<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.LG](#cs.LG) [Total: 18]
- [tldr.article](#tldr.article) [Total: 14]
- [wechat.article](#wechat.article) [Total: 34]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA是一个自动提示优化框架，通过联合优化系统提示和用户提示，使用结构化评分标准和低开销迭代实现快速收敛到高质量提示。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法通常只关注用户提示，依赖非结构化反馈，需要大量样本和长迭代周期，导致成本高且脆弱。

Method: ZERA使用八个可泛化标准对提示进行评分，基于结构化批评修订提示，通过自动推断权重实现快速收敛。

Result: 在五个LLM和九个多样化数据集上的实验显示，ZERA相比强基线实现了持续改进，消融研究验证了各组件对提示构建的有效贡献。

Conclusion: ZERA提供了一种高效、低成本的提示优化方法，通过结构化评分和联合优化显著提升LLM性能。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [2] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出TEMPO算法，通过前缀树结构实现无critic模型的token级信用分配，在数学和医疗QA任务中优于PPO和GRPO。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中稀疏延迟奖励导致的token级信用分配瓶颈问题，特别是在可验证奖励设置下（最终答案可检查且每个提示可生成多个响应）。

Method: 提出Prefix-to-Tree（P2T）方法将响应组转换为前缀树，计算非参数化前缀值。基于此开发TEMPO算法，结合GRPO的组相对结果信号和分支门控的时序差分修正。

Result: 在Qwen3-1.7B/4B模型上，TEMPO在分布内（MATH、MedQA）和分布外（GSM-HARD、AMC23等）基准测试中均优于PPO和GRPO，验证准确率更高且训练时间相近。

Conclusion: TEMPO提供了一种简单有效的token级信用分配方法，无需学习价值网络或额外评判器，在推理任务中表现出色。

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [3] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文探索将LLM作为知识图谱推理路径的奖励模型，通过训练模型判断候选路径是否能正确诊断患者输入，而非直接生成推理路径。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常通过检索增强生成或微调将知识图谱内容插入提示中，但缺乏结构化推理能力。本文受奖励训练增强模型推理能力的启发，提出将LLM作为奖励模型来验证诊断路径的正确性。

Method: 系统评估了五种知识路径判断任务表述和八种训练范式，测试路径判断能力是否能泛化到下游诊断任务（如诊断总结和医学问答）。使用三个开源指令调优LLM进行实验。

Result: 实验显示特定奖励优化和蒸馏能带来强大的路径判断性能，但向下游任务的迁移性较弱。

Conclusion: 这是对临床知识图谱进行"奖励模型风格"推理的首次系统评估，为结构化、基于奖励的监督如何影响医疗GenAI系统的诊断推理提供了见解。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [4] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种LLM辅助的说话人日志校正系统，通过实时用户反馈来修正说话人归属错误，显著降低DER和说话人混淆错误。


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音处理系统在没有用户反馈的情况下运行，而人机协作工作流可以显著提高准确性。

Method: 采用流式ASR和说话人日志，使用LLM生成简洁摘要，接受用户口头反馈并立即整合。开发了SWM技术检测和分割多说话人段，以及在线说话人注册技术。

Result: 在AMI测试集上，系统显著降低了9.92%的DER和44.23%的说话人混淆错误。

Conclusion: 该系统通过人机协作有效提升了说话人日志的准确性，特别是在多说话人场景下表现优异。

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [5] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文对LLM在图数据上的能力进行了系统性评估，发现代码生成方法在图形推理任务中表现最佳，特别是在长文本或高密度图中优势明显，且所有交互策略在异质图中都有效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在文本丰富的图机器学习任务中应用日益广泛，但缺乏对LLM与图数据交互能力的系统性理解，需要进行全面评估以指导实践。

Method: 通过大规模控制实验，评估了LLM-图交互模式（提示、工具使用、代码生成）、数据集领域、结构机制、特征特性和模型配置等多个关键变量。

Result: 代码生成方法整体表现最强，特别是在长文本或高密度图中；所有交互策略在异质图中都有效；代码生成能灵活调整对结构、特征或标签的依赖。

Conclusion: 研究为当前LLM-图交互模式提供了全面的优劣势分析，并为未来方法设计提供了关键原则。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [6] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 本文通过Planorama实验发现，用户偏好和模型偏好并不能准确预测哪些计划真正对用户有帮助，表明基于偏好的对齐方法可能与实际帮助性存在偏差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法（如RLHF和ChatbotArena）基于用户偏好来训练或评估计划的有用性，但缺乏对偏好是否真正反映帮助性的验证。

Method: 开发Planorama界面，让126名用户使用LLM生成的计划回答300个多步骤问题，收集4388次计划执行和5584次比较数据，测量计划帮助性和用户偏好。

Result: 1）用户/模型偏好和代理成功率不能准确预测计划对用户的帮助性；2）这种差距不是用户特定偏好造成的；3）表面特征（如简洁性）与偏好相关但不能预测帮助性。

Conclusion: 需要基于真实用户交互的反馈来对齐有帮助的LLM，而不仅仅是基于看起来有帮助的偏好。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [7] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: 提出了MemOrb，一种轻量级即插即用的语言强化记忆层，通过提炼多轮交互为紧凑的策略反思来增强LLM智能体在客户服务中的长期可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在客户服务中经常出现跨会话遗忘、重复错误和缺乏持续自我改进机制的问题，在需要稳定性和一致性的动态环境中不可靠。

Method: 设计MemOrb记忆层，将多轮交互提炼为策略反思存储在共享记忆库中，检索这些反思来指导决策，无需微调。

Result: MemOrb显著提高了任务成功率和稳定性，在多轮成功率上获得高达63个百分点的提升，并在重复试验中提供更一致的性能。

Conclusion: 结构化反思是增强冻结LLM智能体在客户服务场景中长期可靠性的有效机制。

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [8] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX是一个基于多智能体协作的关键词提取框架，通过动态适应文档长度的双路径策略，显著提升了LLM在关键词提取任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有的无监督提示方法通常采用单阶段推理管道和统一提示策略，无法充分利用LLM的推理和生成能力，特别是在处理不同长度文档时效果有限

Method: MAPEX引入多智能体协作，包含专家招募、候选提取、主题指导、知识增强和后处理模块，采用双路径策略：短文本使用知识驱动提取，长文本使用主题引导提取

Result: 在6个基准数据集和3种不同LLM上的实验表明，MAPEX在F1@5指标上平均优于最先进的无监督方法2.44%，优于标准LLM基线4.01%

Conclusion: MAPEX通过多智能体协作框架有效提升了关键词提取性能，展示了良好的泛化能力和普适性

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [9] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: PoT是一种推理阶段方法，通过建模LLM的推理为迭代决策过程，动态选择认知操作来生成多样化候选响应，然后根据用户偏好聚合得到个性化QA响应。


<details>
  <summary>Details</summary>
Motivation: 个性化QA系统对提高准确性和用户满意度至关重要，但目前面临从长噪声隐式上下文中推断偏好、生成同时正确且符合用户期望的响应的挑战。

Method: 提出思想路径（PoT）方法，将LLM推理建模为迭代决策过程，动态选择推理、修订、个性化和澄清等认知操作，探索多种推理轨迹生成候选响应，然后根据用户偏好进行聚合和重加权。

Result: 在LaMP-QA个性化QA基准测试中，PoT始终优于竞争基线，相对改进高达13.1%。人工评估显示66%的情况下偏好PoT输出，仅有15%平局。

Conclusion: PoT方法有效解决了个性化QA的挑战，无需任务特定微调即可应用于任何LLM，显著提升了响应质量和用户满意度。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [10] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习训练连续思维链的方法，使用软标记（标记混合加噪声）实现可扩展训练，在数学推理任务中表现优于离散标记方法。


<details>
  <summary>Details</summary>
Motivation: 现有连续标记方法存在训练困难：要么仅在推理时使用连续标记，要么需要从离散思维链蒸馏且计算成本高。本文旨在开发可扩展的连续思维链训练方法。

Method: 使用强化学习训练连续思维链，采用软标记（标记混合加噪声）进行探索，计算开销小，可训练数百个标记的连续思维链。

Result: 在Llama和Qwen模型上的数学推理基准测试中，连续思维链训练在pass@1上匹配离散标记方法，在pass@32上超越，显示出更大的思维链多样性。最佳方案是训练时使用连续标记，推理时使用离散标记。

Conclusion: 连续思维链强化学习训练能更好地保持基础模型在域外任务上的预测性能，对基础模型的影响更温和。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [11] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: OPRL是一种用于智能体强化学习的通用信用分配策略，通过交替优化隐式过程奖励模型和策略，将轨迹偏好转化为隐式步骤奖励，解决了稀疏和不可验证奖励环境中的时间信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的智能体在交互环境中面临稀疏和不可验证奖励的挑战，导致时间信用分配困难。现有方法存在标注偏差、奖励攻击、高方差等问题。

Method: OPRL通过轨迹DPO目标优化隐式过程奖励模型，将轨迹偏好转化为步骤奖励，结合结果奖励计算步骤级优势，与回合级优势结合进行策略更新，形成自增强循环。

Result: 在WebShop、VisualSokoban和SOTOPIA等三个不同智能体基准测试中，OPRL表现出优于前沿LLM和强RL基线的性能，实现了最先进结果，具有更高的样本效率和更低的训练方差。

Conclusion: OPRL为智能体学习提供了一种有效的信用分配策略，理论保证和实证结果都证明了其在现实场景中的潜力，特别是在需要高效探索的环境中。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [12] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: RLPT是一种新的训练时扩展范式，通过在预训练数据上使用强化学习来优化大语言模型，无需人工标注奖励信号，直接从预训练数据中推导奖励。


<details>
  <summary>Details</summary>
Motivation: 解决计算资源指数级增长与高质量文本数据有限增长之间的差距，突破传统扩展方法的限制。

Method: 采用下一段推理目标，让策略基于前文准确预测后续文本片段来获得奖励，使强化学习能够在预训练数据上扩展。

Result: 在多个模型和基准测试中验证了RLPT的有效性，如Qwen3-4B-Base在MMLU、MMLU-Pro等基准上获得显著提升（3.0-8.1个绝对百分点）。

Conclusion: RLPT展示了良好的扩展行为，为继续提升性能提供了强有力基础，扩展了LLMs的推理边界并增强了RLVR性能。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出摊销潜在引导（ALS）方法，通过离线计算单个向量在推理时以恒定成本应用，替代昂贵的逐查询优化循环，实现2-5倍加速同时保持或超越贪婪CoT和自一致性基线的性能。


<details>
  <summary>Details</summary>
Motivation: 测试时优化方法因推理成本过高而难以大规模应用，现有方法如迭代优化和多步验证需要10-100倍的计算开销。潜在空间优化方法虽然更直接但仍需昂贵的逐查询优化循环。

Method: ALS计算成功与不成功生成之间隐藏状态的平均差异，使用该方向校准模型的隐藏表示。当解码偏离成功流形时，ALS将激活状态推回正确方向，所有优化计算在离线阶段完成。

Result: 在GSM8K和MATH-500基准测试中，ALS相比迭代方法实现2-5倍加速，同时匹配或超越贪婪CoT和自一致性基线，效率-准确率权衡提升高达101%。

Conclusion: 研究表明潜在优化的主要收益可以通过离线方式获得，使复杂推理技术能够实际部署到生产环境中。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [14] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线强化学习框架，通过难度自适应算法和奖励调整策略，显著提升移动GUI代理的性能，在AndroidWorld和AndroidLab基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 开发有效的移动GUI代理面临任务难度分布不均和大规模环境采样效率低下的挑战，需要新的强化学习框架来解决这些问题。

Method: 提出MOBILERL框架，核心是难度自适应GRPO算法（ADAGRPO），包含难度自适应正向回放、失败课程过滤和最短路径奖励调整策略。

Result: 在Qwen2.5-VL-7B和GLM-4.1V-9B模型上应用，MOBILERL-9B在AndroidWorld达到75.8%成功率，AndroidLab达到46.8%成功率，均为最先进水平。

Conclusion: MOBILERL框架能稳定强化学习训练，提高采样效率，在多样化移动应用和任务中表现优异，已应用于AutoGLM产品并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [15] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL是一个用于解释大型语言模型中稀疏自编码器特征的框架，旨在提升对安全相关行为的机制性理解。


<details>
  <summary>Details</summary>
Motivation: 现有安全研究主要关注评估LLM输出或特定安全任务，难以应对更广泛的未定义风险。稀疏自编码器虽然有助于解释模型行为，但之前的研究未能充分解释与细粒度安全概念相关的特征。

Method: 提出Safe-SAIL框架，系统性地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。

Result: 将发布包含SAE检查点和人类可读神经元解释的全面工具包，支持对安全风险的实证分析。

Conclusion: 该框架有助于促进LLM安全研究，通过机制性理解来应对高风险行为。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [16] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文提出了一个大规模数据集，系统分析LLM结构配置与性能的关系，通过数据挖掘和机制可解释性技术验证结构选择对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在规模和能力上快速增长，但关于结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。

Method: 构建包含多样化开源LLM结构及其在多个基准测试中性能的大规模数据集，采用数据挖掘驱动的方法进行系统性分析，并使用机制可解释性技术验证发现。

Result: 量化了结构配置与性能之间的关系，为LLM优化提供了数据驱动的见解。

Conclusion: 该研究旨在指导未来模型的针对性开发和应用，数据集将在HuggingFace上发布。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [17] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 本文研究卡车与无人机协同配送问题，提出混合强化学习方法，在N=50的实例上比ALNS方法平均提升2.73%，与NN方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 研究最后一公里配送中卡车与无人机的协同作业问题，特别考虑电池续航约束和充电需求，旨在优化整体配送时间。

Method: 采用混合强化学习求解器，结合ALNS卡车路径规划和小型指针/注意力策略来调度无人机任务，使用精确时间线模拟器确保可行性。

Result: 在欧几里得实例上，该方法平均完成时间为5.203±0.093，比ALNS方法提升2.73%，与NN方法性能相当（仅差0.10%）。

Conclusion: 学习的调度器能够平衡卡车等待时间与无人机飞行时间，有效最小化总完成时间，且在所有测试实例上都不劣于ALNS方法。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [18] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种新的训练和推理架构，通过在神经网络中内生集成计算能力，实现计算与推理的融合，相比微调LLM和多智能体系统在精度、延迟、token使用和GPU能耗方面都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法将高精度数值计算作为内在可解释能力集成，而主流多智能体方法存在通信开销大、多模态能力效率低和可扩展性有限的问题。

Method: 提出PiMoE架构，通过分别训练专家模型、文本到计算模块和路由器，在推理时路由器在token级别指导计算和推理，实现单链思维中的迭代交替。

Result: 在两个推理-计算任务上的评估显示，PiMoE不仅比直接微调LLM精度更高，相比主流多智能体方法在响应延迟、token使用和GPU能耗方面都有显著改进。

Conclusion: PiMoE为下一代科学或工业智能系统提供了高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [19] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨了为什么强化学习预训练算法能够生成支持情境强化学习的网络参数，通过理论证明Transformer在策略评估预训练中的全局最小化器可以实现情境时序差分学习。


<details>
  <summary>Details</summary>
Motivation: 当前研究发现一些RL智能体在预训练后无需参数更新就能解决分布外任务，但预训练过程使用标准RL算法，需要理解这些算法为何能产生支持ICRL的参数。

Method: 通过案例研究，理论证明当Transformer被预训练用于策略评估时，预训练损失的全局最小化器能够实现情境时序差分学习。

Result: 提供了初步证据支持假设，证明存在特定的参数配置能够使预训练网络具备情境学习能力。

Conclusion: 预训练损失的最小化器确实能够产生支持情境强化学习的网络参数，这为理解ICRL现象提供了理论基础。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [20] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 该论文提出了KANDI方法，结合Kolmogorov-Arnold网络和扩散策略，用于解决医疗健康领域中离线强化学习的奖励函数定义困难问题，特别是在老年人跌倒风险干预的物理活动促进应用中。


<details>
  <summary>Details</summary>
Motivation: 在医疗健康领域应用离线强化学习面临重大挑战：直接定义奖励函数困难，而逆向强化学习在复杂环境中难以从专家行为中推断准确的奖励函数。离线强化学习也难以将学习到的策略与医疗应用中观察到的人类行为对齐。

Method: 提出KANDI方法，利用Kolmogorov-Arnold网络的灵活函数逼近能力从低跌倒风险的老年人（专家）行为中估计奖励函数，同时在Actor-Critic框架中使用基于扩散的策略进行动作细化和离线强化学习的效率优化。

Result: KANDI在D4RL基准测试中优于最先进方法，并在PEER研究的临床试验中通过可穿戴活动监测数据验证了其在跌倒风险干预项目中促进老年人身体活动的实际应用效果。

Conclusion: KANDI有潜力解决医疗健康应用中离线强化学习的关键挑战，为医疗健康领域的活动促进干预策略提供有效解决方案。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [21] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL是一种新的强化学习框架，通过主动部分rollout策略解决RL训练中长尾响应分布导致的GPU利用率低下问题，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中，rollout生成占用了90%以上的计算时间，且长尾响应分布导致GPU空闲时间增加，限制了RL系统的可扩展性。

Method: APRIL在rollout阶段过度配置请求，一旦达到目标响应数量就终止，并将未完成的响应回收用于后续步骤的继续处理。

Result: 实验显示APRIL在常用RL算法（GRPO、DAPO、GSPO）上最多提升44%的rollout吞吐量，加速收敛，并在任务上实现最多8%的最终准确率提升。

Conclusion: APRIL统一了系统级和算法级考虑，提高了RL训练效率，且与框架和硬件无关，已在slime RL框架中集成。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [22] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT方法通过在LLM决策过程中增加反思步骤，显著提升了交互式决策任务的性能，在多个环境中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM交互决策方法存在错误累积问题，缺乏有效的自我纠正机制，需要一种能够及时修正错误的方法。

Method: 提出'Reflect before Act'方法，在采取下一个动作之前增加关键的反思步骤，实现即时错误纠正和适应性调整。

Result: 在ALFWorld、WebShop和TextCraft三个环境中，REBACT显著优于基线方法，成功率分别提升6.72%、24%和0.5%，最高达到98.51%、61%和99.5%。

Conclusion: REBACT方法通过少量修改步骤实现了显著的性能提升，证明了其计算效率和实用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [23] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于衡量不同马尔可夫决策过程（MDP）之间的状态相似性，解决了传统双模拟度量在多MDP场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量（BSM）在单MDP中有效，但在多MDP场景（如策略迁移）中应用受限，缺乏严格的数学性质分析限制了理论进展。

Method: 通过形式化建立MDP对之间的广义双模拟度量，严格证明其三个基本性质：对称性、MDP间三角不等式和相同状态空间上的距离界限。

Result: GBSM在策略迁移、状态聚合和基于采样的估计中获得了比标准BSM更严格的理论界限，并提供了闭式样本复杂度估计。

Conclusion: GBSM为多MDP场景提供了理论基础，数值结果验证了其有效性，相比现有方法有显著改进。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [24] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出了一种将强化学习与大型语言模型相结合的新型电商支付欺诈检测方法，通过将交易风险建模为多步骤马尔可夫决策过程，利用LLM迭代优化奖励函数，实现了更好的欺诈检测准确性和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统电商支付欺诈检测方法中，设计有效的强化学习奖励函数需要大量人工专业知识，而LLM具有先进的推理和编码能力，可以自动优化这些函数，提高检测效果。

Method: 将交易风险建模为多步骤马尔可夫决策过程，利用大型语言模型迭代优化强化学习的奖励函数设计，实现跨多个支付阶段的风险检测优化。

Result: 在真实世界数据上的实验证实了该方法的有效性、鲁棒性和韧性，通过长期评估展示了LLM增强的RL框架在欺诈检测准确率上的提升。

Conclusion: LLM在推进工业级强化学习应用方面具有巨大潜力，特别是在复杂奖励函数设计的自动化优化方面。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [25] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO算法解决了GRPO在处理同质错误响应时梯度消失的问题，通过优势校准和非对称裁剪机制，将同质错误转化为有效的学习信号，在数学推理任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: GRPO等RLVR算法在处理全对或全错的同质响应组时存在局限性，特别是当所有响应都错误时，优势函数值为零导致梯度消失，无法从错误中学习。

Method: 提出NGRPO算法：1）优势校准机制，假设存在虚拟最大奖励样本来改变组内奖励分布；2）非对称裁剪，放松正样本更新幅度，严格约束负样本更新。

Result: 在Qwen2.5-Math-7B模型上的实验显示，NGRPO在MATH500、AMC23和AIME2025等数学基准上显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法。

Conclusion: NGRPO能够有效从同质错误中学习，实现稳定且显著的数学推理能力提升。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [26] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 本文提出了GNARL框架，将神经算法推理（NAR）重新构建为马尔可夫决策过程，结合模仿学习和强化学习来解决传统NAR方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统神经算法推理存在无法无后处理构建有效解、难以处理多解问题、在NP难问题上表现不佳以及缺乏强算法时无法应用的局限性。

Method: 将算法轨迹学习问题转化为马尔可夫决策过程，提出GNARL框架，结合模仿学习和强化学习技术，适用于广泛的图基问题。

Result: 在多个CLRS-30问题上获得高图精度，在NP难问题上性能匹配或超过更窄的NAR方法，甚至在缺乏专家算法时也能应用。

Conclusion: GNARL框架成功解决了NAR的关键限制，为算法学习提供了更通用和强大的方法。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [27] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 提出完全可学习的神经奖励机（FLNRM），能够端到端学习符号接地函数和自动机，无需依赖先验知识，在非马尔可夫强化学习任务中表现优于基于RNN的方法。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫强化学习任务需要代理基于完整轨迹做出决策，现有符号化方法依赖预定义的符号接地函数或先验知识，限制了应用范围。

Method: 提出完全可学习的神经奖励机（FLNRM），集成深度强化学习，端到端学习符号接地和自动机结构。

Result: FLNRM方法在非马尔可夫任务中优于基于RNN的先前方法，同时保持了可解释性。

Conclusion: FLNRM结合了深度强化学习的易用性和自动机的可解释性，为复杂时序任务提供了有效的解决方案。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [28] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 该论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，涵盖了对抗样本、领域泛化和大型语言模型越狱三个方面的最新算法进展。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型在安全关键应用中的广泛使用，确保模型决策对对抗性攻击具有鲁棒性至关重要。

Method: 提出了新的技术结果、训练范式和认证算法来处理计算机视觉中的对抗样本；开发了在医学影像、分子识别和图像分类中实现最先进泛化的新算法；针对LLM越狱问题提出了新的攻击和防御方法。

Result: 在对抗样本、领域泛化和语言模型鲁棒性方面取得了前沿进展，为设计鲁棒的语言智能体奠定了基础。

Conclusion: 该研究在深度学习模型鲁棒性的多个重要方向上做出了贡献，推动了安全可靠AI系统的发展。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [29] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL是一种用于LLM训练的强化学习方法，通过并发异步数据生成和模型训练，实现了硬件效率与数据在线性的优化平衡，相比传统RL方法学习速度提升约2倍。


<details>
  <summary>Details</summary>
Motivation: 当前RL方法在扩展LLM推理能力时面临挑战，主要是在保持高AI加速器利用率的同时避免产生过时的离线策略数据，这些数据会损害常见的RL算法性能。

Method: PipelineRL采用并发异步数据生成和模型训练，核心创新是飞行中权重更新机制，允许LLM生成引擎在生成token序列时以最小中断接收更新的模型权重。

Result: 在128个H100 GPU上进行的实验表明，PipelineRL相比传统RL基线实现了约2倍的学习速度提升，同时保持了高度在线的训练数据。

Conclusion: PipelineRL在硬件效率和数据在线性之间实现了优越的平衡，为大规模LLM训练提供了有效的解决方案，并发布了开源实现。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [30] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出了功能缩放定律（FSL），通过随机微分方程建模SGD训练过程，揭示了学习率调度对LLM预训练损失动态的影响，并理论验证了常见实践如学习率衰减和WSD调度的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要关注最终损失，忽略了训练过程中的损失动态和学习率调度的影响，本文旨在填补这一空白。

Method: 使用师生核回归设置和在线SGD训练，通过内在时间视角和SDE建模，提出FSL框架来分析不同学习率调度下的种群风险演化。

Result: FSL成功捕捉了学习率调度的影响，理论验证了高容量模型更高效、学习率衰减提升效率、WSD调度优于直接衰减等经验实践。

Conclusion: FSL框架能加深对LLM预训练动态的理解，为大规模模型训练优化提供理论指导。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [31] [Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor (Sponsor) With all the noise about agents, it's easy to forget that an agent is just a looping LLM call + Memory + Tool usage. Use step-by-step tutorial to build an agent that "thinks" with an LLM, calls external tools, remembers context, and keeps looping until it gets the job done. Follow along, get the satisfaction of seeing your first agent in action, and iterate from there to build more complex, enterprise-rea...

</details>


### [32] [How Google's dev tools manager makes AI coding work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google开发者工具经理Ryan Salva分享关于AI编码工具的研究发现、使用经验以及IDE未来的展望，认为IDE使用时间将逐渐减少。


<details>
  <summary>Details</summary>
Motivation: 了解开发者如何实际使用AI工具，探索AI工具对开发流程的影响以及IDE的未来发展方向。

Method: 通过第三方研究分析开发者使用AI工具的行为模式，结合Salva的个人使用经验和团队观察。

Result: 研究发现开发者正在改变工作方式，AI工具正在重塑开发流程，IDE的角色将发生变化。

Conclusion: AI工具将改变开发者的工作模式，IDE的重要性可能相对降低，开发工具需要适应这一趋势。

Abstract: How Google's dev tools manager makes AI coding work (5 minute read) Ryan Salva, Google's project manager for developer tools, is responsible for tools like Gemini CLI and Gemini Code Assist. His team recently released new third-party research that showed how developers actually use AI tools. This article contains an edited interview with Salva where he talks about the research, how he uses AI tools, and the future of IDEs. Salva believes that over time, the time spent in the IDE will graduall...

</details>


### [33] [Your Brain on ChatGPT](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究比较了使用ChatGPT、搜索引擎和仅靠大脑写作对认知的影响，发现依赖LLM会降低神经连接和参与度，并导致记忆回忆和工作归属感减弱


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在写作任务中对人类认知功能的潜在影响，特别是神经活动和记忆表现方面

Method: 使用脑电图(EEG)测量三组参与者（使用LLM、搜索引擎、仅靠大脑）在多次写作任务中的神经活动，并评估记忆回忆和工作归属感

Result: LLM组显示出神经连接和参与度降低，记忆回忆能力较弱，对工作的归属感也较低

Conclusion: 虽然LLM提供即时帮助，但过度依赖可能对认知功能产生负面影响

Abstract: Your Brain on ChatGPT (3 minute read) This study examined the cognitive impact of using ChatGPT for essay writing. It compared participants who used an LLM, a search engine, or only their own brain across multiple sessions. EEG data revealed that reliance on the LLM reduced neural connectivity and engagement compared with the Brain-only and Search Engine groups, and participants using the LLM showed weaker memory recall and lower ownership of their work. While LLMs offer immediate assistance,...

</details>


### [34] [Getting AI to Work in Complex Codebases](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文探讨了AI在复杂代码库中工作的有效方法，强调通过核心上下文工程原则和频繁的有意压缩，当前AI模型能够很好地处理大型详细代码库。


<details>
  <summary>Details</summary>
Motivation: AI在大型代码库中很难一次性生成正确代码，因此需要在实现前进行代码库研究和步骤规划。

Method: 采用核心上下文工程原则和频繁的有意压缩技术，让AI在实现前先进行代码库研究和步骤规划。

Result: 通过这种方法，当前AI模型能够有效地处理大型复杂代码库。

Conclusion: 在复杂代码库中使用AI需要分步方法，包括前期研究和规划，而不仅仅是直接生成代码。

Abstract: Getting AI to Work in Complex Codebases (20 minute read) With core context engineering principles and frequent intentional compaction, current AI models can be great at handling large, detailed codebases. AI can rarely produce correct code in one shot in large code bases, so it needs steps before implementation to do research on the codebase and plan its steps.

</details>


### [35] [Exploring Active Agent, or can we build AI features the Rails way?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Active Agent是一个Ruby gem，帮助使用熟悉的Rails约定和模式将AI功能集成到Rails应用中，引入了"agents"作为封装AI逻辑的新抽象层。


<details>
  <summary>Details</summary>
Motivation: 旨在简化AI功能在Rails应用中的集成，让开发者能够使用熟悉的Rails开发模式和约定来构建AI特性。

Method: 引入"agents"作为新抽象层，采用动作驱动对象、回调和提示渲染，类似于Rails控制器和邮件器的设计模式。

Result: 开发了一个Ruby gem，提供了一种符合Rails开发习惯的方式来构建AI功能。

Conclusion: Active Agent为Rails开发者提供了一种熟悉的方式来集成AI功能，降低了AI技术的学习曲线。

Abstract: Exploring Active Agent, or can we build AI features the Rails way? (8 minute read) Active Agent is a Ruby gem that helps integrate AI features into Rails applications using familiar Rails conventions and patterns. It introduces "agents" as a new abstraction that encapsulates AI-backed logic, using action-driven objects, callbacks, and prompt rendering similar to Rails controllers and mailers.

</details>


### [36] [Context Engineering for AI Agents: Lessons from Building Manus](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus AI代理通过上下文工程技术提升性能，包括优化KV缓存命中率、保持稳定提示前缀、使用文件系统作为扩展内存等方法


<details>
  <summary>Details</summary>
Motivation: 提高AI代理的性能和效率，通过优化上下文管理来改善代理的响应速度和准确性

Method: 采用上下文工程技术，包括维护稳定的提示前缀、仅追加上下文、优化KV缓存命中率、使用文件系统作为扩展内存、以及使用复述方法来操纵注意力

Result: Manus AI代理的性能得到了显著提升

Conclusion: 上下文工程是提升AI代理性能的有效方法，建议避免动态改变动作空间，并合理利用文件系统作为内存扩展

Abstract: Context Engineering for AI Agents: Lessons from Building Manus (11 minute read) Manus is an AI agent that had improved performance through context engineering techniques. The devs optimized the KV-cache hit rate by maintaining a stable prompt prefix with append-only context. They advise against dynamically changing the action space. The file system should be used as an extended memory, along with recitative methods to manipulate attention.

</details>


### [37] [How Google's dev tools manager makes AI coding work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google的AI编码工具如Gemini CLI通过自动化代码生成改变了软件开发流程，将开发者角色转向架构和问题分解


<details>
  <summary>Details</summary>
Motivation: 探索AI编码工具如何提升软件开发效率，通过自动化代码生成来减轻开发者的编码负担

Method: 使用Gemini CLI等AI工具，将自然语言需求自动转换为代码，实现代码生成自动化

Result: AI编码工具能够有效自动化代码生成过程，使开发者更专注于高层次的设计和架构工作

Conclusion: AI编码工具正在改变软件开发的工作方式，将开发者从繁琐的编码任务中解放出来，专注于更具创造性的工作

Abstract: How Google's dev tools manager makes AI coding work (5 minute read) AI coding tools like Gemini CLI are being used for software development by automating code generation from natural language requirements and shifting the developer's role towards architecture and problem decomposition.

</details>


### [38] [Coinbase Announces x402 Foundation with Cloudflare](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Coinbase与Cloudflare联合成立x402基金会，旨在推动链上代理商务开放标准的发展。


<details>
  <summary>Details</summary>
Motivation: 为链上代理商务创建独立的管理机构，促进该领域的标准化和生态发展。

Method: 通过成立独立的x402基金会，由Coinbase和Cloudflare共同管理，未来将引入更多合作伙伴。

Result: 建立了x402基金会作为链上代理商务标准的独立管理机构。

Conclusion: 这是推动链上代理商务标准化的重要一步，有助于该技术的广泛应用。

Abstract: Coinbase Announces x402 Foundation with Cloudflare (5 minute read) x402, the open standard for onchain agentic commerce created by Coinbase, will become controlled by an independent foundation called the x402 Foundation. The foundation will be co-founded by Coinbase and Cloudflare – one of the largest internet infrastructure providers who recently announced an agentic pay-per-crawl option for websites – with more partners to be announced soon. Creating a foundation is a significant step towar...

</details>


### [39] [Greptile Series A and Greptile v3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile v3 AI代码审查代理在Series A融资2500万美元，解决了代码验证的规模问题，检测到的关键bug数量是前代的3倍，已审查超过5亿行代码，防止了18万+个bug


<details>
  <summary>Details</summary>
Motivation: 解决大规模代码验证的挑战，提高代码审查的效率和准确性

Method: 开发AI代码审查代理Greptile v3，集成Jira和Notion等工具进行上下文感知反馈，从PR评论中学习团队实践

Result: 检测到的关键bug数量是前代的3倍，审查了500M+行代码，防止了180,000+个bug，为顶级公司提供服务

Conclusion: Greptile v3在代码审查领域取得了显著成效，证明了AI代理在大规模代码验证中的价值

Abstract: Greptile Series A and Greptile v3 (5 minute read) Greptile raised $25M Series A led by Benchmark Capital to advance its AI code review agent, Greptile v3, which addresses scale issues in code validation. The revamped agent detects 3x more critical bugs than its predecessor and has reviewed over 500M lines of code for top firms, preventing 180,000+ bugs. Greptile integrates with tools like Jira and Notion for context-aware feedback and learns team practices from PR comments to enhance code rev...

</details>


### [40] [ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ShadowLeak是一种零点击漏洞，利用ChatGPT Deep Research代理中的间接提示注入命令，通过隐藏的电子邮件指令窃取Gmail数据而不被用户察觉。


<details>
  <summary>Details</summary>
Motivation: 揭示AI代理在真实应用场景中的安全漏洞，特别是通过间接提示注入实现数据泄露的风险。

Method: 在电子邮件中隐藏不可见的间接提示注入命令，利用代理的browser.open()函数调用将窃取的数据以Base64编码发送到攻击者控制的URL。

Result: 攻击成功率达到100%，能够完全在OpenAI服务器上运行，无需用户交互。

Conclusion: AI代理在处理外部数据时存在严重安全风险，需要更强的安全防护机制。

Abstract: ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent (2 minute read) ShadowLeak is a zero-click vulnerability in OpenAI's ChatGPT Deep Research agent that utilized invisible, indirect prompt injection commands hidden in emails to exfiltrate Gmail data without user knowledge. The attack operated entirely on OpenAI's servers using the agent's browser.open() function call to send stolen data encoded in Base64 to attacker-controlled URLs, achieving a 100% success rate. OpenAI fixed the vul...

</details>


### [41] [Control AI in your SDLC](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SCW Trust Agent是一个AI治理工具，帮助管理者监控和控制AI生成的代码安全风险，包括发现"影子AI"、映射漏洞到技能水平、执行政策


<details>
  <summary>Details</summary>
Motivation: 78%的开发者使用AI编码工具，但一半功能正确的AI生成代码存在安全隐患，需要有效的治理和风险管理

Method: 提供可见性和治理能力，包括检测未经授权的AI使用、将漏洞与开发者技能水平关联、强制执行安全政策

Result: 帮助企业领导者管理AI编码工具带来的安全风险，成为首批加入早期访问等待名单的机会

Conclusion: AI编码工具虽然普及但存在安全风险，SCW Trust Agent提供了必要的治理解决方案

Abstract: Control AI in your SDLC (Sponsor) 78% of developers use AI coding tools, yet half of functionally correct AI-generated code is insecure. SCW Trust Agent: AI gives leaders visibility and governance to manage this risk - spotting “shadow AI,” mapping vulnerabilities to skill level, and enforcing policy. Be among the first to join the early access waitlist!

</details>


### [42] [When LLMs Autonomously Attack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM能够自主协调代理系统重现现实世界攻击，如2017年Equifax数据泄露事件，使小型组织能够将LLM用作专用红队测试工具。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在网络安全攻击中的自主能力，验证其是否能够重现真实世界的复杂攻击场景。

Method: 在卡内基梅隆大学的研究中，研究人员重现了2017年Equifax数据泄露的网络环境，观察LLM如何协调代理系统复制该攻击。

Result: LLM成功复制了Equifax攻击，展示了其作为自主攻击工具的潜力，特别是对小型组织而言可作为专用红队测试工具。

Conclusion: LLM具备自主攻击能力，这既是安全威胁也是安全测试机会，需要相应的防御措施和监管框架。

Abstract: When LLMs Autonomously Attack (5 minute read) A researcher at Carnegie Mellon University demonstrated that an LLM can coordinate a system of agents to recreate real-world attacks. As part of his PhD, Brian Singer recreated the network environment of the 2017 Equifax data breach and observed that an LLM could replicate the attack. These capabilities allow smaller organizations to use LLMs as dedicated red teamers.

</details>


### [43] [Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 苹果正在为其设备开发MCP支持，以实现AI代理功能


<details>
  <summary>Details</summary>
Motivation: 让AI助手能够与传统平台交互，为开发者提供系统级的MCP集成能力

Method: 在最新测试版中开始采用Anthropic的模型上下文协议(MCP)标准

Result: MCP已成为AI助手连接API和数据源的通用途径

Conclusion: 苹果计划让开发者使用系统级MCP集成来暴露操作和功能

Abstract: Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad (4 minute read) Apple has begun to lay the groundwork for adopting Anthropic's Model Context Protocol (MCP) in its latest round of betas. MCP allows AI agents to interface with traditional platforms. The standard has been widely adopted and has become a universal pathway for AI assistants to plug into APIs and data sources. Apple plans to let developers use a system-level MCP integration to expose actions and functiona...

</details>


### [44] [Chrome DevTools for your AI agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Chrome DevTools MCP服务器为AI编程助手提供了Chrome DevTools的调试能力，使其能够直接在Chrome中调试网页并获取性能洞察。


<details>
  <summary>Details</summary>
Motivation: 让AI编程助手能够直接利用Chrome DevTools的强大调试功能，实时验证代码变更、诊断网络和console错误、模拟用户行为以及自动化性能审计。

Method: 通过Model Context Protocol (MCP)服务器将Chrome DevTools的功能暴露给AI编程助手，使其能够与Chrome浏览器进行交互并执行调试操作。

Result: AI编程助手现在可以直接在Chrome中调试网页，利用DevTools的完整调试能力和性能分析工具。

Conclusion: Chrome DevTools MCP为AI编程助手提供了强大的网页调试能力，显著提升了代码验证和问题诊断的效率。

Abstract: Chrome DevTools (MCP) for your AI agent (5 minute read) The Chrome DevTools Model Context Protocol (MCP) server brings the power of Chrome DevTools to AI coding assistants. It allows AI coding assistants to debug web pages directly in Chrome and benefit from DevTools debugging capabilities and performance insights. Use cases include verifying code changes in real-time, diagnosing network and console errors, simulating user behavior, and automating performance audits. A video showing how the C...

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [45] [MAPO：破解<em class="highlight">强化学习</em>优势失真难题，大幅提升基础模型推理性能](http://mp.weixin.qq.com/s?__biz=MzUzNjU2OTkyOA==&mid=2247496839&idx=1&sn=dfad9994bc65c4bca83b60c71dfebeeb&chksm=fb2c1d4eb38211e6b0ec5abc671e083343d3719445aa3dd2984e1095a2b90a1d2627cd848145#rd)
*机器感知*

Main category: wechat.article

TL;DR: 强化学习是提升基础模型推理能力的关键技术，其中组相对策略优化（GRPO）因无需额外奖励模型而广受青睐。其核心通过优势函数排序轨迹重要性，但现有GRPO采用固定优势公式，忽略样本轨迹确定性差异，引发两大问题：优势


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是提升基础模型推理能力的关键技术，其中组相对策略优化（GRPO）因无需额外奖励模型而广受青睐。其核心通过优势函数排序轨迹重要性，但现有GRPO采用固定优势公式，忽略样本轨迹确定性差异，引发两大问题：优势

</details>


### [46] [基于<em class="highlight">强化学习</em>的科技创新与产业融合路径推演与优化策略研究【管理学自科研究题目挖掘】【20250924】](http://mp.weixin.qq.com/s?__biz=Mzk0NzY1MTA4MQ==&mid=2247485275&idx=1&sn=7d0f8ba4a0f571fd53f53c3d851be419&chksm=c2f13edc23dc555985404a2efe3625cef34e4184cedcadaa3d4ca675f7ad9e7451593d1cfd55#rd)
*政管选题研究院*

Main category: wechat.article

TL;DR: 5. 仿真优化：基于强化学习训练政策优化策略6. 验证测试：选取典型民族区域进行实证验证5. 数据需求与算法数据类型来源算法民族数据人口普查社会网络分析（SNA）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 5. 仿真优化：基于强化学习训练政策优化策略6. 验证测试：选取典型民族区域进行实证验证5. 数据需求与算法数据类型来源算法民族数据人口普查社会网络分析（SNA）

</details>


### [47] [广告出价进入“自动驾驶”时代，快手提出生成式<em class="highlight">强化学习</em>出价技术！](http://mp.weixin.qq.com/s?__biz=MzI4NzU3MjA3OQ==&mid=2247503561&idx=1&sn=35be25d44b7b0a82211f20e9db01e10e&chksm=ea52b72532dee1579decf2a6b8342d226ba28d164adb26ddfdc4c6ae168b4be9381f69641764#rd)
*快手大模型*

Main category: wechat.article

TL;DR: 生成式强化学习有两个大方向：1. Generative Model as a world model：建立一个可以模拟不同出价策略下广告投放结果的“数字沙盒”，生成大量训练数据来增强模型学习。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 生成式强化学习有两个大方向：1. Generative Model as a world model：建立一个可以模拟不同出价策略下广告投放结果的“数字沙盒”，生成大量训练数据来增强模型学习。

</details>


### [48] [COMMTR | Towards Fair Lights：一种多智能体遮罩深度<em class="highlight">强化学习</em>方法，用于走廊级公平高效的交通信号控制系统](http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247502301&idx=1&sn=19e195ecbb17ea8627e0c1c19aa7bdf0&chksm=cff718fe7388f2ab1856bc8dbf78c4a144148acc2cb6f0da92a51d23eb87a10e012ab455b715#rd)
*智慧车辆与交通*

Main category: wechat.article

TL;DR: 随着AI技术在交通领域的快速发展，基于深度强化学习（DRL）的自适应交通信号控制（ATSC）成为研究热点。然而，现有大多数方法以机动车行为核心，忽视了行人、公交乘客等交通弱势群体的需求，也难以兼顾公平性与效率。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 随着AI技术在交通领域的快速发展，基于深度强化学习（DRL）的自适应交通信号控制（ATSC）成为研究热点。然而，现有大多数方法以机动车行为核心，忽视了行人、公交乘客等交通弱势群体的需求，也难以兼顾公平性与效率。

</details>


### [49] [从蹒跚学步到奔跑如飞：<em class="highlight">强化学习</em>在控制领域的进阶之路](http://mp.weixin.qq.com/s?__biz=Mzk2NDI1MzMyNg==&mid=2247485565&idx=1&sn=927882c6b21a2f36b0789e316966a5b3&chksm=c52336eb1b684d22423aa40e85e65c29424b09b8ee5410fae8b57ae22f244b917b4ac8edea55#rd)
*人形AI智控先锋*

Main category: wechat.article

TL;DR: 面对这些问题，强化学习并没有停滞不前，研究者们开始深入思考并探索各种解决方案，就像勇敢的冒险者在困境中寻找出路，不断尝试新的方法和技术，努力突破这些瓶颈，为强化学习在控制领域的进一步发展开辟新的道路


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 面对这些问题，强化学习并没有停滞不前，研究者们开始深入思考并探索各种解决方案，就像勇敢的冒险者在困境中寻找出路，不断尝试新的方法和技术，努力突破这些瓶颈，为强化学习在控制领域的进一步发展开辟新的道路

</details>


### [50] [南洋理工大学肖佳平 等 | 基于深度<em class="highlight">强化学习</em>的异构机器人系统目标搜索与导航](http://mp.weixin.qq.com/s?__biz=MzA4NjY1MTI0Mg==&mid=2660026670&idx=1&sn=59981e8465c8d82e0ab9218ba5189c8c&chksm=8567dbc4c7e8f2b973518d5cac9d404d4e49b582896b31eee0ae55488a91d0e62db10fae7a3a#rd)
*机器智能研究MIR*

Main category: wechat.article

TL;DR: 第4节提出了一个多阶段强化学习框架。第5节对所提的方法进行了仿真实验验证并讨论了结果。第6节总结了本工作并对未来工作进行了展望。· 本文作者 ·


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 第4节提出了一个多阶段强化学习框架。第5节对所提的方法进行了仿真实验验证并讨论了结果。第6节总结了本工作并对未来工作进行了展望。· 本文作者 ·

</details>


### [51] [<em class="highlight">强化学习</em>遇见大语言模型：贯穿 LLM 生命周期的进展与应用综述](http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671018&idx=1&sn=6d2a80cf3c0d87d3a7a09f672a106dc1&chksm=fda0bf7de271a1b4fbee5cd2ee63f4f9503f8a3e7b8ea3b35c91a706ba18cb88fc0174827242#rd)
*专知*

Main category: wechat.article

TL;DR: 近年来，以强化学习（Reinforcement Learning， RL）为核心的训练方法显著提升了大语言模型（Large Language Models， LLMs）的推理与对齐性能，特别是在理解人类意图、遵循用户指令以及增强推理能力方面。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近年来，以强化学习（Reinforcement Learning， RL）为核心的训练方法显著提升了大语言模型（Large Language Models， LLMs）的推理与对齐性能，特别是在理解人类意图、遵循用户指令以及增强推理能力方面。

</details>


### [52] [预测信息如何优化HVAC控制？GRU-RL算法突破<em class="highlight">强化学习</em>局限](http://mp.weixin.qq.com/s?__biz=MzE5MTE1MDc0NQ==&mid=2247487186&idx=1&sn=4e1b75bbc82507fca8a46edf57d6b4b6&chksm=97c42b37c47e93d4f6a1c55b4ef44b57f8f2a55d4870496f94c9f42a3a67e16a549bcdda2a61#rd)
*暖通前瞻*

Main category: wechat.article

TL;DR: 传统强化学习用于空调系统控制时存在未充分利用未来预测信息的问题，本研究旨在解决将预测信息与强化学习结合以优化空调系统运行的难题。研究借助开源框架测试不同预测信息策略的影响，还提出GRU - RL算法来处理系统状


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 传统强化学习用于空调系统控制时存在未充分利用未来预测信息的问题，本研究旨在解决将预测信息与强化学习结合以优化空调系统运行的难题。研究借助开源框架测试不同预测信息策略的影响，还提出GRU - RL算法来处理系统状

</details>


### [53] [香港大学提出Duramax<em class="highlight">强化学习</em>框架，实现心血管疾病长期精准脂质控制](http://mp.weixin.qq.com/s?__biz=Mzg5MDU5NTQ2MQ==&mid=2247485505&idx=2&sn=cb9f142aa431db32f1f3faacc3dc453c&chksm=ce0528ed34ae7c3630332635272ed9a14e5fb40f2453ee89b279a6b9b966092fe4f01a9ad734#rd)
*AI4CNS*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）因其在序列决策优化中的优势，理论上适用于慢性病长期管理。但将其应用于CVD一级预防面临三大挑战：延迟奖励建模——如何将数十年后的心血管事件与当前治疗决策关联；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）因其在序列决策优化中的优势，理论上适用于慢性病长期管理。但将其应用于CVD一级预防面临三大挑战：延迟奖励建模——如何将数十年后的心血管事件与当前治疗决策关联；

</details>


### [54] [清华最新发布114页大型推理模型的<em class="highlight">强化学习</em>综述](http://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247712292&idx=1&sn=1b3548eb06e5a9396de0d1ba8f1a875b&chksm=e9dd38ad0579e0cb7a1ecaa2e6f3be8861c4a31fb48413537bdee61b66ef290258946a10da54#rd)
*Datawhale*

Main category: wechat.article

TL;DR: 本文综述了强化学习在大型语言模型 （LLMs） 推理能力发展中的最新进展，特别是自 DeepSeek-R1 发布以来，RL 已成为将 LLMs 转化为大型推理模型的基础方法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文综述了强化学习在大型语言模型 （LLMs） 推理能力发展中的最新进展，特别是自 DeepSeek-R1 发布以来，RL 已成为将 LLMs 转化为大型推理模型的基础方法。

</details>


### [55] [AI <em class="highlight">代码Agent</em>“三国志”：Grok × Claude × GPT-5 Codex](http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489623&idx=1&sn=a96f61d9d535c2162e64220afad90459&chksm=cf281d32cd9d24cb281ac756c8bbacd4cfa916ca9ec805cf086c0c719e1ba0e4db3350d15c0a#rd)
*行客科技*

Main category: wechat.article

TL;DR: 一位合格的代码agent，不是“能写几段函数”，而是能稳定跑完这条闭环：感知（读仓库/工单/上下文）→ 规划（拆解任务、选工具）→ 执行（改代码/跑测/发包/GUI 操作）→ 观测（日志/测试/指标）→ 自检与回滚 → 形成可审


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一位合格的代码agent，不是“能写几段函数”，而是能稳定跑完这条闭环：感知（读仓库/工单/上下文）→ 规划（拆解任务、选工具）→ 执行（改代码/跑测/发包/GUI 操作）→ 观测（日志/测试/指标）→ 自检与回滚 → 形成可审

</details>


### [56] [智能编程助手 Neovate <em class="highlight">Code</em> 正式开源](http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247491901&idx=1&sn=ccb9ec1739032a0eea21d8b4b8dd67fe&chksm=cfe492b6ce1bceec1948a7b2e9cb7420ac18d2bf378b344da4e4f3265f882996cb7f90acddb4#rd)
*蚂蚁开源*

Main category: wechat.article

TL;DR: 它集成了 code agent 所需的核心能力。github：https：//github.com/neovateai/neovate-code 目前，Neovate Code 以 CLI 工具的形态提供，但其架构设计高度灵活，未来将支持多种客户端形态，适配更多开发场景。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它集成了 code agent 所需的核心能力。github：https：//github.com/neovateai/neovate-code 目前，Neovate Code 以 CLI 工具的形态提供，但其架构设计高度灵活，未来将支持多种客户端形态，适配更多开发场景。

</details>


### [57] [Neovate <em class="highlight">Code</em> 现已开源](http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487526&idx=1&sn=16669046bfbcf91aac3f01416fdf605d&chksm=a7da276a8a30351bdb2b98eee6325c251843740bd12ac9de1b2b7528aa14221a3e88c43bfe95#rd)
*云谦和他的朋友们*

Main category: wechat.article

TL;DR: 市面上有这么多 Code Agent。以下是让 Neovate Code 与其他 Code Agent 不同的一些特性： 开放的 Claude Code 易于扩展 多客户端支持Claude Code 是一个很棒的代码智能体，但它不是开源的，想要用上它还得费一番力气，同时默认情况下也无


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 市面上有这么多 Code Agent。以下是让 Neovate Code 与其他 Code Agent 不同的一些特性： 开放的 Claude Code 易于扩展 多客户端支持Claude Code 是一个很棒的代码智能体，但它不是开源的，想要用上它还得费一番力气，同时默认情况下也无

</details>


### [58] [秘塔AI放大招！「边想边搜边做」，内置20+<em class="highlight">智能体</em>，想法一键实现](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652630127&idx=1&sn=540858f320d6d69762df1e19bc0586b2&chksm=f08be02f7cabcacb3cea10fd856cd8b49d4146f13cbafdc035142af6ad9d8737a719805d3efc#rd)
*新智元*

Main category: wechat.article

TL;DR: 借助「Agentic Search」的多模态能力，秘塔AI可以更好的「理解」输入的内容。上传一张图片，就能「洞察」图片所属人的性格底色。图源自小红书秘塔AI按照指令，从性别、性格、习惯、MBTI、专业等角度进行了分析，甚至还能据


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 借助「Agentic Search」的多模态能力，秘塔AI可以更好的「理解」输入的内容。上传一张图片，就能「洞察」图片所属人的性格底色。图源自小红书秘塔AI按照指令，从性别、性格、习惯、MBTI、专业等角度进行了分析，甚至还能据

</details>


### [59] [构建可投入生产的<em class="highlight">Agentic</em>系统：来自Shopify助手的经验教训](http://mp.weixin.qq.com/s?__biz=MzU4MDQ0NTU3MQ==&mid=2247483856&idx=1&sn=fb24ad6a061637caa0362976c02d4eae&chksm=fc0c8898f4757f6f31b98f6da06c9d74194e4a1888d148744e76e8ca47121ad0358c577f6fa7#rd)
*Hus的Ai手札记*

Main category: wechat.article

TL;DR: 分享一篇来自于 Shopify 搭建可生产环境的 Agentic 系统，进行了逐字翻译，并保留了英语原文，删除了 "GRPO Training and Reward Hacking" 章节，对此章节感兴趣的可以点击文章底部"原文阅读"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 分享一篇来自于 Shopify 搭建可生产环境的 Agentic 系统，进行了逐字翻译，并保留了英语原文，删除了 "GRPO Training and Reward Hacking" 章节，对此章节感兴趣的可以点击文章底部"原文阅读"

</details>


### [60] [火到出圈的<em class="highlight">Agentic</em> AI 概念图？超详细解析，小白一看就会！](http://mp.weixin.qq.com/s?__biz=Mzg5ODkxNTc5Mw==&mid=2247484759&idx=1&sn=6d117076c4f2f11114c7e2303b5c0717&chksm=c12e281a3962d45abd42ce16ed58263ad0c6301d470e9dd2d3e9ef5d45be60387565a72134fc#rd)
*大模型白白*

Main category: wechat.article

TL;DR: 这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1 最核心：LLM's（大语言模型）=AI 的 “大脑”咱们熟悉的 GPT、文心一言都在这一层！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1 最核心：LLM's（大语言模型）=AI 的 “大脑”咱们熟悉的 GPT、文心一言都在这一层！

</details>


### [61] [大数据 AI 平台：构筑 <em class="highlight">Agentic</em> AI 的核心基石](http://mp.weixin.qq.com/s?__biz=Mzk3NTEwNTIyOA==&mid=2247500895&idx=1&sn=7888b82a09a64e0adfb56345b7e31270&chksm=c514cf7e23ea228242c96c75f1dd905a47f122f7d8dbe8fe1939a1aa543ed0fe6d4cb1f589ed#rd)
*阿里云大数据AI平台*

Main category: wechat.article

TL;DR: 在大模型能力加速进化的今天，除了推理模型和各类 Agentic 能力增强模型，世界模型同样备受关注。世界模型能够理解和遵循物理规律，具备因果推理、时间推演等能力，是大模型真正深入现实物理世界的关键。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在大模型能力加速进化的今天，除了推理模型和各类 Agentic 能力增强模型，世界模型同样备受关注。世界模型能够理解和遵循物理规律，具备因果推理、时间推演等能力，是大模型真正深入现实物理世界的关键。

</details>


### [62] [数据即智能，<em class="highlight">Agentic</em> AI 驱动存储范式改变](http://mp.weixin.qq.com/s?__biz=MzI2NzYyMDgyOA==&mid=2247519830&idx=2&sn=5f8a672a785ee1dcb35da87255adf0b8&chksm=ebcf47c9f4f7eb9eb392937fc8baef42da541b99184a55d9311dadbdcbc4e2e6d7d209c6ef89#rd)
*思瀚产业研究院*

Main category: wechat.article

TL;DR: 无限上下文 数字世界映射物理世界 数据规模扩展定理 记忆规模扩展定理 环境规模扩展定理 数据决定模型智能的高度 记忆决定agentic应用智能 环境决定模型自演进 数据规模扩展定律，数据驱动模型智能


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 无限上下文 数字世界映射物理世界 数据规模扩展定理 记忆规模扩展定理 环境规模扩展定理 数据决定模型智能的高度 记忆决定agentic应用智能 环境决定模型自演进 数据规模扩展定律，数据驱动模型智能

</details>


### [63] [ERP不再手工操作，知微行易借<em class="highlight">Agentic</em> AI 驱动企业智能运营！](http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651227789&idx=2&sn=fa2e4db714c3a31e8a80e085414c97a8&chksm=8aaf1e9669e5ef47ed5347ca8171ab7046dad5061dd0acc53064b268b4ad25e8c6b4acfde038#rd)
*亚马逊云科技*

Main category: wechat.article

TL;DR: 亚马逊云科技for software and technology agentic ai isv & startup 客户成功实践。亚马逊云科技是构建agentic al的不二选择。概述 知微行易（上海）智能科技有限公司（以下简称“知微行易”）作为专注于服务中国500强企业的智能企业运营


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 亚马逊云科技for software and technology agentic ai isv & startup 客户成功实践。亚马逊云科技是构建agentic al的不二选择。概述 知微行易（上海）智能科技有限公司（以下简称“知微行易”）作为专注于服务中国500强企业的智能企业运营

</details>


### [64] [范式转移！无问芯穹推出基础设施<em class="highlight">智能体</em>蜂群，开启<em class="highlight">Agentic智能体</em>基础设施新纪元](http://mp.weixin.qq.com/s?__biz=MzU5MDA5NjYyMg==&mid=2247553012&idx=2&sn=14a9a1b202c217cde26cd613be7d970b&chksm=fc5cb2439f458e3ba4a7df50ce4c62199cf03916390b0cfa0ec82be318ade592fc522d58ca50#rd)
*前润母基金*

Main category: wechat.article

TL;DR: infinigence 释放无穹算力 无问芯穹 agentic infra： 重构人工智能及智能体生产新范式 让agi触手可及 agent应用 具身智能 自动驾驶 图片/视频生成 deep research vibe coding 无问芯穹基础设施智能体蜂群 maas 通用大模型 代码模型 视觉模型 语


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: infinigence 释放无穹算力 无问芯穹 agentic infra： 重构人工智能及智能体生产新范式 让agi触手可及 agent应用 具身智能 自动驾驶 图片/视频生成 deep research vibe coding 无问芯穹基础设施智能体蜂群 maas 通用大模型 代码模型 视觉模型 语

</details>


### [65] [【Agent专题】硬核干货！<em class="highlight">Agentic</em> AI 架构全解析，一份生产就绪的实战指南](http://mp.weixin.qq.com/s?__biz=Mzk5MDczNTMyMQ==&mid=2247485029&idx=1&sn=fc2b27cc1362d3e2b61179602c0c5000&chksm=c4f894cf3b982f0ed0ef07c93fcbfefad59aa9d0211d6c2c8505163991f4108daadd673e2372#rd)
*苏州如姆AI人工智能*

Main category: wechat.article

TL;DR: agentic ai architecture agent orchestration ai agents 但实际情况是，只需将AI想象成一个能执行任务的小助手，再稍加思考，便能轻松步入代理AI架构的逻辑殿堂。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai architecture agent orchestration ai agents 但实际情况是，只需将AI想象成一个能执行任务的小助手，再稍加思考，便能轻松步入代理AI架构的逻辑殿堂。

</details>


### [66] [全网最火<em class="highlight">Agentic</em> AI概念图解析！小白也能看懂！](http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357070&idx=1&sn=7df71df7fe4f3dbf0bb96048ec59817a&chksm=8c90132c5ad9863f4c067eaa57e1d36000c1f27b2b744bff94966fdd75f88e1afa8ce1361951#rd)
*AI Agent 领域*

Main category: wechat.article

TL;DR: 全网最火Agentic AI概念图解析！小白也能看懂！这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1. LLM's（大语言模型）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全网最火Agentic AI概念图解析！小白也能看懂！这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1. LLM's（大语言模型）

</details>


### [67] [咨询 | 听听麦肯锡说企业落地<em class="highlight">智能体</em>的避坑指南](http://mp.weixin.qq.com/s?__biz=Mzg4OTk2MDY0Nw==&mid=2247491732&idx=1&sn=3bf7838e2601a709f434eb05de112b1f&chksm=ce91a3145400ef292777ca2e5d740f2cbf0245d4656cdfb428349a7fde37df792103d7c335cc#rd)
*增长 Growth Croissance*

Main category: wechat.article

TL;DR: 什么是Agentic AI？挫折是任何新技术发展过程中的自然阶段，此前其他创新技术的发展也呈现过类似规律。为总结早期经验，我们近期深入研究了麦肯锡主导的50多个智能体AI开发项目，以及市场上数十个其他相关项目，并将分析


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是Agentic AI？挫折是任何新技术发展过程中的自然阶段，此前其他创新技术的发展也呈现过类似规律。为总结早期经验，我们近期深入研究了麦肯锡主导的50多个智能体AI开发项目，以及市场上数十个其他相关项目，并将分析

</details>


### [68] [C端热战，B端暗涌：<em class="highlight">大模型</em>真正的战场才刚刚开始 | 《中国<em class="highlight">大模型</em>落地应用研究报告 2025》正式发布](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257061&idx=2&sn=bdc1e2aa94d023fc6bbfa70199430950&chksm=bcefabff39fc7fa74e3ce406bd61eae5f6dfe0d4704dea7e9ffd93876d7d1aa3a40a57417ccc#rd)
*InfoQ*

Main category: wechat.article

TL;DR: 过去很长一段时间，大模型技术迭代的速度几乎是每季度一次大更新——超大参数模型、MOE、推理成本下降、多模态登场、Agent 涌现，推理模型轮番登场，似乎一切都在变得更快。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 过去很长一段时间，大模型技术迭代的速度几乎是每季度一次大更新——超大参数模型、MOE、推理成本下降、多模态登场、Agent 涌现，推理模型轮番登场，似乎一切都在变得更快。

</details>


### [69] [我国人工智能<em class="highlight">大模型</em>实现批量“上车”](http://mp.weixin.qq.com/s?__biz=MzUzNzA5MjEyNQ==&mid=2247548699&idx=2&sn=80a3c1b75b2cd4cafb0a91f445d6286a&chksm=fb4b63ffcd88444a07e3b109f206eee6b7620bd388033066c7612f468a84b735573b6211c7e6#rd)
*装备强国*

Main category: wechat.article

TL;DR: 组织建设综合交通运输大模型，加快普及智能体应用。作为我国首个经国务院批准的国家级智能网联汽车专业会议，自2018年起，世界智能网联汽车大会已连续举办七届。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 组织建设综合交通运输大模型，加快普及智能体应用。作为我国首个经国务院批准的国家级智能网联汽车专业会议，自2018年起，世界智能网联汽车大会已连续举办七届。

</details>


### [70] [ICML 2025 | 会做题≠会思考？首个反例驱动推理基准：揭穿<em class="highlight">大模型</em>“刷题式假象”](http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585622&idx=3&sn=9ca738e956632aceb8f5181f6ad50364&chksm=fa9b0686c0143920a02bb137a9e30980956849975e2a396ed298cc81a6b720f519d2f141e884#rd)
*AI思想会*

Main category: wechat.article

TL;DR: “大模型能解高数题了，但它是真的理解了数学概念，还是只背会了题库套路？”随着大语言模型（LLMs）在数学领域的应用越来越广，“模型是否真的具备数学推理能力” 成了学界热议的焦点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “大模型能解高数题了，但它是真的理解了数学概念，还是只背会了题库套路？”随着大语言模型（LLMs）在数学领域的应用越来越广，“模型是否真的具备数学推理能力” 成了学界热议的焦点。

</details>


### [71] [燃气发电首个垂类<em class="highlight">大模型</em>发布](http://mp.weixin.qq.com/s?__biz=MzU1MjkwMDQ4NQ==&mid=2247500253&idx=2&sn=19538621cf50a6744abd23c23549309b&chksm=fa2cef2d51a7ed5486e06328b66b9b7e69e60a73c1534b3a5476fc80ee5abb2f7f1bcf8c6eac#rd)
*振邦天然气LNG新能源*

Main category: wechat.article

TL;DR: 京能集团发布行业首个燃机大模型——京能“擎睿”燃机大模型。作为首个燃气发电领域垂类大模型，京能“擎睿”燃机大模型依托全栈国产算力底座，自主可控、训推一体，实现从软硬件开发到落地应用的全链路突破，助力“


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 京能集团发布行业首个燃机大模型——京能“擎睿”燃机大模型。作为首个燃气发电领域垂类大模型，京能“擎睿”燃机大模型依托全栈国产算力底座，自主可控、训推一体，实现从软硬件开发到落地应用的全链路突破，助力“

</details>


### [72] [<em class="highlight">大模型</em>将吞噬软件](http://mp.weixin.qq.com/s?__biz=MzI4OTc4MzI5OA==&mid=2247852911&idx=1&sn=e7c803863b8caa3ed4ed48b4223788ba&chksm=eda3cf473cbb8b6705e51b81928796f565ed093d34466917c0c35fc63baf91f0b9540319fe0b#rd)
*云头条*

Main category: wechat.article

TL;DR: 大模型作为下一代的操作系统，将允许任何人用自然语言，创造无限多的应用。未来几乎所有与计算世界打交道的软件可能都是由大模型产生的 Agent，而不是现在的商业软件。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型作为下一代的操作系统，将允许任何人用自然语言，创造无限多的应用。未来几乎所有与计算世界打交道的软件可能都是由大模型产生的 Agent，而不是现在的商业软件。

</details>


### [73] [医疗<em class="highlight">大模型</em>从小事做起（六）：氛围编程](http://mp.weixin.qq.com/s?__biz=MzA4NzAwMjMyOQ==&mid=2649890691&idx=1&sn=8c6b40d201888c6927459f557c572dee&chksm=860a590275eb4511ffcd13d47d3705b3da882ed48ff748314695adf40b9a79300999a4e0bb9e#rd)
*CHIMA*

Main category: wechat.article

TL;DR: 近日，我拜读了薛万国主任的专栏文章《大模型-我的思与惑》，收获颇丰。文中提到，传统编程要求开发者使用严格、精确且符合特定语法的代码指令与计算机进行交互，而当下，我们正越来越多地借助自然语言，以更接近人


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，我拜读了薛万国主任的专栏文章《大模型-我的思与惑》，收获颇丰。文中提到，传统编程要求开发者使用严格、精确且符合特定语法的代码指令与计算机进行交互，而当下，我们正越来越多地借助自然语言，以更接近人

</details>


### [74] [硬科技看名企 | 文旅专属AI智能体来了！全国首个省级文旅<em class="highlight">大模型</em>“博观”发布](http://mp.weixin.qq.com/s?__biz=MzIzMjg0MTk0Nw==&mid=2247582224&idx=1&sn=a303545f104d3ee197013468157daa8a&chksm=e98ccd2b48f69d518494f492c54395c8e1aa59aa45b5f2d980c6f7e254e52dfcff1119ac4325#rd)
*创新西安*

Main category: wechat.article

TL;DR: 据悉，9月18日在上海举行的华为全联接大会上，陕西文旅行业人工智能大模型——“博观”正式发布。“博观”由陕文投集团与华为公司合作打造，是全国首个省级文旅大模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 据悉，9月18日在上海举行的华为全联接大会上，陕西文旅行业人工智能大模型——“博观”正式发布。“博观”由陕文投集团与华为公司合作打造，是全国首个省级文旅大模型。

</details>


### [75] [一文掌握<em class="highlight">大模型</em>与智能体：AI时代的“思考者”与“行动者”](http://mp.weixin.qq.com/s?__biz=Mzk5MDQyNjk2Mw==&mid=2247484597&idx=1&sn=c6ccfa5434665bd297a4b58cc190d643&chksm=c4f651b22e80d5133a3b3dab294ad128c09ae659217ceab127b63b9243be06d52fe528051765#rd)
*中商数智浪潮*

Main category: wechat.article

TL;DR: 大模型（Large Language Models， LLMs），顾名思义，是通过海量文本数据训练而成的大型人工智能系统。它具有强大的语言理解和生成能力，能够回答问题、撰写文章、翻译语言，甚至进行一定程度的逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型（Large Language Models， LLMs），顾名思义，是通过海量文本数据训练而成的大型人工智能系统。它具有强大的语言理解和生成能力，能够回答问题、撰写文章、翻译语言，甚至进行一定程度的逻辑推理。

</details>


### [76] [阿里云<em class="highlight">大模型</em>产品七连发](http://mp.weixin.qq.com/s?__biz=MjM5MTM3NTMwNA==&mid=2661636475&idx=2&sn=423e82dc3a2c5306cf2fd11870007359&chksm=bcaec1c2c7c5c5f37885775d6a39d9fbdbd6d4b826a92651756298cb6740db3b951a79002a81#rd)
*第一财经*

Main category: wechat.article

TL;DR: 2025云栖大会现场，阿里云CTO周靖人发布七款大模型产品，包括大语言模型通义旗舰模型Qwen3-Max、下一代基础模型架构Qwen3-Next及系列模型、千问编程模型Qwen3-Coder、视觉理解模型Qwen3-VL、全模态模型Qwen3-Omni、视觉基础模型Wan2.5-prev


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025云栖大会现场，阿里云CTO周靖人发布七款大模型产品，包括大语言模型通义旗舰模型Qwen3-Max、下一代基础模型架构Qwen3-Next及系列模型、千问编程模型Qwen3-Coder、视觉理解模型Qwen3-VL、全模态模型Qwen3-Omni、视觉基础模型Wan2.5-prev

</details>


### [77] [阿里股价刷新4年新高，阿里通义<em class="highlight">大模型</em>“全家桶”出炉](http://mp.weixin.qq.com/s?__biz=MzA5NTI1MDEyNA==&mid=2652724663&idx=1&sn=c0d2e4bec228abc8d8d41bc78e22d518&chksm=8a56ad6baea434e22c86b7327fc9acb3cbdd544865c20bf3b4a8f1520ff08b31aa7df4cba8ff#rd)
*亿欧网*

Main category: wechat.article

TL;DR: 通义千问qwen模型家族 大语言模型 专项模型 全尺寸 全模态 多场景 300+开源模型 开源全球第一 ·17万+全球衍生模型。通义万相wan模型家族 阿里云：全球领先的全栈人工智能服务商 无影 qoder ai agent 通义灵码 llm os qwen3-next 下一代


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 通义千问qwen模型家族 大语言模型 专项模型 全尺寸 全模态 多场景 300+开源模型 开源全球第一 ·17万+全球衍生模型。通义万相wan模型家族 阿里云：全球领先的全栈人工智能服务商 无影 qoder ai agent 通义灵码 llm os qwen3-next 下一代

</details>


### [78] [AI <em class="highlight">大模型</em>驱动汽车产业三大变革——竞争力、产业链、盈利模式](http://mp.weixin.qq.com/s?__biz=Mzg3MDUzOTU5Mw==&mid=2247545923&idx=2&sn=7010cd07b04f082bd831c6ccebd5d254&chksm=cf6854d51d42b6d86337a2e895945f5ca36c2ae688e4b2f7d97206369de774d58cdc04e5dc6d#rd)
*车百会研究院*

Main category: wechat.article

TL;DR: 大模型具备泛在的理解能力，结合多年发展的人工智能技术，将给汽车带来全面、彻底的变化，让汽车成为更高层次、更多内容的超级智能体，形成会思考、懂用户的 AI汽车。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型具备泛在的理解能力，结合多年发展的人工智能技术，将给汽车带来全面、彻底的变化，让汽车成为更高层次、更多内容的超级智能体，形成会思考、懂用户的 AI汽车。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [79] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出CoRaCMG框架，通过检索相似的diff-message对来增强LLM生成提交消息的性能，显著提升了BLEU、Rouge-L、METEOR和CIDEr等指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息在记录代码变更意图中起关键作用，但现有提交消息质量普遍较低，限制了其实用性。虽然LLM在自动生成提交消息方面显示出潜力，但性能仍有待提升。

Method: CoRaCMG框架包含三个阶段：(1)检索相似的diff-message对；(2)将检索结果与查询diff结合成结构化提示；(3)通过LLM生成对应的提交消息。该方法使LLM能从检索到的示例中学习项目特定术语和写作风格。

Result: 实验表明CoRaCMG显著提升了多种LLM的性能，DeepSeek-R1在BLEU和CIDEr上分别提升了76%和71%，GPT-4o的BLEU提升了89%。性能提升在超过3个示例后趋于稳定。

Conclusion: 检索增强方法能有效提升提交消息生成质量，改进主要归因于模型从检索示例中学习人类编写提交消息的术语和写作风格的能力。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [80] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: 本文介绍了SC2Tools工具集，用于处理星际争霸2游戏数据，简化数据收集和预处理工作，支持游戏和电子竞技研究。


<details>
  <summary>Details</summary>
Motivation: 游戏环境作为受控模拟环境在强化学习研究中很重要，但数据收集和预处理工作繁重，阻碍了技术能力较弱的研究者参与游戏和电子竞技研究。

Method: 开发了SC2Tools工具集，包含多个子模块，采用模块化结构设计，支持星际争霸2比赛数据集创建，并提供PyTorch和PyTorch Lightning API接口。

Result: 创建了迄今为止最大的星际争霸2比赛数据集之一，工具集不仅限于星际争霸2，还可用于其他类型的数据集创建。

Conclusion: 减轻数据收集和预处理负担对于促进游戏和电子竞技研究至关重要，该工具为星际争霸2实验工作流程标准化提供了基础工作。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [81] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval是一个专门评估LLMs在逐步需求细化下迭代代码生成能力的基准测试，涵盖函数级和仓库级任务，结果显示当前模型在此任务上表现仍极具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要将代码生成视为静态、单轮问题，忽略了真实软件开发中的逐步需求变化和迭代工作流程，限制了理解LLMs支持真实开发工作流程的能力。

Method: 采用多智能体需求生成方法模拟开发过程，从最终需求中恢复多轮交互过程，并使用语义感知的判别性测试用例生成组件确保每轮评估的一致性和判别性。

Result: 评估11个代表性LLMs，最佳模型在函数级任务上仅达到22.67%完成率，仓库级任务为20.00%，提示策略对性能有显著影响。

Conclusion: 逐步需求细化下的迭代代码生成仍然极具挑战性，需要开发更先进的方法。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [82] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究了使用大型语言模型（LLM）代理直接执行自然语言测试用例的可行性，重点关注NL测试用例的不健全性和执行一致性问题，并提出了防护机制和专门代理来验证测试步骤的正确执行。


<details>
  <summary>Details</summary>
Motivation: 传统的手写可执行测试脚本开发成本高且难以维护，而使用自然语言测试用例验证图形用户界面（GUI）应用程序是一个有前景的方向。LLM的进展使得直接执行NL测试用例成为可能，但存在不健全性和执行一致性的挑战。

Method: 提出了一种执行NL测试用例的算法，采用防护机制和专门代理动态验证每个测试步骤的正确执行。引入了评估LLM测试执行能力的指标和一个量化执行一致性的指标，并提出了弱不健全性定义来表征NL测试用例执行可接受的上下文。

Result: 实验评估了8个公开可用的LLM（参数从3B到70B），结果显示Meta Llama 3.1 70B在NL测试用例执行中表现出可接受的能力，执行一致性高（超过3-sigma水平）。

Conclusion: 当前LLM代理在GUI测试方面既有潜力也存在局限性，Meta Llama 3.1 70B在NL测试用例执行方面表现最佳，但整体技术仍需改进。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [83] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该研究首次对AI代理生态系统中的测试实践进行了大规模实证研究，分析了39个开源代理框架和439个代理应用，发现测试努力存在根本性倒置：确定性组件消耗了70%以上的测试努力，而基于基础模型的计划主体和触发组件（提示）被严重忽视。


<details>
  <summary>Details</summary>
Motivation: 基础模型驱动的AI代理日益普及，但其固有的非确定性和不可重现性给测试和质量保证带来了挑战。现有基准主要提供任务级评估，缺乏对开发过程中如何验证代理内部正确性的理解。

Method: 通过对39个开源代理框架和439个代理应用进行大规模实证分析，识别出十种不同的测试模式，并将这些模式映射到代理框架和应用的规范架构组件上。

Result: 研究发现传统测试模式（如负面测试和成员测试）被广泛采用以管理基础模型的不确定性，而新型代理特定方法（如DeepEval）使用率仅为1%左右。测试努力存在严重不平衡：确定性组件占70%以上测试努力，而计划主体和触发组件分别只占不到5%和1%。

Conclusion: 研究揭示了在非确定性适应方面的理性但不完整的调整，建议框架开发者改进对新型测试方法的支持，应用开发者必须采用提示回归测试，研究人员应探索采用障碍。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过单一自然语言提示自动化整个OpenFOAM工作流程，包括网格生成、HPC脚本生成和后处理可视化，在110个仿真任务中达到88.2%的成功率。


<details>
  <summary>Details</summary>
Motivation: 计算流体动力学(CFD)仿真工具学习曲线陡峭且设置复杂，现有系统存在功能不完整的问题，需要开发端到端的自动化解决方案来降低使用门槛。

Method: 采用多智能体框架，包含网格生成代理、HPC脚本生成代理等；使用模型上下文协议(MCP)暴露核心功能为可调用工具；采用分层多索引RAG进行精确上下文检索和依赖感知的配置生成。

Result: 在110个仿真任务基准测试中，使用Claude 3.5 Sonnet达到88.2%的成功率，显著优于MetaOpenFOAM的55.5%。

Conclusion: Foam-Agent显著降低了CFD的专业门槛，展示了专用多智能体系统如何民主化复杂科学计算。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [85] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 本文提出了一种'N-Plus-1'GPT代理机构，用于机械工程问题的初始低成本分析。该机构通过多个独立求解代理生成解决方案，再通过比较代理进行汇总和比较，以提高解决方案的可靠性。


<details>
  <summary>Details</summary>
Motivation: GPT在解决机械工程分析问题时存在不可靠性（成功率仅85%），这使得'开箱即用'的GPT不适合在教育和工程实践中部署。需要一种方法来提高解决方案的可靠性。

Method: 采用'N-Plus-1'代理架构：首先启动N个独立求解代理生成N个解决方案，然后调用比较代理对这些方案进行汇总、比较，并推荐最佳解决方案。基于孔多塞陪审团定理，当每个求解代理的成功概率大于1/2时，多数解决方案很可能是正确的。

Result: 与商业多代理模型Grok Heavy相比，该机构在设计和性能上有相似之处，但更注重透明度和教学价值。该方法能够有效提高解决方案的可靠性。

Conclusion: 提出的代理机构能够显著提高GPT在机械工程问题分析中的可靠性，特别适合教育和工程实践应用，同时保持了透明度和教学价值。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [86] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: 本文提出了ComputerAgent，一个轻量级分层强化学习框架，用于桌面应用控制，通过两级选项过程（管理器和子策略）解决现有MLLM方法的高延迟、低样本效率和无法在设备上部署的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLM）的桌面应用控制方法存在推理延迟高、长视野稀疏奖励任务样本效率差、无法在设备上部署等问题，需要更实用的解决方案。

Method: 采用分层强化学习框架，包含管理器和子策略两级选项过程；使用三重模态状态编码器（截图、任务ID、数值状态）处理视觉和上下文多样性；集成元动作和早停机制减少无效交互；使用紧凑视觉骨干网络和小型策略网络（1500万参数）实现设备上推理。

Result: 在135个真实世界桌面任务测试中，ComputerAgent在简单任务（<8步）上达到92.1%成功率，在困难任务（≥8步）上达到58.8%成功率，在简单场景中匹配或超过200B参数MLLM基线，同时模型大小减少四个数量级，推理时间减半。

Conclusion: 分层强化学习为计算机控制提供了一个实用、可扩展的替代方案，优于单一MLLM自动化方法。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [87] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个新的函数调用基准测试，专注于评估大语言模型对参数描述中格式指令的遵循能力，填补了现有基准测试只关注参数正确性而忽略格式要求的空白。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准测试（如BFCL、tau^2-Bench、ACEBench）只评估参数正确性，不测试格式指令遵循能力，这在现实AI代理系统中是一个重要缺陷。

Method: 基于IFEval设计，在JSON schema描述中直接编码可验证的格式要求（如必须用双引号、ISO日期格式等），包含750个测试用例，每个用例包含一个函数和对应的用户查询，采用全自动算法评估。

Result: 即使是GPT-5和Claude 4.1 Opus等最先进的专有模型也经常无法遵循基本格式规则，揭示了现实世界代理系统的实际局限性。

Conclusion: 格式指令遵循是函数调用的关键能力，现有模型在这方面存在明显不足，IFEval-FC为评估和改进这一能力提供了重要工具。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [88] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: 本文提出LLMZ+方法，通过提示白名单机制保护智能体AI免受攻击，相比传统检测方法更安全高效。


<details>
  <summary>Details</summary>
Motivation: 智能体AI具有特权数据访问和API工具调用能力，其非确定性行为特性带来了重大安全风险，传统检测机制存在局限。

Method: 采用提示白名单方法，只允许上下文相关且安全的消息与智能体LLM交互，确保所有交换符合预定义用例和操作边界。

Result: 实证评估显示LLMZ+对常见越狱提示具有强韧性，误报率和漏报率在实验环境下可降至0，同时不影响合法业务通信。

Conclusion: LLMZ+方法简化了安全框架，增强了长期韧性，减少了维持LLM信息安全所需的资源。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [89] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 该论文提出自主数据代理（DataAgents）的概念，通过集成LLM推理与任务分解、行动推理和工具调用，实现从复杂数据到知识的自主转换。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性的增长，数据准备、转换和分析工作变得劳动密集且难以扩展。数据与AI之间的对齐至关重要，但现有数据结构往往不适合AI利用。

Method: DataAgents通过动态规划工作流、调用强大工具和适应多样化数据任务，能够处理数据收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等操作。

Result: DataAgents能够将复杂和非结构化数据转换为连贯且可操作的知识，代表了向自主数据到知识系统的范式转变。

Conclusion: 需要协同努力推进行动工作流优化、建立开放数据集和基准生态系统、保护隐私、平衡效率与可扩展性，并开发可信赖的DataAgent防护机制。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [90] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: 本文提出了Agent Directory Service (ADS)，一个用于AI代理能力发现的分布式目录服务，采用内容寻址存储、分层分类和加密签名技术，支持异构多代理系统中的高效、可验证、多维发现。


<details>
  <summary>Details</summary>
Motivation: 随着多代理系统(MAS)的异构性增加，需要一种能够有效发现和管理AI代理能力、元数据和来源的分布式目录服务，以解决代理发现和互操作性问题。

Method: 基于Open Agentic Schema Framework (OASF)，采用两级映射架构，利用Kademlia分布式哈希表(DHT)实现能力索引与内容位置的解耦，重用OCI/ORAS基础设施进行工件分发，集成Sigstore用于来源验证。

Result: ADS实现了高效、可验证的代理能力发现，支持多种新兴代理模式（如LLM提示代理、MCP服务器、A2A组件），并具有良好的安全性和性能特性。

Conclusion: ADS为新兴代理注册和互操作性倡议提供了一个重要的架构模型，解决了多代理系统中的关键发现和互操作挑战。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [91] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于模型检查的验证方法，用于验证LLM文本生成过程的概率计算树逻辑(PCTL)属性。该方法通过α-k有界文本生成来限制生成过程，只关注每一步中top-k令牌的α最大累积概率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本生成过程缺乏形式化验证方法，无法保证生成文本的一致性和可靠性。作者发现文本生成过程中通常只有有限数量的令牌被选择，且这些选择并不总是相同的。

Method: 提出α-k有界文本生成方法，在每一步只考虑top-k令牌中累积概率超过阈值α的令牌。LLMCHECKER利用模型检查技术验证PCTL属性，支持多种文本量化方法。

Result: 在多个LLM模型（包括Llama、Gemma、Mistral、Genstruct和BERT）上验证了方法的适用性，能够有效检查文本生成过程的一致性。

Conclusion: 这是首次将PCTL模型检查应用于LLM文本生成过程验证，为LLM的可靠性和安全性提供了形式化保证。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [92] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的LLM记忆定义和四部分分类法（参数化、上下文、外部、程序性/情景性），建立了记忆四元组框架，并设计了分层评估协议来标准化LLM记忆能力的评测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM记忆研究缺乏统一的定义和评估标准，导致不同设置下的结果难以比较。需要建立一个可复现、可比较、可治理的坐标系统来推动研究和部署。

Method: 采用三设置协议（仅参数化、离线检索、在线检索）解耦能力与信息可用性，构建分层评估框架，整合时间治理和泄漏审计，并提出DMM Gov更新和遗忘协调机制。

Result: 建立了一个包含记忆定义、分类、评估协议和治理框架的完整体系，为LLM记忆研究提供了标准化的方法论基础。

Conclusion: 该框架为LLM记忆研究提供了可复现、可比较、可治理的坐标系统，包含四个可测试的命题，有助于推动该领域的标准化发展。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [93] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个5600亿参数的开放源代码MoE推理模型，通过精心设计的训练流程（包括长链思维数据冷启动和大规模强化学习）实现高效推理能力，在复杂推理任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的大规模推理模型，通过创新的训练策略解决传统模型在推理任务中的效率问题，特别是在智能体推理领域减少token消耗。

Method: 采用长链思维数据冷启动训练策略，结合领域并行训练方案（将不同领域的专家模型解耦优化后融合），并使用DORA系统进行大规模异步强化学习训练。

Result: 在AIME-25基准测试中，智能体推理的平均token消耗减少了64.5%（从19,653降至6,965），且不降低任务准确率，在复杂推理任务上达到开源模型的最先进性能。

Conclusion: LongCat-Flash-Thinking展示了通过创新的训练策略可以实现高效的大规模推理模型，为推理系统和智能体AI研究提供了重要贡献。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [94] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本文首次对基于LLM的智能体中的幻觉问题进行了全面调查，提出了新的分类法，分析了18种触发原因，并总结了幻觉缓解和检测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的智能体在认知、推理和交互方面展现出强大能力，但它们仍然容易产生幻觉问题，这会严重影响任务执行的准确性和系统可靠性。

Method: 通过仔细分析智能体的完整工作流程，提出了新的分类法来识别不同阶段出现的幻觉类型，并对大量现有研究进行了详细回顾。

Result: 系统性地识别了18种导致智能体幻觉的触发原因，总结了幻觉缓解和检测的方法，并指出了未来研究的重点方向。

Conclusion: 这项调查将为解决基于LLM智能体的幻觉问题提供重要参考，有助于开发更鲁棒和可靠的智能体系统。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [95] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC提出了一种基于代码驱动规划和领域自适应评估器的AI代理规划方法，通过生成多样化高层规划程序并利用训练好的评估器选择长期奖励最优的计划，显著减少LLM查询次数并提高规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划方法依赖频繁查询和环境反馈进行迭代优化，导致查询成本高且难以实现长期奖励对齐。需要一种既能减少查询成本又能优化长期规划效果的方法。

Method: 使用LLM生成多样化高层规划程序，这些程序迭代产生和优化候选计划；训练领域自适应评估器评估候选计划与长期奖励的对齐程度；选择最优计划执行。

Result: 在ALFWorld、NetHack和StarCraft II Unit Building三个基准测试中，CoPiC相比AdaPlanner和Reflexion基线方法，平均成功率提升23.33%，查询成本降低91.27%。

Conclusion: CoPiC通过代码驱动规划和领域自适应评估器的结合，有效解决了LLM规划中的查询成本高和长期奖励对齐问题，在多个复杂环境中表现出优越性能。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [96] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一种多智能体系统初始化方法，通过优化智能体团队结构来提升系统性能，结合自然语言到格式机制和帕累托原则的平衡团队选择策略。


<details>
  <summary>Details</summary>
Motivation: 现有MAS初始化方法未能充分考虑后续阶段智能体的协作需求，需要一种能够优化智能体团队结构的初始化方法。

Method: AgentInit包含多轮智能体交互与反思、自然语言到格式机制确保一致性，以及基于帕累托原则的平衡团队选择策略来兼顾多样性和任务相关性。

Result: 实验表明AgentInit在多种框架和任务中优于现有初始化方法和预定义策略，性能提升达1.2-1.6倍，同时显著减少token消耗。

Conclusion: AgentInit具有良好的可迁移性和关键组件的有效性，证明其作为可靠MAS初始化方法的能力和适应性。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>
