{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "FRIT\u662f\u4e00\u79cd\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\u63d0\u9ad8\u601d\u7ef4\u94fe\u63a8\u7406\u5fe0\u5b9e\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u63a8\u7406\u5bf9\u6765\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5fe0\u5b9e\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6b65\u9aa4\u4e0e\u6700\u7ec8\u7b54\u6848\u7f3a\u4e4f\u56e0\u679c\u5173\u8054\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u8106\u5f31\u4e14\u4e0d\u53ef\u4fe1\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6d4b\u91cf\u5fe0\u5b9e\u6027\uff0c\u4f46\u7cfb\u7edf\u6027\u6539\u8fdb\u65b9\u6cd5\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51faFRIT\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u5728\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u4e2d\u5bf9\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5e72\u9884\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff08\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u63a8\u7406\u5bf9\uff09\uff1b2\uff09\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Qwen3-8B\u548cMistral-7B-v0.1\u6a21\u578b\u4e0a\uff0cFRIT\u5728GSM8K\u4efb\u52a1\u4e0a\u5c06Mistral\u7684\u5fe0\u5b9e\u63a8\u7406\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u51c6\u786e\u6027\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FRIT\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2509.13471", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13471", "abs": "https://arxiv.org/abs/2509.13471", "authors": ["Sina Gogani-Khiabani", "Ashutosh Trivedi", "Diptikalyan Saha", "Saeid Tizpaz-Niari"], "title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software", "comment": "To appear at ICSE 26. 12 pages", "summary": "Large language models (LLMs) show promise for translating natural-language\nstatutes into executable logic, but reliability in legally critical settings\nremains challenging due to ambiguity and hallucinations. We present an agentic\napproach for developing legal-critical software, using U.S. federal tax\npreparation as a case study. The key challenge is test-case generation under\nthe oracle problem, where correct outputs require interpreting law. Building on\nmetamorphic testing, we introduce higher-order metamorphic relations that\ncompare system outputs across structured shifts among similar individuals.\nBecause authoring such relations is tedious and error-prone, we use an\nLLM-driven, role-based framework to automate test generation and code\nsynthesis. We implement a multi-agent system that translates tax code into\nexecutable software and incorporates a metamorphic-testing agent that searches\nfor counterexamples. In experiments, our framework using a smaller model\n(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier\nmodels (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results\nsupport agentic LLM methodologies as a path to robust, trustworthy\nlegal-critical software from natural-language specifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u6cd5\u5f8b\u6761\u6587\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u8f6f\u4ef6\uff0c\u901a\u8fc7\u9ad8\u9636\u8715\u53d8\u6d4b\u8bd5\u548c\u89d2\u8272\u6846\u67b6\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\uff0c\u5728\u590d\u6742\u7a0e\u6cd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u524d\u6cbf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u6cd5\u5f8b\u5173\u952e\u73af\u5883\u4e2d\u56e0\u6b67\u4e49\u548c\u5e7b\u89c9\u5bfc\u81f4\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f8e\u56fd\u8054\u90a6\u7a0e\u52a1\u51c6\u5907\u7b49\u9700\u8981\u7cbe\u786e\u6cd5\u5f8b\u89e3\u91ca\u7684\u9886\u57df\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u9636\u8715\u53d8\u5173\u7cfb\u8fdb\u884c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u548c\u4ee3\u7801\u5408\u6210\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u8f83\u5c0f\u6a21\u578b(GPT-4o-mini)\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u8fbe\u523045%\u7684\u901a\u8fc7\u7387\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude 3.5\u6a21\u578b(9-15%)\u3002", "conclusion": "\u4ee3\u7406\u5f0fLLM\u65b9\u6cd5\u4e3a\u4ece\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u5f00\u53d1\u5065\u58ee\u3001\u53ef\u4fe1\u8d56\u7684\u6cd5\u5f8b\u5173\u952e\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. G\u00fczel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rockt\u00e4schel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "IMAC\u65b9\u6cd5\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u60f3\u8c61\u73af\u5883\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\u81ea\u52a8\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "motivation": "\u89e3\u51b3\u5728\u5b9e\u4f53\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u7cbe\u786e\u4eff\u771f\u7684\u95ee\u9898\uff0c\u5229\u7528\u79bb\u7ebf\u88ab\u52a8\u6536\u96c6\u6570\u636e\u6784\u5efa\u4e16\u754c\u6a21\u578b\u6765\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u73af\u5883", "method": "\u63d0\u51faIMAC\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u548c\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1(UED)\uff0c\u81ea\u52a8\u751f\u6210\u8bfe\u7a0b\u5316\u7684\u60f3\u8c61\u73af\u5883\u6765\u8bad\u7ec3\u667a\u80fd\u4f53", "result": "\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u73af\u5883\u4e2d\uff0c\u4ec5\u4f7f\u7528\u8f83\u7a84\u6570\u636e\u96c6\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u5c31\u80fd\u5728\u4fdd\u7559\u73af\u5883\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u8fc1\u79fb\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5229\u7528\u66f4\u5927\u89c4\u6a21\u7684\u57fa\u7840\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "2509.13487", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13487", "abs": "https://arxiv.org/abs/2509.13487", "authors": ["Abubakari Alidu", "Michele Ciavotta", "Flavio DePaoli"], "title": "Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation", "comment": null, "summary": "Developing reliable data enrichment pipelines demands significant engineering\nexpertise. We present Prompt2DAG, a methodology that transforms natural\nlanguage descriptions into executable Apache Airflow DAGs. We evaluate four\ngeneration approaches -- Direct, LLM-only, Hybrid, and Template-based -- across\n260 experiments using thirteen LLMs and five case studies to identify optimal\nstrategies for production-grade automation. Performance is measured using a\npenalized scoring framework that combines reliability with code quality (SAT),\nstructural integrity (DST), and executability (PCT). The Hybrid approach\nemerges as the optimal generative method, achieving a 78.5% success rate with\nrobust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly\noutperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.\nOur findings show that reliability, not intrinsic code quality, is the primary\ndifferentiator. Cost-effectiveness analysis reveals the Hybrid method is over\ntwice as efficient as Direct prompting per successful DAG. We conclude that a\nstructured, hybrid approach is essential for balancing flexibility and\nreliability in automated workflow generation, offering a viable path to\ndemocratize data pipeline development.", "AI": {"tldr": "Prompt2DAG\u662f\u4e00\u79cd\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884cApache Airflow DAG\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u5b9e\u73b078.5%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u7eafLLM\u548c\u76f4\u63a5\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684\u6570\u636e\u4e30\u5bcc\u7ba1\u9053\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u964d\u4f4e\u6280\u672f\u95e8\u69db\u5e76\u6c11\u4e3b\u5316\u6570\u636e\u7ba1\u9053\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u751f\u6210\u65b9\u6cd5\uff08\u76f4\u63a5\u3001\u7eafLLM\u3001\u6df7\u5408\u548c\u57fa\u4e8e\u6a21\u677f\uff09\uff0c\u4f7f\u752813\u4e2aLLM\u5728260\u4e2a\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\uff0c\u91c7\u7528\u60e9\u7f5a\u6027\u8bc4\u5206\u6846\u67b6\u8861\u91cf\u53ef\u9760\u6027\u3001\u4ee3\u7801\u8d28\u91cf\u3001\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u53ef\u6267\u884c\u6027\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u6210\u529f\u738778.5%\uff08SAT:6.79, DST:7.67, PCT:7.76\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u7eafLLM\uff0866.2%\uff09\u548c\u76f4\u63a5\u65b9\u6cd5\uff0829.2%\uff09\u3002\u53ef\u9760\u6027\u662f\u4e3b\u8981\u533a\u5206\u56e0\u7d20\uff0c\u6df7\u5408\u65b9\u6cd5\u7684\u6210\u672c\u6548\u76ca\u662f\u76f4\u63a5\u63d0\u793a\u7684\u4e24\u500d\u3002", "conclusion": "\u7ed3\u6784\u5316\u6df7\u5408\u65b9\u6cd5\u5bf9\u4e8e\u5e73\u8861\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u6c11\u4e3b\u5316\u6570\u636e\u7ba1\u9053\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "topic": "swe application"}}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chain of Action (CoA)\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u96be\u9898\uff0c\u5728Minecraft\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u8fd9\u4e00\u5173\u952e\u4f46\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u7814\u7a76\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u52a8\u4f5c\u7a7a\u95f4\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u6700\u4f18\uff0c\u8fd9\u7ed9\u6784\u5efa\u901a\u7528\u667a\u80fd\u4f53\u5e26\u6765\u4e86\u56f0\u5883\u3002", "method": "\u5f15\u5165Chain of Action (CoA)\u6846\u67b6\uff0c\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u800c\u975e\u5355\u72ec\u7b56\u7565\u7684\u547d\u4ee4\uff0c\u7edf\u4e00\u4e86\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\uff1b\u4f7f\u7528\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\u6df7\u5408\u8bad\u7ec3All-in-One\u667a\u80fd\u4f53\u3002", "result": "CoA\u6846\u67b6\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u57fa\u7ebf\u6a21\u578b\uff1b\u8bad\u7ec3\u51fa\u7684\u7edf\u4e00\u667a\u80fd\u4f53\u5b66\u4e60\u5230\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u7b56\u7565\u3002", "conclusion": "CoA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u56f0\u5883\uff0c\u901a\u8fc7\u7edf\u4e00\u89c4\u5212\u548c\u63a7\u5236\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff1b\u540c\u65f6\u53d1\u5e03\u4e86OpenHA\u5957\u4ef6\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.13650", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13650", "abs": "https://arxiv.org/abs/2509.13650", "authors": ["Amena Amro", "Manar H. Alalfi"], "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?", "comment": null, "summary": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security.", "AI": {"tldr": "GitHub Copilot\u7684\u4ee3\u7801\u5ba1\u67e5\u529f\u80fd\u5728\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u5173\u6ce8\u4f4e\u4e25\u91cd\u6027\u95ee\u9898\u800c\u975e\u5173\u952e\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u8bc4\u4f30AI\u8f85\u52a9\u5de5\u5177\u5728\u5b89\u5168\u7f16\u7801\u652f\u6301\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u7684\u80fd\u529b", "method": "\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u7f16\u7a0b\u8bed\u8a00\u548c\u5e94\u7528\u9886\u57df\u7684\u6807\u8bb0\u6f0f\u6d1e\u4ee3\u7801\u6837\u672c\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30Copilot\u68c0\u6d4b\u5e38\u89c1\u5b89\u5168\u7f3a\u9677\u7684\u80fd\u529b", "result": "Copilot\u7ecf\u5e38\u65e0\u6cd5\u68c0\u6d4bSQL\u6ce8\u5165\u3001XSS\u548c\u4e0d\u5b89\u5168\u53cd\u5e8f\u5217\u5316\u7b49\u5173\u952e\u6f0f\u6d1e\uff0c\u53cd\u9988\u4e3b\u8981\u9488\u5bf9\u7f16\u7801\u98ce\u683c\u548c\u62fc\u5199\u9519\u8bef\u7b49\u4f4e\u4e25\u91cd\u6027\u95ee\u9898", "conclusion": "AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u9884\u671f\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4ecd\u9700\u4e13\u7528\u5b89\u5168\u5de5\u5177\u548c\u4eba\u5de5\u4ee3\u7801\u5ba1\u8ba1\u6765\u786e\u4fdd\u8f6f\u4ef6\u5b89\u5168", "topic": "swe application"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\u6765\u7814\u7a76LLM\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\u548c\u526f\u4f5c\u7528\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u53d7\u9690\u85cf\u7684\u7edf\u8ba1\u56e0\u7d20\u548c\u8bed\u8a00\u7279\u5f81\u5f71\u54cd\uff0c\u800c\u975e\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u90e8\u7f72\u5230\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u65b0\u4efb\u52a1\u65f6\uff0c\u679a\u4e3e\u6240\u6709\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u4f9d\u8d56\u4e0d\u540c\u7279\u6027\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u56e0\u6b64\u9700\u8981\u5206\u6790\u8de8\u4efb\u52a1\u4ea4\u4e92\u7684\u590d\u6742\u52a8\u6001\u3002", "method": "\u6784\u5efa\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u8bad\u7ec310\u4e2a\u6a21\u578b\u6765\u8bc6\u522b\u6f5c\u5728\u80fd\u529b\uff08\u63a8\u7406\u3001\u60c5\u611f\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7b97\u672f\u7b49\uff09\uff0c\u5e76\u5206\u6790\u8fc1\u79fb\u5b66\u4e60\u7684\u526f\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u5f80\u5f80\u65e0\u6cd5\u7528\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\u6765\u89e3\u91ca\uff0c\u800c\u662f\u53d7\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u5982\u7c7b\u522b\u5206\u5e03\u3001\u751f\u6210\u957f\u5ea6\u503e\u5411\uff09\u548c\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u7684\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\uff0c\u4e3a\u66f4\u53ef\u9884\u6d4b\u548c\u6709\u6548\u7684LLM\u9002\u5e94\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6846\u67b6Agentic UAVs\uff0c\u901a\u8fc7\u4e94\u5c42\u67b6\u6784\u5b9e\u73b0\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u548c\u5b66\u4e60\uff0c\u5728\u641c\u6551\u6a21\u62df\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u548c\u7a84AI\uff0c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u81ea\u4e3b\u51b3\u7b56\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7075\u6d3b\u9002\u5e94", "method": "\u8bbe\u8ba1\u4e94\u5c42\u67b6\u6784\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u3001\u5b66\u4e60\uff09\uff0c\u96c6\u6210YOLOv11\u76ee\u6807\u68c0\u6d4b\u3001GPT-4\u63a8\u7406\u548c\u672c\u5730Gemma-3\u90e8\u7f72\uff0c\u57fa\u4e8eROS2\u548cGazebo\u6784\u5efa\u539f\u578b\u7cfb\u7edf", "result": "\u5728\u6a21\u62df\u641c\u6551\u573a\u666f\u4e2d\uff0c\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u4ece0.72\u63d0\u5347\u52300.79\uff0c\u4eba\u5458\u68c0\u6d4b\u7387\u4ece75%\u63d0\u5347\u523091%\uff0c\u884c\u52a8\u5efa\u8bae\u7387\u4ece4.5%\u5927\u5e45\u63d0\u5347\u523092%", "conclusion": "\u9002\u5ea6\u7684\u8ba1\u7b97\u5f00\u9500\u80fd\u591f\u5b9e\u73b0\u8d28\u7684\u81ea\u4e3b\u6027\u63d0\u5347\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2509.13656", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13656", "abs": "https://arxiv.org/abs/2509.13656", "authors": ["Yingao Elaine Yao", "Vedant Nimje", "Varun Viswanath", "Saikat Dutta"], "title": "A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks", "comment": "22 pages, 2 figures, 6 tables", "summary": "Notebooks have become the de-facto choice for data scientists and machine\nlearning engineers for prototyping and experimenting with machine learning (ML)\npipelines. Notebooks provide an interactive interface for code, data, and\nvisualization. However, notebooks provide very limited support for testing.\nThus, during continuous development, many subtle bugs that do not lead to\ncrashes often go unnoticed and cause silent errors that manifest as performance\nregressions.\n  To address this, we introduce NBTest - the first regression testing framework\nthat allows developers to write cell-level assertions in notebooks and run such\nnotebooks in pytest or in continuous integration (CI) pipelines. NBTest offers\na library of assertion APIs, and a JupyterLab plugin that enables executing\nassertions. We also develop the first automated approach for generating\ncell-level assertions for key components in ML notebooks, such as data\nprocessing, model building, and model evaluation. NBTest aims to improve the\nreliability and maintainability of ML notebooks without adding developer\nburden.\n  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163\nassertions (35.75 on average per notebook). The generated assertions obtain a\nmutation score of 0.57 in killing ML-specific mutations. NBTest can catch\nregression bugs in previous versions of the Kaggle notebooks using assertions\ngenerated for the latest versions. Because ML pipelines involve non\ndeterministic computations, the assertions can be flaky. Hence, we also show\nhow NBTest leverages statistical techniques to minimize flakiness while\nretaining high fault-detection effectiveness. NBTest has been adopted in the CI\nof a popular ML library. Further, we perform a user study with 17 participants\nthat shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful\nin writing assertions and testing notebooks (Rating 4.24/5).", "AI": {"tldr": "NBTest\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u7684\u56de\u5f52\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u5355\u5143\u7ea7\u65ad\u8a00\u548c\u81ea\u52a8\u5316\u65ad\u8a00\u751f\u6210\uff0c\u63d0\u9ad8ML\u7b14\u8bb0\u672c\u7684\u53ef\u9760\u6027", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u5728\u6301\u7eed\u5f00\u53d1\u4e2d\u7f3a\u4e4f\u6d4b\u8bd5\u652f\u6301\u7684\u95ee\u9898\uff0c\u9632\u6b62\u56e0\u7ec6\u5faebug\u5bfc\u81f4\u7684\u6027\u80fd\u56de\u5f52\u548c\u9759\u9ed8\u9519\u8bef", "method": "\u5f00\u53d1\u4e86NBTest\u6846\u67b6\uff0c\u5305\u62ec\u65ad\u8a00API\u5e93\u3001JupyterLab\u63d2\u4ef6\u548c\u81ea\u52a8\u5316\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u5728pytest\u548cCI\u4e2d\u8fd0\u884c\u7b14\u8bb0\u672c\u6d4b\u8bd5", "result": "\u5728592\u4e2aKaggle\u7b14\u8bb0\u672c\u4e2d\u751f\u621021163\u4e2a\u65ad\u8a00\uff0c\u53d8\u5f02\u5f97\u52060.57\uff0c\u80fd\u591f\u6355\u83b7\u56de\u5f52\u9519\u8bef\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u6613\u7528\u6027\u8bc4\u52064.3/5\uff0c\u5b9e\u7528\u6027\u8bc4\u52064.24/5", "conclusion": "NBTest\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u53ef\u9760\u6027\uff0c\u5df2\u88ab\u4e3b\u6d41ML\u5e93\u91c7\u7528\uff0c\u901a\u8fc7\u7edf\u8ba1\u6280\u672f\u6700\u5c0f\u5316\u975e\u786e\u5b9a\u6027\u8ba1\u7b97\u5e26\u6765\u7684\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027", "topic": "swe application"}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u5185\u90e8\u8868\u5f81\u4e2d\u7ebf\u6027\u7f16\u7801\u95ee\u9898\u6b67\u4e49\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u64cd\u63a7\u6b67\u4e49\u7f16\u7801\u795e\u7ecf\u5143(AENs)\uff0c\u53ef\u4ee5\u68c0\u6d4b\u548c\u63a7\u5236\u6a21\u578b\u4ece\u76f4\u63a5\u56de\u7b54\u5230\u5f03\u6743\u7684\u884c\u4e3a", "motivation": "\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u666e\u904d\u5b58\u5728\u6b67\u4e49\u6027\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f80\u5f80\u81ea\u4fe1\u56de\u7b54\u800c\u975e\u5bfb\u6c42\u6f84\u6e05\uff0c\u9700\u8981\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5185\u90e8\u5904\u7406\u6b67\u4e49\u4fe1\u606f", "method": "\u5728\u6a21\u578b\u9884\u586b\u5145\u9636\u6bb5\u8bc6\u522b\u6b67\u4e49\u7f16\u7801\u795e\u7ecf\u5143\uff0c\u8bad\u7ec3\u63a2\u9488\u8fdb\u884c\u6b67\u4e49\u68c0\u6d4b\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u64cd\u63a7\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "\u53d1\u73b0\u5c11\u91cf\u795e\u7ecf\u5143(\u5c11\u81f31\u4e2a)\u7f16\u7801\u6b67\u4e49\u4fe1\u606f\uff0cAENs\u63a2\u9488\u5728\u6b67\u4e49\u68c0\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u5177\u6709\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u6d45\u5c42\u51fa\u73b0AENs\u8868\u660e\u65e9\u671f\u7f16\u7801\u6b67\u4e49\u4fe1\u53f7", "conclusion": "LLMs\u5f62\u6210\u7d27\u51d1\u7684\u5185\u90e8\u6b67\u4e49\u8868\u5f81\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u884c\u4e3a\uff0c\u4e3a\u6a21\u578b\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3", "topic": "agent analysis"}}
{"id": "2509.13680", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13680", "abs": "https://arxiv.org/abs/2509.13680", "authors": ["Wei Ma", "Yixiao Yang", "Jingquan Ge", "Xiaofei Xie", "Lingxiao Jiang"], "title": "Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations", "comment": null, "summary": "Code generation models are widely used in software development, yet their\nsensitivity to prompt phrasing remains under-examined. Identical requirements\nexpressed with different emotions or communication styles can yield divergent\noutputs, while most benchmarks emphasize only peak performance. We present\nPromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically\nequivalent prompt variants with emotion and personality templates, and that\nevaluates stability using probability aware continuous scoring or using binary\npass rates when logits are unavailable. The results are aggregated into a\nproposed area under curve metric (AUC-E) for cross model comparison. Across 14\nmodels from three families (Llama, Qwen, and DeepSeek), our study shows that\nperformance and stability behave as largely decoupled optimization objectives,\nand it reveals architectural and scale related patterns that challenge common\nassumptions about model robustness. The framework supports rapid screening for\nclosed-source models as well as detailed stability analysis in research\nsettings. PromptSE enables practitioners to quantify performance stability\ntrade offs for deployment and model selection, positioning prompt stability as\na complementary evaluation dimension alongside performance and fairness, and\ncontributing to more trustworthy AI-assisted software development tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86PromptSE\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u8bed\u4e49\u76f8\u540c\u4f46\u60c5\u611f\u548c\u98ce\u683c\u4e0d\u540c\u7684\u63d0\u793a\u53d8\u4f53\u6765\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\u662f\u89e3\u8026\u7684\u4f18\u5316\u76ee\u6807\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd phrasing \u7684\u654f\u611f\u6027\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u76f8\u540c\u9700\u6c42\u7528\u4e0d\u540c\u60c5\u611f\u6216\u6c9f\u901a\u98ce\u683c\u8868\u8fbe\u4f1a\u4ea7\u751f\u4e0d\u540c\u8f93\u51fa\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u53ea\u5173\u6ce8\u5cf0\u503c\u6027\u80fd\u3002", "method": "\u521b\u5efa\u8bed\u4e49\u7b49\u4ef7\u7684\u63d0\u793a\u53d8\u4f53\uff08\u542b\u60c5\u611f\u548c\u4e2a\u6027\u6a21\u677f\uff09\uff0c\u4f7f\u7528\u6982\u7387\u611f\u77e5\u8fde\u7eed\u8bc4\u5206\u6216\u4e8c\u5143\u901a\u8fc7\u7387\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u63d0\u51faAUC-E\u6307\u6807\u8fdb\u884c\u8de8\u6a21\u578b\u6bd4\u8f83\u3002", "result": "\u572814\u4e2a\u6a21\u578b\uff08Llama\u3001Qwen\u3001DeepSeek\uff09\u4e0a\u7684\u7814\u7a76\u8868\u660e\uff0c\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u662f\u89e3\u8026\u7684\u4f18\u5316\u76ee\u6807\uff0c\u63ed\u793a\u4e86\u67b6\u6784\u548c\u89c4\u6a21\u76f8\u5173\u7684\u6a21\u5f0f\uff0c\u6311\u6218\u4e86\u5173\u4e8e\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5e38\u89c1\u5047\u8bbe\u3002", "conclusion": "PromptSE\u6846\u67b6\u652f\u6301\u5feb\u901f\u7b5b\u9009\u95ed\u6e90\u6a21\u578b\u548c\u8be6\u7ec6\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u91cf\u5316\u6027\u80fd\u7a33\u5b9a\u6027\u6743\u8861\uff0c\u5c06\u63d0\u793a\u7a33\u5b9a\u6027\u5b9a\u4f4d\u4e3a\u4e0e\u6027\u80fd\u548c\u516c\u5e73\u6027\u4e92\u8865\u7684\u8bc4\u4f30\u7ef4\u5ea6\u3002", "topic": "code agent"}}
{"id": "2509.13755", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13755", "abs": "https://arxiv.org/abs/2509.13755", "authors": ["Zhaoyang Chu", "Yao Wan", "Zhikun Zhang", "Di Wang", "Zhou Yang", "Hongyu Zhang", "Pan Zhou", "Xuanhua Shi", "Hai Jin", "David Lo"], "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "While Code Language Models (CLMs) have demonstrated superior performance in\nsoftware engineering tasks such as code generation and summarization, recent\nempirical studies reveal a critical privacy vulnerability: these models exhibit\nunintended memorization of sensitive training data, enabling verbatim\nreproduction of confidential information when specifically prompted. To address\nthis issue, several approaches, including training data de-duplication and\ndifferential privacy augmentation, have been proposed. However, these methods\nrequire full-model retraining for deployed CLMs, which incurs substantial\ncomputational costs. In this paper, we aim to answer the following research\nquestion: Can sensitive information memorized by CLMs be erased effectively and\nefficiently?\n  We conduct a pioneering investigation into erasing sensitive memorization in\nCLMs through machine unlearning - a post-hoc modification method that removes\nspecific information from trained models without requiring full retraining.\nSpecifically, we first quantify the memorization risks of sensitive data within\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\nmemorized samples as unlearning targets. We study two widely used gradient\nascent-based unlearning approaches: the vanilla and constraint-based methods,\nand introduce CodeEraser, an advanced variant that selectively unlearns\nsensitive memorized segments in code while preserving the structural integrity\nand functional correctness of the surrounding code. Extensive experiments on\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\nsensitive memorization while maintaining model utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodeEraser\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u9057\u5fd8\u6280\u672f\u6709\u6548\u64e6\u9664\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u4e2d\u654f\u611f\u4fe1\u606f\u7684\u8bb0\u5fc6\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u529f\u80fd\u7684\u540c\u65f6\u89e3\u51b3\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u654f\u611f\u4fe1\u606f\u8bb0\u5fc6\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u4e0a\u5347\u4e3a\u57fa\u7840\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u5305\u62ecvanilla\u3001constraint-based\u65b9\u6cd5\u548c\u63d0\u51fa\u7684CodeEraser\u53d8\u4f53\uff0c\u9009\u62e9\u6027\u64e6\u9664\u4ee3\u7801\u4e2d\u7684\u654f\u611f\u8bb0\u5fc6\u7247\u6bb5\u3002", "result": "\u5728CodeParrot\u3001CodeGen-Mono\u548cQwen2.5-Coder\u4e09\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CodeEraser\u80fd\u6709\u6548\u64e6\u9664\u76ee\u6807\u654f\u611f\u8bb0\u5fc6\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "conclusion": "\u673a\u5668\u9057\u5fd8\u662f\u89e3\u51b3\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u9690\u79c1\u6f0f\u6d1e\u7684\u6709\u6548\u65b9\u6cd5\uff0cCodeEraser\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "Agent^2\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u667a\u80fd\u751f\u6210\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u548c\u73af\u5883\u4ee3\u7801\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fdRL\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u4f20\u7edfRL\u4ee3\u7406\u5f00\u53d1\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u957f\u65f6\u95f4\u8fed\u4ee3\uff0c\u5931\u8d25\u7387\u9ad8\u4e14\u53ef\u8bbf\u95ee\u6027\u6709\u9650\u3002\u9700\u8981\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684RL\u4ee3\u7406\u8bbe\u8ba1\u6765\u964d\u4f4e\u95e8\u69db\u548c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff1a\u751f\u6210\u5668\u4ee3\u7406\u4f5c\u4e3a\u81ea\u4e3bAI\u8bbe\u8ba1\u5668\u5206\u6790\u4efb\u52a1\u5e76\u751f\u6210\u53ef\u6267\u884cRL\u4ee3\u7406\uff0c\u76ee\u6807\u4ee3\u7406\u662f\u81ea\u52a8\u751f\u6210\u7684RL\u4ee3\u7406\u3002\u6846\u67b6\u5c06RL\u5f00\u53d1\u5206\u89e3\u4e3aMDP\u5efa\u6a21\u548c\u7b97\u6cd5\u4f18\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5728MuJoCo\u3001MetaDrive\u3001MPE\u548cSMAC\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgent^2\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe55%\uff0c\u5e73\u5747\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u667a\u80fd\u4ee3\u7406\u8bbe\u8ba1\u548c\u4f18\u5316\u5176\u4ed6\u4ee3\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7aef\u5230\u7aef\u7684\u95ed\u73af\u81ea\u52a8\u5316\uff0c\u662f\u81ea\u52a8\u5316AI\u7cfb\u7edf\u7684\u6839\u672c\u6027\u7a81\u7834\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.13758", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13758", "abs": "https://arxiv.org/abs/2509.13758", "authors": ["Kevin Halim", "Sin G. Teo", "Ruitao Feng", "Zhenpeng Chen", "Yang Gu", "Chong Wang", "Yang Liu"], "title": "A Study on Thinking Patterns of Large Reasoning Models in Code Generation", "comment": null, "summary": "Currently, many large language models (LLMs) are utilized for software\nengineering tasks such as code generation. The emergence of more advanced\nmodels known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek\nR1, and Qwen3. They have demonstrated the capability of performing multi-step\nreasoning. Despite the advancement in LRMs, little attention has been paid to\nsystematically analyzing the reasoning patterns these models exhibit and how\nsuch patterns influence the generated code. This paper presents a comprehensive\nstudy aimed at investigating and uncovering the reasoning behavior of LRMs\nduring code generation. We prompted several state-of-the-art LRMs of varying\nsizes with code generation tasks and applied open coding to manually annotate\nthe reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning\nbehaviors, encompassing 15 reasoning actions across four phases.\n  Our empirical study based on the taxonomy reveals a series of findings.\nFirst, we identify common reasoning patterns, showing that LRMs generally\nfollow a human-like coding workflow, with more complex tasks eliciting\nadditional actions such as scaffolding, flaw detection, and style checks.\nSecond, we compare reasoning across models, finding that Qwen3 exhibits\niterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like\napproach. Third, we analyze the relationship between reasoning and code\ncorrectness, showing that actions such as unit test creation and scaffold\ngeneration strongly support functional outcomes, with LRMs adapting strategies\nbased on task context. Finally, we evaluate lightweight prompting strategies\ninformed by these findings, demonstrating the potential of context- and\nreasoning-oriented prompts to improve LRM-generated code. Our results offer\ninsights and practical implications for advancing automatic code generation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u63a8\u7406\u8f68\u8ff9\u5efa\u7acb\u4e86\u5305\u542b15\u79cd\u63a8\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u5dee\u5f02\u53ca\u5176\u4e0e\u4ee3\u7801\u6b63\u786e\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u63a8\u7406\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u7684\u7cfb\u7edf\u6027\u5206\u6790\u8fd8\u5f88\u7f3a\u4e4f\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u793a\u591a\u4e2a\u6700\u5148\u8fdb\u7684LRMs\uff0c\u5e94\u7528\u5f00\u653e\u5f0f\u7f16\u7801\u624b\u52a8\u6807\u6ce8\u63a8\u7406\u8f68\u8ff9\uff0c\u4ece\u4e2d\u63a8\u5bfc\u51faLRM\u63a8\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u542b15\u79cd\u63a8\u7406\u52a8\u4f5c\u548c\u56db\u4e2a\u9636\u6bb5\u3002", "result": "\u53d1\u73b0LRMs\u9075\u5faa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\uff0c\u590d\u6742\u4efb\u52a1\u4f1a\u5f15\u53d1\u989d\u5916\u52a8\u4f5c\uff1b\u4e0d\u540c\u6a21\u578b\u63a8\u7406\u65b9\u5f0f\u5dee\u5f02\u660e\u663e(Qwen3\u8fed\u4ee3\u5f0f\uff0cDeepSeek-R1-7B\u7ebf\u6027)\uff1b\u5355\u5143\u6d4b\u8bd5\u521b\u5efa\u548c\u811a\u624b\u67b6\u751f\u6210\u7b49\u52a8\u4f5c\u4e0e\u4ee3\u7801\u6b63\u786e\u6027\u5f3a\u76f8\u5173\uff1b\u57fa\u4e8e\u53d1\u73b0\u7684\u8f7b\u91cf\u7ea7\u63d0\u793a\u7b56\u7565\u80fd\u6539\u5584\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63a8\u8fdb\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u5b9e\u8df5\u610f\u4e49\uff0c\u63ed\u793a\u4e86LRMs\u7684\u63a8\u7406\u884c\u4e3a\u6a21\u5f0f\u53ca\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2509.13782", "categories": ["cs.SE", "cs.AI", "cs.MA", "D.2.2; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.13782", "abs": "https://arxiv.org/abs/2509.13782", "authors": ["Yu Ge", "Linna Xie", "Zhong Li", "Yu Pei", "Tian Zhang"], "title": "Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis", "comment": "20 pages, 6 figures", "summary": "Large Language Model Powered Multi-Agent Systems (MASs) are increasingly\nemployed to automate complex real-world problems, such as programming and\nscientific discovery. Despite their promising, MASs are not without their\nflaws. However, failure attribution in MASs - pinpointing the specific agent\nactions responsible for failures - remains underexplored and labor-intensive,\nposing significant challenges for debugging and system improvement. To bridge\nthis gap, we propose FAMAS, the first spectrum-based failure attribution\napproach for MASs, which operates through systematic trajectory replay and\nabstraction, followed by spectrum analysis.The core idea of FAMAS is to\nestimate, from variations across repeated MAS executions, the likelihood that\neach agent action is responsible for the failure. In particular, we propose a\nnovel suspiciousness formula tailored to MASs, which integrates two key factor\ngroups, namely the agent behavior group and the action behavior group, to\naccount for the agent activation patterns and the action activation patterns\nwithin the execution trajectories of MASs. Through expensive evaluations\nagainst 12 baselines on the Who and When benchmark, FAMAS demonstrates superior\nperformance by outperforming all the methods in comparison.", "AI": {"tldr": "FAMAS\u662f\u9996\u4e2a\u57fa\u4e8e\u9891\u8c31\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u91cd\u653e\u548c\u9891\u8c31\u5206\u6790\u6765\u8bc6\u522b\u5bfc\u81f4\u5931\u8d25\u7684\u667a\u80fd\u4f53\u884c\u4e3a", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u65f6\u5b58\u5728\u6545\u969c\uff0c\u4f46\u6545\u969c\u5f52\u56e0\u7814\u7a76\u4e0d\u8db3\u4e14\u4eba\u5de5\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u8c03\u8bd5\u548c\u6539\u8fdb", "method": "\u63d0\u51faFAMAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8f68\u8ff9\u91cd\u653e\u548c\u62bd\u8c61\uff0c\u7136\u540e\u8fdb\u884c\u9891\u8c31\u5206\u6790\uff0c\u4f7f\u7528\u4e13\u95e8\u4e3aMAS\u8bbe\u8ba1\u7684\u53ef\u7591\u5ea6\u516c\u5f0f\uff0c\u6574\u5408\u667a\u80fd\u4f53\u884c\u4e3a\u7ec4\u548c\u52a8\u4f5c\u884c\u4e3a\u7ec4\u4e24\u4e2a\u5173\u952e\u56e0\u7d20", "result": "\u5728Who and When\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf912\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0cFAMAS\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4f18\u4e8e\u6240\u6709\u5bf9\u6bd4\u65b9\u6cd5", "conclusion": "FAMAS\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u8c03\u8bd5\u548c\u6539\u8fdb", "topic": "agent analysis"}}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "SteeringControl\u662f\u4e00\u4e2a\u8bc4\u4f30\u8868\u793a\u5f15\u5bfc\u65b9\u6cd5\u7684\u57fa\u51c6\uff0c\u91cd\u70b9\u5173\u6ce8\u504f\u89c1\u3001\u6709\u5bb3\u751f\u6210\u548c\u5e7b\u89c9\u7b49\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6b21\u8981\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u5de5\u4f5c\u901a\u5e38\u53ea\u5173\u6ce8\u771f\u5b9e\u6027\u6216\u63a8\u7406\u80fd\u529b\u6765\u5c55\u793a\u8868\u793a\u5f15\u5bfc\u7684\u526f\u4f5c\u7528\uff0c\u4f46\u8bb8\u591a\u6743\u8861\u5173\u7cfb\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u6536\u96c6\u4e86\u5b89\u5168\u76f8\u5173\u7684\u4e3b\u8981\u548c\u6b21\u8981\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6d41\u884c\u5f15\u5bfc\u65b9\u6cd5\u5728Qwen-2.5-7B\u548cLlama-3.1-8B\u6a21\u578b\u4e0a\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u5f3a\u5f15\u5bfc\u6027\u80fd\u53d6\u51b3\u4e8e\u5f15\u5bfc\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7279\u5b9a\u7ec4\u5408\uff0c\u4e0d\u826f\u7ec4\u5408\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6982\u5ff5\u7ea0\u7f20\u3002", "conclusion": "\u8868\u793a\u5f15\u5bfc\u7684\u6548\u679c\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u7684\u7ec4\u5408\u9009\u62e9\u3002", "topic": "agent analysis"}}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "\u4e3aClaude Code\u4ee3\u7406\u914d\u5907\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u5fd7\u5de5\u5177\u540e\uff0c\u572834\u4e2aAider\u591a\u8bed\u8a00Python\u7f16\u7a0b\u6311\u6218\u4e2d\uff0c\u534f\u4f5c\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86\u6700\u96be\u95ee\u9898\u7684\u89e3\u51b3\u6027\u80fd\uff0c\u6210\u672c\u964d\u4f4e15-40%\uff0c\u8f6e\u6b21\u51cf\u5c1112-27%\uff0c\u5b8c\u6210\u65f6\u95f4\u52a0\u5feb12-38%\u3002", "motivation": "\u7814\u7a76\u662f\u5426\u901a\u8fc7\u8d4b\u4e88LLM\u4ee3\u7406\u4eba\u7c7b\u81ea\u7136\u4f7f\u7528\u7684\u534f\u4f5c\u5de5\u5177\u548c\u81ea\u4e3b\u6743\uff0c\u80fd\u591f\u63d0\u5347\u5176\u95ee\u9898\u89e3\u51b3\u6027\u80fd\u3002", "method": "\u4e3aClaude Code\u4ee3\u7406\u914d\u5907\u57fa\u4e8eMCP\u7684\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u5fd7\u5de5\u5177\uff0c\u5141\u8bb8\u5b83\u4eec\u81ea\u4e3b\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u6765\u89e3\u51b3\u7f16\u7a0b\u6311\u6218\u3002", "result": "\u534f\u4f5c\u5de5\u5177\u5bf9\u6700\u96be\u95ee\u9898\u8868\u73b0\u63d0\u5347\u663e\u8457\uff1a\u6210\u672c\u964d\u4f4e15-40%\uff0c\u8f6e\u6b21\u51cf\u5c1112-27%\uff0c\u5b8c\u6210\u65f6\u95f4\u52a0\u5feb12-38%\u3002\u4e0d\u540c\u6a21\u578b\u81ea\u7136\u91c7\u7528\u4e0d\u540c\u534f\u4f5c\u7b56\u7565\uff0cSonnet 3.7\u5e7f\u6cdb\u4f7f\u7528\u5de5\u5177\uff0cSonnet 4\u9009\u62e9\u6027\u5730\u4f9d\u8d56\u57fa\u4e8e\u65e5\u5fd7\u7684\u8bed\u4e49\u641c\u7d22\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u5176\u80fd\u529b\u8fb9\u754c\u5904\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u4ece\u4eba\u7c7b\u542f\u53d1\u7684\u534f\u4f5c\u5de5\u5177\u4e2d\u53d7\u76ca\uff0c\u8868\u660e\u81ea\u9002\u5e94\u534f\u4f5c\u754c\u9762\u53ef\u4f5c\u4e3a\u63a8\u7406\u589e\u5f3a\u5668\u800c\u975e\u901a\u7528\u6548\u7387\u63d0\u5347\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2509.13642", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13642", "abs": "https://arxiv.org/abs/2509.13642", "authors": ["Zirun Guo", "Feng Zhang", "Kai Jia", "Tao Jin"], "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators", "comment": null, "summary": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\nare limited to synthetic imagery and struggle with tasks requiring factual\ngrounding or programmatic precision. Our framework empowers a central LLM or\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\ntools, including online image search, diffusion-based generation, code\nexecution, and image editing. The agent is trained to select and apply these\ntools proficiently via a Reinforcement Learning (RL) framework that features a\nhybrid reward system combining rule-based logic with judgments from LLM and\nMLLM evaluators. Trained on a diverse new dataset using four different model\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\nexisting methods by a large margin across four benchmarks. We also introduce a\nnovel test-time scaling strategy that provides further performance gains.\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.", "AI": {"tldr": "LLM-Interleaved\u662f\u4e00\u4e2a\u52a8\u6001\u6846\u67b6\uff0c\u5c06\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5de5\u5177\u4f7f\u7528\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u667a\u80fd\u534f\u8c03\u591a\u79cd\u89c6\u89c9\u5de5\u5177\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u6a21\u578b\u5b58\u5728\"\u5355\u4e00\u5de5\u5177\"\u74f6\u9888\uff0c\u4ec5\u9650\u4e8e\u5408\u6210\u56fe\u50cf\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u4e8b\u5b9e\u57fa\u7840\u6216\u7f16\u7a0b\u7cbe\u5ea6\u7684\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6846\u67b6\u6765\u534f\u8c03\u591a\u79cd\u4e13\u4e1a\u89c6\u89c9\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u4e2d\u5fc3LLM/MLLM\u4ee3\u7406\u667a\u80fd\u534f\u8c03\u5728\u7ebf\u56fe\u50cf\u641c\u7d22\u3001\u6269\u6563\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4e13\u4e1a\u5de5\u5177\uff0c\u91c7\u7528\u7ed3\u5408\u89c4\u5219\u903b\u8f91\u548cLLM\u8bc4\u4f30\u7684\u6df7\u5408\u5956\u52b1\u7cfb\u7edf\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u83b7\u5f97\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LLM-I\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5de5\u5177\u534f\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u51c6\u786e\u7684\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u751f\u6210\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.13941", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13941", "abs": "https://arxiv.org/abs/2509.13941", "authors": ["Simiao Liu", "Fang Liu", "Liehao Li", "Xin Tan", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "title": "An Empirical Study on Failures in Automated Issue Solving", "comment": null, "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86SWE-Bench\u4e2d\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u5305\u542b3\u4e2a\u9636\u6bb5\u30019\u4e2a\u7c7b\u522b\u548c25\u4e2a\u5b50\u7c7b\u522b\u7684\u5931\u8d25\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u5bb6-\u6267\u884c\u8005\u534f\u4f5c\u6846\u67b6\u6765\u89e3\u51b3\u8ba4\u77e5\u6b7b\u9501\u548c\u63a8\u7406\u9519\u8bef\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5de5\u5177\u5728\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u4e2d\u4ecd\u6709\u5927\u91cf\u5931\u8d25\u6848\u4f8b\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u6307\u6807\uff0c\u96be\u4ee5\u8bca\u65ad\u5177\u4f53\u5931\u8d25\u539f\u56e0\u548c\u6307\u5bfc\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "method": "\u9996\u5148\u5206\u6790\u4e09\u79cdSOTA\u5de5\u5177\u5728SWE-Bench-Verified\u4e2d\u7684\u8868\u73b0\uff0c\u7136\u540e\u5bf9150\u4e2a\u5931\u8d25\u6848\u4f8b\u8fdb\u884c\u7cfb\u7edf\u624b\u52a8\u5206\u6790\u5efa\u7acb\u5931\u8d25\u5206\u7c7b\u6cd5\uff0c\u6700\u540e\u63d0\u51fa\u4e13\u5bb6-\u6267\u884c\u8005\u534f\u4f5c\u6846\u67b6\u6765\u7ea0\u6b63\u63a8\u7406\u9519\u8bef\u548c\u6253\u7834\u8ba4\u77e5\u6b7b\u9501\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u5931\u8d25\u6a21\u5f0f\u5206\u7c7b\u6cd5\uff0c\u53d1\u73b0\u4ee3\u7406\u5f0f\u67b6\u6784\u4e3b\u8981\u5931\u8d25\u6e90\u4e8e\u63a8\u7406\u9519\u8bef\u548c\u8ba4\u77e5\u6b7b\u9501\uff0c\u63d0\u51fa\u7684\u534f\u4f5c\u6846\u67b6\u89e3\u51b3\u4e86\u9886\u5148\u5355\u4ee3\u740622.2%\u7684\u5148\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u8bca\u65ad\u6027\u8bc4\u4f30\u548c\u534f\u4f5c\u8bbe\u8ba1\u53ef\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\uff0c\u5931\u8d25\u6a21\u5f0f\u5206\u6790\u4e3a\u6539\u8fdb\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u57fa\u4e8eLLM\u7684\u793e\u4f1a\u6a21\u62df\u4e2d\u7cfb\u7edf\u5316\u6307\u5b9a\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7a0b\u8ba4\u77e5\u504f\u89c1\u6765\u89e3\u51b3\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u9690\u5f0f\u63cf\u8ff0\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u4ea7\u751f\u4e00\u81f4\u884c\u4e3a\uff0c\u4e14\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u63cf\u8ff0\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u884c\u4e3a\u89c4\u8303\u65b9\u6cd5", "method": "CoBRA\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u8ba4\u77e5\u504f\u89c1\u6307\u6570 - \u901a\u8fc7\u7ecf\u5178\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u91cf\u5316\u667a\u80fd\u4f53\u53cd\u5e94\u6765\u6d4b\u91cf\u8ba4\u77e5\u504f\u89c1\uff1b2\uff09\u884c\u4e3a\u8c03\u8282\u5f15\u64ce - \u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u53d7\u63a7\u8ba4\u77e5\u504f\u89c1\u5bf9\u9f50", "result": "\u8bc4\u4f30\u663e\u793aCoBRA\u80fd\u591f\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u7cbe\u786e\u7f16\u7a0b\u793e\u4ea4\u667a\u80fd\u4f53\u4e2d\u5c55\u793a\u7684\u8ba4\u77e5\u504f\u89c1", "conclusion": "CoBRA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\u5305\uff0c\u80fd\u591f\u7cfb\u7edf\u5316\u5730\u6307\u5b9a\u548c\u63a7\u5236\u57fa\u4e8eLLM\u7684\u793e\u4ea4\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u504f\u89c1\u884c\u4e3a", "topic": "agent analysis"}}
{"id": "2509.13942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13942", "abs": "https://arxiv.org/abs/2509.13942", "authors": ["Duc Minh Ha", "Phu Trac Kien", "Tho Quan", "Anh Nguyen-Duc"], "title": "Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation", "comment": null, "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u5c06\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff08\u7011\u5e03\u6a21\u578b\u3001V\u6a21\u578b\u3001\u654f\u6377\uff09\u4f5c\u4e3a\u534f\u8c03\u6846\u67b6\u5e94\u7528\u4e8e\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7132\u6b21\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u6d41\u7a0b\u5728\u4ee3\u7801\u8d28\u91cf\u3001\u6210\u672c\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u80fd\u5426\u4f5c\u4e3aLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6846\u67b6\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u3001\u6210\u672c\u548c\u751f\u4ea7\u7387\u7684\u5f71\u54cd\u3002", "method": "\u57283\u79cd\u6d41\u7a0b\u6a21\u578b\u548c4\u79cdGPT\u53d8\u4f53\u4e0b\u6267\u884c11\u4e2a\u8f6f\u4ef6\u9879\u76ee\uff0c\u5171132\u6b21\u8fd0\u884c\uff0c\u4f7f\u7528\u6807\u51c6\u5316\u6307\u6807\u8bc4\u4f30\u8f93\u51fa\u7ed3\u679c\uff08\u6587\u4ef6\u6570\u3001\u4ee3\u7801\u884c\u6570\u3001\u6267\u884c\u65f6\u95f4\u3001token\u4f7f\u7528\u91cf\u3001\u4ee3\u7801\u5f02\u5473\u548cbug\u68c0\u6d4b\uff09\u3002", "result": "\u6d41\u7a0b\u6a21\u578b\u548cLLM\u9009\u62e9\u90fd\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\uff1a\u7011\u5e03\u6a21\u578b\u6700\u6709\u6548\u7387\uff0cV\u6a21\u578b\u4ea7\u751f\u6700\u5197\u957f\u7684\u4ee3\u7801\uff0c\u654f\u6377\u5f00\u53d1\u83b7\u5f97\u6700\u9ad8\u4ee3\u7801\u8d28\u91cf\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "\u4f20\u7edf\u8f6f\u4ef6\u6d41\u7a0b\u53ef\u6709\u6548\u5e94\u7528\u4e8eLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u9700\u8981\u5728\u8d28\u91cf\u3001\u6210\u672c\u548c\u9002\u5e94\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u6d41\u7a0b\u9009\u62e9\u5e94\u6839\u636e\u9879\u76ee\u76ee\u6807\uff08\u6548\u7387\u3001\u5065\u58ee\u6027\u6216\u7ed3\u6784\u5316\u9a8c\u8bc1\uff09\u6765\u51b3\u5b9a\u3002", "topic": "agent analysis"}}
{"id": "2509.13615", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86State-aware Reasoning (StaR)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u4ee3\u7406\u5728GUI\u5207\u6362\u63a7\u5236\u4e2d\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f53\u524d\u72b6\u6001\u4e0e\u671f\u671b\u72b6\u6001\u5339\u914d\u65f6\u7684\u6267\u884c\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u4ee3\u7406\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u63a7\u5236\u4e2d\uff0c\u7279\u522b\u662f\u5728\u5207\u6362\u63a7\u5236\u6307\u4ee4\u6267\u884c\u65b9\u9762\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u8fd9\u6210\u4e3aGUI\u4ea4\u4e92\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u6784\u5efa\u72b6\u6001\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faStaR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6559\u5bfc\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u3001\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u671f\u671b\u72b6\u6001\uff0c\u5e76\u76f8\u5e94\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cStaR\u80fd\u5c06\u5207\u6362\u6307\u4ee4\u6267\u884c\u51c6\u786e\u7387\u63d0\u9ad830%\u4ee5\u4e0a\uff0c\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u63d0\u5347\u4e86\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "StaR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GUI\u5207\u6362\u63a7\u5236\u95ee\u9898\uff0c\u5728\u52a8\u6001\u73af\u5883\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.14093", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14093", "abs": "https://arxiv.org/abs/2509.14093", "authors": ["Kerui Huang", "Shuhan Liu", "Xing Hu", "Tongtong Xu", "Lingfeng Bao", "Xin Xia"], "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.", "AI": {"tldr": "Chain-of-Thought\u63a8\u7406\u867d\u7136\u63d0\u5347LLM\u6027\u80fd\u4f46\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u7814\u7a76\u53d1\u73b0\u8fc7\u957f\u63a8\u7406\u53cd\u800c\u6709\u5bb3\u3002\u63d0\u51faSEER\u81ea\u9002\u5e94\u6846\u67b6\u538b\u7f29\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c1142.1%\u63a8\u7406\u957f\u5ea6\u3002", "motivation": "Chain-of-Thought\u63a8\u7406\u867d\u7136\u80fd\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u672f\u3001\u903b\u8f91\u548c\u5e38\u8bc6\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5e26\u6765\u4e86\u9ad8\u8ba1\u7b97\u6210\u672c\uff08\u5ef6\u8fdf\u3001\u5185\u5b58\u4f7f\u7528\u548cKV\u7f13\u5b58\u9700\u6c42\uff09\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7b80\u6d01\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u8fc7\u957f\u63a8\u7406\u4f1a\u5bfc\u81f4\u622a\u65ad\u3001\u7cbe\u5ea6\u4e0b\u964d\u548c\u9ad8\u8fbe5\u500d\u7684\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faSEER\uff08Self-Enhancing Efficient Reasoning\uff09\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7ed3\u5408Best-of-N\u91c7\u6837\u548c\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u8fc7\u6ee4\uff0c\u901a\u8fc7\u9884\u63a8\u7406\u8f93\u51fa\u52a8\u6001\u8c03\u6574\u9608\u503c\u6765\u538b\u7f29Chain-of-Thought\u63a8\u7406\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e09\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u548c\u4e00\u4e2a\u6570\u5b66\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSEER\u5e73\u5747\u7f29\u77edChain-of-Thought\u63a8\u7406\u957f\u5ea642.1%\uff0c\u901a\u8fc7\u51cf\u5c11\u622a\u65ad\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u6d88\u9664\u4e86\u5927\u591a\u6570\u65e0\u9650\u5faa\u73af\u95ee\u9898\u3002", "conclusion": "SEER\u662f\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u80fd\u4f7fChain-of-Thought\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u66f4\u52a0\u9ad8\u6548\u548c\u9c81\u68d2\uff0c\u6311\u6218\u4e86\"\u63a8\u7406\u8d8a\u957f\u8d8a\u597d\"\u7684\u5047\u8bbe\u3002", "topic": "code agent"}}
{"id": "2509.13704", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.", "AI": {"tldr": "InfraMind\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5de5\u4e1a\u7ba1\u7406\u7cfb\u7edf\u8bbe\u8ba1\u7684\u57fa\u4e8e\u63a2\u7d22\u7684GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u5de5\u4e1a\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728DCIM\u5e73\u53f0\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "motivation": "\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u9762\u4e34\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u3001\u591a\u4f9b\u5e94\u5546\u96c6\u6210\u548c\u4e13\u5bb6\u64cd\u4f5c\u5458\u77ed\u7f3a\u7b49\u6311\u6218\uff0c\u73b0\u6709RPA\u548c\u901a\u7528LLM GUI\u4ee3\u7406\u5728\u5de5\u4e1a\u7ba1\u7406\u4e2d\u5b58\u5728\u4e94\u4e2a\u5173\u952e\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u63d0\u51faInfraMind\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6a21\u5757\uff1a\u7cfb\u7edf\u641c\u7d22\u63a2\u7d22\u3001\u5185\u5b58\u9a71\u52a8\u89c4\u5212\u3001\u9ad8\u7ea7\u72b6\u6001\u8bc6\u522b\u3001\u7ed3\u6784\u5316\u77e5\u8bc6\u84b8\u998f\u548c\u591a\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aDCIM\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "InfraMind\u4e3a\u5de5\u4e1a\u7ba1\u7406\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2509.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.", "AI": {"tldr": "THOR\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5de5\u5177\u96c6\u6210\u5c42\u6b21\u4f18\u5316\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u6784\u5efa\u3001\u7ec6\u7c92\u5ea6\u4f18\u5316\u548c\u63a8\u7406\u589e\u5f3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6570\u503c\u8ba1\u7b97\u548c\u5f62\u5f0f\u7b26\u53f7\u64cd\u4f5c\u7b49\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u96c6\u6210\u5916\u90e8\u5de5\u5177\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faTHOR\u6846\u67b6\uff1a1) TIRGen\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u6784\u5efa\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u7ea7\u95ee\u9898\u89e3\u51b3\u548c\u6b65\u9aa4\u7ea7\u4ee3\u7801\u751f\u6210\uff1b3) \u63a8\u7406\u65f6\u5229\u7528\u5de5\u5177\u53cd\u9988\u8fdb\u884c\u52a8\u6001\u9519\u8bef\u4fee\u6b63\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u540c\u7c7b\u89c4\u6a21\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e5f\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "THOR\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u9ad8\u7cbe\u5ea6\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.14030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.", "AI": {"tldr": "CrowdAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u548c\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\uff0c\u4e3aLLM\u3001SLM\u548c\u4eba\u7c7b\u4e13\u5bb6\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u534f\u540c\u6807\u6ce8\u6d41\u7a0b\u63a7\u5236\u3002", "motivation": "\u5f53\u524dNLP\u6807\u6ce8\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6807\u6ce8\u6b65\u9aa4\u672c\u8eab\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6e90\u6807\u6ce8\uff08LLM\u3001SLM\u3001\u4eba\u7c7b\u4e13\u5bb6\uff09\u7684\u52a8\u6001\u8c03\u5ea6\u548c\u8d28\u91cf-\u6210\u672c\u6743\u8861\u7684\u6574\u4f53\u6d41\u7a0b\u63a7\u5236\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5b9e\u73b0\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u548c\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\u7684\u7aef\u5230\u7aef\u96c6\u6210\uff0c\u901a\u8fc7\u5408\u7406\u4efb\u52a1\u5206\u914d\u4f7f\u4e0d\u540c\u6807\u6ce8\u6e90\u5728\u534f\u540c\u5de5\u4f5c\u6d41\u4e2d\u534f\u540c\u63a8\u8fdb\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86CrowdAgent\u7684\u6709\u6548\u6027\u3002", "conclusion": "CrowdAgent\u4e3a\u591a\u6e90\u6807\u6ce8\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6d41\u7a0b\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7ba1\u7406\u590d\u6742\u7684\u8c03\u5ea6\u548c\u8d28\u91cf-\u6210\u672c\u6743\u8861\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "Slim-SC\u662f\u4e00\u79cd\u901a\u8fc7\u9010\u6b65\u526a\u679d\u5197\u4f59\u63a8\u7406\u94fe\u6765\u52a0\u901fSelf-Consistency\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "Self-Consistency\u867d\u7136\u80fd\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u6570\u91cf\u7ea7\u7684\u8ba1\u7b97\u5f00\u9500\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u57fa\u4e8e\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u63d0\u51faSlim-SC\u65b9\u6cd5\uff0c\u5229\u7528\u601d\u7ef4\u5c42\u9762\u7684\u94fe\u95f4\u76f8\u4f3c\u6027\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u63a8\u7406\u94fe", "result": "\u5728\u4e09\u4e2aSTEM\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\uff0cSlim-SC\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e45%\uff0cKVC\u4f7f\u7528\u964d\u4f4e26%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u7cbe\u5ea6", "conclusion": "Slim-SC\u4e3aSelf-Consistency\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u66ff\u4ee3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "ES-CoT\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u6765\u63d0\u524d\u7ec8\u6b62\u601d\u7ef4\u94fe\u751f\u6210\uff0c\u5e73\u5747\u51cf\u5c1141%\u7684\u63a8\u7406token\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u9700\u8981\u751f\u6210\u957f\u601d\u7ef4\u94fe\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u4ea7\u751f\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u3002\u7814\u7a76\u65e8\u5728\u51cf\u5c11\u63a8\u7406token\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7ed3\u675f\u65f6\u63d0\u793aLLM\u8f93\u51fa\u5f53\u524d\u6700\u7ec8\u7b54\u6848\uff08\u6b65\u9aa4\u7b54\u6848\uff09\uff0c\u8ddf\u8e2a\u8fde\u7eed\u76f8\u540c\u6b65\u9aa4\u7b54\u6848\u7684\u8fd0\u884c\u957f\u5ea6\u4f5c\u4e3a\u6536\u655b\u5ea6\u91cf\u3002\u5f53\u8fd0\u884c\u957f\u5ea6\u51fa\u73b0\u6025\u5267\u589e\u52a0\u5e76\u8d85\u8fc7\u6700\u5c0f\u9608\u503c\u65f6\u7ec8\u6b62\u751f\u6210\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cES-CoT\u5e73\u5747\u51cf\u5c11\u7ea641%\u7684\u63a8\u7406token\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u4e0e\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u5728\u8d85\u53c2\u6570\u9009\u62e9\u4e0a\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "ES-CoT\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u5b9e\u73b0\u65e9\u671f\u505c\u6b62\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u91d1\u878d\u987e\u95ee\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\u6784\u5efa\u76d1\u7763\u6570\u636e\uff0c\u4f7f8B\u53c2\u6570\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e14-32B\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e80%", "motivation": "\u73b0\u6709\u91d1\u878d\u987e\u95ee\u7cfb\u7edf\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u6536\u76ca\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7efc\u5408\u8003\u8651\u7528\u6237\u76ee\u6807\u3001\u7ea6\u675f\u3001\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u4e2a\u6027\u5316\u91d1\u878d\u5efa\u8bae\u65b9\u6cd5", "method": "\u521b\u5efa\u5305\u542b19k\u6837\u672c\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6574\u5408\u76f8\u5173\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\uff0c\u5bf9Qwen-3-8B\u6a21\u578b\u8fdb\u884c\u5168\u9762\u5fae\u8c03", "result": "8B\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e2a\u6027\u5316\u6307\u6807\u4e0a\u8fbe\u5230\u4e0e14-32B\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e80%", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u6574\u5408\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u65b9\u6cd5\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c", "topic": "agent analysis"}}
{"id": "2509.14077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14077", "abs": "https://arxiv.org/abs/2509.14077", "authors": ["Yuhao Wang", "Enlu Zhou"], "title": "Online Bayesian Risk-Averse Reinforcement Learning", "comment": null, "summary": "In this paper, we study the Bayesian risk-averse formulation in reinforcement\nlearning (RL). To address the epistemic uncertainty due to a lack of data, we\nadopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the\nparameter uncertainty of the unknown underlying model. We derive the asymptotic\nnormality that characterizes the difference between the Bayesian risk value\nfunction and the original value function under the true unknown distribution.\nThe results indicate that the Bayesian risk-averse approach tends to\npessimistically underestimate the original value function. This discrepancy\nincreases with stronger risk aversion and decreases as more data become\navailable. We then utilize this adaptive property in the setting of online RL\nas well as online contextual multi-arm bandits (CMAB), a special case of online\nRL. We provide two procedures using posterior sampling for both the general RL\nproblem and the CMAB problem. We establish a sub-linear regret bound, with the\nregret defined as the conventional regret for both the RL and CMAB settings.\nAdditionally, we establish a sub-linear regret bound for the CMAB setting with\nthe regret defined as the Bayesian risk regret. Finally, we conduct numerical\nexperiments to demonstrate the effectiveness of the proposed algorithm in\naddressing epistemic uncertainty and verifying the theoretical properties.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\uff0c\u901a\u8fc7BRMDP\u5904\u7406\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\uff0c\u63a8\u5bfc\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u503c\u51fd\u6570\u4e0e\u771f\u5b9e\u503c\u51fd\u6570\u4e4b\u95f4\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u540e\u9a8c\u91c7\u6837\u7684\u5728\u7ebfRL\u548cCMAB\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7531\u4e8e\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u6765\u5904\u7406\u6a21\u578b\u53c2\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u98ce\u9669\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(BRMDP)\uff0c\u63a8\u5bfc\u6e10\u8fd1\u6b63\u6001\u6027\u7406\u8bba\uff0c\u63d0\u51fa\u57fa\u4e8e\u540e\u9a8c\u91c7\u6837\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u4f1a\u60b2\u89c2\u5730\u4f4e\u4f30\u539f\u59cb\u503c\u51fd\u6570\uff0c\u8fd9\u79cd\u5dee\u5f02\u968f\u98ce\u9669\u89c4\u907f\u5f3a\u5ea6\u589e\u52a0\u800c\u589e\u5927\uff0c\u968f\u6570\u636e\u91cf\u589e\u52a0\u800c\u51cf\u5c0f\u3002\u5efa\u7acb\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548cCMAB\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.14234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14234", "abs": "https://arxiv.org/abs/2509.14234", "authors": ["Dulhan Jayalath", "Shashwat Goel", "Thomas Foster", "Parag Jain", "Suchin Gururangan", "Cheng Zhang", "Anirudh Goyal", "Alan Schelten"], "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision", "comment": "22 pages, 8 figures, 2 tables", "summary": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.", "AI": {"tldr": "Compute as Teacher (CaT) \u662f\u4e00\u79cd\u5c06\u63a8\u7406\u65f6\u63a2\u7d22\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u591a\u4e2a\u5e76\u884crollout\u7684\u7ed3\u679c\u751f\u6210\u53c2\u8003\u7b54\u6848\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u4f18\u5316\u6a21\u578b\u6027\u80fd", "motivation": "\u89e3\u51b3\u5728\u65e0\u76d1\u7763\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\u5982\u4f55\u83b7\u5f97\u5b66\u4e60\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5c06\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u8d44\u6e90\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7", "method": "\u4f7f\u7528\u5f53\u524d\u7b56\u7565\u751f\u6210\u4e00\u7ec4\u5e76\u884crollout\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u521d\u59cb\u7b56\u7565\uff08anchor\uff09\u534f\u8c03\u51b2\u7a81\u548c\u9057\u6f0f\u6765\u5408\u6210\u5355\u4e00\u53c2\u8003\u7b54\u6848\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u5bf9\u4e8e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4f7f\u7528\u7a0b\u5e8f\u7b49\u4ef7\u6027\uff0c\u5bf9\u4e8e\u4e0d\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4f7f\u7528\u81ea\u63d0\u51fa\u7684\u53ef\u5ba1\u8ba1\u6807\u51c6\u5e76\u7531\u72ec\u7acbLLM\u8bc4\u5206", "result": "\u5728Gemma 3 4B\u3001Qwen 3 4B\u548cLlama 3.1 8B\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08MATH-500\u4e0a\u6700\u9ad8+27%\uff0cHealthBench\u4e0a+12%\uff09\u3002\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08CaT-RL\uff09\u540e\u83b7\u5f97\u8fdb\u4e00\u6b65\u589e\u76ca\uff08\u6700\u9ad8+33%\u548c+30%\uff09", "conclusion": "CaT\u80fd\u591f\u6709\u6548\u5c06\u63a8\u7406\u65f6\u8ba1\u7b97\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u6027\u80fd\u968frollout\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\uff0c\u5408\u6210\u65b9\u6cd5\u53ef\u80fd\u4f18\u4e8e\u591a\u6570\u6295\u7968\uff0c\u8bad\u7ec3\u540e\u7684\u7b56\u7565\u53ef\u4ee5\u8d85\u8d8a\u521d\u59cb\u6559\u5e08\u4fe1\u53f7", "topic": "agentic reinforcement learning"}}
