<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [tldr.article](#tldr.article) [Total: 1]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 12]
- [wechat.article](#wechat.article) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
*Gyubok Lee,Elea Bach,Eric Yang,Tom Pollard,Alistair Johnson,Edward Choi,Yugang jia,Jong Ha Lee*

Main category: cs.CL

TL;DR: FHIR-AgentBench是一个新的临床AI基准测试，基于HL7 FHIR标准构建了2,931个真实临床问题，用于评估LLM代理在处理互操作性临床数据方面的能力。


<details>
  <summary>Details</summary>
Motivation: 随着医疗行业向HL7 FHIR标准转变，现有的基准测试缺乏对互操作性临床数据的真实评估能力，需要新的基准来评估LLM代理在复杂资源数据模型上的表现。

Method: 系统评估了代理框架，比较了不同的数据检索策略（直接FHIR API调用vs专用工具）、交互模式（单轮vs多轮）和推理策略（自然语言vs代码生成）。

Result: 实验揭示了从复杂FHIR资源中检索数据的实际挑战以及对其进行推理的困难，这些因素严重影响问答性能。

Conclusion: 发布FHIR-AgentBench数据集和评估套件，以促进可重复研究和开发稳健可靠的临床应用LLM代理。

Abstract: The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

</details>


### [2] [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
*Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的双视角方法，结合基于词典的情感分析和深度学习分类模型，分析Google Play商店中ChatGPT和DeepSeek的用户评论。研究发现ChatGPT获得更积极的情感反馈，且CNN模型在情感分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单独使用基于词典的策略或预测性深度学习模型，缺乏对基于大语言模型应用用户满意度的全面调查。本研究旨在填补这一空白。

Method: 采用双视角方法：结合TextBlob词典情感分析和CNN、Bi-LSTM深度学习模型。收集4000条真实用户评论，经过预处理和过采样平衡后，使用1700条评论进行模型测试。

Result: ChatGPT获得比DeepSeek更积极的情感反馈。深度学习分类优于词典分析，CNN模型表现最佳（96.41%准确率），在负面评论分类上接近完美，中性和正面情感也获得高F1分数。

Conclusion: 本研究为衡量基于大语言模型应用的情感设定了新的方法论标准，为开发者和研究人员改进以用户为中心的AI系统设计提供了实用见解。

Abstract: This study presents a novel dual-perspective approach to analyzing user
reviews for ChatGPT and DeepSeek on the Google Play Store, integrating
lexicon-based sentiment analysis (TextBlob) with deep learning classification
models, including Convolutional Neural Networks (CNN) and Bidirectional Long
Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on
either lexicon-based strategies or predictive deep learning models in
isolation, this study conducts an extensive investigation into user
satisfaction with Large Language Model (LLM) based applications. A Dataset of
4,000 authentic user reviews was collected, which were carefully preprocessed
and subjected to oversampling to achieve balanced classes. The balanced test
set of 1,700 Reviews were used for model testing. Results from the experiments
reveal that ChatGPT received significantly more positive sentiment than
DeepSeek. Furthermore, deep learning based classification demonstrated superior
performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving
96.41 percent accuracy and near perfect classification of negative reviews,
alongside high F1-scores for neutral and positive sentiments. This research
sets a new methodological standard for measuring sentiment in LLM-based
applications and provides practical insights for developers and researchers
seeking to improve user-centric AI system design.

</details>


### [3] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: ShinkaEvolve是一个开源框架，利用LLM作为变异算子进行代码进化，通过三项创新技术显著提升样本效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 当前代码进化方法存在样本效率低（需要数千样本）和闭源的局限性，阻碍了广泛采用和扩展。

Method: 引入三项关键技术：平衡探索与利用的父代采样、代码新颖性拒绝采样、基于多臂老虎机的LLM集成选择策略。

Result: 在多个任务中表现优异：仅用150样本发现新的最优圆包装解、设计高性能AIME数学推理代理、改进ALE-Bench编程解、发现新颖的专家混合负载均衡损失函数。

Conclusion: ShinkaEvolve通过开源和成本效益，实现了跨领域计算问题的民主化开放式发现。

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [4] [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
*Chimaobi Okite,Naihao Deng,Kiran Bodipati,Huaidian Hou,Joyce Chai,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文提出了PERG框架来评估LLM在个性化响应中的稳健性，强调事实准确性是重要但常被忽视的维度，并开发了Pref-Aligner方法提升稳健性25%。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注LLM响应是否与用户偏好一致，但忽视了事实准确性这一重要维度。作者认为在个性化场景中，模型应同时保证响应的事实准确性和用户偏好一致性。

Method: 引入PERG可扩展评估框架和PERGData数据集，评估14个模型；提出Pref-Aligner两阶段方法，通过偏好对齐提升模型稳健性。

Result: 当前LLM在稳健个性化方面表现不佳：最强模型(GPT-4.1, LLaMA3-70B)在5%的无个性化成功案例中无法保持正确性，小模型(7B规模)失败率超过20%。

Conclusion: 研究揭示了当前评估实践的关键缺陷，提供了支持更可靠、用户对齐LLM部署的工具和指标。

Abstract: Recent years have witnessed a growing interest in personalizing the responses
of large language models (LLMs). While existing evaluations primarily focus on
whether a response aligns with a user's preferences, we argue that factuality
is an equally important yet often overlooked dimension. In the context of
personalization, we define a model as robust if its responses are both
factually accurate and align with the user preferences. To assess this, we
introduce PERG, a scalable framework for evaluating robustness in LLMs, along
with a new dataset, PERGData. We evaluate fourteen models from five different
model families using different prompting methods. Our findings show that
current LLMs struggle with robust personalization: even the strongest models
(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously
successful cases without personalization, while smaller models (e.g., 7B-scale)
can fail more than 20% of the time. Further analysis reveals that robustness is
significantly affected by the nature of the query and the type of user
preference. To mitigate these failures, we propose Pref-Aligner, a two-stage
approach that improves robustness by an average of 25% across models. Our work
highlights critical gaps in current evaluation practices and introduces tools
and metrics to support more reliable, user-aligned LLM deployments.

</details>


### [5] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: 本文提出了一种新的语义表示攻击方法，通过利用语义表示空间而非精确文本模式来绕过LLM的对齐防护，解决了现有方法在攻击效果和提示自然性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前针对对齐LLM的攻击方法主要针对精确的肯定响应模式，存在收敛性有限、提示不自然和计算成本高等问题，需要一种更有效的攻击范式。

Method: 提出语义表示攻击范式，利用包含等效有害含义的多样化响应的语义表示空间，并设计了语义表示启发式搜索算法来高效生成语义连贯且简洁的对抗性提示。

Result: 该方法在18个LLM上实现了前所未有的攻击成功率（平均89.41%，其中11个模型达到100%），同时保持了隐蔽性和效率。

Conclusion: 语义表示攻击在整体上优于现有方法，为对抗对齐LLM提供了新的有效途径。

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [6] [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
*Changhyun Jeon,Jinhee Park,Jungwoo Choi,Keonwoo Kim,Jisu Kim,Minji Hong*

Main category: cs.CL

TL;DR: 提出了一个专门针对韩语工具使用的小型语言模型代理架构P-C-G（Planner-Caller-Generator），通过角色分离和韩语优先策略提升韩语环境下的工具使用效果。


<details>
  <summary>Details</summary>
Motivation: 解决韩语环境中频繁的韩英代码切换导致的执行失败问题，为韩语工具使用提供成本效益更高的解决方案。

Method: 采用Planner-Caller-Generator三阶段架构：Planner进行初始批量规划和按需重规划，Caller进行联合模式值验证并返回规范化调用对象，Generator整合工具输出生成最终答案。应用韩语优先值策略。

Result: P-C-G在韩语查询和工具规范下，在单链、多链、缺失参数和缺失函数等场景中表现出竞争力的工具使用准确性和端到端质量，同时减少token使用并保持可接受的延迟。

Conclusion: 角色专门化的小型语言模型是韩语工具使用代理的成本效益替代方案。

Abstract: We propose a small-scale language model (SLM) based agent architecture,
Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G
separates planning, calling, and generation by role: the Planner produces an
initial batch plan with limited on-demand replanning; the Caller returns a
normalized call object after joint schema-value validation; and the Generator
integrates tool outputs to produce the final answer. We apply a Korean-first
value policy to reduce execution failures caused by frequent Korean-to-English
code switching in Korean settings. Evaluation assumes Korean queries and Korean
tool/parameter specifications; it covers single-chain, multi-chain,
missing-parameters, and missing-functions scenarios, and is conducted via an
LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.
Results show that P-C-G delivers competitive tool-use accuracy and end-to-end
quality while reducing tokens and maintaining acceptable latency, indicating
that role-specialized SLMs are a cost-effective alternative for Korean tool-use
agents.

</details>


### [7] [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
*Dylan Hutson,Daniel Vennemeyer,Aneesh Deshmukh,Justin Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: GuessingGame是一个评估LLMs作为战略提问者的协议，通过自由形式提问来识别隐藏对象，提出两种信息增益指标来衡量问题质量，并证明高信息增益能显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 动机是评估大型语言模型在开放领域设置中作为战略提问者的能力，特别是在没有预定义选择或候选列表的情况下，通过提问来识别隐藏对象。

Method: 方法包括引入GuessingGame协议，提出两种信息增益指标：基于贝叶斯的方法跟踪语义概念的信念更新，以及基于熵的方法通过ConceptNet过滤候选。这些指标是模型无关的，支持事后分析。

Result: 在858个游戏中，更高的信息增益强烈预测效率：信息增益增加一个标准差，预期游戏长度减少43%。通过信息增益引导的提示约束，如强制问题多样性，使较弱模型显著提高性能。

Conclusion: 结论是LLMs中的提问能力是可测量和可改进的，对于交互式推理至关重要。

Abstract: We introduce GuessingGame, a protocol for evaluating large language models
(LLMs) as strategic question-askers in open-ended, open-domain settings. A
Guesser LLM identifies a hidden object by posing free-form questions to an
Oracle without predefined choices or candidate lists. To measure question
quality, we propose two information gain (IG) metrics: a Bayesian method that
tracks belief updates over semantic concepts using LLM-scored relevance, and an
entropy-based method that filters candidates via ConceptNet. Both metrics are
model-agnostic and support post hoc analysis. Across 858 games with multiple
models and prompting strategies, higher IG strongly predicts efficiency: a
one-standard-deviation IG increase reduces expected game length by 43\%.
Prompting constraints guided by IG, such as enforcing question diversity,
enable weaker models to significantly improve performance. These results show
that question-asking in LLMs is both measurable and improvable, and crucial for
interactive reasoning.

</details>


### [8] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: AutoSpec是一个用于自动起草专利说明书的保密性代理框架，通过将起草过程分解为可管理的子任务，使用开源语言模型和定制工具来解决专利起草的挑战。


<details>
  <summary>Details</summary>
Motivation: 专利起草过程昂贵且耗时，但现有语言模型面临保密性要求和专利技术性写作的挑战，需要开发安全的自动化解决方案。

Method: 将专利起草过程分解为序列化子任务，使用开源语言模型结合定制工具来逐步完成专利说明书的起草工作。

Result: 通过与专利律师合作设计的评估协议显示，AutoSpec在专利起草任务上优于现有基线方法。

Conclusion: AutoSpec框架成功解决了专利起草自动化的保密性和技术性挑战，为专利自动化提供了可行的解决方案。

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [9] [Future Policy Aware Preference Learning for Mathematical Reasoning](https://arxiv.org/abs/2509.19893)
*Minjae Oh,Yunho Choi,Dongmin Choi,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了Future Policy Aware (FPA)偏好学习方法，通过用未来策略替换当前策略作为正则化项，解决数学推理中DPO等方法因token重叠导致的过度惩罚问题。


<details>
  <summary>Details</summary>
Motivation: 传统偏好学习方法在数学推理任务中效果不佳，主要原因是偏好和非偏好轨迹间存在大量token重叠，降低非偏好轨迹概率时会过度惩罚共享的有用token，导致性能崩溃。

Method: 提出FPA方法，在正则化项中使用未来策略而非当前策略，通过轻量级的logit空间外推从参考模型向当前模型估计未来策略，实现主动正则化。

Result: 在MATH和GSM8K基准测试中，FPA应用于DPO、RPO和SimPER均带来一致性能提升，其中SimPER提升最大达5.75%，且能实现更长的无退化训练。

Conclusion: FPA通过前瞻性正则化有效保护共享数学token的概率，以可忽略的计算开销实现更安全的训练。

Abstract: Preference learning methods such as Direct Preference Optimization (DPO) have
become standard for Large Language Model (LLM) post-training, yet they are
often ineffective for mathematical reasoning. A key challenge is the large
token overlap between preferred and dispreferred trajectories; lowering the
probability of dispreferred trajectories also reduces the probability of shared
useful tokens, leading to over-penalization and overall performance collapse.
As a mitigation, existing algorithms include the probability of a trajectory
under the current policy as a regularization term, which decreases the effect
of the gradient when the probability is low. However, by the time this effect
takes hold, useful tokens may have already been over-penalized as the model has
begun to degrade. To address this, we propose Future Policy Aware (FPA)
preference learning, which replaces the current policy with a future policy in
the regularization term. This future policy is estimated via lightweight,
logit-space extrapolation from a reference model toward the current model. FPA
enables safer training by preemptively regularizing potentially problematic
gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH
and GSM8K benchmarks. FPA yields consistent performance gains, with the largest
improvements observed with SimPER, achieving gains of up to 5.75%. We
demonstrate that FPA provides proactive regularization while preserving the
probability of shared, useful mathematical tokens, and enables longer,
degradation-free training with negligible computational overhead. We will
release our code publicly upon publication.

</details>


### [10] [The Knowledge-Behaviour Disconnect in LLM-based Chatbots](https://arxiv.org/abs/2509.20004)
*Jan Broersen*

Main category: cs.CL

TL;DR: 本文认为大型语言模型存在"脱节"问题，即模型的知识与其对话行为之间缺乏内在联系，这种脱节是根本性的，无法通过更多数据或训练解决。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否真正将知识作为对话行为的基础，分析模型内部知识与外部行为之间的脱节现象及其根本原因。

Method: 通过哲学论证分析LLM训练的核心技术（预测下一个词）无法建立知识到行为的连接，并考察伦理对齐技术的局限性。

Result: 发现脱节现象是LLM的根本限制，解释了幻觉的来源，伦理对齐技术不仅无法解决脱节问题，反而可能加剧问题。

Conclusion: LLM的知识与行为脱节是结构性的根本问题，现有技术无法解决这一限制。

Abstract: Large language model-based artificial conversational agents (like ChatGPT)
give answers to all kinds of questions, and often enough these answers are
correct. Just on the basis of that capacity alone, we may attribute knowledge
to them. But do these models use this knowledge as a basis for their own
conversational behaviour? I argue this is not the case, and I will refer to
this failure as a `disconnect'. I further argue this disconnect is fundamental
in the sense that with more data and more training of the LLM on which a
conversational chatbot is based, it will not disappear. The reason is, as I
will claim, that the core technique used to train LLMs does not allow for the
establishment of the connection we are after. The disconnect reflects a
fundamental limitation on the capacities of LLMs, and explains the source of
hallucinations. I will furthermore consider the ethical version of the
disconnect (ethical conversational knowledge not being aligned with ethical
conversational behaviour), since in this domain researchers have come up with
several additional techniques to influence a chatbot's behaviour. I will
discuss how these techniques do nothing to solve the disconnect and can make it
worse.

</details>


### [11] [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
*Sujeong Lee,Hayoung Lee,Seongsoo Heo,Wonik Choi*

Main category: cs.CL

TL;DR: 提出了SPEED框架，通过专家模型进行多维度描述性评估，解决传统基准测试依赖固定参考答案的局限性


<details>
  <summary>Details</summary>
Motivation: 传统基于基准测试的评估方法依赖固定参考答案，无法捕捉生成响应的定性特征，需要更全面的评估框架

Method: 使用专门的功能专家模型进行综合分析，包括幻觉检测、毒性评估和词汇上下文适当性等多个维度，并整合专家反馈

Result: SPEED在不同领域和数据集上实现稳健一致的评估性能，使用相对紧凑的专家模型，比大规模评估器具有更好的资源效率

Conclusion: SPEED显著提升了LLM评估的公平性和可解释性，为现有评估方法提供了有前景的替代方案

Abstract: Reliable evaluation of large language models is essential to ensure their
applicability in practical scenarios. Traditional benchmark-based evaluation
methods often rely on fixed reference answers, limiting their ability to
capture important qualitative aspects of generated responses. To address these
shortcomings, we propose an integrated evaluation framework called
\textit{self-refining descriptive evaluation with expert-driven diagnostics},
SPEED, which utilizes specialized functional experts to perform comprehensive,
descriptive analyses of model outputs. Unlike conventional approaches, SPEED
actively incorporates expert feedback across multiple dimensions, including
hallucination detection, toxicity assessment, and lexical-contextual
appropriateness. Experimental results demonstrate that SPEED achieves robust
and consistent evaluation performance across diverse domains and datasets.
Additionally, by employing relatively compact expert models, SPEED demonstrates
superior resource efficiency compared to larger-scale evaluators. These
findings illustrate that SPEED significantly enhances fairness and
interpretability in LLM evaluations, offering a promising alternative to
existing evaluation methodologies.

</details>


### [12] [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
*Chaojun Nie,Jun Zhou,Guanxiang Wang,Shisong Wud,Zichen Wang*

Main category: cs.CL

TL;DR: 本文提出RLAG方法，通过强化学习从增强生成中优化LLMs在专业领域的知识嵌入，解决传统方法在知识优先级和连贯性方面的不足。


<details>
  <summary>Details</summary>
Motivation: LLMs在专业领域任务中表现有限，主要由于训练数据中专业信息比例不足且存在时间滞后。现有方法如持续预训练对所有token同等对待，而监督微调难以构建连贯的知识结构。

Method: 提出RLAG方法，通过迭代生成采样和基于奖励的模型优化，选择最高对数概率的生成结果，计算三个定制奖励指标来指导优化过程。

Result: 在医学、法律、天文学和时事等多个数据集上的实验结果表明，该方法显著优于基线方法。

Conclusion: RLAG方法能有效嵌入关键且上下文连贯的专业领域知识，提升LLMs在专业任务中的表现。

Abstract: Large language models (LLMs) often exhibit limited performance on
domain-specific tasks due to the natural disproportionate representation of
specialized information in their training data and the static nature of these
datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain
applications. While post-training on domain datasets can embed knowledge into
models, existing approaches have some limitations. Continual Pre-Training (CPT)
treats all tokens in domain documents with equal importance, failing to
prioritize critical knowledge points, while supervised fine-tuning (SFT) with
question-answer pairs struggles to develop the coherent knowledge structures
necessary for complex reasoning tasks. To address these challenges, we propose
Reinforcement Learning from Augmented Generation (RLAG). Our approach
iteratively cycles between sampling generations and optimizing the model
through calculated rewards, effectively embedding critical and contextually
coherent domain knowledge. We select generated outputs with the highest log
probabilities as the sampling result, then compute three tailored reward
metrics to guide the optimization process. To comprehensively evaluate domain
expertise, we assess answer accuracy and the rationality of explanations
generated for correctly answered questions. Experimental results across
medical, legal, astronomy, and current events datasets demonstrate that our
proposed method significantly outperforms baseline approaches. Our code and
data are open sourced at https://github.com/ChaojunNie/RLAG.

</details>


### [13] [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
*Zipeng Ling,Yuehao Tang,Chen Huang,Shuliang Liu,Gaoyang Jiang,Shenghong Fu,Junqi Yang,Yao Wan,Jiawan Zhang,Kejia Huang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文研究了LLM推理中的指令边界问题，即用户提供有偏见或不完整的提示时会导致LLM产生偏差。作者提出了BiasDetector框架来测量三种指令类型（完整、冗余、不足）产生的偏差，发现主流LLM在下游任务中存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: LLM推理虽然强大，但提示设计的局限性尚未充分探索。用户可能无意中提供有偏见或不完整的提示，这会误导LLM，降低可靠性并带来风险。

Method: 将指令边界问题提炼为八个具体方面，引入BiasDetector框架来测量由完整、冗余和不足三种指令类型产生的偏差，并对多个主流LLM进行评估。

Result: 尽管主流LLM在标题准确性方面表现良好，但由于提示覆盖范围的问题，许多下游任务中仍存在显著偏差。实证研究证实LLM推理可靠性仍有很大改进空间。

Conclusion: LLM推理可靠性需要显著改进，开发者需要解决偏差问题，用户需要谨慎设计提示。分析了这些偏差的实际影响并概述了缓解策略。

Abstract: Large-language-model (LLM) reasoning has long been regarded as a powerful
tool for problem solving across domains, providing non-experts with valuable
advice. However, their limitations - especially those stemming from prompt
design - remain underexplored. Because users may supply biased or incomplete
prompts - often unintentionally - LLMs can be misled, undermining reliability
and creating risks. We refer to this vulnerability as the Instruction Boundary.
To investigate the phenomenon, we distill it into eight concrete facets and
introduce BiasDetector, a framework that measures biases arising from three
instruction types: complete, redundant, and insufficient. We evaluate several
mainstream LLMs and find that, despite high headline accuracy, substantial
biases persist in many downstream tasks as a direct consequence of prompt
coverage. Our empirical study confirms that LLM reasoning reliability can still
be significantly improved. We analyze the practical impact of these biases and
outline mitigation strategies. Our findings underscore the need for developers
to tackle biases and for users to craft options carefully.

</details>


### [14] [Language Models that Think, Chat Better](https://arxiv.org/abs/2509.20357)
*Adithya Bhaskar,Xi Ye,Danqi Chen*

Main category: cs.CL

TL;DR: RLMT（模型奖励思考强化学习）通过让语言模型生成长链思考后响应，并使用在线RL优化偏好奖励模型，在通用对话任务上超越标准RLHF流程，显著提升聊天、创意写作等能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在可验证领域有效，但在开放式任务中泛化有限。本文旨在将强化学习范式扩展到非可验证领域，提升语言模型的通用对话能力。

Method: 提出RLMT方法：使用真实世界提示让模型生成长链思考（CoT），然后通过在线RL（DPO、PPO、GRPO等算法）针对RLHF中的偏好奖励模型进行优化。

Result: 在40个训练实验中，RLMT在三个聊天基准上获得3-7分提升，创意写作和常识任务提升1-3分。8B模型在聊天和创意写作上超越GPT-4o，媲美Claude-3.7-Sonnet。仅用7K提示训练的模型优于使用2500万+样本的多阶段流程后训练的模型。

Conclusion: RLMT重新思考了后训练流程，证明思考机制在广泛任务中的有效性，呼吁未来工作更广泛理解和应用思考机制。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves language model
reasoning by using rule-based rewards in verifiable domains such as mathematics
and code. However, RLVR leads to limited generalization for open-ended tasks --
such as writing outline essays or making meal plans -- where humans reason
routinely. This paper shows that the RLVR paradigm is effective beyond
verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking
(**RLMT**) for general-purpose chat capabilities. Using diverse real-world
prompts, RLMT requires LMs to generate long CoT reasoning before response, and
optimizes them with online RL against a preference-based reward model used in
RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and
instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT
consistently outperforms standard RLHF pipelines. This includes substantial
gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and
ArenaHardV2), along with 1-3 point improvements on other tasks like creative
writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and
creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be
applied directly to base models without an SFT stage, akin to R1-Zero training.
Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT
recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex
multi-staged pipeline with 25M+ examples. We close with qualitative and
quantitative analyses of how trained models plan their responses. Our results
rethink the post-training pipeline and call upon future work to understand and
employ thinking more broadly.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [15] [Top CDOs are turning to Maia - a team of agentic data engineers - to scale data delivery](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.matillion.com%2Fmaia%3Fsrc=web-generated%26utm_medium=email-newsletter%26utm_source=tldr%26utm_campaign=2025-00-web-generated%26utm_content=sept-25-newsletter%26utm_partner=%26utm_term=pp/2/0100019980574e3a-42991f65-26a7-4adf-b838-8256c8168427-000000/mNKBrTk3hdPP21rEoiITFcjSqen8vQn3qwYd5Eq4TH8=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Maia是一个由智能数据工程师代理组成的团队，旨在帮助企业规模化数据交付，解决数据团队面临的繁重工作积压问题。


<details>
  <summary>Details</summary>
Motivation: 传统AI工具无法解决数据团队面临的重复性工作和遗留系统现代化等挑战，企业需要更高效的解决方案来加速可信数据的交付。

Method: 部署智能数据工程师代理作为AI团队成员，构建和优化数据管道，自动化重复性任务，处理遗留ETL系统现代化和持续模型维护。

Result: Merck、GE Healthcare和Precision for Medicine等顶级CDO已采用Maia解决方案，成功解决了其他AI工具无法处理的数据交付瓶颈问题。

Conclusion: 智能数据工程师代理团队能够有效提升数据交付效率，解决数据团队的工作积压问题，是企业数据现代化的重要工具。

Abstract: Top CDOs are turning to Maia - a team of agentic data engineers - to scale data delivery (Sponsor) Organizations like Merck, GE Healthcare and Precision for Medicine are using Maia to solve what other AI tools can't: the grinding work that backlogs data teams.Maia deploys agentic data engineers that act as AI teammates to build and optimize pipelines, automate repetitive tasks, and deliver trusted data faster. Whether you're modernizing 15-year-old legacy ETL systems or handling ongoing model...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning](https://arxiv.org/abs/2509.19305)
*Yifu Luo,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: 本文提出WFDiffuser，一种基于扩散模型的离线强化学习框架，通过集成离散小波变换和短时傅里叶变换来解决现有方法忽视频域特征导致的频率偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散概率模型的离线强化学习方法主要关注时域特征，忽视了频域特征，导致频率偏移和性能下降。作者观察到仅使用时域方法会无意中引入频域低频分量的偏移，导致轨迹不稳定。

Method: 提出Wavelet Fourier Diffuser (WFDiffuser)框架：1）使用离散小波变换将轨迹分解为低频和高频分量；2）采用短时傅里叶变换提取频域特征；3）使用交叉注意力机制促进跨频率交互。

Result: 在D4RL基准测试上的大量实验结果表明，WFDiffuser有效缓解了频率偏移问题，产生了更平滑、更稳定的轨迹，并在决策性能上优于现有方法。

Conclusion: 从频域视角研究强化学习问题具有重要价值，WFDiffuser通过结合时域和频域特征分析，显著提升了扩散模型在离线强化学习中的性能。

Abstract: Diffusion probability models have shown significant promise in offline
reinforcement learning by directly modeling trajectory sequences. However,
existing approaches primarily focus on time-domain features while overlooking
frequency-domain features, leading to frequency shift and degraded performance
according to our observation. In this paper, we investigate the RL problem from
a new perspective of the frequency domain. We first observe that
time-domain-only approaches inadvertently introduce shifts in the low-frequency
components of the frequency domain, which results in trajectory instability and
degraded performance. To address this issue, we propose Wavelet Fourier
Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates
Discrete Wavelet Transform to decompose trajectories into low- and
high-frequency components. To further enhance diffusion modeling for each
component, WFDiffuser employs Short-Time Fourier Transform and cross attention
mechanisms to extract frequency-domain features and facilitate cross-frequency
interaction. Extensive experiment results on the D4RL benchmark demonstrate
that WFDiffuser effectively mitigates frequency shift, leading to smoother,
more stable trajectories and improved decision-making performance over existing
methods.

</details>


### [17] [Learning from Observation: A Survey of Recent Advances](https://arxiv.org/abs/2509.19379)
*Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 这篇论文提出了一个学习从观察（LfO）的框架，用于分类和调查现有的LfO方法，并讨论了相关领域如离线强化学习、基于模型的强化学习和分层强化学习的联系。


<details>
  <summary>Details</summary>
Motivation: 模仿学习算法通常需要专家演示的状态和动作信息，但在实际应用中，专家动作可能难以获取。LfO或仅状态模仿学习（SOIL）通过仅使用专家状态访问信息来解决这一限制。

Method: 论文提出了一个LfO框架，用于分类和调查现有的LfO方法，包括轨迹构建、假设和算法设计选择。

Result: 通过框架分析了现有LfO方法，并识别了开放问题和未来研究方向。

Conclusion: LfO框架有助于系统化现有方法，并指出了与相关领域的联系及未来研究的方向。

Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent
by mimicking an expert's behavior without requiring a reward function. IL
algorithms often necessitate access to state and action information from expert
demonstrations. Although expert actions can provide detailed guidance,
requiring such action information may prove impractical for real-world
applications where expert actions are difficult to obtain. To address this
limitation, the concept of learning from observation (LfO) or state-only
imitation learning (SOIL) has recently gained attention, wherein the imitator
only has access to expert state visitation information. In this paper, we
present a framework for LfO and use it to survey and classify existing LfO
methods in terms of their trajectory construction, assumptions and algorithm's
design choices. This survey also draws connections between several related
fields like offline RL, model-based RL and hierarchical RL. Finally, we use our
framework to identify open problems and suggest future research directions.

</details>


### [18] [DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions](https://arxiv.org/abs/2509.19538)
*Zongyue Li,Xiao Han,Yusong Li,Niklas Strauss,Matthias Schubert*

Main category: cs.LG

TL;DR: DAWM是一种基于扩散的世界模型，通过生成状态-奖励轨迹并配合逆动力学模型进行动作推断，为离线强化学习提供完整的合成转换数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的世界模型通常不直接生成动作，限制了与标准值函数离线强化学习算法的兼容性。虽然已有研究尝试联合建模状态、奖励和动作，但这种方法增加了训练复杂性并降低了实际性能。

Method: 提出DAWM模型，生成基于当前状态、动作和回报的未来状态-奖励轨迹，同时使用逆动力学模型进行高效的动作推断。这种模块化设计产生适合一步TD学习的完整合成转换数据。

Result: 在D4RL基准测试中，保守型离线RL算法如TD3BC和IQL在使用DAWM生成的增强轨迹进行训练时，性能显著优于先前的基于扩散的基线方法。

Conclusion: DAWM的模块化设计能够有效支持基于一步TD学习的离线强化学习算法，在多个任务上展现出优越性能。

Abstract: Diffusion-based world models have demonstrated strong capabilities in
synthesizing realistic long-horizon trajectories for offline reinforcement
learning (RL). However, many existing methods do not directly generate actions
alongside states and rewards, limiting their compatibility with standard
value-based offline RL algorithms that rely on one-step temporal difference
(TD) learning. While prior work has explored joint modeling of states, rewards,
and actions to address this issue, such formulations often lead to increased
training complexity and reduced performance in practice. We propose
\textbf{DAWM}, a diffusion-based world model that generates future state-reward
trajectories conditioned on the current state, action, and return-to-go, paired
with an inverse dynamics model (IDM) for efficient action inference. This
modular design produces complete synthetic transitions suitable for one-step
TD-based offline RL, enabling effective and computationally efficient training.
Empirically, we show that conservative offline RL algorithms such as TD3BC and
IQL benefit significantly from training on these augmented trajectories,
consistently outperforming prior diffusion-based baselines across multiple
tasks in the D4RL benchmark.

</details>


### [19] [Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks](https://arxiv.org/abs/2509.19554)
*Yi Ren*

Main category: cs.LG

TL;DR: 该论文提出了一种基于力分析思想的深度学习模型学习过程分析框架，将训练样本间的影响分解为相似性和更新强度两个部分，用于解释模型在不同系统中的行为。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型如何随时间学习，特别是训练样本之间如何相互影响，类似于力分析中物体间的相互作用。

Method: 采用力分析的思想框架，将训练样本间的影响分解为相似性和更新强度两个维度，应用于各种学习任务中分析模型行为。

Result: 该框架能解释模型的各种行为，包括特定样本的学习路径、LLM微调方法的效果原因，以及为什么结构化模式更容易学习。同时发现了改进模型训练的新策略。

Conclusion: 虽然该方法仍在发展中，但为系统解释模型行为提供了一种新的分析视角。

Abstract: This thesis explores how deep learning models learn over time, using ideas
inspired by force analysis. Specifically, we zoom in on the model's training
procedure to see how one training example affects another during learning, like
analyzing how forces move objects. We break this influence into two parts: how
similar the two examples are, and how strong the updating force is. This
framework helps us understand a wide range of the model's behaviors in
different real systems. For example, it explains why certain examples have
non-trivial learning paths, why (and why not) some LLM finetuning methods work,
and why simpler, more structured patterns tend to be learned more easily. We
apply this approach to various learning tasks and uncover new strategies for
improving model training. While the method is still developing, it offers a new
way to interpret models' behaviors systematically.

</details>


### [20] [C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning](https://arxiv.org/abs/2509.19674)
*Kunlun Xu,Yibo Feng,Jiangmeng Li,Yongsheng Qi,Jiahuan Zhou*

Main category: cs.LG

TL;DR: 提出C²Prompt方法解决联邦持续学习中提示通信的类间知识一致性问题，通过局部类分布补偿和类感知提示聚合提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的联邦持续学习方法存在类间知识一致性问题，包括客户端间的类内分布差距和提示间的类间相关性，导致知识冲突和遗忘加剧

Method: 提出C²Prompt方法，包含局部类分布补偿机制(LCDC)减少客户端间类内分布差异，以及类感知提示聚合方案(CPA)选择性增强类相关知识聚合

Result: 在多个联邦持续学习基准测试中达到最先进性能

Conclusion: C²Prompt通过增强提示通信中的类间知识一致性，有效缓解了联邦持续学习中的时空遗忘问题

Abstract: Federated continual learning (FCL) tackles scenarios of learning from
continuously emerging task data across distributed clients, where the key
challenge lies in addressing both temporal forgetting over time and spatial
forgetting simultaneously. Recently, prompt-based FCL methods have shown
advanced performance through task-wise prompt communication.In this study, we
underscore that the existing prompt-based FCL methods are prone to class-wise
knowledge coherence between prompts across clients. The class-wise knowledge
coherence includes two aspects: (1) intra-class distribution gap across
clients, which degrades the learned semantics across prompts, (2) inter-prompt
class-wise relevance, which highlights cross-class knowledge confusion. During
prompt communication, insufficient class-wise coherence exacerbates knowledge
conflicts among new prompts and induces interference with old prompts,
intensifying both spatial and temporal forgetting. To address these issues, we
propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method
that explicitly enhances class-wise knowledge coherence during prompt
communication. Specifically, a local class distribution compensation mechanism
(LCDC) is introduced to reduce intra-class distribution disparities across
clients, thereby reinforcing intra-class knowledge consistency. Additionally, a
class-aware prompt aggregation scheme (CPA) is designed to alleviate
inter-class knowledge confusion by selectively strengthening class-relevant
knowledge aggregation. Extensive experiments on multiple FCL benchmarks
demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our
source code is available at
https://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt

</details>


### [21] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 该论文提出Frictional Q-learning算法，通过类比经典力学中的静摩擦力来防止策略漂移到不支持的动作用，从而减少外推误差。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中的外推误差问题，防止策略在连续控制任务中漂移到数据分布之外的无效动作。

Method: 扩展批量约束强化学习，通过约束智能体的动作空间，使其行为与回放缓冲区中的行为相似，同时与正交动作空间的流形保持距离。

Result: 算法在标准连续控制基准测试中表现出鲁棒的训练效果和具有竞争力的性能。

Conclusion: Frictional Q-learning算法简单有效，为外推误差提供了直观的物理解释，并在实践中表现良好。

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [22] [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)
*Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim*

Main category: cs.LG

TL;DR: RDAR提出了一种学习每个智能体相关性的策略，通过识别哪些智能体可以从预训练行为模型的输入中排除，从而减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶员每次只关注少数几个智能体，而自动驾驶系统需要处理复杂场景中的所有智能体，无论其是否影响决策。现有注意力机制计算复杂度高，需要更高效的方法。

Method: 将智能体选择过程建模为马尔可夫决策过程，动作是表示智能体选择的二进制掩码。通过识别可排除的智能体来学习每个智能体的相关性。

Result: 在大规模驾驶数据集上评估，RDAR能够学习准确的智能体相关性度量，在保持可比驾驶性能的同时显著减少处理的智能体数量。

Conclusion: RDAR提供了一种有效的方法来减少自动驾驶系统的计算负担，同时保持性能。

Abstract: Human drivers focus only on a handful of agents at any one time. On the other
hand, autonomous driving systems process complex scenes with numerous agents,
regardless of whether they are pedestrians on a crosswalk or vehicles parked on
the side of the road. While attention mechanisms offer an implicit way to
reduce the input to the elements that affect decisions, existing attention
mechanisms for capturing agent interactions are quadratic, and generally
computationally expensive. We propose RDAR, a strategy to learn per-agent
relevance -- how much each agent influences the behavior of the controlled
vehicle -- by identifying which agents can be excluded from the input to a
pre-trained behavior model. We formulate the masking procedure as a Markov
Decision Process where the action consists of a binary mask indicating agent
selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate
its ability to learn an accurate numerical measure of relevance by achieving
comparable driving performance, in terms of overall progress, safety and
performance, while processing significantly fewer agents compared to a state of
the art behavior model.

</details>


### [23] [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2509.19803)
*Guochao Jiang,Wenfeng Feng,Guofeng Quan,Chuzhan Hao,Yuewei Zhang,Guohua Liu,Hao Wang*

Main category: cs.LG

TL;DR: VCRL是一个基于奖励方差动态控制训练样本难度的课程强化学习框架，用于改进LLM在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于rollout的强化学习方法没有考虑LLM对不同难度样本的学习能力，这与人类从易到难的数学推理认知过程相悖。

Method: 基于发现奖励方差反映样本难度的事实，提出VCRL框架，通过动态调整训练样本难度来优化学习过程。

Result: 在五个数学基准和两个模型上的实验表明，VCRL优于当前的LLM强化学习基线方法。

Conclusion: VCRL通过模拟人类从易到难的学习过程，有效提升了LLM在数学推理任务上的性能。

Abstract: Policy-based reinforcement learning currently plays an important role in
improving LLMs on mathematical reasoning tasks. However, existing rollout-based
reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly
consider LLMs' learning ability for samples of different difficulty levels,
which is contrary to the human cognitive process of mathematical reasoning
tasks from easy to difficult. Intuitively, we find that the variance of the
rollout group's reward in RLVR partly reflects the difficulty of the current
sample for LLMs. Samples that are too easy or too difficult have a lower
variance, while samples with moderate difficulty have a higher variance. Based
on this, we propose VCRL, a curriculum reinforcement learning framework that
dynamically controls the difficulty of training samples based on the variance
of group rewards. Experiments on five mathematical benchmarks and two models
reveal the advantages of VCRL over the current LLM RL baselines.

</details>


### [24] [BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management](https://arxiv.org/abs/2509.19846)
*Kevin Bradley Dsouza,Enoch Ofosu,Daniel Chukwuemeka Amaogu,Jérôme Pigeon,Richard Boudreault,Pooneh Maghoul,Juan Moreno-Cruz,Yuri Leonenko*

Main category: cs.LG

TL;DR: BoreaRL是第一个用于气候适应性北方森林管理的多目标强化学习环境，揭示了碳目标比永久冻土保护目标更容易优化，标准方法在通用设置中失败，而课程学习方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 北方森林储存大量碳，但优化森林管理以同时实现碳封存和永久冻土保护存在复杂权衡，现有工具无法充分解决这一问题。

Method: 开发了基于物理的耦合能量、碳和水通量模拟器，支持站点特定模式和通用模式两种训练范式，评估了多目标强化学习算法。

Result: 发现碳目标优化明显比永久冻土保护目标容易，标准偏好条件方法在通用设置中完全失败，而课程学习方法通过策略性选择训练片段获得更好性能。

Conclusion: 当前多目标强化学习方法在稳健的气候适应性森林管理方面仍面临挑战，BoreaRL为开发更有效方法提供了有价值的基准。

Abstract: Boreal forests store 30-40% of terrestrial carbon, much in climate-vulnerable
permafrost soils, making their management critical for climate mitigation.
However, optimizing forest management for both carbon sequestration and
permafrost preservation presents complex trade-offs that current tools cannot
adequately address. We introduce $\textbf{BoreaRL}$, the first multi-objective
reinforcement learning environment for climate-adaptive boreal forest
management, featuring a physically-grounded simulator of coupled energy,
carbon, and water fluxes. BoreaRL supports two training paradigms:
site-specific mode for controlled studies and generalist mode for learning
robust policies under environmental stochasticity. Through evaluation of
multi-objective RL algorithms, we reveal a fundamental asymmetry in learning
difficulty: carbon objectives are significantly easier to optimize than thaw
(permafrost preservation) objectives, with thaw-focused policies showing
minimal learning progress across both paradigms. In generalist settings,
standard preference-conditioned approaches fail entirely, while a naive
curriculum learning approach achieves superior performance by strategically
selecting training episodes. Analysis of learned strategies reveals distinct
management philosophies, where carbon-focused policies favor aggressive
high-density coniferous stands, while effective multi-objective policies
balance species composition and density to protect permafrost while maintaining
carbon gains. Our results demonstrate that robust climate-adaptive forest
management remains challenging for current MORL methods, establishing BoreaRL
as a valuable benchmark for developing more effective approaches. We
open-source BoreaRL to accelerate research in multi-objective RL for climate
applications.

</details>


### [25] [Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches](https://arxiv.org/abs/2509.19924)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: 本文研究了基础模型在稀疏奖励强化学习环境中的零样本探索能力，发现视觉语言模型（VLM）存在"知-行差距"，并提出了一种混合框架来利用VLM指导探索而非端到端控制。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习中的稀疏奖励环境具有挑战性，基础模型具有强大的语义先验知识，但其在经典RL基准测试中作为零样本探索代理的能力尚未得到充分理解。

Method: 在老虎机、Gridworld和稀疏奖励Atari游戏中测试LLM和VLM的零样本探索能力，并研究了一个简单的在线混合框架来分析VLM指导探索的潜力。

Result: VLM能够从视觉输入推断高级目标，但在精确低级控制方面持续失败。在理想化设置中，VLM指导显著提高了早期样本效率。

Conclusion: 基础模型更适合用于指导探索而非端到端控制，VLM指导在早期探索阶段具有潜力但存在局限性。

Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly
in sparse-reward settings. While foundation models possess strong semantic
priors, their capabilities as zero-shot exploration agents in classic RL
benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed
bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our
investigation reveals a key limitation: while VLMs can infer high-level
objectives from visual input, they consistently fail at precise low-level
control: the "knowing-doing gap". To analyze a potential bridge for this gap,
we investigate a simple on-policy hybrid framework in a controlled, best-case
scenario. Our results in this idealized setting show that VLM guidance can
significantly improve early-stage sample efficiency, providing a clear analysis
of the potential and constraints of using foundation models to guide
exploration rather than for end-to-end control.

</details>


### [26] [Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation](https://arxiv.org/abs/2509.20008)
*Raphael Simon,Pieter Libin,Wim Mees*

Main category: cs.LG

TL;DR: 该论文研究了在部分可观测环境下使用强化学习进行渗透测试，比较了多种PPO变体来处理部分可观测性挑战，发现历史聚合方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 渗透测试作为网络安全攻击模拟，是一个适合强化学习自动化的序列决策问题。但现实世界中的部分可观测性挑战了马尔可夫性质，需要开发更鲁棒和可迁移的策略。

Method: 使用vanilla PPO作为基线，比较了多种PPO变体：帧堆叠、历史信息增强观测、循环神经网络和基于Transformer的架构，在可变规模的网络主机上进行系统实证分析。

Result: 研究发现该任务极大受益于历史聚合方法，收敛速度比其他方法快三倍。对学习策略的手动检查揭示了超越定量结果的清晰区别和深入见解。

Conclusion: 在部分可观测的渗透测试场景中，历史聚合方法表现出显著优势，为开发更鲁棒和可迁移的网络安全策略提供了重要见解。

Abstract: Penetration testing, the simulation of cyberattacks to identify security
vulnerabilities, presents a sequential decision-making problem well-suited for
reinforcement learning (RL) automation. Like many applications of RL to
real-world problems, partial observability presents a major challenge, as it
invalidates the Markov property present in Markov Decision Processes (MDPs).
Partially Observable MDPs require history aggregation or belief state
estimation to learn successful policies. We investigate stochastic, partially
observable penetration testing scenarios over host networks of varying size,
aiming to better reflect real-world complexity through more challenging and
representative benchmarks. This approach leads to the development of more
robust and transferable policies, which are crucial for ensuring reliable
performance across diverse and unpredictable real-world environments. Using
vanilla Proximal Policy Optimization (PPO) as a baseline, we compare a
selection of PPO variants designed to mitigate partial observability, including
frame-stacking, augmenting observations with historical information, and
employing recurrent or transformer-based architectures. We conduct a systematic
empirical analysis of these algorithms across different host network sizes. We
find that this task greatly benefits from history aggregation. Converging three
times faster than other approaches. Manual inspection of the learned policies
by the algorithms reveals clear distinctions and provides insights that go
beyond quantitative results.

</details>


### [27] [Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](https://arxiv.org/abs/2509.20230)
*Wenhan Wu,Zheyuan Liu,Chongyang Gao,Ren Wang,Kaize Ding*

Main category: cs.LG

TL;DR: 本文揭示了当前LLM遗忘方法的安全漏洞：被遗忘的信息可通过重新学习攻击恢复，原因是传统方法将模型参数推向损失函数的不稳定区域。作者提出StableUN框架，通过邻域感知优化寻找更稳定的参数区域，显著提高了对抗重新学习和越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法存在严重安全漏洞，虽然表面上成功移除了敏感知识，但这些被遗忘的信息仍可通过少量微调样本快速恢复。这种脆弱性源于传统方法将模型参数优化到损失函数的不稳定区域。

Method: 提出StableUN双级反馈引导优化框架，通过邻域感知优化寻找稳定参数区域。框架集成遗忘反馈（使用对抗扰动探测参数邻域）和记忆反馈（保持模型效用），通过梯度投影对齐两个目标。

Result: 在WMDP和MUSE基准测试中，该方法显著提高了对抗重新学习和越狱攻击的鲁棒性，同时保持了竞争力的模型效用性能。

Conclusion: StableUN框架有效解决了LLM遗忘方法的安全漏洞问题，证明了通过寻找稳定参数区域可以显著提升遗忘方法的鲁棒性。

Abstract: Current LLM unlearning methods face a critical security vulnerability that
undermines their fundamental purpose: while they appear to successfully remove
sensitive or harmful knowledge, this ``forgotten" information remains
precariously recoverable through relearning attacks. We identify that the root
cause is that conventional methods optimizing the forgetting loss at individual
data points will drive model parameters toward sharp minima in the loss
landscape. In these unstable regions, even minimal parameter perturbations can
drastically alter the model's behaviors. Consequently, relearning attacks
exploit this vulnerability by using just a few fine-tuning samples to navigate
the steep gradients surrounding these unstable regions, thereby rapidly
recovering knowledge that was supposedly erased. This exposes a critical
robustness gap between apparent unlearning and actual knowledge removal. To
address this issue, we propose StableUN, a bi-level feedback-guided
optimization framework that explicitly seeks more stable parameter regions via
neighborhood-aware optimization. It integrates forgetting feedback, which uses
adversarial perturbations to probe parameter neighborhoods, with remembering
feedback to preserve model utility, aligning the two objectives through
gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that
our method is significantly more robust against both relearning and
jailbreaking attacks while maintaining competitive utility performance.

</details>


### [28] [Failure Modes of Maximum Entropy RLHF](https://arxiv.org/abs/2509.20265)
*Ömer Veysel Çağatan,Barış Akgün*

Main category: cs.LG

TL;DR: 本文揭示了SimPO作为最大熵强化学习的理论基础，并对比了其在离线与在线RLHF设置中的表现差异，发现最大熵RL在在线设置中会出现过优化和不稳定的KL动态。


<details>
  <summary>Details</summary>
Motivation: 受SimPO在离线偏好优化中优异表现的启发，研究最大熵RL是否能在在线RLHF设置中取得类似效果。

Method: 通过实验比较最大熵RL与KL约束方法在在线RLHF设置中的表现，分析其训练稳定性和过优化问题。

Result: 最大熵RL在在线设置中表现出持续的过优化和不稳定KL动态，即使学习率很低也无法避免奖励破解问题。

Conclusion: SimPO在离线设置中成功而最大熵RL在在线设置中失败，表明无参考方法在在线和离线偏好学习中面临不同的挑战。

Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be
derived as Maximum Entropy Reinforcement Learning with length-normalized
temperature, providing a theoretical foundation for this reference-free method.
Motivated by SimPO's strong performance in offline preference optimization, we
investigate whether Maximum Entropy RL can achieve similar results in online
RLHF settings. Our experiments find that Maximum Entropy RL consistently
exhibits overoptimization and unstable KL dynamics, even at very low learning
rates. Unlike KL-constrained methods that maintain stable training, entropy
regularization fails to prevent reward hacking and appears to correlate with
overoptimization. Lastly, we discuss possible explanations for why SimPO
succeeds in offline settings while Maximum Entropy RL struggles in online
scenarios. Our findings suggest that reference-free approaches may face
distinct challenges when applied to online or offline preference learning.

</details>


### [29] [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](https://arxiv.org/abs/2509.20293)
*Benjamin Feuer,Chiung-Yi Tseng,Astitwa Sarthak Lathe,Oussama Elachqar,John P Dickerson*

Main category: cs.LG

TL;DR: 该论文分析了LLM评判基准的设计问题，提出了两种诊断机制（模式依从性和心理测量有效性）来识别基准排名中的噪声，并在Arena-Hard Auto基准上发现严重的模式不一致和因子崩溃问题。


<details>
  <summary>Details</summary>
Motivation: LLM评判基准在评估复杂模型行为时被广泛使用，但其设计存在传统基于真实标签基准所没有的故障模式。作者认为，没有严格目标和可验证构建的基准排名可能会产生高置信度但实际上主要是噪声的排名。

Method: 引入了两种诊断机制：1）模式依从性 - 量化评判者整体裁决中有多少是由显式评估模式解释的；2）心理测量有效性 - 聚合内部一致性和区分效度信号来量化任何基准运行中的不可约不确定性。将这些工具应用于Arena-Hard Auto基准进行分析。

Result: 在Arena-Hard Auto基准上发现严重的模式不一致和因子崩溃：例如DeepSeek-R1-32B的未解释方差超过90%，大多数标准的因子相关性高于0.93。还发现ELO风格的聚合会掩盖真实的排名不确定性。

Conclusion: 研究结果突出了破坏有效性的设计故障，并为构建更好范围、可靠性感知的LLM评判基准提供了可操作的原则。

Abstract: LLM-judged benchmarks are increasingly used to evaluate complex model
behaviors, yet their design introduces failure modes absent in conventional
ground-truth based benchmarks. We argue that without tight objectives and
verifiable constructions, benchmark rankings can produce high-confidence
rankings that are in fact largely noise. We introduce two mechanisms to
diagnose these issues. Schematic adherence quantifies how much of a judge's
overall verdict is explained by the explicit evaluation schema, revealing
unexplained variance when judges deviate from their own rubric. Psychometric
validity aggregates internal consistency and discriminant validity signals to
quantify irreducible uncertainty in any benchmarking run. Applying these tools
to Arena-Hard Auto, we find severe schema incoherence and factor collapse
across popular judges: for example, unexplained variance exceeding 90 percent
for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We
also show that the ELO-style aggregation used by Arena-Hard Auto collapses and
masks genuine ranking uncertainty. Our results highlight design failures that
undermine validity and offer actionable principles for building better-scoped,
reliability-aware LLM-judged benchmarks. We release our code at
https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型从C++源代码自动恢复用户故事的能力，发现所有模型在200行代码内平均F1分数达0.8，小模型通过单示例提示可达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决遗留系统和文档不完善系统中用户故事缺失或过时的问题，探索LLM直接从源代码恢复用户故事的可行性。

Method: 使用1,750个标注的C++代码片段，评估5个最先进的LLM在6种提示策略下的表现，包括单示例提示和思维链推理。

Result: 所有模型在200NLOC内平均F1分数为0.8，小模型通过单示例提示可匹配大模型性能，思维链推理仅对大模型有边际提升。

Conclusion: LLM能有效从源代码恢复用户故事，提示设计对性能有重要影响，简单的单示例提示对小模型特别有效。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [31] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文评估了四种最先进的FILL-in-the-Middle LLM在生成Java测试断言消息方面的能力，发现Codestral-22B表现最佳，但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 断言消息对单元测试至关重要，但开发者和自动化工具经常忽略它们。LLM在此任务上的能力尚未得到系统评估。

Method: 使用包含216个Java测试方法的数据集，评估了Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B和StarCoder四种模型，采用类人评估方法进行质量评分。

Result: Codestral-22B获得最高质量分2.76/5（人工编写为3.24），包含描述性测试注释后性能提升至2.97。所有模型都倾向于复制开发者的语言模式。

Conclusion: LLM在生成断言消息方面有潜力，但需要更多上下文信息。传统文本评估指标难以捕捉断言消息的多样性结构。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [32] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 本文对在企业规模部署的AI辅助软件开发工具进行了为期一年的真实评估，涉及300名工程师使用内部AI平台(DeputyDev)，结果显示PR审查周期时间减少31.8%，开发者满意度达85%，代码推送量增加61%。


<details>
  <summary>Details</summary>
Motivation: 评估AI工具在真实企业环境中的实际效果，填补受控基准测试与实际生产环境评估之间的差距，为AI在企业软件开发中的集成提供实证依据。

Method: 采用纵向队列分析方法，让300名工程师在一年内将结合代码生成和自动审查功能的AI平台集成到日常工作流程中，通过系统数据收集和满意度调查进行评估。

Result: PR审查周期时间整体减少31.8%；开发者满意度85%；使用率从第1个月的4%增长到第6个月的83%峰值；顶级采用者代码推送量增加61%；约30-40%的生产代码通过该工具生成。

Conclusion: AI工具在企业软件开发中具有变革潜力，能显著提升生产力和代码质量，但实际部署面临挑战；开发者接受度高，工具能有效集成到现有工作流程中。

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [33] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen提出了一种协调的多智能体架构，通过数据驱动的桥接语言选择机制，显著提升了多编程语言代码生成的质量，在13种语言上相比最强基线提升了13个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不同编程语言上的代码生成能力差异很大，特别是对于训练数据有限的语言（如Rust、Perl、OCaml等）。现有方法大多孤立处理每种语言，未能充分利用跨语言的知识共享和模式复用。

Method: 采用协调的多智能体架构，整合中间表示、代码生成、翻译和自动修复。核心创新是数据驱动的桥接语言选择机制，基于经验推导的转移矩阵选择最佳中间语言，并进行早期输出验证和迭代错误修正。

Result: 在广泛实验中，XL-CoGen相比最强微调基线提升了13个百分点，相比现有单语言多智能体方法提升高达30个百分点。消融研究证实兼容性引导的桥接显著优于基于LLM的启发式方法。

Conclusion: XL-CoGen通过累积的跨语言知识转移机制，有效解决了多语言代码生成中的语言间能力不均衡问题，证明了跨语言模式复用和知识共享的重要性。

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [34] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的多模态基准测试，包含2,219个高质量样本，通过自动化LLM驱动管道评估游戏开发中的可玩性、视觉美学和用户参与度等关键指标。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM基准主要关注单模态编程任务，忽视了游戏开发所需的视觉美学、可玩性和用户参与度等实际需求，存在与现实游戏开发工作流的差距。

Method: 采用基于聚类的筛选方法从真实仓库中提取100个主题集群的2,219个样本，建立多模态评估框架和自动化LLM驱动管道，在完整UI沙盒环境中进行视觉代码合成评估。

Result: V-GameGym有效弥合了代码生成准确性与实际游戏开发工作流之间的差距，为视觉编程和交互元素生成提供了可量化的质量指标。

Conclusion: 该基准测试填补了现有代码LLM评估在游戏开发领域的空白，为多模态代码生成提供了更全面的评估标准。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [35] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 本文提出了一种利用大语言模型进行数据增强的方法来解决需求追踪中的数据稀缺问题，通过提示模板生成需求-代码追踪链接，并优化追踪模型的编码器，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 需求追踪在软件工程中至关重要，但现有自动化方法受限于训练数据稀缺和需求与代码之间的语义鸿沟问题。

Method: 使用四种大语言模型（Gemini 1.5 Pro、Claude 3、GPT-3.5、GPT-4）进行数据增强，采用零样本和少样本提示模板生成追踪链接，并优化追踪模型的编码器组件。

Result: 实验结果表明该方法显著提升了模型性能，F1分数最高提升了28.59%。

Conclusion: 基于LLM的数据增强方法能有效解决需求追踪中的数据稀缺问题，具有实际应用潜力。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [36] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文评估了大型语言模型在生成Web API集成代码方面的能力，发现现有开源模型表现不佳，无法解决超过40%的任务。


<details>
  <summary>Details</summary>
Motivation: API集成是数字基础设施的核心，但编写正确的API调用代码具有挑战性。尽管LLMs在软件开发中越来越受欢迎，但它们在自动化生成Web API集成代码方面的有效性尚未被充分探索。

Method: 作者创建了一个数据集和评估流程，用于评估多个开源LLMs生成Web API调用代码的能力。

Result: 实验结果显示，生成API调用存在显著挑战，包括产生虚构的端点、错误的参数使用等错误。所有评估的开源模型都无法解决超过40%的任务。

Conclusion: 当前的开源LLMs在Web API集成代码生成方面能力有限，需要进一步改进。

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: 本文提出了评估感知强化学习（EvA-RL）框架，通过在训练策略时同时最小化评估误差，使策略既高效又易于评估。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在策略评估时面临高方差（数据有限、长时程任务）和高偏差（支持度不均、环境模型不准确）的问题，这些挑战源于标准RL范式在训练时未显式考虑评估需求。

Method: 设计EvA-RL框架，训练策略同时最大化期望回报和最小化给定价值预测方案下的评估误差；扩展方法共同学习评估条件状态价值预测器。

Result: 在离散和连续动作领域的实验表明，EvA-RL能显著降低评估误差，同时保持有竞争力的回报，缓解了评估准确性与策略性能之间的权衡。

Conclusion: EvA-RL为RL方法开辟了新方向，将可靠评估作为训练过程中的首要原则。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [38] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 本文分析了在固定计算预算下，LLM自一致性估计器的权衡，提出了m和n与B的平方根成比例的分配策略。


<details>
  <summary>Details</summary>
Motivation: 系统通常重复相同的提示给大语言模型并聚合响应以提高可靠性，需要分析在固定计算预算下的最优分配策略。

Method: 分析自一致性估计器，在固定计算预算B=mn下研究m（提示样本数）和n（每个提示的重复调用次数）之间的权衡关系。

Result: 分析结果表明，最优的分配策略是m和n都与B的平方根成比例，即m,n∝√B。

Conclusion: 在固定计算预算下，应该将资源大致平均分配给提示采样和重复调用，以获得最佳的自一致性估计效果。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [39] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 该研究探索了使用小型语言模型（<100亿参数）通过智能体框架进行基因组问答，以解决幻觉问题和计算成本挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在基因组问答中的幻觉问题和计算成本高昂的挑战，探索小型模型的潜力。

Method: 开发了Nano Bio-Agent（NBA）框架，包含任务分解、工具编排和API访问（NCBI、AlphaGenome等系统）。

Result: 小型模型（3-10B参数）结合智能体框架在GeneTuring基准测试中达到85-97%准确率，最佳组合达到98%准确率，性能优于使用大型模型的方法。

Conclusion: 小型模型结合智能体框架在保持高准确性的同时，显著降低了计算资源需求，有望实现效率提升、成本节约和基因组学工具的民主化。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [40] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL是一个统一的框架，通过标准化gym环境和模拟用户来训练和评估以用户为中心的智能体能力，研究了奖励分配和轨迹评分对GRPO算法学习的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练智能体模型方面显示出潜力，但智能体的最终价值在于协助用户的能力，而用户交互的多样性和动态性带来了挑战。

Method: 提出UserRL框架，系统性地变化回合级奖励分配和轨迹级分数计算，使用GRPO算法在Qwen3模型上进行实验分析。

Result: 发现三个关键结果：SFT冷启动对解锁初始交互能力至关重要；精心设计的轨迹评分能产生更有效的多轮交互；开源模拟器是成本效益高且可迁移的选择。

Conclusion: 奖励塑造和用户模拟选择的精心设计对模型规模同样重要，UserRL为开发稳健的用户中心智能体模型提供了实用途径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [41] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 本文介绍了一种优化的推理工作流程（cepo），旨在解决现代LLM推理中计算效率低下的问题，使较小的开源模型能够超越比其大数倍的模型。


<details>
  <summary>Details</summary>
Motivation: 现代LLM推理依赖于大量的测试时计算，但模型冗长和指令遵循能力差导致计算资源浪费，需要优化能力与成本之间的权衡。

Method: 引入优化的推理工作流程（cepo），通过协同设计编排框架与底层模型能力，提高推理效率。

Result: 该方法使较小的开源模型能够超越比其大数倍的模型性能。

Conclusion: 研究展示了通过协同设计编排框架与模型能力，可以在中小型模型中实现强大的推理能力。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [42] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种在低代码/无代码环境中集成元认知层的架构模式，通过主动监控和预测任务失败来增强自主代理的可靠性，并在预测失败时主动发起人工交接。


<details>
  <summary>Details</summary>
Motivation: 自主代理在低代码/无代码环境中的非确定性特性导致可靠性问题，如陷入循环、生成错误输出或遇到不可恢复的故障，这会破坏用户体验和信任。

Method: 引入一个次级元认知层来监控主代理，基于延迟过长或重复动作等触发器预测任务失败，并在预测失败时主动发起人工交接，提供代理的思考过程和失败原因说明。

Result: 原型系统的实证分析表明，该方法显著提高了整体任务成功率，但带来了显著的计算开销增加。

Conclusion: 将人工交接重新定义为增强系统韧性、改善用户体验和建立信任的核心设计特征，而非失败的标志。论文讨论了该方法的实践和伦理影响，并指出了未来研究方向。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [43] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 本文提出了一种使用对数障碍函数将基于线性规划的马尔可夫决策过程转化为无约束优化问题的新方法，使梯度下降能够有效求解。


<details>
  <summary>Details</summary>
Motivation: 基于线性规划的MDP方法虽然在某些场景（如离线强化学习）中受到关注，但由于其导致的不等式约束优化问题比基于贝尔曼方程的方法更难求解，因此使用较少。本文旨在为更有效实用的LP-based MDP求解建立理论基础。

Method: 利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP表述转化为无约束优化问题，从而可以通过梯度下降获得近似解。

Result: 该方法虽然看似简单，但据作者所知，这种方法的完整理论解释尚未建立，本文旨在填补这一空白。

Conclusion: 通过将LP-based MDP转化为无约束优化问题，为更有效实用的求解方法奠定了理论基础。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [44] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD是一个三阶段框架，通过行为对齐和监督潜在空间控制实现LLM的安全对齐，在保持实用性的同时提高安全可控性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在表示层面平衡全面安全性和细粒度可控性，需要一种既能确保安全又能保持实用性的新方法

Method: 三阶段框架：1）在包含推理增强拒绝响应和正常响应的数据集上微调LLM；2）训练结构化VAE学习解耦的潜在表示；3）通过潜在维度操作实现选择性拒绝行为

Result: 在Qwen3-8B上实验显示安全可控性和响应可解释性显著提升，在Mistral-7B上的跨架构验证证实了方法的通用性

Conclusion: 结构化表示级干预为构建更安全实用的LLM系统提供了有前景的途径

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [45] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本文综述了具身人工智能（Embodied AI）的发展现状，重点分析了大型语言模型（LLMs）和世界模型（WMs）在具身AI中的关键作用，并提出了联合MLLM-WM驱动的具身AI架构的未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和世界模型的突破性进展，具身AI作为实现通用人工智能的重要范式，需要系统性地梳理其技术发展脉络和应用前景。

Method: 采用文献综述方法，从基础到前沿全面梳理具身AI的发展历程、关键技术、核心组件和硬件系统，重点分析LLM驱动和WM驱动的具身AI工作。

Result: 系统阐述了具身AI从单模态到多模态的发展路径，深入剖析了LLMs在语义推理和任务分解、WMs在世界表示和未来预测方面的核心作用。

Conclusion: 提出了联合多模态LLM和世界模型的具身AI架构是未来重要发展方向，能够更好地支持物理世界中的复杂任务执行。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [46] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: 该研究提出了一个多智能体临床诊断框架（MACD），通过多智能体管道让LLM自我学习临床知识，显著提高了复杂临床诊断的准确性，在4390个真实病例上表现优于传统临床指南和人类医生。


<details>
  <summary>Details</summary>
Motivation: 传统提示方法和多智能体方法在处理复杂临床诊断时存在局限，无法积累可重用的临床经验，需要一种能够模拟医生经验积累过程的框架。

Method: 提出MACD框架，通过多智能体管道（总结、精炼和应用诊断见解）让LLM自我学习临床知识，并扩展到MACD-人类协作工作流，包含诊断智能体、评估智能体和人类监督。

Result: 在7种疾病的4390个真实病例上，MACD显著提高诊断准确率，最高提升22.3%，部分数据上表现优于人类医生（最高提升16%），MACD-人类工作流相比纯医生诊断提升18.6%。

Conclusion: 该工作为LLM辅助诊断提供了一个可扩展的自学习范式，弥合了LLM内在知识与真实临床实践之间的差距。

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [47] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 该研究建立了秀丽隐杆线虫中信息素介导的聚集行为与强化学习之间的理论等价性，展示了信息素作为分布式奖励机制的功能，并通过计算实验验证了在动态环境中引入探索性代理可以恢复群体可塑性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索生物群体智能与强化学习算法之间的理论联系，特别是信息素介导的群体行为如何实现分布式决策和集体问题解决能力。

Method: 方法包括建立数学模型将信息素动态与强化学习更新算法对应，使用文献中的实验数据进行验证，并在多臂老虎机场景中进行计算实验来测试群体适应性。

Result: 研究结果显示信息素系统本质上编码了分布式强化学习过程，环境信号作为集体信用分配的外部记忆。在动态环境中，引入对信息素不敏感的探索性代理可以平衡探索-利用权衡。

Conclusion: 结论是信息素系统实现了分布式强化学习过程，通过行为异质性可以增强群体在波动环境中的适应性和决策弹性。

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [48] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA是一个分布式编排框架，通过版本化能力向量实现动态多智能体协作，在HealthBench基准上比单模型基线提升13倍性能


<details>
  <summary>Details</summary>
Motivation: 解决静态多智能体协调的局限性，实现基于能力的动态协作，释放异构AI智能体联邦的集体智能

Method: 结合语义路由、动态任务分解和智能聚类三大创新：语义路由通过HNSW索引匹配任务与智能体；动态任务分解让兼容智能体协作将复杂任务分解为DAG子任务；智能聚类将处理相似子任务的智能体分组进行多轮优化

Result: 在HealthBench基准测试中表现出13倍于单模型基线的性能提升，特别是在需要多视角的复杂推理任务上效果显著

Conclusion: 基于语义编排和结构化协作的FoA框架能够有效释放异构AI智能体联邦的集体智能，系统具有水平扩展能力并保持稳定性能

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [49] [DeepSeek-R1通过<em class="highlight">强化学习</em>激励大语言模型的推理能力](http://mp.weixin.qq.com/s?__biz=MzIxODg4MjY2Mg==&mid=2247484265&idx=1&sn=e0ffcc6a33fd53ed04447e17f11a5f63&chksm=9634e54c76f48dfa2f03582de309621a39b5c3f9f6be19a5b490c624a3da0cbdb3f3ce639fcd#rd)
*所系皆山海*

Main category: wechat.article

TL;DR: 这对我们来说也是一个啊哈时刻，让我们见证了强化学习的力量与美。“思考时间”的增加促进了复杂行为的自发发展。模型愈发表现出诸如反思式推理与系统性备选解探索等高级策略（见扩展数据图1a），显著提升了在数学与


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这对我们来说也是一个啊哈时刻，让我们见证了强化学习的力量与美。“思考时间”的增加促进了复杂行为的自发发展。模型愈发表现出诸如反思式推理与系统性备选解探索等高级策略（见扩展数据图1a），显著提升了在数学与

</details>


### [50] [RLPT：腾讯混元在预训练数据上实现自主<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461154578&idx=1&sn=1ece7a66957a7b2f001bfb99befe8d7a&chksm=867eb9ebe4f9994c40e7305efa7a9360a0ce59f5d20d23a00d1403d2954689c6dd6669a622cf#rd)
*AI工程化*

Main category: wechat.article

TL;DR: 使强化学习能够直接在预训练数据上进行扩展。81 mmlu 65 mmlu-pro 46 gpqa-diamond supergpqa y=105.625 ·expt-0.300x-0.829） y = 79.827 -exp（-0.300x-0.093） $ y = 32.220 - exp（0.015x012） y= 31.412 · exp（0.004x0.779 64 34 42- accu...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 使强化学习能够直接在预训练数据上进行扩展。81 mmlu 65 mmlu-pro 46 gpqa-diamond supergpqa y=105.625 ·expt-0.300x-0.829） y = 79.827 -exp（-0.300x-0.093） $ y = 32.220 - exp（0.015x012） y= 31.412 · exp（0.004x0.779 64 34 42- accuracy（%） 80 63 40 62 33- 36 79 61 32 09 3

</details>


### [51] [如何为<em class="highlight">强化学习</em>设计“元学习”算法？](http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649428159&idx=1&sn=b518133e9f0f33f7509834520bd15135&chksm=898f493af41d58b974662296a15432fbd841b7ced9e96bad681f6470f284d4ebe9c2f0f51a13#rd)
*CreateAMind*

Main category: wechat.article

TL;DR: 3 背景强化学习。强化学习（rl）问题通常被建模为马尔可夫。强化学习强化学习（RL）问题通常被建模为马尔可夫决策过程（Sutton &Barto，2020，MDPs）。一个mdp通常表示为一个元组。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 3 背景强化学习。强化学习（rl）问题通常被建模为马尔可夫。强化学习强化学习（RL）问题通常被建模为马尔可夫决策过程（Sutton &Barto，2020，MDPs）。一个mdp通常表示为一个元组。

</details>


### [52] [信息工程大学越奇强等：基于深度<em class="highlight">强化学习</em>的算网协同动态路由调度算法](http://mp.weixin.qq.com/s?__biz=MzAxMjI4ODQwOA==&mid=2649680373&idx=1&sn=3eb70f9a4ea0cb79a2cc443cd54239ec&chksm=82e871033755290a58f684b7c29d259144d690933e8fded25976c507510da3517e7c24f4a39c#rd)
*电信科学*

Main category: wechat.article

TL;DR: 关键词 算力路由；算网融合；多场景优化；序列决策；深度强化学习 0 引言 随着信息技术的快速发展，特别是大数据、人工智能、云计算和边缘计算的广泛应用，各行业对算力的需求呈现出高度动态化、复杂化的趋势，为了应


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 关键词 算力路由；算网融合；多场景优化；序列决策；深度强化学习 0 引言 随着信息技术的快速发展，特别是大数据、人工智能、云计算和边缘计算的广泛应用，各行业对算力的需求呈现出高度动态化、复杂化的趋势，为了应

</details>


### [53] [推荐阅读 | 基于<em class="highlight">强化学习</em>与ADRC相结合的隔离段边界层抽吸最优智能控制方法研究](http://mp.weixin.qq.com/s?__biz=MzIxMjE3MzA3MQ==&mid=2247503456&idx=1&sn=b7fd175f4f58c64bbe8e7c67e42117f9&chksm=96e2914e0541223e27e473f8afb9b4fd57d81416731472392c0092bbaf36930da3feb0f66cfd#rd)
*空天技术*

Main category: wechat.article

TL;DR: 隔离段；边界层抽吸；强化学习；自抗扰控制（长按识别二维码获取PDF版文章）1 引 言冲压发动机与传统的航空发动机相比，具有结构简单、单位推力高、有效载荷更大和速度更快等优点，已经成为航空航天领域的研究热点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 隔离段；边界层抽吸；强化学习；自抗扰控制（长按识别二维码获取PDF版文章）1 引 言冲压发动机与传统的航空发动机相比，具有结构简单、单位推力高、有效载荷更大和速度更快等优点，已经成为航空航天领域的研究热点。

</details>


### [54] [北京科技大学与清华大学联合研究成果：自动驾驶车辆轨迹跟踪避撞的扩散<em class="highlight">强化学习</em>方法研究](http://mp.weixin.qq.com/s?__biz=MzU5MzQwODE5OQ==&mid=2247513633&idx=1&sn=ecf62b713fb408f33909da3b900a0c97&chksm=ff828a0768cd693e1da5b2e1c4b575a395bd6e7dee3c16829d94efcf0329db654a370f69c839#rd)
*汽车工程编辑部*

Main category: wechat.article

TL;DR: 为此，文章提出了一种扩散型强化学习算法，通过将扩散模型与强化学习框架相结合，以扩散式生成策略网络替代传统策略网络，显著增强了策略的探索能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 为此，文章提出了一种扩散型强化学习算法，通过将扩散模型与强化学习框架相结合，以扩散式生成策略网络替代传统策略网络，显著增强了策略的探索能力。

</details>


### [55] [117页论文！大型推理模型的<em class="highlight">强化学习</em>最新综述，清华联合发表](http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994357&idx=1&sn=e4cdca480812bb1a0432b3412316b0dd&chksm=b0d49d2b08b17325d4f655254e75f42430e98679c8a42329cb62cda10d761b2a27e39de51c8d#rd)
*智猩猩GenAI*

Main category: wechat.article

TL;DR: 2025]；pag [jiang et al.， 2025e]；urpo [lu et al.， 2025e]；pcl [fei et al.， 2025b]；k2 [team， 2025c]；cooper [hong et al.， 2025a]；Critique-GRPO [Zhang et al.， 2025m]Token-level： e.g.， Implicit PRM [Yuan et...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025]；pag [jiang et al.， 2025e]；urpo [lu et al.， 2025e]；pcl [fei et al.， 2025b]；k2 [team， 2025c]；cooper [hong et al.， 2025a]；Critique-GRPO [Zhang et al.， 2025m]Token-level： e.g.， Implicit PRM [Yuan et al.， 2025d]；

</details>


### [56] [DQN：让AI玩游戏的深度<em class="highlight">强化学习</em>革命](http://mp.weixin.qq.com/s?__biz=MzA5MzY1MTQ4Mw==&mid=2247484354&idx=1&sn=a62c0edcb6c9807c8e2996f90c5f004d&chksm=914116414563d79f154531af579df0df5beb7d603edb1b14edb759ffb9acf0c6f20a836d7ac0#rd)
*Wonderful仿真*

Main category: wechat.article

TL;DR: 这标志着深度强化学习时代的到来。今天，我们来深入了解这个改变游戏规则的算法——DQN （Deep Q-Network）。什么是DQN？DQN是Deep Q-Network的缩写，它将深度学习的强大表示能力与Q-Learning的决策能力完美结合。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这标志着深度强化学习时代的到来。今天，我们来深入了解这个改变游戏规则的算法——DQN （Deep Q-Network）。什么是DQN？DQN是Deep Q-Network的缩写，它将深度学习的强大表示能力与Q-Learning的决策能力完美结合。

</details>


### [57] [GUI智能体训练迎来新范式！半在线<em class="highlight">强化学习</em>让7B模型媲美GPT-4o](http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574020&idx=3&sn=4ec5dc4738b3f2347366907dc566013a&chksm=eae6c6ac5c5f923ca51b4a2f0b799e0fd93e3efd7d893cb5cb63febd7298664433ef9b5f900c#rd)
*机器学习算法与自然语言处理*

Main category: wechat.article

TL;DR: 在线强化学习（Online RL）通过与真实环境持续交互获取反馈，能够捕捉到任务完成与否的全局奖励信号，适用于多步决策优化，但面临奖励稀疏、试错成本高昂以及训练不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在线强化学习（Online RL）通过与真实环境持续交互获取反馈，能够捕捉到任务完成与否的全局奖励信号，适用于多步决策优化，但面临奖励稀疏、试错成本高昂以及训练不稳定等问题。

</details>


### [58] [优化大模型<em class="highlight">强化学习</em>训练，上交大联合微软清北提出FlowRL，让AI推理更具泛化力](http://mp.weixin.qq.com/s?__biz=MzE5OTExNjAzNw==&mid=2247485103&idx=2&sn=f766477e339ab0c84789bcfffb2c99e1&chksm=97e01816dd036649a14ccfc3e515260079ddcf0b9f99340a62b6236163d1972285b741d4af75#rd)
*算泥*

Main category: wechat.article

TL;DR: 强化学习（reinforcement learning）是训练大模型推理能力的一把好手。从早期的REINFORCE算法，到后来更稳定高效的PPO（近端策略优化），再到简化版的GRPO（分组奖励策略优化），这些方法本质上都是一个逻辑：奖励最大化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（reinforcement learning）是训练大模型推理能力的一把好手。从早期的REINFORCE算法，到后来更稳定高效的PPO（近端策略优化），再到简化版的GRPO（分组奖励策略优化），这些方法本质上都是一个逻辑：奖励最大化。

</details>


### [59] [<em class="highlight">强化学习</em> x 自动驾驶：现实落地、技术挑战与未来趋势全景解读](http://mp.weixin.qq.com/s?__biz=MzA4MzIzODIzMA==&mid=2257484245&idx=1&sn=edd669a5ceb9ed9cd8dfd18686ec5b3c&chksm=9d168a1962d86200241836a74d0fb6a8c7b56b3674ca97026f0d976d9eb6e3eea9b18d52dbed#rd)
*皮浪日知录*

Main category: wechat.article

TL;DR: 强化学习是除监督学习和无监督学习之外的一个重要机器学习子领域，其核心思想是智能体（agent）通过与环境（environment）的交互，从交互中学习如何基于环境而行动，以最大化预期的长期累积奖励 1。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是除监督学习和无监督学习之外的一个重要机器学习子领域，其核心思想是智能体（agent）通过与环境（environment）的交互，从交互中学习如何基于环境而行动，以最大化预期的长期累积奖励 1。

</details>


### [60] [<em class="highlight">Agent</em>+<em class="highlight">Code</em>，打开AI开发新范式](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257116&idx=1&sn=15c50dba14db176342d0a19e4d7a51f4&chksm=bc3c17c335fca8301cf857608c28bef4e740ec2f2e74e701965bc184bd632a94a9f02197b7d3#rd)
*InfoQ*

Main category: wechat.article

TL;DR: JoyCode 2.0 作为“Agent+Code”范式的另一核心，则专注于解决传统开发模式中“使用门槛高”的难题。凭借多 Agent 协作架构和 CSR 上下文增强引擎，已经让“0 手写代码”的自动化编程成为可能 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: JoyCode 2.0 作为“Agent+Code”范式的另一核心，则专注于解决传统开发模式中“使用门槛高”的难题。凭借多 Agent 协作架构和 CSR 上下文增强引擎，已经让“0 手写代码”的自动化编程成为可能 。

</details>


### [61] [阿里云无影发布首个<em class="highlight">Agentic</em> Computer形态的个人计算产品](http://mp.weixin.qq.com/s?__biz=MzAxNjM3NzYyNg==&mid=2651705390&idx=2&sn=19af265c70986a993d0b9457a3004031&chksm=814a24575417a99545ae7a5868ce6aff8da001c0e7f51518bb394943e99a9f59f0e4bf0725b1#rd)
*科技明说*

Main category: wechat.article

TL;DR: 并首次展示全新的个人计算产品——无影Agentic Computer，拥有全新的人机交互方式，革命性的“记忆”能力和近乎无穷的云上算力。AI Agent是当下全球科技和商业的焦点，过去半年涌现的Agent相关产品，超过了2024年的总和，10个创


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 并首次展示全新的个人计算产品——无影Agentic Computer，拥有全新的人机交互方式，革命性的“记忆”能力和近乎无穷的云上算力。AI Agent是当下全球科技和商业的焦点，过去半年涌现的Agent相关产品，超过了2024年的总和，10个创

</details>


### [62] [从 LLM 到自动化<em class="highlight">智能体</em>：<em class="highlight">Agentic</em> RL 的力量](http://mp.weixin.qq.com/s?__biz=Mzk4ODI5NjUxMA==&mid=2247483697&idx=1&sn=edb4332f89114677ec4d7e2ee5a1f398&chksm=c42b869e834389ea4a07c5e3ff95ba06a8a5576410e38112694467122ed3dc7797c237a14202#rd)
*ClayX AI*

Main category: wechat.article

TL;DR: Agentic RL 的关键技术与挑战如何结合 多智能体系统、记忆机制与自进化范式 构建真正自主的 LLM-based Agents 时间：本周日 下午 4：30（北京时间） 地点：Zoom 线上会议


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic RL 的关键技术与挑战如何结合 多智能体系统、记忆机制与自进化范式 构建真正自主的 LLM-based Agents 时间：本周日 下午 4：30（北京时间） 地点：Zoom 线上会议

</details>


### [63] [「指哪打哪」的 AI 助手｜知乎直答智能上新](http://mp.weixin.qq.com/s?__biz=MTUzODMyMzcwMQ==&mid=2651436879&idx=1&sn=6e88e980c92bb1811cbc3f41eccb3ab6&chksm=699938d0c3c865b14eefa3537443c67ac266f9087958689b6613c4f9a403e7ca8b66748d0e24#rd)
*知乎*

Main category: wechat.article

TL;DR: 而在今天，我们终于可以（骄傲地）宣布，直答已经进一步升级为 Agentic 助手啦！知乎直答。知乎直答全面升级 agentic助手。面对搜索 / 研究 / 学习 / 创作各类需求，无论你希望「深入而全面的研究」、「只要论文的资料」、「


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而在今天，我们终于可以（骄傲地）宣布，直答已经进一步升级为 Agentic 助手啦！知乎直答。知乎直答全面升级 agentic助手。面对搜索 / 研究 / 学习 / 创作各类需求，无论你希望「深入而全面的研究」、「只要论文的资料」、「

</details>


### [64] [创业者说｜<em class="highlight">Agentic</em>时代，不需要学编程也可以AI创业？](http://mp.weixin.qq.com/s?__biz=Mzg2NDAyNzc3MQ==&mid=2247494845&idx=1&sn=4c860ef9faf64e292e2fadf43908fa95&chksm=cf8f7634ac61bacb9ba7ca541da5d6d4279a15a4ad2d101b1b21ef2701f10b414ef36abc528f#rd)
*wteam官方号*

Main category: wechat.article

TL;DR: 所谓“Agentic”，指的是能够独立完成任务闭环的系统或能力。这类应用不仅局限于编程，在各行各业都具备广阔前景。ask → edit → agent。ic：agentic 时代已经到来。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 所谓“Agentic”，指的是能够独立完成任务闭环的系统或能力。这类应用不仅局限于编程，在各行各业都具备广阔前景。ask → edit → agent。ic：agentic 时代已经到来。

</details>


### [65] [AI产品经理必知：<em class="highlight">Agentic</em> Workflow的四种主流设计模型及思路](http://mp.weixin.qq.com/s?__biz=MzkxMDMzNzM3MQ==&mid=2247485545&idx=1&sn=5f6760e03713c98d261274c2c052ac40&chksm=c06e53b48e4cf3c8faf3d3e8d420e15c308e7a0e95ff875382bb919c4c574fdf6ce81edf3ee3#rd)
*大模型老蓝*

Main category: wechat.article

TL;DR: 飞书云文档 … >【llm]agentic workflow的四种常... 分享 .. + 最近修改：8分钟前 ai产品经理知识库 reflexion 互联网公开。下图是reflexion agent通过试错和自我反思（self-reflection）解决决策、编程、推 搜索 理任务的示例。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 飞书云文档 … >【llm]agentic workflow的四种常... 分享 .. + 最近修改：8分钟前 ai产品经理知识库 reflexion 互联网公开。下图是reflexion agent通过试错和自我反思（self-reflection）解决决策、编程、推 搜索 理任务的示例。

</details>


### [66] [知乎直答全面转向<em class="highlight">Agentic</em>模式：从“问啥答啥”到主动解决问题](http://mp.weixin.qq.com/s?__biz=MzE5ODI2NDM3Mw==&mid=2247486524&idx=3&sn=978e4dc051912a24b1303b847e72a768&chksm=97063ac767909e210a6440732a5c7a9fade7b7bd3153d1a79b7b3f8c8f783a832f5afe3fe0f8#rd)
*AI普瑞斯*

Main category: wechat.article

TL;DR: 9月25日消息，知乎旗下AI问答工具“直答”已完成全面升级，正式转型为Agentic（代理式）智能助手，从传统被动响应式AI转向具备自主规划能力的智能体，覆盖搜索、研究、学习、创作全场景需求。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 9月25日消息，知乎旗下AI问答工具“直答”已完成全面升级，正式转型为Agentic（代理式）智能助手，从传统被动响应式AI转向具备自主规划能力的智能体，覆盖搜索、研究、学习、创作全场景需求。

</details>


### [67] [麦肯锡报告揭示 2025 拐点：别再只是把 AI 当成工具了](http://mp.weixin.qq.com/s?__biz=Mzk4ODE0OTc2Nw==&mid=2247484484&idx=1&sn=ba4a12dc3e5f2bdb912668c8eaf4e14c&chksm=c42fe71b552f6d65018bd332aa032f89419420705b72e1ecbaaf2339708f80cb48b410adf424#rd)
*硅基生命AIGC*

Main category: wechat.article

TL;DR: Agentic AI 能够独立规划和执行复杂的多步骤任务，它不仅能调用工具，还能与其它 AI 协作。01。multistep reasoning to write， deploy， and test code，create software that could act autonomously，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 能够独立规划和执行复杂的多步骤任务，它不仅能调用工具，还能与其它 AI 协作。01。multistep reasoning to write， deploy， and test code，create software that could act autonomously，

</details>


### [68] [从Agent到<em class="highlight">Agentic</em>，一词之差，顶尖PM的思维跃迁｜吴恩达](http://mp.weixin.qq.com/s?__biz=MzkyOTk1MTc0OQ==&mid=2247484136&idx=1&sn=526d1df048cdc328e194d586a997552b&chksm=c3bfa6e47c9d1aad95557ad4f813baf1fa2c0bc2269a876e210d543c441e71d14bde0c7c26ef#rd)
*辛康在进化*

Main category: wechat.article

TL;DR: “agentic”作为形容词，则是一张进化地图。它承认了“智能”和“自主”是一个连续的光谱。一个AI产品可以只有10%的Agentic特性，也可以达到90%。这种视角将产品经理从技术定义的泥潭中解放出来，转而思考一个更具实践价值


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “agentic”作为形容词，则是一张进化地图。它承认了“智能”和“自主”是一个连续的光谱。一个AI产品可以只有10%的Agentic特性，也可以达到90%。这种视角将产品经理从技术定义的泥潭中解放出来，转而思考一个更具实践价值

</details>


### [69] [重磅！阿里开源6大AI<em class="highlight">代理</em>，<em class="highlight">Agentic</em>时代来袭：从工具调用到全自主决策，一夜颠覆程序员效率](http://mp.weixin.qq.com/s?__biz=MzYyMTEzNzI4Mg==&mid=2247483988&idx=1&sn=f4aa8a5dbf5039d6f2f970f97dbdca63&chksm=fef6e45d8d5c496e9ea94b7df677dea4c1715a7b26e99c249656aea5521cef98715fbbd80cc3#rd)
*每日科技简报引擎*

Main category: wechat.article

TL;DR: OpenAI意外开源gpt-oss模型，针对推理、工具使用和代理交互进行优化，支持Mixture-of-Experts和Grouped Query Attention。亮点：参数高效，提升长上下文处理速度2倍；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: OpenAI意外开源gpt-oss模型，针对推理、工具使用和代理交互进行优化，支持Mixture-of-Experts和Grouped Query Attention。亮点：参数高效，提升长上下文处理速度2倍；

</details>


### [70] [麦肯锡：AI<em class="highlight">智能体</em>（<em class="highlight">Agentic</em> AI）重构生命科学企业](http://mp.weixin.qq.com/s?__biz=MzIzNjUyNzU4MA==&mid=2247484882&idx=2&sn=f9e649bda39150aad53d0999c0d5ee39&chksm=e9bc3d2ef5246c761d3b4eaa6b4ee3b2124ba862b39278fb34f2c172be4c659c95a07449364a#rd)
*悉见Alpha*

Main category: wechat.article

TL;DR: 并且智能体可以促进业务增长并提升利润，有望在未来3-5年内，使制药企业的增长提升5-13个百分点、EBITDA提高3.4-5.4个百分点；在医疗器械企业中，增长提升3-7个百分点、EBITDA提高2.2-4.7个百分点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 并且智能体可以促进业务增长并提升利润，有望在未来3-5年内，使制药企业的增长提升5-13个百分点、EBITDA提高3.4-5.4个百分点；在医疗器械企业中，增长提升3-7个百分点、EBITDA提高2.2-4.7个百分点。

</details>


### [71] [德勤审计 Agent 深度解读：从 PairD 到 Omnia <em class="highlight">Agentic</em> AI 的演进路径](http://mp.weixin.qq.com/s?__biz=Mzk3NTkwNjU1OA==&mid=2247483694&idx=1&sn=c077d9f07a5ac3d475fd41d97f14e2cf&chksm=c52b6cffd8da60a75710aa5cf85730b85c5770080d6fb65f57ac5ae9acd24a78380a41d7e405#rd)
*大车谈AI*

Main category: wechat.article

TL;DR: 2025 年最新公告：在 Omnia 内部嵌入 GenAI/Agentic AI 功能，覆盖文档审阅、报表解读、草稿起草、风险识别、会计研究等模块。paird omnia copilot omnia agentic ai document document task summarization review orchestiration research report data q&a inter...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025 年最新公告：在 Omnia 内部嵌入 GenAI/Agentic AI 功能，覆盖文档审阅、报表解读、草稿起草、风险识别、会计研究等模块。paird omnia copilot omnia agentic ai document document task summarization review orchestiration research report data q&a interpretation ag

</details>


### [72] [可观测性的第四大支柱：<em class="highlight">Agentic</em> AI 的关键](http://mp.weixin.qq.com/s?__biz=MzIzMzcxMTUxOQ==&mid=2247505248&idx=1&sn=96d7886b0e5905f3caf7dcff59dd0378&chksm=e93b0abe7bf2edf2760b6a390e759135bdf614a0585ae697957af6d088da175a7b609d7a92f4#rd)
*云云众生s*

Main category: wechat.article

TL;DR: 译自：Observability’s Overlooked Fourth Pillar： Key for Agentic AI[1]作者：Amnon Heiman从可观测性 1.0 到 2.0 的转变增加了很多东西，但我认为我们在此过程中忘记了一些东西。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 译自：Observability’s Overlooked Fourth Pillar： Key for Agentic AI[1]作者：Amnon Heiman从可观测性 1.0 到 2.0 的转变增加了很多东西，但我认为我们在此过程中忘记了一些东西。

</details>
