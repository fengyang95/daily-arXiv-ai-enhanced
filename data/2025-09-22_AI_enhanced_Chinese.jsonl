{"id": "2509.15237", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15237", "abs": "https://arxiv.org/abs/2509.15237", "authors": ["Di Wen", "Kunyu Peng", "Junwei Zheng", "Yufan Chen", "Yitain Shi", "Jiale Wei", "Ruiping Liu", "Kailun Yang", "Rainer Stiefelhagen"], "title": "MICA: Multi-Agent Industrial Coordination Assistant", "comment": "The source code will be made publicly available at\n  https://github.com/Kratos-Wen/MICA", "summary": "Industrial workflows demand adaptive and trustworthy assistance that can\noperate under limited computing, connectivity, and strict privacy constraints.\nIn this work, we present MICA (Multi-Agent Industrial Coordination Assistant),\na perception-grounded and speech-interactive system that delivers real-time\nguidance for assembly, troubleshooting, part queries, and maintenance. MICA\ncoordinates five role-specialized language agents, audited by a safety checker,\nto ensure accurate and compliant support. To achieve robust step understanding,\nwe introduce Adaptive Step Fusion (ASF), which dynamically blends expert\nreasoning with online adaptation from natural speech feedback. Furthermore, we\nestablish a new multi-agent coordination benchmark across representative task\ncategories and propose evaluation metrics tailored to industrial assistance,\nenabling systematic comparison of different coordination topologies. Our\nexperiments demonstrate that MICA consistently improves task success,\nreliability, and responsiveness over baseline structures, while remaining\ndeployable on practical offline hardware. Together, these contributions\nhighlight MICA as a step toward deployable, privacy-preserving multi-agent\nassistants for dynamic factory environments. The source code will be made\npublicly available at https://github.com/Kratos-Wen/MICA.", "AI": {"tldr": "MICA\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u5de5\u4e1a\u534f\u8c03\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u97f3\u4ea4\u4e92\u63d0\u4f9b\u5b9e\u65f6\u88c5\u914d\u3001\u6545\u969c\u6392\u9664\u7b49\u6307\u5bfc\uff0c\u91c7\u7528\u4e94\u4e2a\u89d2\u8272\u4e13\u4e1a\u5316\u7684\u8bed\u8a00\u4ee3\u7406\u548c\u5b89\u5168\u68c0\u67e5\u5668\u786e\u4fdd\u51c6\u786e\u5408\u89c4\uff0c\u5728\u79bb\u7ebf\u786c\u4ef6\u4e0a\u90e8\u7f72\u4e14\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u3001\u8fde\u63a5\u6027\u548c\u4e25\u683c\u9690\u79c1\u7ea6\u675f\u4e0b\u8fd0\u884c\u7684\u9002\u5e94\u6027\u5f3a\u7684\u53ef\u4fe1\u52a9\u624b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e94\u4e2a\u89d2\u8272\u4e13\u4e1a\u5316\u7684\u8bed\u8a00\u4ee3\u7406\u534f\u8c03\u5de5\u4f5c\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6b65\u9aa4\u878d\u5408(ASF)\u52a8\u6001\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u548c\u81ea\u7136\u8bed\u97f3\u53cd\u9988\u7684\u5728\u7ebf\u9002\u5e94\uff0c\u5efa\u7acb\u65b0\u7684\u591a\u4ee3\u7406\u534f\u8c03\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMICA\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u53ef\u9760\u6027\u548c\u54cd\u5e94\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u7ed3\u6784\uff0c\u53ef\u5728\u5b9e\u9645\u79bb\u7ebf\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002", "conclusion": "MICA\u662f\u671d\u7740\u53ef\u90e8\u7f72\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u591a\u4ee3\u7406\u52a9\u624b\u5728\u52a8\u6001\u5de5\u5382\u73af\u5883\u4e2d\u5e94\u7528\u7684\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2509.15291", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.15291", "abs": "https://arxiv.org/abs/2509.15291", "authors": ["Federico Taschin", "Abderrahmane Lazaraq", "Ozan K. Tonguz", "Inci Ozgunes"], "title": "The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI", "comment": null, "summary": "The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart\ntransportation networks has increased significantly in the last few years.\nAmong these ML and AI approaches, Reinforcement Learning (RL) has been shown to\nbe a very promising approach by several authors. However, a problem with using\nReinforcement Learning in Traffic Signal Control is the reliability of the\ntrained RL agents due to the dynamically changing distribution of the input\ndata with respect to the distribution of the data used for training. This\npresents a major challenge and a reliability problem for the trained network of\nAI agents and could have very undesirable and even detrimental consequences if\na suitable solution is not found. Several researchers have tried to address\nthis problem using different approaches. In particular, Meta Reinforcement\nLearning (Meta RL) promises to be an effective solution. In this paper, we\nevaluate and analyze a state-of-the-art Meta RL approach called MetaLight and\nshow that, while under certain conditions MetaLight can indeed lead to\nreasonably good results, under some other conditions it might not perform well\n(with errors of up to 22%), suggesting that Meta RL schemes are often not\nrobust enough and can even pose major reliability problems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86MetaLight\u8fd9\u4e00\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u867d\u7136\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u53ef\u80fd\u51fa\u73b0\u9ad8\u8fbe22%\u7684\u9519\u8bef\uff0c\u8868\u660e\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u5f80\u5f80\u4e0d\u591f\u7a33\u5065\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7f51\u7edc\u4e2d\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u8f93\u5165\u6570\u636e\u5206\u5e03\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u8bad\u7ec3\u4ee3\u7406\u53ef\u9760\u6027\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u548c\u5206\u6790\u6700\u5148\u8fdb\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5MetaLight\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "MetaLight\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u80fd\u4ea7\u751f\u76f8\u5f53\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9519\u8bef\u7387\u9ad8\u8fbe22%\u3002", "conclusion": "\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u901a\u5e38\u4e0d\u591f\u7a33\u5065\uff0c\u751a\u81f3\u53ef\u80fd\u5e26\u6765\u91cd\u5927\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.15283", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "I.2.7; F.2.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2509.15283", "abs": "https://arxiv.org/abs/2509.15283", "authors": ["Kadin Matotek", "Heather Cassel", "Md Amiruzzaman", "Linh B. Ngo"], "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges", "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern\n  2025", "summary": "This study examines the performance of today's open-source, locally hosted\nlarge-language models (LLMs) in handling complex competitive programming tasks\nwith extended problem descriptions and contexts. Building on the original\nFramework for AI-driven Code Generation Evaluation (FACE), the authors retrofit\nthe pipeline to work entirely offline through the Ollama runtime, collapsing\nFACE's sprawling per-problem directory tree into a handful of consolidated JSON\nfiles, and adding robust checkpointing so multi-day runs can resume after\nfailures. The enhanced framework generates, submits, and records solutions for\nthe full Kattis corpus of 3,589 problems across eight code-oriented models\nranging from 6.7-9 billion parameters. The submission results show that the\noverall pass@1 accuracy is modest for the local models, with the best models\nperforming at approximately half the acceptance rate of the proprietary models,\nGemini 1.5 and ChatGPT-4. These findings expose a persistent gap between\nprivate, cost-controlled LLM deployments and state-of-the-art proprietary\nservices, yet also highlight the rapid progress of open models and the\npractical benefits of an evaluation workflow that organizations can replicate\non in-house hardware.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5f00\u6e90\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7f16\u7a0b\u7ade\u8d5b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u7387\u7ea6\u4e3a\u4e13\u6709\u6a21\u578b\u7684\u4e00\u534a\uff0c\u4f46\u5c55\u793a\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u5feb\u901f\u8fdb\u6b65\u548c\u672c\u5730\u8bc4\u4f30\u6d41\u7a0b\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u5f00\u6e90\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u6269\u5c55\u95ee\u9898\u63cf\u8ff0\u548c\u4e0a\u4e0b\u6587\u7684\u590d\u6742\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u5176\u4e0e\u4e13\u6709\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8eFACE\u6846\u67b6\u6539\u9020\u4e3a\u5b8c\u5168\u79bb\u7ebf\u8fd0\u884c\u7684\u8bc4\u4f30\u7ba1\u9053\uff0c\u4f7f\u7528Ollama\u8fd0\u884c\u65f6\uff0c\u5c06\u76ee\u5f55\u7ed3\u6784\u6574\u5408\u4e3aJSON\u6587\u4ef6\u5e76\u6dfb\u52a0\u68c0\u67e5\u70b9\u529f\u80fd\uff0c\u5bf9Kattis\u8bed\u6599\u5e93\u76843,589\u4e2a\u95ee\u9898\u57288\u4e2a\u4ee3\u7801\u5bfc\u5411\u6a21\u578b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u672c\u5730\u6a21\u578b\u7684\u6574\u4f53pass@1\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u6700\u4f73\u6a21\u578b\u7684\u8868\u73b0\u7ea6\u4e3a\u4e13\u6709\u6a21\u578b(Gemini 1.5\u548cChatGPT-4)\u63a5\u53d7\u7387\u7684\u4e00\u534a\u3002", "conclusion": "\u5f00\u6e90\u672c\u5730\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u4e13\u6709\u670d\u52a1\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u8fdb\u6b65\u8fc5\u901f\uff0c\u4e14\u672c\u5730\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u5177\u6709\u53ef\u5728\u5185\u90e8\u786c\u4ef6\u4e0a\u590d\u5236\u7684\u5b9e\u9645\u4f18\u52bf\u3002", "topic": "swe benchmark"}}
{"id": "2509.15336", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15336", "abs": "https://arxiv.org/abs/2509.15336", "authors": ["Humam Kourani", "Anton Antonov", "Alessandro Berti", "Wil M. P. van der Aalst"], "title": "Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling", "comment": "The Version of Record of this contribution will be published in the\n  proceedings of the 2nd International Workshop on Generative AI for Process\n  Mining (GenAI4PM 2025). This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "The utility of Large Language Models (LLMs) in analytical tasks is rooted in\ntheir vast pre-trained knowledge, which allows them to interpret ambiguous\ninputs and infer missing information. However, this same capability introduces\na critical risk of what we term knowledge-driven hallucination: a phenomenon\nwhere the model's output contradicts explicit source evidence because it is\noverridden by the model's generalized internal knowledge. This paper\ninvestigates this phenomenon by evaluating LLMs on the task of automated\nprocess modeling, where the goal is to generate a formal business process model\nfrom a given source artifact. The domain of Business Process Management (BPM)\nprovides an ideal context for this study, as many core business processes\nfollow standardized patterns, making it likely that LLMs possess strong\npre-trained schemas for them. We conduct a controlled experiment designed to\ncreate scenarios with deliberate conflict between provided evidence and the\nLLM's background knowledge. We use inputs describing both standard and\ndeliberately atypical process structures to measure the LLM's fidelity to the\nprovided evidence. Our work provides a methodology for assessing this critical\nreliability issue and raises awareness of the need for rigorous validation of\nAI-generated artifacts in any evidence-based domain.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u9a71\u52a8\u5e7b\u89c9\u65b9\u9762\u7684\u98ce\u9669\uff0c\u5373\u5728\u81ea\u52a8\u5316\u6d41\u7a0b\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u4f1a\u8986\u76d6\u660e\u786e\u7684\u6e90\u8bc1\u636e\u5bfc\u81f4\u8f93\u51fa\u77db\u76fe\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5206\u6790\u4efb\u52a1\u4e2d\u7531\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u5bfc\u81f4\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bc1\u636e\u4e0e\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u51b2\u7a81\u65f6\u4ea7\u751f\u7684\u77e5\u8bc6\u9a71\u52a8\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5728\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u9886\u57df\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u4f7f\u7528\u6807\u51c6\u548c\u975e\u5178\u578b\u6d41\u7a0b\u7ed3\u6784\u4f5c\u4e3a\u8f93\u5165\uff0c\u6d4b\u91cfLLMs\u5bf9\u63d0\u4f9b\u8bc1\u636e\u7684\u5fe0\u5b9e\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u786e\u5b9e\u5b58\u5728\u77e5\u8bc6\u9a71\u52a8\u5e7b\u89c9\u95ee\u9898\uff0c\u5f53\u63d0\u4f9b\u7684\u8bc1\u636e\u4e0e\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u51b2\u7a81\u65f6\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u5f0f\u800c\u975e\u5b9e\u9645\u8bc1\u636e\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bc4\u4f30\u8fd9\u79cd\u5173\u952e\u53ef\u9760\u6027\u95ee\u9898\u7684\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u5728\u4efb\u4f55\u57fa\u4e8e\u8bc1\u636e\u7684\u9886\u57df\u4e2d\u90fd\u9700\u8981\u5bf9AI\u751f\u6210\u4ea7\u7269\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2509.15397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15397", "abs": "https://arxiv.org/abs/2509.15397", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "LoCaL: Countering Surface Bias in Code Evaluation Metrics", "comment": null, "summary": "With the increasing popularity of large language models (LLMs) and LLM-based\nagents, reliable and effective code evaluation metrics (CEMs) have become\ncrucial for progress across several software engineering tasks. While popular\nbenchmarks often provide test cases to assess the correctness of generated\ncode, crafting and executing test cases is expensive. Reference-based CEMs\nprovide a cheaper alternative by scoring a candidate program based on its\nfunctional similarity to a reference. Although prior research has focused on\nreporting the weak correlation between these CEMs and functional correctness,\nthe causes are only assumed, and plausible solutions remain unexplored. In this\nwork, we critically evaluate four state-of-the-art reference-based CEMs,\nrevealing their strong bias towards surface-level features rather than code\nfunctionality. Despite this surface bias, current evaluation datasets for these\nCEMs rarely include code pairs that are surface-similar yet functionally\ndissimilar, or functionally similar yet surface-dissimilar. To mitigate this\ngap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117\ncode pairs at both the method and program levels. Each pair is labeled with a\nfunctional similarity score and aims to target regions where CEMs are likely to\nperform poorly. The functional similarity scores are calculated through\ndifferential fuzzing, which eliminates the need for predefined test cases and,\nat the same time, improves the reliability of the scores by executing an order\nof magnitude more tests than prior work. We find that all four CEMs show\nsignificant performance degradation on LoCaL, compared to the baselines.\nFinally, based on our findings, we draw the implication that exposing CEMs to\nLoCaL-like data might facilitate the development of metrics that are robust to\nsurface bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LoCaL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u53c2\u8003\u7684\u4ee3\u7801\u8bc4\u4f30\u6307\u6807(CEMs)\uff0c\u53d1\u73b0\u73b0\u6709CEMs\u5b58\u5728\u8868\u9762\u7279\u5f81\u504f\u89c1\uff0c\u5728\u529f\u80fd\u76f8\u4f3c\u4f46\u8868\u9762\u4e0d\u540c\u6216\u8868\u9762\u76f8\u4f3c\u4f46\u529f\u80fd\u4e0d\u540c\u7684\u4ee3\u7801\u5bf9\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740LLM\u548c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65e5\u76ca\u6d41\u884c\uff0c\u53ef\u9760\u7684\u4ee3\u7801\u8bc4\u4f30\u6307\u6807\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684CEMs\u4e0e\u529f\u80fd\u6b63\u786e\u6027\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u4f46\u539f\u56e0\u672a\u88ab\u6df1\u5165\u63a2\u7a76\uff0c\u89e3\u51b3\u65b9\u6848\u4e5f\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86LoCaL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3117\u4e2a\u65b9\u6cd5\u7ea7\u548c\u7a0b\u5e8f\u7ea7\u4ee3\u7801\u5bf9\uff0c\u901a\u8fc7\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u8ba1\u7b97\u529f\u80fd\u76f8\u4f3c\u6027\u5206\u6570\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6d4b\u8bd5\u7528\u4f8b\u4e14\u6267\u884c\u66f4\u591a\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684\u53c2\u8003\u578bCEMs\u3002", "result": "\u53d1\u73b0\u6240\u6709\u56db\u79cdCEMs\u5728LoCaL\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u76f8\u6bd4\u57fa\u7ebf\u90fd\u6709\u663e\u8457\u4e0b\u964d\uff0c\u663e\u793a\u51fa\u5bf9\u8868\u9762\u7279\u5f81\u7684\u5f3a\u70c8\u504f\u89c1\u800c\u975e\u4ee3\u7801\u529f\u80fd\u3002", "conclusion": "\u5c06CEMs\u66b4\u9732\u4e8eLoCaL\u7c7b\u4f3c\u6570\u636e\u53ef\u80fd\u6709\u52a9\u4e8e\u5f00\u53d1\u5bf9\u8868\u9762\u504f\u89c1\u5177\u6709\u9c81\u68d2\u6027\u7684\u8bc4\u4f30\u6307\u6807\u3002", "topic": "swe benchmark"}}
{"id": "2509.15366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15366", "abs": "https://arxiv.org/abs/2509.15366", "authors": ["Andrejs Sorstkins", "Josh Bailey", "Dr Alistair Baron"], "title": "Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context", "comment": "Dissertation and research project created in collaboration with\n  JobFair LTD", "summary": "The rapid evolution of neural architectures - from multilayer perceptrons to\nlarge-scale Transformer-based models - has enabled language models (LLMs) to\nexhibit emergent agentic behaviours when equipped with memory, planning, and\nexternal tool use. However, their inherent stochasticity and multi-step\ndecision processes render classical evaluation methods inadequate for\ndiagnosing agentic performance. This work introduces a diagnostic framework for\nexpert systems that not only evaluates but also facilitates the transfer of\nexpert behaviour into LLM-powered agents. The framework integrates (i) curated\ngolden datasets of expert annotations, (ii) silver datasets generated through\ncontrolled behavioural mutation, and (iii) an LLM-based Agent Judge that scores\nand prescribes targeted improvements. These prescriptions are embedded into a\nvectorized recommendation map, allowing expert interventions to propagate as\nreusable improvement trajectories across multiple system instances. We\ndemonstrate the framework on a multi-agent recruiter-assistant system, showing\nthat it uncovers latent cognitive failures - such as biased phrasing,\nextraction drift, and tool misrouting - while simultaneously steering agents\ntoward expert-level reasoning and style. The results establish a foundation for\nstandardized, reproducible expert behaviour transfer in stochastic,\ntool-augmented LLM agents, moving beyond static evaluation to active expert\nsystem refinement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5177\u6709\u4e13\u5bb6\u884c\u4e3a\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u9ec4\u91d1\u6570\u636e\u96c6\u3001\u94f6\u6570\u636e\u96c6\u548c\u667a\u80fd\u4f53\u6cd5\u5b98\u6765\u5b9e\u73b0\u4e13\u5bb6\u884c\u4e3a\u7684\u6807\u51c6\u5316\u8f6c\u79fb\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8bca\u65ad\u5177\u6709\u968f\u673a\u6027\u548c\u591a\u6b65\u51b3\u7b56\u8fc7\u7a0b\u7684LLM\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u4fc3\u8fdb\u4e13\u5bb6\u884c\u4e3a\u5411LLM\u667a\u80fd\u4f53\u7684\u8f6c\u79fb\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u4e13\u5bb6\u6807\u6ce8\u7684\u9ec4\u91d1\u6570\u636e\u96c6\uff0c(2)\u901a\u8fc7\u53d7\u63a7\u884c\u4e3a\u7a81\u53d8\u751f\u6210\u7684\u94f6\u6570\u636e\u96c6\uff0c(3)\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6cd5\u5b98\u8fdb\u884c\u8bc4\u5206\u548c\u9488\u5bf9\u6027\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u62db\u8058\u52a9\u7406\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u53d1\u73b0\u6f5c\u5728\u8ba4\u77e5\u5931\u8d25\uff08\u5982\u504f\u89c1\u63aa\u8f9e\u3001\u63d0\u53d6\u6f02\u79fb\u3001\u5de5\u5177\u8bef\u8def\u7531\uff09\uff0c\u540c\u65f6\u5f15\u5bfc\u667a\u80fd\u4f53\u8fbe\u5230\u4e13\u5bb6\u7ea7\u63a8\u7406\u548c\u98ce\u683c\u3002", "conclusion": "\u4e3a\u968f\u673a\u6027\u3001\u5de5\u5177\u589e\u5f3a\u7684LLM\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u4e13\u5bb6\u884c\u4e3a\u8f6c\u79fb\u57fa\u7840\uff0c\u4ece\u9759\u6001\u8bc4\u4f30\u8f6c\u5411\u4e3b\u52a8\u7684\u4e13\u5bb6\u7cfb\u7edf\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2509.15567", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15567", "abs": "https://arxiv.org/abs/2509.15567", "authors": ["Hongyu Kuang", "Ning Zhang", "Hui Gao", "Xin Zhou", "Wesley K. G. Assun\u00e7\u00e3o", "Xiaoxing Ma", "Dong Shao", "Guoping Rong", "He Zhang"], "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "comment": null, "summary": "Commit messages are valuable resources for describing why code changes are\ncommitted to repositories in version control systems (e.g., Git). They\neffectively help developers understand code changes and better perform software\nmaintenance tasks. Unfortunately, developers often neglect to write\nhigh-quality commit messages in practice. Therefore, a growing body of work is\nproposed to generate commit messages automatically. These works all\ndemonstrated that how to organize and represent code changes is vital in\ngenerating good commit messages, including the use of fine-grained graphs or\nembeddings to better represent code changes. In this study, we choose an\nalternative way to condense code changes before generation, i.e., proposing\nbrief yet concise text templates consisting of the following three parts: (1)\nsummarized code changes, (2) elicited comments, and (3) emphasized code\nidentifiers. Specifically, we first condense code changes by using our proposed\ntemplates with the help of a heuristic-based tool named ChangeScribe, and then\nfine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding\ncommit messages. Our proposed templates better utilize pre-trained language\nmodels, while being naturally brief and readable to complement generated commit\nmessages for developers. Our evaluation based on a widely used dataset showed\nthat our approach can outperform six baselines in terms of BLEU-Norm, METEOR,\nand ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,\nrespectively. The ablation study and human evaluation also provide further\ninsights into the effectiveness of our approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u6a21\u677f\u6765\u538b\u7f29\u4ee3\u7801\u53d8\u66f4\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528ChangeScribe\u5de5\u5177\u751f\u6210\u5305\u542b\u4ee3\u7801\u53d8\u66f4\u6458\u8981\u3001\u6ce8\u91ca\u63d0\u53d6\u548c\u91cd\u8981\u6807\u8bc6\u7b26\u7684\u6a21\u677f\uff0c\u7136\u540e\u57fa\u4e8eCodeLlama-7B\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u63d0\u4ea4\u6d88\u606f\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u5b9e\u8df5\u4e2d\u7ecf\u5e38\u5ffd\u89c6\u7f16\u5199\u9ad8\u8d28\u91cf\u7684\u63d0\u4ea4\u6d88\u606f\uff0c\u800c\u73b0\u6709\u7684\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u9700\u8981\u66f4\u597d\u5730\u7ec4\u7ec7\u548c\u8868\u793a\u4ee3\u7801\u53d8\u66f4\u4fe1\u606f\u3002", "method": "\u9996\u5148\u4f7f\u7528\u542f\u53d1\u5f0f\u5de5\u5177ChangeScribe\u751f\u6210\u5305\u542b\u4e09\u90e8\u5206\u5185\u5bb9\u7684\u6587\u672c\u6a21\u677f\uff08\u4ee3\u7801\u53d8\u66f4\u6458\u8981\u3001\u63d0\u53d6\u7684\u6ce8\u91ca\u3001\u5f3a\u8c03\u7684\u4ee3\u7801\u6807\u8bc6\u7b26\uff09\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u6a21\u677f\u5bf9CodeLlama-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6765\u751f\u6210\u63d0\u4ea4\u6d88\u606f\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728BLEU-Norm\u3001METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u4f18\u4e8e\u516d\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u5347\u5206\u522b\u4e3a51.7%\u300178.7%\u548c62.5%\u3002\u6d88\u878d\u7814\u7a76\u548c\u4eba\u5de5\u8bc4\u4f30\u4e5f\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6587\u672c\u6a21\u677f\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u7b80\u6d01\u53ef\u8bfb\u7684\u63d0\u4ea4\u6d88\u606f\uff0c\u6709\u6548\u8f85\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u4ee3\u7801\u53d8\u66f4\u3002", "topic": "swe application"}}
{"id": "2509.15635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15635", "abs": "https://arxiv.org/abs/2509.15635", "authors": ["Pan Tang", "Shixiang Tang", "Huanqi Pu", "Zhiqing Miao", "Zhixing Wang"], "title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents", "comment": "18 pages, 22 figures", "summary": "This paper presents MicroRCA-Agent, an innovative solution for microservice\nroot cause analysis based on large language model agents, which constructs an\nintelligent fault root cause localization system with multimodal data fusion.\nThe technical innovations are embodied in three key aspects: First, we combine\nthe pre-trained Drain log parsing algorithm with multi-level data filtering\nmechanism to efficiently compress massive logs into high-quality fault\nfeatures. Second, we employ a dual anomaly detection approach that integrates\nIsolation Forest unsupervised learning algorithms with status code validation\nto achieve comprehensive trace anomaly identification. Third, we design a\nstatistical symmetry ratio filtering mechanism coupled with a two-stage LLM\nanalysis strategy to enable full-stack phenomenon summarization across\nnode-service-pod hierarchies. The multimodal root cause analysis module\nleverages carefully designed cross-modal prompts to deeply integrate multimodal\nanomaly information, fully exploiting the cross-modal understanding and logical\nreasoning capabilities of large language models to generate structured analysis\nresults encompassing fault components, root cause descriptions, and reasoning\ntrace. Comprehensive ablation studies validate the complementary value of each\nmodal data and the effectiveness of the system architecture. The proposed\nsolution demonstrates superior performance in complex microservice fault\nscenarios, achieving a final score of 50.71. The code has been released at:\nhttps://github.com/tangpan360/MicroRCA-Agent.", "AI": {"tldr": "MicroRCA-Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5fae\u670d\u52a1\u6839\u56e0\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u521b\u65b0\u7684\u6280\u672f\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u7684\u6545\u969c\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u5fae\u670d\u52a1\u73af\u5883\u4e2d\u590d\u6742\u6545\u969c\u6839\u56e0\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u6765\u63d0\u5347\u6545\u969c\u5206\u6790\u6548\u7387\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684Drain\u65e5\u5fd7\u89e3\u6790\u7b97\u6cd5\u548c\u591a\u7ea7\u6570\u636e\u8fc7\u6ee4\u673a\u5236\u538b\u7f29\u65e5\u5fd7\uff1b\u91c7\u7528Isolation Forest\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u72b6\u6001\u7801\u9a8c\u8bc1\u7684\u53cc\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff1b\u8bbe\u8ba1\u7edf\u8ba1\u5bf9\u79f0\u6bd4\u8fc7\u6ee4\u673a\u5236\u548c\u4e24\u9636\u6bb5LLM\u5206\u6790\u7b56\u7565\uff1b\u5229\u7528\u8de8\u6a21\u6001\u63d0\u793a\u6df1\u5ea6\u6574\u5408\u591a\u6a21\u6001\u5f02\u5e38\u4fe1\u606f\u3002", "result": "\u5728\u590d\u6742\u5fae\u670d\u52a1\u6545\u969c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6700\u7ec8\u5f97\u5206\u4e3a50.71\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u6001\u6570\u636e\u7684\u4e92\u8865\u4ef7\u503c\u548c\u7cfb\u7edf\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "MicroRCA-Agent\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548cLLM\u9a71\u52a8\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u5fae\u670d\u52a1\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2509.15971", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15971", "abs": "https://arxiv.org/abs/2509.15971", "authors": ["Owen Truong", "Terrence Zhang", "Arnav Marchareddy", "Ryan Lee", "Jeffery Busold", "Michael Socas", "Eman Abdullah AlOmar"], "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines", "comment": null, "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLeakageDetector\u7684VS Code\u6269\u5c55\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u4fee\u590dJupyter Notebook\u4e2d\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5305\u62ec\u91cd\u53e0\u6cc4\u9732\u3001\u9884\u5904\u7406\u6cc4\u9732\u548c\u591a\u6d4b\u8bd5\u6cc4\u9732\uff0c\u5e76\u63d0\u4f9b\u4f20\u7edf\u4fee\u590d\u548cLLM\u9a71\u52a8\u7684\u4fee\u590d\u65b9\u6cd5\u3002", "motivation": "\u5e2e\u52a9\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u8bc6\u522b\u548c\u7ea0\u6b63\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u907f\u514d\u6d4b\u8bd5\u6570\u636e\u4fe1\u606f\u610f\u5916\u5305\u542b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u8bef\u5bfc\u3002", "method": "\u5f00\u53d1VS Code\u6269\u5c55LeakageDetector\uff0c\u68c0\u6d4b\u4e09\u79cd\u4e3b\u8981\u6570\u636e\u6cc4\u9732\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4e24\u79cd\u4fee\u590d\u673a\u5236\uff1a\u4f20\u7edf\u5feb\u901f\u4fee\u590d\u548cLLM\u9a71\u52a8\u7684\u6307\u5bfc\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6570\u636e\u6cc4\u9732\u95ee\u9898\u7684\u5de5\u5177\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4fee\u590d\u6307\u5bfc\u3002", "conclusion": "LeakageDetector\u6269\u5c55\u80fd\u591f\u5e2e\u52a9ML\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7ba1\u7406\u6570\u636e\u5206\u5272\uff0c\u907f\u514d\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2509.15269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15269", "abs": "https://arxiv.org/abs/2509.15269", "authors": ["Elisabetta Rocchetti"], "title": "Modeling Transformers as complex networks to analyze learning dynamics", "comment": null, "summary": "The process by which Large Language Models (LLMs) acquire complex\ncapabilities during training remains a key open question in mechanistic\ninterpretability. This project investigates whether these learning dynamics can\nbe characterized through the lens of Complex Network Theory (CNT). I introduce\na novel methodology to represent a Transformer-based LLM as a directed,\nweighted graph where nodes are the model's computational components (attention\nheads and MLPs) and edges represent causal influence, measured via an\nintervention-based ablation technique. By tracking the evolution of this\ncomponent-graph across 143 training checkpoints of the Pythia-14M model on a\ncanonical induction task, I analyze a suite of graph-theoretic metrics. The\nresults reveal that the network's structure evolves through distinct phases of\nexploration, consolidation, and refinement. Specifically, I identify the\nemergence of a stable hierarchy of information spreader components and a\ndynamic set of information gatherer components, whose roles reconfigure at key\nlearning junctures. This work demonstrates that a component-level network\nperspective offers a powerful macroscopic lens for visualizing and\nunderstanding the self-organizing principles that drive the formation of\nfunctional circuits in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u590d\u6742\u7f51\u7edc\u7406\u8bba\u5206\u6790LLM\u8bad\u7ec3\u52a8\u6001\uff0c\u5c06Transformer\u6a21\u578b\u8868\u793a\u4e3a\u6709\u5411\u52a0\u6743\u56fe\uff0c\u8ffd\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f51\u7edc\u7ed3\u6784\u7684\u6f14\u5316\uff0c\u53d1\u73b0\u63a2\u7d22\u3001\u5de9\u56fa\u548c\u7cbe\u70bc\u4e09\u4e2a\u9636\u6bb5\uff0c\u8bc6\u522b\u51fa\u4fe1\u606f\u4f20\u64ad\u5668\u548c\u6536\u96c6\u5668\u7684\u5c42\u7ea7\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u4f55\u83b7\u5f97\u590d\u6742\u80fd\u529b\u662f\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5173\u952e\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u590d\u6742\u7f51\u7edc\u7406\u8bba\u7684\u89c6\u89d2\u6765\u8868\u5f81\u8fd9\u4e9b\u5b66\u4e60\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u65b0\u65b9\u6cd5\u5c06\u57fa\u4e8eTransformer\u7684LLM\u8868\u793a\u4e3a\u6709\u5411\u52a0\u6743\u56fe\uff08\u8282\u70b9\u4e3a\u6ce8\u610f\u529b\u5934\u548cMLP\uff0c\u8fb9\u8868\u793a\u56e0\u679c\u5f71\u54cd\uff09\uff0c\u5728Pythia-14M\u6a21\u578b\u7684143\u4e2a\u8bad\u7ec3\u68c0\u67e5\u70b9\u4e0a\u8ffd\u8e2a\u56fe\u8bba\u6307\u6807\u6f14\u5316\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u7ed3\u6784\u7ecf\u5386\u63a2\u7d22\u3001\u5de9\u56fa\u548c\u7cbe\u70bc\u4e09\u4e2a\u660e\u663e\u9636\u6bb5\uff0c\u8bc6\u522b\u51fa\u7a33\u5b9a\u7684\u4fe1\u606f\u4f20\u64ad\u5668\u5c42\u7ea7\u7ed3\u6784\u548c\u52a8\u6001\u7684\u4fe1\u606f\u6536\u96c6\u5668\u7ec4\u4ef6\uff0c\u8fd9\u4e9b\u89d2\u8272\u5728\u5173\u952e\u5b66\u4e60\u8282\u70b9\u91cd\u65b0\u914d\u7f6e\u3002", "conclusion": "\u7ec4\u4ef6\u7ea7\u7f51\u7edc\u89c6\u89d2\u4e3a\u53ef\u89c6\u5316\u548c\u7406\u89e3LLM\u4e2d\u529f\u80fd\u7535\u8def\u5f62\u6210\u7684\u81ea\u7ec4\u7ec7\u539f\u5219\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b8f\u89c2\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2509.15690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15690", "abs": "https://arxiv.org/abs/2509.15690", "authors": ["Weixuan Sun", "Jucai Zhai", "Dengfeng Liu", "Xin Zhang", "Xiaojun Wu", "Qiaobo Hao", "AIMgroup", "Yang Fang", "Jiuyang Tang"], "title": "CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair", "comment": null, "summary": "The automated repair of C++ compilation errors presents a significant\nchallenge, the resolution of which is critical for developer productivity.\nProgress in this domain is constrained by two primary factors: the scarcity of\nlarge-scale, high-fidelity datasets and the limitations of conventional\nsupervised methods, which often fail to generate semantically correct\npatches.This paper addresses these gaps by introducing a comprehensive\nframework with three core contributions. First, we present CCrepair, a novel,\nlarge-scale C++ compilation error dataset constructed through a sophisticated\ngenerate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)\nparadigm guided by a hybrid reward signal, shifting the focus from mere\ncompilability to the semantic quality of the fix. Finally, we establish the\nrobust, two-stage evaluation system providing this signal, centered on an\nLLM-as-a-Judge whose reliability has been rigorously validated against the\ncollective judgments of a panel of human experts. This integrated approach\naligns the training objective with generating high-quality, non-trivial patches\nthat are both syntactically and semantically correct. The effectiveness of our\napproach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct\nmodel achieved performance comparable to a Qwen2.5-14B-Instruct model,\nvalidating the efficiency of our training paradigm. Our work provides the\nresearch community with a valuable new dataset and a more effective paradigm\nfor training and evaluating robust compilation repair models, paving the way\nfor more practical and reliable automated programming assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86CCrepair\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684C++\u7f16\u8bd1\u9519\u8bef\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\u548cLLM\u8bc4\u4f30\u7cfb\u7edf\u63d0\u5347\u4fee\u590d\u8d28\u91cf", "motivation": "\u89e3\u51b3C++\u7f16\u8bd1\u9519\u8bef\u81ea\u52a8\u4fee\u590d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u8bed\u4e49\u6b63\u786e\u7684\u8865\u4e01", "method": "\u6784\u5efaCCrepair\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u914d\u5408\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u8fdb\u884c\u4e24\u9636\u6bb5\u9a8c\u8bc1", "result": "RL\u8bad\u7ec3\u7684Qwen2.5-1.5B\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u4e0eQwen2.5-14B\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u8303\u5f0f\u7684\u6548\u7387", "conclusion": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u8bc4\u4f30\u8303\u5f0f\uff0c\u4e3a\u5b9e\u7528\u53ef\u9760\u7684\u81ea\u52a8\u5316\u7f16\u7a0b\u52a9\u624b\u94fa\u5e73\u9053\u8def", "topic": "swe application"}}
{"id": "2509.16187", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16187", "abs": "https://arxiv.org/abs/2509.16187", "authors": ["Ali Reza Ibrahimzada", "Brandon Paulsen", "Reyhaneh Jabbarvand", "Joey Dodds", "Daniel Kroening"], "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair", "comment": null, "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.", "AI": {"tldr": "MatchFixAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ee3\u7801\u7ffb\u8bd1\u7684\u7b49\u4ef7\u6027\u9a8c\u8bc1\u548c\u4fee\u590d\uff0c\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u57282199\u4e2a\u7ffb\u8bd1\u5bf9\u4e2d\u5b9e\u73b0\u4e8699.2%\u7684\u9a8c\u8bc1\u8986\u76d6\u7387\u548c50.6%\u7684\u4fee\u590d\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7ffb\u8bd1\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u5de5\u7a0b\u5f00\u9500\u5927\u3001\u4f9d\u8d56\u4e0d\u5145\u5206\u6d4b\u8bd5\u5957\u4ef6\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b49\u4ef7\u6027\u8bef\u5224\u548c\u4fee\u590d\u6548\u679c\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u548c\u51c6\u786e\u7684\u9a8c\u8bc1\u4fee\u590d\u6846\u67b6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u7b49\u4ef7\u6027\u9a8c\u8bc1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff08\u8bed\u4e49\u5206\u6790\u3001\u6d4b\u8bd5\u751f\u6210\u6267\u884c\u3001\u4fee\u590d\u3001\u6700\u7ec8\u88c1\u51b3\uff09\uff0c\u5229\u7528LLM\u8fdb\u884c\u7f16\u7a0b\u8bed\u8a00\u65e0\u5173\u7684\u5206\u6790\u548c\u4fee\u590d\u3002", "result": "\u57286\u79cd\u7f16\u7a0b\u8bed\u8a00\u5bf9\u76842199\u4e2a\u7ffb\u8bd1\u5bf9\u4e0a\uff0c\u5b9e\u73b0\u4e8699.2%\u7684\u9a8c\u8bc1\u8986\u76d6\u7387\uff0c72.8%\u4e0e\u5148\u524d\u5de5\u4f5c\u4e00\u81f4\uff0c60.7%\u4e0d\u4e00\u81f4\u65f6MatchFixAgent\u6b63\u786e\uff1b\u4fee\u590d\u6210\u529f\u738750.6% vs \u5148\u524d\u5de5\u4f5c\u768418.5%\u3002", "conclusion": "MatchFixAgent\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7f16\u7a0b\u8bed\u8a00\u9002\u5e94\u6027\u548c\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4ee3\u7801\u7ffb\u8bd1\u7684\u7b49\u4ef7\u6027\u9a8c\u8bc1\u548c\u4fee\u590d\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2509.16112", "categories": ["cs.CL", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16112", "abs": "https://arxiv.org/abs/2509.16112", "authors": ["Sheng Zhang", "Yifan Ding", "Shuquan Lian", "Shun Song", "Hui Li"], "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion", "comment": "EMNLP 2025", "summary": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.", "AI": {"tldr": "CodeRAG\u662f\u4e00\u4e2a\u9488\u5bf9\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u67e5\u8be2\u6784\u5efa\u3001\u591a\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u548c\u6700\u4f73\u5339\u914d\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u8865\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u65b9\u6cd5\u5b58\u5728\u67e5\u8be2\u6784\u5efa\u4e0d\u5f53\u3001\u5355\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u4ee5\u53ca\u4ee3\u7801\u68c0\u7d22\u5668\u4e0eLLM\u4e0d\u5bf9\u9f50\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u3002", "method": "\u63d0\u51faCodeRAG\u6846\u67b6\uff0c\u5305\u542b\u5bf9\u6570\u6982\u7387\u5f15\u5bfc\u7684\u67e5\u8be2\u6784\u5efa\u3001\u591a\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u548c\u504f\u597d\u5bf9\u9f50\u7684BestFit\u91cd\u6392\u5e8f\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u5728ReccEval\u548cCCEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodeRAG\u663e\u8457\u4e14\u6301\u7eed\u5730\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CodeRAG\u901a\u8fc7\u6539\u8fdb\u7684\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4ee3\u7801\u8865\u5168\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2509.15957", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.15957", "abs": "https://arxiv.org/abs/2509.15957", "authors": ["Kanato Masayoshi", "Masahiro Hashimoto", "Ryoichi Yokoyama", "Naoki Toda", "Yoshifumi Uwamino", "Shogo Fukuda", "Ho Namkoong", "Masahiro Jinzaki"], "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol", "comment": null, "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.", "AI": {"tldr": "LLM\u901a\u8fc7MCP\u534f\u8bae\u8fde\u63a5\u533b\u9662EHR\u6570\u636e\u5e93\uff0c\u5728\u771f\u5b9e\u533b\u9662\u73af\u5883\u4e2d\u81ea\u4e3b\u68c0\u7d22\u4e34\u5e8a\u4fe1\u606f\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\uff0c\u5728\u533b\u9662\u90e8\u7f72\u53d7\u9650\u3002Model Context Protocol (MCP) \u4e3aLLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1EHR-MCP\u6846\u67b6\uff0c\u5c06\u81ea\u5b9a\u4e49MCP\u5de5\u5177\u4e0e\u533b\u9662EHR\u6570\u636e\u5e93\u96c6\u6210\uff0c\u4f7f\u7528GPT-4.1\u901a\u8fc7LangGraph ReAct\u4ee3\u7406\u8fdb\u884c\u4ea4\u4e92\u3002\u6d4b\u8bd56\u4e2a\u611f\u67d3\u63a7\u5236\u56e2\u961f\u7528\u4f8b\u4efb\u52a1\uff0c\u56de\u987e\u6027\u5206\u67908\u540d\u60a3\u8005\u6570\u636e\uff0c\u4e0e\u533b\u751f\u751f\u6210\u7684\u91d1\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u80fd\u6b63\u786e\u9009\u62e9\u548c\u6267\u884cMCP\u5de5\u5177\uff0c\u9664\u4e24\u4e2a\u4efb\u52a1\u5916\uff0c\u6240\u6709\u4efb\u52a1\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\u3002\u5728\u9700\u8981\u65f6\u95f4\u76f8\u5173\u8ba1\u7b97\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002\u9519\u8bef\u4e3b\u8981\u6765\u81ea\u53c2\u6570\u4e0d\u6b63\u786e\u6216\u5de5\u5177\u7ed3\u679c\u8bef\u89e3\u3002\u54cd\u5e94\u53ef\u9760\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u98ce\u9669\u3002", "conclusion": "LLM\u53ef\u901a\u8fc7MCP\u5de5\u5177\u4eceEHR\u68c0\u7d22\u4e34\u5e8a\u6570\u636e\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u4ecd\u6709\u6311\u6218\u3002EHR-MCP\u4e3a\u533b\u9662AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u4e00\u81f4\u7684\u6570\u636e\u8bbf\u95ee\u57fa\u7840\u67b6\u6784\u3002", "topic": "agent analysis"}}
{"id": "2509.16198", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16198", "abs": "https://arxiv.org/abs/2509.16198", "authors": ["Jane Luo", "Xin Zhang", "Steven Liu", "Jie Wu", "Yiming Huang", "Yangyu Huang", "Chengyu Yin", "Ying Xin", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Qi Chen", "Scarlett Li", "Mao Yang"], "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation", "comment": null, "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86Repository Planning Graph (RPG)\u8868\u793a\u6cd5\u548cZeroRepo\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u96f6\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u4ed3\u5e93\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u548c\u6587\u4ef6\u7ea7\u522b\u4ee3\u7801\u751f\u6210\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u4ece\u96f6\u751f\u6210\u5b8c\u6574\u4ed3\u5e93\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u7cca\u6027\u4e0d\u9002\u5408\u8868\u793a\u590d\u6742\u8f6f\u4ef6\u7ed3\u6784", "method": "\u5f15\u5165RPG\u56fe\u8868\u793a\u6cd5\u7edf\u4e00\u63d0\u6848\u548c\u5b9e\u73b0\u7ea7\u522b\u89c4\u5212\uff0c\u6784\u5efaZeroRepo\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u63d0\u6848\u89c4\u5212\u3001\u5b9e\u73b0\u7ec6\u5316\u3001\u56fe\u5f15\u5bfc\u4ee3\u7801\u751f\u6210\u4e0e\u6d4b\u8bd5\u9a8c\u8bc1", "result": "\u5728RepoCraft\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751f\u6210\u5e73\u574736K\u884c\u4ee3\u7801\uff0c\u529f\u80fd\u8986\u76d6\u7387\u8fbe81.5%\uff0c\u901a\u8fc7\u738769.7%\uff0c\u8fdc\u8d85Claude Code\u7b49\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "RPG\u80fd\u6709\u6548\u5efa\u6a21\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u8fd1\u7ebf\u6027\u6269\u5c55\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u590d\u6742\u89c4\u5212\uff0c\u63d0\u5347LLM\u5bf9\u4ed3\u5e93\u7684\u7406\u89e3\u5e76\u52a0\u901f\u667a\u80fd\u4f53\u5b9a\u4f4d", "topic": "code agent"}}
{"id": "2509.15356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15356", "abs": "https://arxiv.org/abs/2509.15356", "authors": ["Kevin Ren", "Santiago Cortes-Gomez", "Carlos Miguel Pati\u00f1o", "Ananya Joshi", "Ruiqi Lyu", "Jingjing Tang", "Alistair Turcan", "Khurram Yamin", "Steven Wu", "Bryan Wilder"], "title": "Predicting Language Models' Success at Zero-Shot Probabilistic Prediction", "comment": "EMNLP Findings 2025. We release our code at:\n  https://github.com/kkr36/llm-eval/tree/camera-ready", "summary": "Recent work has investigated the capabilities of large language models (LLMs)\nas zero-shot models for generating individual-level characteristics (e.g., to\nserve as risk models or augment survey datasets). However, when should a user\nhave confidence that an LLM will provide high-quality predictions for their\nparticular task? To address this question, we conduct a large-scale empirical\nstudy of LLMs' zero-shot predictive capabilities across a wide range of tabular\nprediction tasks. We find that LLMs' performance is highly variable, both on\ntasks within the same dataset and across different datasets. However, when the\nLLM performs well on the base prediction task, its predicted probabilities\nbecome a stronger signal for individual-level accuracy. Then, we construct\nmetrics to predict LLMs' performance at the task level, aiming to distinguish\nbetween tasks where LLMs may perform well and where they are likely unsuitable.\nWe find that some of these metrics, each of which are assessed without labeled\ndata, yield strong signals of LLMs' predictive performance on new tasks.", "AI": {"tldr": "LLM\u5728\u96f6\u6837\u672c\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u4f46\u9884\u6d4b\u6982\u7387\u53ef\u4f5c\u4e3a\u4e2a\u4f53\u51c6\u786e\u5ea6\u7684\u4fe1\u53f7\u3002\u7814\u7a76\u6784\u5efa\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u6307\u6807\u6765\u9884\u6d4bLLM\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u96f6\u6837\u672c\u6a21\u578b\u751f\u6210\u4e2a\u4f53\u7279\u5f81\u65f6\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63a2\u8ba8\u7528\u6237\u4f55\u65f6\u53ef\u4ee5\u76f8\u4fe1LLM\u80fd\u4e3a\u5176\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u9884\u6d4b\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30LLM\u5728\u5404\u79cd\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u6307\u6807\u6765\u9884\u6d4bLLM\u6027\u80fd\u3002", "result": "\u53d1\u73b0LLM\u6027\u80fd\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u4f46\u5f53LLM\u5728\u57fa\u7840\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u826f\u597d\u65f6\uff0c\u5176\u9884\u6d4b\u6982\u7387\u6210\u4e3a\u4e2a\u4f53\u51c6\u786e\u5ea6\u7684\u66f4\u5f3a\u4fe1\u53f7\u3002\u67d0\u4e9b\u65e0\u6807\u6ce8\u6570\u636e\u8bc4\u4f30\u6307\u6807\u80fd\u6709\u6548\u9884\u6d4bLLM\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "LLM\u5728\u96f6\u6837\u672c\u8868\u683c\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u5177\u6709\u53ef\u53d8\u6027\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u5408\u9002\u7684\u6307\u6807\u6765\u9884\u6d4b\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u9002\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.15518", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15518", "abs": "https://arxiv.org/abs/2509.15518", "authors": ["Siyang Wu", "Zhewei Sun"], "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages", "comment": null, "summary": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u7684\u4fda\u8bed\u7528\u6cd5\uff0c\u53d1\u73b0LLM\u5728\u4fda\u8bed\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u867d\u7136\u638c\u63e1\u4e86\u521b\u9020\u6027\u7279\u5f81\u4f46\u4e0d\u8db3\u4ee5\u652f\u6301\u8bed\u8a00\u5b66\u5206\u6790\u7b49\u5916\u63a8\u4efb\u52a1\u3002", "motivation": "\u4fda\u8bed\u4f5c\u4e3a\u975e\u6b63\u5f0f\u8bed\u8a00\u5bf9NLP\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30LLM\u662f\u5426\u638c\u63e1\u4e86\u4e0e\u4eba\u7c7b\u4e00\u81f4\u7684\u4fda\u8bed\u7ed3\u6784\u77e5\u8bc6\uff0c\u4ee5\u786e\u5b9a\u5176\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u7cfb\u7edf\u6027\u504f\u89c1\u3001\u521b\u9020\u6027\u3001\u4fe1\u606f\u6027\uff09\u6bd4\u8f83Online Slang Dictionary\u7684\u4eba\u7c7b\u4fda\u8bed\u4e0eGPT-4o\u3001Llama-3\u751f\u6210\u7684\u4fda\u8bed\u3002", "result": "\u53d1\u73b0LLM\u5728\u4fda\u8bed\u611f\u77e5\u4e0a\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u867d\u7136\u638c\u63e1\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0d\u8db3\u4ee5\u652f\u6301\u5916\u63a8\u6027\u4efb\u52a1\u5982\u8bed\u8a00\u5b66\u5206\u6790\u3002", "conclusion": "LLM\u867d\u7136\u5b66\u4e60\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u65b9\u9762\uff0c\u4f46\u5176\u77e5\u8bc6\u7ed3\u6784\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5728\u4fda\u8bed\u68c0\u6d4b\u548c\u89e3\u91ca\u7b49\u4e2d\u4ecb\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.15498", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15498", "abs": "https://arxiv.org/abs/2509.15498", "authors": ["Zahra Aref", "Narayan B. Mandayam"], "title": "Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers", "comment": null, "summary": "Transformers have emerged as a compelling architecture for sequential\ndecision-making by modeling trajectories via self-attention. In reinforcement\nlearning (RL), they enable return-conditioned control without relying on value\nfunction approximation. Decision Transformers (DTs) exploit this by casting RL\nas supervised sequence modeling, but they are restricted to offline data and\nlack exploration. Online Decision Transformers (ODTs) address this limitation\nthrough entropy-regularized training on on-policy rollouts, offering a stable\nalternative to traditional RL methods like Soft Actor-Critic, which depend on\nbootstrapped targets and reward shaping. Despite these advantages, ODTs use\nstandard attention, which lacks explicit memory of action-specific outcomes.\nThis leads to inefficiencies in learning long-term action effectiveness.\nInspired by cognitive models such as Experience-Weighted Attraction (EWA), we\npropose Experience-Weighted Attraction with Vector Quantization for Online\nDecision Transformers (EWA-VQ-ODT), a lightweight module that maintains\nper-action mental accounts summarizing recent successes and failures.\nContinuous actions are routed via direct grid lookup to a compact\nvector-quantized codebook, where each code stores a scalar attraction updated\nonline through decay and reward-based reinforcement. These attractions modulate\nattention by biasing the columns associated with action tokens, requiring no\nchange to the backbone or training objective. On standard continuous-control\nbenchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,\nparticularly in early training. The module is computationally efficient,\ninterpretable via per-code traces, and supported by theoretical guarantees that\nbound the attraction dynamics and its impact on attention drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86EWA-VQ-ODT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u7684\u7ecf\u9a8c\u52a0\u6743\u5438\u5f15\u529b\u673a\u5236\u589e\u5f3a\u5728\u7ebf\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u957f\u671f\u52a8\u4f5c\u6548\u679c\u5b66\u4e60\u80fd\u529b", "motivation": "\u4f20\u7edf\u5728\u7ebf\u51b3\u7b56\u53d8\u6362\u5668\u4f7f\u7528\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u4f5c\u7279\u5b9a\u7ed3\u679c\u7684\u663e\u5f0f\u8bb0\u5fc6\uff0c\u5bfc\u81f4\u5b66\u4e60\u957f\u671f\u52a8\u4f5c\u6709\u6548\u6027\u6548\u7387\u4f4e\u4e0b", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u6a21\u5757\u7ef4\u62a4\u6bcf\u4e2a\u52a8\u4f5c\u7684\u5fc3\u7406\u8d26\u6237\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u7801\u672c\u5b58\u50a8\u6807\u91cf\u5438\u5f15\u529b\u503c\uff0c\u5728\u7ebf\u66f4\u65b0\u5e76\u901a\u8fc7\u8870\u51cf\u548c\u5956\u52b1\u5f3a\u5316\u673a\u5236\u8c03\u6574\u6ce8\u610f\u529b\u504f\u7f6e", "result": "\u5728\u6807\u51c6\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEWA-VQ-ODT\u76f8\u6bd4ODT\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u5e73\u5747\u56de\u62a5\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u8868\u73b0\u66f4\u4f18", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e8f\u5217\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236", "topic": "agentic reinforcement learning"}}
{"id": "2509.15519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15519", "abs": "https://arxiv.org/abs/2509.15519", "authors": ["Chao Li", "Bingkun Bao", "Yang Gao"], "title": "Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem", "comment": null, "summary": "This paper studies fully decentralized cooperative multi-agent reinforcement\nlearning, where each agent solely observes the states, its local actions, and\nthe shared rewards. The inability to access other agents' actions often leads\nto non-stationarity during value function updates and relative\novergeneralization during value function estimation, hindering effective\ncooperative policy learning. However, existing works fail to address both\nissues simultaneously, due to their inability to model the joint policy of\nother agents in a fully decentralized setting. To overcome this limitation, we\npropose a novel method named Dynamics-Aware Context (DAC), which formalizes the\ntask, as locally perceived by each agent, as an Contextual Markov Decision\nProcess, and further addresses both non-stationarity and relative\novergeneralization through dynamics-aware context modeling. Specifically, DAC\nattributes the non-stationary local task dynamics of each agent to switches\nbetween unobserved contexts, each corresponding to a distinct joint policy.\nThen, DAC models the step-wise dynamics distribution using latent variables and\nrefers to them as contexts. For each agent, DAC introduces a context-based\nvalue function to address the non-stationarity issue during value function\nupdate. For value function estimation, an optimistic marginal value is derived\nto promote the selection of cooperative actions, thereby addressing the\nrelative overgeneralization issue. Experimentally, we evaluate DAC on various\ncooperative tasks (including matrix game, predator and prey, and SMAC), and its\nsuperior performance against multiple baselines validates its effectiveness.", "AI": {"tldr": "\u63d0\u51faDAC\u65b9\u6cd5\u89e3\u51b3\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u548c\u76f8\u5bf9\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u4e0a\u4e0b\u6587\u5efa\u6a21\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4e0a\u4e0b\u6587\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b", "motivation": "\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u667a\u80fd\u4f53\u65e0\u6cd5\u89c2\u6d4b\u5176\u4ed6\u667a\u80fd\u4f53\u52a8\u4f5c\u5bfc\u81f4\u4ef7\u503c\u51fd\u6570\u66f4\u65b0\u7684\u975e\u5e73\u7a33\u6027\u548c\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u7684\u76f8\u5bf9\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898", "method": "DAC\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u667a\u80fd\u4f53\u611f\u77e5\u7684\u5c40\u90e8\u4efb\u52a1\u52a8\u6001\u5f52\u56e0\u4e8e\u672a\u89c2\u6d4b\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u5207\u6362\uff0c\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u9010\u6b65\u52a8\u6001\u5206\u5e03\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u5f15\u5165\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u4ef7\u503c\u51fd\u6570\u89e3\u51b3\u975e\u5e73\u7a33\u6027\uff0c\u63a8\u5bfc\u4e50\u89c2\u8fb9\u9645\u4ef7\u503c\u4fc3\u8fdb\u5408\u4f5c\u52a8\u4f5c\u9009\u62e9", "result": "\u5728\u77e9\u9635\u6e38\u620f\u3001\u6355\u98df\u8005-\u730e\u7269\u548cSMAC\u7b49\u591a\u79cd\u5408\u4f5c\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cDAC\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "DAC\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u4e0a\u4e0b\u6587\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u548c\u76f8\u5bf9\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2509.15676", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15676", "abs": "https://arxiv.org/abs/2509.15676", "authors": ["Vaibhav Singh", "Soumya Suvra Ghosal", "Kapu Nirmal Joshua", "Soumyabrata Pal", "Sayak Ray Chowdhury"], "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning", "comment": null, "summary": "In-context learning (ICL) has emerged as a powerful paradigm for adapting\nlarge language models (LLMs) to new and data-scarce tasks using only a few\ncarefully selected task-specific examples presented in the prompt. However,\ngiven the limited context size of LLMs, a fundamental question arises: Which\nexamples should be selected to maximize performance on a given user query?\nWhile nearest-neighbor-based methods like KATE have been widely adopted for\nthis purpose, they suffer from well-known drawbacks in high-dimensional\nembedding spaces, including poor generalization and a lack of diversity. In\nthis work, we study this problem of example selection in ICL from a principled,\ninformation theory-driven perspective. We first model an LLM as a linear\nfunction over input embeddings and frame the example selection task as a\nquery-specific optimization problem: selecting a subset of exemplars from a\nlarger example bank that minimizes the prediction error on a specific query.\nThis formulation departs from traditional generalization-focused learning\ntheoretic approaches by targeting accurate prediction for a specific query\ninstance. We derive a principled surrogate objective that is approximately\nsubmodular, enabling the use of a greedy algorithm with an approximation\nguarantee. We further enhance our method by (i) incorporating the kernel trick\nto operate in high-dimensional feature spaces without explicit mappings, and\n(ii) introducing an optimal design-based regularizer to encourage diversity in\nthe selected examples. Empirically, we demonstrate significant improvements\nover standard retrieval methods across a suite of classification tasks,\nhighlighting the benefits of structure-aware, diverse example selection for ICL\nin real-world, label-scarce scenarios.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u7814\u7a76ICL\u4e2d\u7684\u793a\u4f8b\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u67e5\u8be2\u7279\u5b9a\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u5b50\u6a21\u4f18\u5316\u548c\u6838\u6280\u5de7\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u68c0\u7d22\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u793a\u4f8b\u9009\u62e9\u7684\u5173\u952e\u95ee\u9898\uff0c\u4f20\u7edf\u6700\u8fd1\u90bb\u65b9\u6cd5\u5728\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u7406\u8bba\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06LLM\u5efa\u6a21\u4e3a\u8f93\u5165\u5d4c\u5165\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u5c06\u793a\u4f8b\u9009\u62e9\u6784\u5efa\u4e3a\u67e5\u8be2\u7279\u5b9a\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u8fd1\u4f3c\u5b50\u6a21\u7684\u4ee3\u7406\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u6838\u6280\u5de7\u548c\u57fa\u4e8e\u6700\u4f18\u8bbe\u8ba1\u7684\u6b63\u5219\u5316\u5668\u6765\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u68c0\u7d22\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u548c\u591a\u6837\u5316\u793a\u4f8b\u9009\u62e9\u5728\u5b9e\u9645\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u7406\u7684\u67e5\u8be2\u7279\u5b9a\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347ICL\u6027\u80fd\uff0c\u6838\u6280\u5de7\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u793a\u4f8b\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2509.15839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15839", "abs": "https://arxiv.org/abs/2509.15839", "authors": ["Zhongze Luo", "Zhenshuai Yin", "Yongxin Guo", "Zhichao Wang", "Jionghao Zhu", "Xiaoying Tang"], "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems", "comment": null, "summary": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multi-Physics\u4e2d\u6587\u7269\u7406\u63a8\u7406\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u96be\u5ea6\u7ea7\u522b\u30011412\u9053\u56fe\u50cf\u5173\u8054\u9009\u62e9\u9898\uff0c\u8986\u76d611\u4e2a\u9ad8\u4e2d\u7269\u7406\u4e3b\u9898\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5728\u4e13\u4e1a\u79d1\u5b66\u9886\u57df\uff08\u5982\u7269\u7406\uff09\u5b58\u5728\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5b66\u79d1\u8986\u76d6\u3001\u5ffd\u89c6\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u3001\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u3001\u672a\u80fd\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b5\u4e2a\u96be\u5ea6\u7ea7\u522b\u30011412\u9053\u56fe\u50cf\u5173\u8054\u9009\u62e9\u9898\u7684\u4e2d\u6587\u7269\u7406\u57fa\u51c6\uff1b\u91c7\u7528\u53cc\u91cd\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f3020\u4e2a\u4e0d\u540cMLLM\u6a21\u578b\uff0c\u5206\u6790\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u548c\u9010\u6b65\u63a8\u7406\u5b8c\u6574\u6027\uff1b\u7cfb\u7edf\u7814\u7a76\u96be\u5ea6\u7ea7\u522b\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u793e\u533a\u8d44\u6e90\uff0c\u5e76\u63d0\u4f9b\u4e86\u5256\u6790\u6700\u5148\u8fdbMLLM\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u7684\u7a33\u5065\u65b9\u6cd5\uff1b\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "Multi-Physics\u57fa\u51c6\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u5728\u79d1\u5b66\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2509.15652", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15652", "abs": "https://arxiv.org/abs/2509.15652", "authors": ["Kyohei Suzuki", "Konstantinos Slavakis"], "title": "Nonconvex Regularization for Feature Selection in Reinforcement Learning", "comment": null, "summary": "This work proposes an efficient batch algorithm for feature selection in\nreinforcement learning (RL) with theoretical convergence guarantees. To\nmitigate the estimation bias inherent in conventional regularization schemes,\nthe first contribution extends policy evaluation within the classical\nleast-squares temporal-difference (LSTD) framework by formulating a\nBellman-residual objective regularized with the sparsity-inducing, nonconvex\nprojected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC\npenalty, this formulation can be interpreted as a special instance of a general\nnonmonotone-inclusion problem. The second contribution establishes novel\nconvergence conditions for the forward-reflected-backward splitting (FRBS)\nalgorithm to solve this class of problems. Numerical experiments on benchmark\ndatasets demonstrate that the proposed approach substantially outperforms\nstate-of-the-art feature-selection methods, particularly in scenarios with many\nnoisy features.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u6279\u91cf\u7b97\u6cd5\uff0c\u901a\u8fc7PMC\u60e9\u7f5a\u6b63\u5219\u5316Bellman\u6b8b\u5dee\u76ee\u6807\u6765\u7f13\u89e3\u4f30\u8ba1\u504f\u5dee\uff0c\u5e76\u4f7f\u7528FRBS\u7b97\u6cd5\u6c42\u89e3\uff0c\u5728\u542b\u566a\u58f0\u7279\u5f81\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6848\u5b58\u5728\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u6765\u5904\u7406\u9ad8\u7ef4\u566a\u58f0\u7279\u5f81\u573a\u666f", "method": "\u6269\u5c55LSTD\u6846\u67b6\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\uff0c\u4f7f\u7528\u975e\u51f8PMC\u60e9\u7f5a\u6b63\u5219\u5316Bellman\u6b8b\u5dee\u76ee\u6807\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u975e\u5355\u8c03\u5305\u542b\u95ee\u9898\uff0c\u5e76\u5e94\u7528FRBS\u7b97\u6cd5\u6c42\u89e3", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u542b\u5927\u91cf\u566a\u58f0\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u51fa\u7684PMC\u6b63\u5219\u5316\u65b9\u6cd5\u548cFRBS\u7b97\u6cd5\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf", "topic": "agentic reinforcement learning"}}
{"id": "2509.15927", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15927", "abs": "https://arxiv.org/abs/2509.15927", "authors": ["Zhiyu Mou", "Yiqin Lv", "Miao Xu", "Cheems Wang", "Yixiu Mao", "Qichen Ye", "Chao Li", "Rongquan Bai", "Chuan Yu", "Jian Xu", "Bo Zheng"], "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search", "comment": null, "summary": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach.", "AI": {"tldr": "AIGB-Pearl\u662f\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u89c4\u5212\u548c\u7b56\u7565\u4f18\u5316\u7684\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8f68\u8ff9\u8bc4\u4f30\u5668\u6765\u6307\u5bfc\u7b56\u7565\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AIGB\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u751f\u6210\u8d28\u91cf\u8bc4\u4f30\u548c\u79bb\u7ebf\u6570\u636e\u63a2\u7d22\u65b9\u9762\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u7ade\u4ef7(AIGB)\u65b9\u6cd5\u867d\u7136\u4f18\u4e8e\u4f20\u7edf\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u751f\u6210\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e14\u65e0\u6cd5\u8d85\u8d8a\u9759\u6001\u6570\u636e\u96c6\u8fdb\u884c\u63a2\u7d22\u3002", "method": "\u63d0\u51faAIGB-Pearl\u65b9\u6cd5\uff0c\u6574\u5408\u751f\u6210\u5f0f\u89c4\u5212\u548c\u7b56\u7565\u4f18\u5316\uff1a1\uff09\u6784\u5efa\u975e\u81ea\u4e3e\u7684\u8f68\u8ff9\u8bc4\u4f30\u5668\u6765\u5206\u914d\u5956\u52b1\u548c\u6307\u5bfc\u7b56\u7565\u641c\u7d22\uff1b2\uff09\u91c7\u7528LLM\u67b6\u6784\u589e\u5f3a\u8868\u793a\u80fd\u529b\uff1b3\uff09\u4f7f\u7528\u6df7\u5408\u70b9\u5bf9\u548c\u914d\u5bf9\u635f\u5931\u8fdb\u884c\u66f4\u597d\u7684\u5206\u6570\u5b66\u4e60\uff1b4\uff09\u81ea\u9002\u5e94\u6574\u5408\u4e13\u5bb6\u53cd\u9988\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5e7f\u544a\u7cfb\u7edf\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "AIGB-Pearl\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u8bc4\u4f30\u5668\u548c\u591a\u79cd\u6280\u672f\u6539\u8fdb\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u7ade\u4ef7\u7684\u751f\u6210\u8d28\u91cf\u548c\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.16093", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16093", "abs": "https://arxiv.org/abs/2509.16093", "authors": ["Fangyi Yu", "Nabeel Seedat", "Dasha Herrmannova", "Frank Schilder", "Jonathan Richard Schwarz"], "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses", "comment": null, "summary": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.", "AI": {"tldr": "DeCE\u662f\u4e00\u4e2a\u5206\u89e3\u5f0fLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u7b54\u6848\u8d28\u91cf\u5206\u89e3\u4e3a\u7cbe\u786e\u5ea6\uff08\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u548c\u53ec\u56de\u7387\uff08\u6982\u5ff5\u8986\u76d6\u7387\uff09\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u5b9e\u73b0\u4e86\u4e0e\u4e13\u5bb6\u5224\u65ad\u66f4\u5f3a\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5982BLEU\u548cROUGE\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u73b0\u6709LLM\u8bc4\u4f30\u5668\u5f80\u5f80\u5c06\u7b54\u6848\u8d28\u91cf\u7684\u7ec6\u5fae\u5dee\u522b\u7b80\u5316\u4e3a\u5355\u4e00\u5206\u6570\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u98ce\u9669\u9886\u57df\u5982\u6cd5\u5f8b\u6216\u533b\u5b66\u7684\u8bc4\u4f30\u9700\u6c42\u3002", "method": "DeCE\u6846\u67b6\u81ea\u52a8\u4ece\u6807\u51c6\u7b54\u6848\u8981\u6c42\u4e2d\u63d0\u53d6\u5b9e\u4f8b\u7279\u5b9a\u6807\u51c6\uff0c\u5c06\u8bc4\u4f30\u5206\u89e3\u4e3a\u7cbe\u786e\u5ea6\uff08\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\uff09\u548c\u53ec\u56de\u7387\uff08\u6240\u9700\u6982\u5ff5\u8986\u76d6\u7387\uff09\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5206\u7c7b\u6cd5\u6216\u624b\u5de5\u5236\u5b9a\u7684\u8bc4\u5206\u6807\u51c6\u3002", "result": "DeCE\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u76f8\u5173\u6027\u8fbe\u52300.78\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff080.12\uff09\u3001\u70b9\u5f0fLLM\u8bc4\u5206\uff080.35\uff09\u548c\u591a\u7ef4\u8bc4\u4f30\u5668\uff080.48\uff09\u3002\u4ec511.95%\u7684LLM\u751f\u6210\u6807\u51c6\u9700\u8981\u4e13\u5bb6\u4fee\u8ba2\u3002", "conclusion": "DeCE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u63ed\u793a\u901a\u7528\u6a21\u578b\u504f\u5411\u53ec\u56de\u7387\u800c\u4e13\u4e1a\u6a21\u578b\u504f\u5411\u7cbe\u786e\u5ea6\u7684\u53ef\u89e3\u91ca\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2509.15965", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15965", "abs": "https://arxiv.org/abs/2509.15965", "authors": ["Chao Yu", "Yuanqing Wang", "Zhen Guo", "Hao Lin", "Si Xu", "Hongzhi Zang", "Quanlu Zhang", "Yongji Wu", "Chunyang Zhu", "Junhao Hu", "Zixiao Huang", "Mingjie Wei", "Yuqing Xie", "Ke Yang", "Bo Dai", "Zhexuan Xu", "Xiangyuan Wang", "Xu Fu", "Zhihao Liu", "Kang Chen", "Weilin Liu", "Gang Liu", "Boxun Li", "Jianlei Yang", "Zhi Yang", "Guohao Dai", "Yu Wang"], "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation", "comment": "GitHub Repo: https://github.com/RLinf/RLinf", "summary": "Reinforcement learning (RL) has demonstrated immense potential in advancing\nartificial general intelligence, agentic intelligence, and embodied\nintelligence. However, the inherent heterogeneity and dynamicity of RL\nworkflows often lead to low hardware utilization and slow training on existing\nsystems. In this paper, we present RLinf, a high-performance RL training system\nbased on our key observation that the major roadblock to efficient RL training\nlies in system flexibility. To maximize flexibility and efficiency, RLinf is\nbuilt atop a novel RL system design paradigm called macro-to-micro flow\ntransformation (M2Flow), which automatically breaks down high-level,\neasy-to-compose RL workflows at both the temporal and spatial dimensions, and\nrecomposes them into optimized execution flows. Supported by RLinf worker's\nadaptive communication capability, we devise context switching and elastic\npipelining to realize M2Flow transformation, and a profiling-guided scheduling\npolicy to generate optimal execution plans. Extensive evaluations on both\nreasoning RL and embodied RL tasks demonstrate that RLinf consistently\noutperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in\nend-to-end training throughput.", "AI": {"tldr": "RLinf\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b8f-\u5fae\u6d41\u8f6c\u6362(M2Flow)\u8303\u5f0f\u7684\u9ad8\u6027\u80fd\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5206\u89e3\u548c\u91cd\u7ec4RL\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u4e861.1x-2.13x\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u5bfc\u81f4\u73b0\u6709\u7cfb\u7edf\u7684\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u3001\u8bad\u7ec3\u901f\u5ea6\u6162\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u7cfb\u7edf\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faM2Flow\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u81ea\u52a8\u5206\u89e3RL\u5de5\u4f5c\u6d41\u5e76\u91cd\u7ec4\u4e3a\u4f18\u5316\u6267\u884c\u6d41\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u901a\u4fe1\u3001\u4e0a\u4e0b\u6587\u5207\u6362\u3001\u5f39\u6027\u6d41\u6c34\u7ebf\u548c\u57fa\u4e8e\u6027\u80fd\u5206\u6790\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u63a8\u7406RL\u548c\u5177\u8eabRL\u4efb\u52a1\u4e0a\uff0cRLinf\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.1-2.13\u500d\u3002", "conclusion": "RLinf\u901a\u8fc7\u63d0\u9ad8\u7cfb\u7edf\u7075\u6d3b\u6027\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0cM2Flow\u8303\u5f0f\u4e3a\u9ad8\u6027\u80fdRL\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.15738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15738", "abs": "https://arxiv.org/abs/2509.15738", "authors": ["Musen Lin", "Minghao Liu", "Taoran Lu", "Lichen Yuan", "Yiwei Liu", "Haonan Xu", "Yu Miao", "Yuhao Chao", "Zhaojian Li"], "title": "GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning", "comment": null, "summary": "Graphical User Interface (GUI) Agents, powered by large language and\nvision-language models, hold promise for enabling end-to-end automation in\ndigital environments. However, their progress is fundamentally constrained by\nthe scarcity of scalable, high-quality trajectory data. Existing data\ncollection strategies either rely on costly and inconsistent manual annotations\nor on synthetic generation methods that trade off between diversity and\nmeaningful task coverage. To bridge this gap, we present GUI-ReWalk: a\nreasoning-enhanced, multi-stage framework for synthesizing realistic and\ndiverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase\nthat emulates human trial-and-error behaviors, and progressively transitions\ninto a reasoning-guided phase where inferred goals drive coherent and\npurposeful interactions. Moreover, it supports multi-stride task generation,\nenabling the construction of long-horizon workflows across multiple\napplications. By combining randomness for diversity with goal-aware reasoning\nfor structure, GUI-ReWalk produces data that better reflects the intent-aware,\nadaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B\non the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including\nScreenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results\ndemonstrate that GUI-ReWalk enables superior coverage of diverse interaction\nflows, higher trajectory entropy, and more realistic user intent. These\nfindings establish GUI-ReWalk as a scalable and data-efficient framework for\nadvancing GUI agent research and enabling robust real-world automation.", "AI": {"tldr": "GUI-ReWalk\u662f\u4e00\u4e2a\u7528\u4e8e\u5408\u6210GUI\u8f68\u8ff9\u6570\u636e\u7684\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u63a2\u7d22\u548c\u63a8\u7406\u5f15\u5bfc\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u4eba\u673a\u4ea4\u4e92\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "GUI\u4ee3\u7406\u7684\u53d1\u5c55\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u73b0\u6709\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u8981\u4e48\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u4e00\u81f4\uff0c\u8981\u4e48\u5728\u591a\u6837\u6027\u548c\u4efb\u52a1\u8986\u76d6\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u63d0\u51faGUI-ReWalk\u6846\u67b6\uff1a1\uff09\u968f\u673a\u63a2\u7d22\u9636\u6bb5\u6a21\u62df\u4eba\u7c7b\u8bd5\u9519\u884c\u4e3a\uff1b2\uff09\u63a8\u7406\u5f15\u5bfc\u9636\u6bb5\u57fa\u4e8e\u63a8\u65ad\u76ee\u6807\u8fdb\u884c\u8fde\u8d2f\u4ea4\u4e92\uff1b3\uff09\u652f\u6301\u591a\u6b65\u957f\u4efb\u52a1\u751f\u6210\uff0c\u6784\u5efa\u8de8\u5e94\u7528\u7684\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08Screenspot-Pro\u3001OSWorld-G\u7b49\uff09\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cGUI-ReWalk\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u4ea4\u4e92\u6d41\u8986\u76d6\u3001\u66f4\u9ad8\u7684\u8f68\u8ff9\u71b5\u548c\u66f4\u771f\u5b9e\u7684\u7528\u6237\u610f\u56fe\u3002", "conclusion": "GUI-ReWalk\u4e3aGUI\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63a8\u52a8\u7a33\u5065\u7684\u5b9e\u65f6\u81ea\u52a8\u5316\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.15981", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15981", "abs": "https://arxiv.org/abs/2509.15981", "authors": ["Yujie Zhu", "Charles A. Hepburn", "Matthew Thorpe", "Giovanni Montana"], "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations", "comment": null, "summary": "In reinforcement learning with sparse rewards, demonstrations can accelerate\nlearning, but determining when to imitate them remains challenging. We propose\nSmooth Policy Regularisation from Demonstrations (SPReD), a framework that\naddresses the fundamental question: when should an agent imitate a\ndemonstration versus follow its own policy? SPReD uses ensemble methods to\nexplicitly model Q-value distributions for both demonstration and policy\nactions, quantifying uncertainty for comparisons. We develop two complementary\nuncertainty-aware methods: a probabilistic approach estimating the likelihood\nof demonstration superiority, and an advantage-based approach scaling imitation\nby statistical significance. Unlike prevailing methods (e.g. Q-filter) that\nmake binary imitation decisions, SPReD applies continuous,\nuncertainty-proportional regularisation weights, reducing gradient variance\nduring training. Despite its computational simplicity, SPReD achieves\nremarkable gains in experiments across eight robotics tasks, outperforming\nexisting approaches by up to a factor of 14 in complex tasks while maintaining\nrobustness to demonstration quality and quantity. Our code is available at\nhttps://github.com/YujieZhu7/SPReD.", "AI": {"tldr": "SPReD\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u5efa\u6a21Q\u503c\u5206\u5e03\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u6982\u7387\u548c\u4f18\u52bf\u4e24\u79cd\u65b9\u6cd5\u51b3\u5b9a\u4f55\u65f6\u6a21\u4eff\u6f14\u793a\uff0c\u76f8\u6bd4\u4e8c\u5143\u51b3\u7b56\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fde\u7eed\u4e0d\u786e\u5b9a\u6027\u6bd4\u4f8b\u6b63\u5219\u5316\uff0c\u57288\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7a00\u758f\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6f14\u793a\u53ef\u4ee5\u52a0\u901f\u5b66\u4e60\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u6a21\u4eff\u6f14\u793a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982Q-filter\uff09\u505a\u51fa\u4e8c\u5143\u6a21\u4eff\u51b3\u7b56\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u65b9\u6cd5\u663e\u5f0f\u5efa\u6a21\u6f14\u793a\u548c\u7b56\u7565\u52a8\u4f5c\u7684Q\u503c\u5206\u5e03\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\u6982\u7387\u65b9\u6cd5\u4f30\u8ba1\u6f14\u793a\u4f18\u8d8a\u6027\u7684\u53ef\u80fd\u6027\uff0c\u4f18\u52bf\u65b9\u6cd5\u901a\u8fc7\u7edf\u8ba1\u663e\u8457\u6027\u7f29\u653e\u6a21\u4eff\u3002\u5e94\u7528\u8fde\u7eed\u7684\u4e0d\u786e\u5b9a\u6027\u6bd4\u4f8b\u6b63\u5219\u5316\u6743\u91cd\u3002", "result": "\u57288\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe14\u500d\uff0c\u540c\u65f6\u5bf9\u6f14\u793a\u8d28\u91cf\u548c\u6570\u91cf\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "SPReD\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8fde\u7eed\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f55\u65f6\u6a21\u4eff\u6f14\u793a\u7684\u95ee\u9898\uff0c\u8ba1\u7b97\u7b80\u5355\u4f46\u6548\u679c\u663e\u8457\uff0c\u4e3a\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6f14\u793a\u5229\u7528\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.16117", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16117", "abs": "https://arxiv.org/abs/2509.16117", "authors": ["Kaiwen Zheng", "Huayu Chen", "Haotian Ye", "Haoxiang Wang", "Qinsheng Zhang", "Kai Jiang", "Hang Su", "Stefano Ermon", "Jun Zhu", "Ming-Yu Liu"], "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process", "comment": null, "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.", "AI": {"tldr": "DiffusionNFT\u662f\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76f4\u63a5\u5728\u6b63\u5411\u8fc7\u7a0b\u4e2d\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u6548\u7387\u63d0\u534725\u500d\u4e14\u65e0\u9700\u5206\u7c7b\u5668\u5f15\u5bfc\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u3001\u6c42\u89e3\u5668\u9650\u5236\u3001\u6b63\u53cd\u5411\u4e0d\u4e00\u81f4\u4ee5\u53ca\u4e0e\u5206\u7c7b\u5668\u5f15\u5bfc\u590d\u6742\u96c6\u6210\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffusionNFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u5728\u6b63\u5411\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u5bf9\u6bd4\u6b63\u8d1f\u751f\u6210\u6765\u5b9a\u4e49\u9690\u5f0f\u7b56\u7565\u6539\u8fdb\u65b9\u5411\uff0c\u5c06\u5f3a\u5316\u4fe1\u53f7\u81ea\u7136\u878d\u5165\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u3002", "result": "DiffusionNFT\u6bd4FlowGRPO\u6548\u7387\u63d0\u534725\u500d\uff0c\u57281k\u6b65\u5185\u5c06GenEval\u5206\u6570\u4ece0.24\u63d0\u5347\u52300.98\uff0c\u65e0\u9700\u5206\u7c7b\u5668\u5f15\u5bfc\u3002\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86SD3.5-Medium\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "DiffusionNFT\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6839\u672c\u7f3a\u9677\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.16060", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16060", "abs": "https://arxiv.org/abs/2509.16060", "authors": ["Maithili Joshi", "Palash Nandi", "Tanmoy Chakraborty"], "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection", "comment": "Accepted in EMNLP'25 Main", "summary": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSABER\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u767d\u76d2\u8bbe\u7f6e\u4e0b\u8fde\u63a5LLM\u4e2d\u95f4\u5c42\u7684\u6b8b\u5dee\u8fde\u63a5\u6765\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u5728HarmBench\u6d4b\u8bd5\u96c6\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534751%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\u8bad\u7ec3\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u7814\u7a76\u53d1\u73b0\u5b89\u5168\u673a\u5236\u4e3b\u8981\u5d4c\u5165\u5728\u6a21\u578b\u4e2d\u540e\u5c42\uff0c\u8fd9\u4e3a\u767d\u76d2\u653b\u51fb\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faSABER\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u63a5\u4e24\u4e2a\u4e2d\u95f4\u5c42s\u548ce\uff08s<e\uff09\u7684\u6b8b\u5dee\u8fde\u63a5\u6765\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u8fd9\u662f\u4e00\u79cd\u767d\u76d2\u8d8a\u72f1\u65b9\u6cd5\u3002", "result": "\u5728HarmBench\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8651%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u9a8c\u8bc1\u96c6\u4e0a\u4ec5\u5f15\u8d77\u8fb9\u9645\u56f0\u60d1\u5ea6\u53d8\u5316\uff0c\u8868\u660e\u653b\u51fb\u6709\u6548\u4e14\u5bf9\u6b63\u5e38\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "conclusion": "SABER\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86LLM\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2509.15828", "categories": ["cs.LG", "cs.DM"], "pdf": "https://arxiv.org/pdf/2509.15828", "abs": "https://arxiv.org/abs/2509.15828", "authors": ["Ning Xu", "Junkai Zhang", "Yang Wu", "Huigen Ye", "Hua Xu", "Huiling Xu", "Yifan Zhang"], "title": "HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs", "comment": null, "summary": "Directly solving large-scale Integer Linear Programs (ILPs) using traditional\nsolvers is slow due to their NP-hard nature. While recent frameworks based on\nLarge Neighborhood Search (LNS) can accelerate the solving process, their\nperformance is often constrained by the difficulty in generating sufficiently\neffective neighborhoods. To address this challenge, we propose HyP-ASO, a\nhybrid policy-based adaptive search optimization framework that combines a\ncustomized formula with deep Reinforcement Learning (RL). The formula leverages\nfeasible solutions to calculate the selection probabilities for each variable\nin the neighborhood generation process, and the RL policy network predicts the\nneighborhood size. Extensive experiments demonstrate that HyP-ASO significantly\noutperforms existing LNS-based approaches for large-scale ILPs. Additional\nexperiments show it is lightweight and highly scalable, making it well-suited\nfor solving large-scale ILPs.", "AI": {"tldr": "HyP-ASO\u662f\u4e00\u4e2a\u6df7\u5408\u7b56\u7565\u7684\u81ea\u9002\u5e94\u641c\u7d22\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u5236\u516c\u5f0f\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u52a0\u901f\u5927\u89c4\u6a21\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u7684\u6c42\u89e3\u3002", "motivation": "\u4f20\u7edf\u6c42\u89e3\u5668\u5904\u7406\u5927\u89c4\u6a21\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u901f\u5ea6\u6162\uff0c\u73b0\u6709\u57fa\u4e8e\u5927\u90bb\u57df\u641c\u7d22\u7684\u6846\u67b6\u5728\u751f\u6210\u6709\u6548\u90bb\u57df\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7b56\u7565\uff1a\u5b9a\u5236\u516c\u5f0f\u5229\u7528\u53ef\u884c\u89e3\u8ba1\u7b97\u53d8\u91cf\u9009\u62e9\u6982\u7387\uff0c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7f51\u7edc\u9884\u6d4b\u90bb\u57df\u5927\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHyP-ASO\u663e\u8457\u4f18\u4e8e\u73b0\u6709LNS\u65b9\u6cd5\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u9ad8\u53ef\u6269\u5c55\u6027\u7279\u70b9\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u5408\u89e3\u51b3\u5927\u89c4\u6a21\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.16151", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16151", "abs": "https://arxiv.org/abs/2509.16151", "authors": ["Isaiah J. King", "Benjamin Bowman", "H. Howie Huang"], "title": "Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents", "comment": null, "summary": "Deep reinforcement learning (RL) is emerging as a viable strategy for\nautomated cyber defense (ACD). The traditional RL approach represents networks\nas a list of computers in various states of safety or threat. Unfortunately,\nthese models are forced to overfit to specific network topologies, rendering\nthem ineffective when faced with even small environmental perturbations. In\nthis work, we frame ACD as a two-player context-based partially observable\nMarkov decision problem with observations represented as attributed graphs.\nThis approach allows our agents to reason through the lens of relational\ninductive bias. Agents learn how to reason about hosts interacting with other\nsystem entities in a more general manner, and their actions are understood as\nedits to the graph representing the environment. By introducing this bias, we\nwill show that our agents can better reason about the states of networks and\nzero-shot adapt to new ones. We show that this approach outperforms the\nstate-of-the-art by a wide margin, and makes our agents capable of defending\nnever-before-seen networks against a wide range of adversaries in a variety of\ncomplex, and multi-agent environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\uff0c\u901a\u8fc7\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u96f6\u6837\u672c\u9002\u5e94\u65b0\u7f51\u7edc\u62d3\u6251\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u7f51\u7edc\u9632\u5fa1\u4e2d\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u7f51\u7edc\u62d3\u6251\uff0c\u9762\u5bf9\u73af\u5883\u6270\u52a8\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7f51\u7edc\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u7f51\u7edc\u9632\u5fa1\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5c5e\u6027\u56fe\u8868\u793a\u89c2\u6d4b\uff0c\u8ba9\u667a\u80fd\u4f53\u901a\u8fc7\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u8fdb\u884c\u63a8\u7406\uff0c\u5c06\u52a8\u4f5c\u7406\u89e3\u4e3a\u5bf9\u56fe\u7684\u7f16\u8f91\u3002", "result": "\u8be5\u65b9\u6cd5\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u5404\u79cd\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u9632\u5fa1\u4ece\u672a\u89c1\u8fc7\u7684\u7f51\u7edc\uff0c\u5bf9\u6297\u591a\u79cd\u653b\u51fb\u8005\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u8868\u793a\u548c\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u9632\u5fa1RL\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.16203", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16203", "abs": "https://arxiv.org/abs/2509.16203", "authors": ["Zhengxing Li", "Guangmingmei Yang", "Jayaram Raghuram", "David J. Miller", "George Kesidis"], "title": "Inverting Trojans in LLMs", "comment": null, "summary": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u540e\u95e8\u653b\u51fb\u7684\u89e6\u53d1\u5668\u53cd\u8f6c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u641c\u7d22\u3001\u9690\u5f0f\u9ed1\u540d\u5355\u548c\u7f6e\u4fe1\u5ea6\u68c0\u6d4b\u6765\u6709\u6548\u8bc6\u522b\u548c\u53cd\u8f6c\u771f\u5b9e\u540e\u95e8\u89e6\u53d1\u5668", "motivation": "\u73b0\u6709\u540e\u95e8\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u56fe\u50cfAI\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8eLLM\uff0c\u56e0\u4e3aLLM\u8f93\u5165\u7a7a\u95f4\u79bb\u6563\u3001\u53ef\u80fd\u7684\u89e6\u53d1\u5668\u7ec4\u5408\u6570\u91cf\u5de8\u5927\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u9ed1\u540d\u5355\u673a\u5236", "method": "\u91c7\u7528\u4e09\u7ec4\u4ef6\u65b9\u6cd5\uff1a1\uff09\u4ece\u5355\u4f8b\u5f00\u59cb\u7684\u8d2a\u5a6a\u79bb\u6563\u641c\u7d22\uff1b2\uff09\u901a\u8fc7\u6fc0\u6d3b\u7a7a\u95f4\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5b9e\u73b0\u9690\u5f0f\u9ed1\u540d\u5355\uff1b3\uff09\u57fa\u4e8e\u9ad8\u8bef\u5206\u7c7b\u7387\u548c\u5f02\u5e38\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u68c0\u6d4b\u673a\u5236", "result": "\u4e0e\u8bb8\u591a\u8fd1\u671f\u5de5\u4f5c\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u5e76\u6210\u529f\u53cd\u8f6c\u771f\u5b9e\u7684\u540e\u95e8\u89e6\u53d1\u5668\u77ed\u8bed", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u540e\u95e8\u68c0\u6d4b\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u68c0\u6d4b\u65b9\u6848", "topic": "agent analysis"}}
{"id": "wechat.2509.83610edd", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNTAyMDY0MQ==&mid=2652723537&idx=4&sn=dc259480691d7514b5a7eee397104a5f&chksm=8164014f2dbe0e5f32f81a98c7066b73f35613b83174f5ee91fa4eca19e7aa60854b7d3be033#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNTAyMDY0MQ==&mid=2652723537&idx=4&sn=dc259480691d7514b5a7eee397104a5f&chksm=8164014f2dbe0e5f32f81a98c7066b73f35613b83174f5ee91fa4eca19e7aa60854b7d3be033#rd", "authors": ["\u81ea\u7136\u7cfb\u5217"], "title": "DeepSeek-R1\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6fc0\u52b1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b |\u300a\u81ea\u7136\u300b\u8bba\u6587", "comment": "Source: WeChat, Published: 2025-09-22 04:41:15", "summary": "\u4ed6\u4eec\u6240\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u4fc3\u751f\u51fa\u9ad8\u7ea7\u63a8\u7406\u6a21\u5f0f\uff0c\u4f8b\u5982\u81ea\u6211\u53cd\u601d\u3001\u9a8c\u8bc1\u4ee5\u53ca\u52a8\u6001\u7b56\u7565\u8c03\u6574\u3002\u56e0\u6b64\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7ade\u8d5b\u4ee5\u53caSTEM\u9886\u57df\u7b49\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u66f4\u4f18\u6027\u80fd\uff0c\u5176\u8868\u73b0\u8d85\u8d8a\u4e86\u901a\u8fc7\u57fa\u4e8e\u4eba\u5de5\u793a\u8303\u7684\u4f20\u7edf\u76d1\u7763", "AI": {"tldr": "\u4ed6\u4eec\u6240\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u4fc3\u751f\u51fa\u9ad8\u7ea7\u63a8\u7406\u6a21\u5f0f\uff0c\u4f8b\u5982\u81ea\u6211\u53cd\u601d\u3001\u9a8c\u8bc1\u4ee5\u53ca\u52a8\u6001\u7b56\u7565\u8c03\u6574\u3002\u56e0\u6b64\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7ade\u8d5b\u4ee5\u53caSTEM\u9886\u57df\u7b49\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u66f4\u4f18\u6027\u80fd\uff0c\u5176\u8868\u73b0\u8d85\u8d8a\u4e86\u901a\u8fc7\u57fa\u4e8e\u4eba\u5de5\u793a\u8303\u7684\u4f20\u7edf\u76d1\u7763", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.42fea328", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5Mzg3ODEwNA==&mid=2247488852&idx=1&sn=88b7c95f60037d88785f46b2af2dbf5e&chksm=c1c96d1f05791828fba29028aaa56f3261f09db4494365df2dc4749760c6538551b3a1569eed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5Mzg3ODEwNA==&mid=2247488852&idx=1&sn=88b7c95f60037d88785f46b2af2dbf5e&chksm=c1c96d1f05791828fba29028aaa56f3261f09db4494365df2dc4749760c6538551b3a1569eed#rd", "authors": ["human five"], "title": "\u7528\u4e8e\u673a\u5668\u4eba\u73b0\u5b9e\u4e16\u754c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684VLAC", "comment": "Source: WeChat, Published: 2025-09-22 04:36:41", "summary": "\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u4e00\u79cd\u76f4\u63a5\u65b9\u6cd5\u662f\u63d0\u4f9b\u5bc6\u96c6\u7684\u8fdb\u5ea6\u5956\u52b1\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u8bbe\u8ba1\u4ecd\u9762\u4e34\u8bf8\u591a\u96be\u9898\u3002\u8bb8\u591a\u65b9\u6cd5\u4f9d\u8d56\u975e\u901a\u7528\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4e14\u6bcf\u4e2a\u573a\u666f\u90fd\u9700\u5355\u72ec\u8bbe\u8ba1\u3002", "AI": {"tldr": "\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u4e00\u79cd\u76f4\u63a5\u65b9\u6cd5\u662f\u63d0\u4f9b\u5bc6\u96c6\u7684\u8fdb\u5ea6\u5956\u52b1\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u8bbe\u8ba1\u4ecd\u9762\u4e34\u8bf8\u591a\u96be\u9898\u3002\u8bb8\u591a\u65b9\u6cd5\u4f9d\u8d56\u975e\u901a\u7528\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4e14\u6bcf\u4e2a\u573a\u666f\u90fd\u9700\u5355\u72ec\u8bbe\u8ba1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.e04e8631", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MzY1MTQ4Mw==&mid=2247484352&idx=1&sn=3545da2c960a375f99d35b59da3aeba6&chksm=916b75a5288b3512e267e2fafd4695e29a3be4b82809a86f3162200fbbfa84b1aaf42cafb293#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MzY1MTQ4Mw==&mid=2247484352&idx=1&sn=3545da2c960a375f99d35b59da3aeba6&chksm=916b75a5288b3512e267e2fafd4695e29a3be4b82809a86f3162200fbbfa84b1aaf42cafb293#rd", "authors": ["Wonderful\u4eff\u771f"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4bQ-Learning\u8ba9\u673a\u5668\u50cf\u4eba\u4e00\u6837\u5b66\u4f1a\u505a\u51b3\u7b56", "comment": "Source: WeChat, Published: 2025-09-22 04:21:22", "summary": "\u8fd9\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u5c31\u5f88\u50cf\u4eca\u5929\u8981\u4ecb\u7ecd\u7684Q-Learning\u7b97\u6cd5\u2014\u2014\u4e00\u79cd\u8ba9\u673a\u5668\u901a\u8fc7\u8bd5\u9519\u6765\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u4ec0\u4e48\u662fQ-Learning\uff1fQ-Learning\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u6700\u7ecf\u5178\u7684\u7b97\u6cd5\u4e4b\u4e00\uff0c\u7531Christopher Watkins\u57281989\u5e74\u63d0\u51fa\u3002", "AI": {"tldr": "\u8fd9\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u5c31\u5f88\u50cf\u4eca\u5929\u8981\u4ecb\u7ecd\u7684Q-Learning\u7b97\u6cd5\u2014\u2014\u4e00\u79cd\u8ba9\u673a\u5668\u901a\u8fc7\u8bd5\u9519\u6765\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u4ec0\u4e48\u662fQ-Learning\uff1fQ-Learning\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u6700\u7ecf\u5178\u7684\u7b97\u6cd5\u4e4b\u4e00\uff0c\u7531Christopher Watkins\u57281989\u5e74\u63d0\u51fa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.41480bf7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MzU2NzM5MA==&mid=2649679058&idx=1&sn=cbb482d6c0543f7e7e11b537c6190e75&chksm=bf97f26cfdd38a28d5a20a2c98e09d60167627815a578b295023c946d2b029a854c100598a1f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MzU2NzM5MA==&mid=2649679058&idx=1&sn=cbb482d6c0543f7e7e11b537c6190e75&chksm=bf97f26cfdd38a28d5a20a2c98e09d60167627815a578b295023c946d2b029a854c100598a1f#rd", "authors": ["\u4fe1\u606f\u7f51\u7edc\u5de5\u7a0b\u7814\u7a76\u4e2d\u5fc3"], "title": "\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u57fa\u7840\uff1a\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-09-22 02:32:12", "summary": "\u636e\u6211\u4eec\u6240\u77e5\uff0c\u672c\u7efc\u8ff0\u662f\u9996\u4e2a\u4e13\u95e8\u805a\u7126\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u7684\u5de5\u4f5c\u3002\u672c\u6587\u6cbf\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u5316\u4e86 DeepSeek-R1 \u4e4b\u540e\u7684\u7814\u7a76\uff1a\uff08i\uff09 \u6570\u636e\u5408\u6210\u4e0e\u6570\u636e\u6574\u7406\uff1b", "AI": {"tldr": "\u636e\u6211\u4eec\u6240\u77e5\uff0c\u672c\u7efc\u8ff0\u662f\u9996\u4e2a\u4e13\u95e8\u805a\u7126\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u7684\u5de5\u4f5c\u3002\u672c\u6587\u6cbf\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u5316\u4e86 DeepSeek-R1 \u4e4b\u540e\u7684\u7814\u7a76\uff1a\uff08i\uff09 \u6570\u636e\u5408\u6210\u4e0e\u6570\u636e\u6574\u7406\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.ba1821eb", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDkxMzM0MA==&mid=2247486246&idx=1&sn=dfabd2fc3f493d3a22de5c083d583535&chksm=c58b55e49141d7045d84a9e67e5e3db3f8bed984324d69102372048767c1a22bc51e553c9941#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDkxMzM0MA==&mid=2247486246&idx=1&sn=dfabd2fc3f493d3a22de5c083d583535&chksm=c58b55e49141d7045d84a9e67e5e3db3f8bed984324d69102372048767c1a22bc51e553c9941#rd", "authors": ["\u6e17\u900f\u667a\u80fdAGI"], "title": "9/21/2025 AI\u901f\u9012 | \u7845\u8c37\u521b\u4e1a\u516c\u53f8\u6253\u9020<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u73af\u5883\uff0c\u63a8\u52a8AI\u8bad\u7ec3\u65b0\u70ed\u6f6e", "comment": "Source: WeChat, Published: 2025-09-22 01:21:59", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u667a\u80fd\u4ee3\u7406\u5728\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u5b66\u4e60\u548c\u4f18\u5316\u5176\u884c\u4e3a\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\u3002\u5f53\u524d\uff0c\u8bb8\u591aAI\u5b9e\u9a8c\u5ba4\u6b63\u9762\u4e34\u6570\u636e\u83b7\u53d6\u548c\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u800c\u8fd9\u4e9b\u521d\u521b\u516c\u53f8\u63d0\u4f9b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u663e\u8457\u63d0", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u667a\u80fd\u4ee3\u7406\u5728\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u5b66\u4e60\u548c\u4f18\u5316\u5176\u884c\u4e3a\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\u3002\u5f53\u524d\uff0c\u8bb8\u591aAI\u5b9e\u9a8c\u5ba4\u6b63\u9762\u4e34\u6570\u636e\u83b7\u53d6\u548c\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u800c\u8fd9\u4e9b\u521d\u521b\u516c\u53f8\u63d0\u4f9b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u663e\u8457\u63d0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.e068b9e1", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNTc2NDA2Mg==&mid=2247494075&idx=1&sn=7dc797715f961c9e50383f89da4ec601&chksm=fbeacdb6b7a8e9d1f4e715d9cf8509f7c637b9beb46c956fe179252216cf01077406a45044a3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNTc2NDA2Mg==&mid=2247494075&idx=1&sn=7dc797715f961c9e50383f89da4ec601&chksm=fbeacdb6b7a8e9d1f4e715d9cf8509f7c637b9beb46c956fe179252216cf01077406a45044a3#rd", "authors": ["\u5783\u573e\u5206\u7c7b\u7ad9"], "title": "\u4f7f\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u89e3\u51b3\u9910\u996e\u914d\u9001\u670d\u52a1\u4e2d\u7684\u9a91\u624b\u8def\u5f84\u89c4\u5212\u4e0e\u5206\u914d\u95ee\u9898", "comment": "Source: WeChat, Published: 2025-09-22 00:01:40", "summary": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\uff08agent\uff09\u5728\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u4e2d\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u9884\u671f\u7684\u7d2f\u8ba1\u5956\u52b1\uff0c\u4e3a\u89e3\u51b3\u8fd9\u79cd\u590d\u6742\u7684\u52a8\u6001\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002\u7814\u7a76\u95ee\u9898", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\uff08agent\uff09\u5728\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u4e2d\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u9884\u671f\u7684\u7d2f\u8ba1\u5956\u52b1\uff0c\u4e3a\u89e3\u51b3\u8fd9\u79cd\u590d\u6742\u7684\u52a8\u6001\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002\u7814\u7a76\u95ee\u9898", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.26579847", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNTcyOTExNQ==&mid=2247485451&idx=1&sn=d9b9658629aac9845ff5cd8843d30c14&chksm=c01f26b5d8fce9486bf21a0beb66e6e75559b30033fade3bced2086c5625d36afaeffc68103c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNTcyOTExNQ==&mid=2247485451&idx=1&sn=d9b9658629aac9845ff5cd8843d30c14&chksm=c01f26b5d8fce9486bf21a0beb66e6e75559b30033fade3bced2086c5625d36afaeffc68103c#rd", "authors": ["AI\u79d1\u7814\u8fdb\u9636\u793e"], "title": "\u6e05\u534e\u5468\u4f2f\u6587\u6559\u6388\u8bfe\u9898\u7ec4\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b0\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-09-21 12:00:00", "summary": "\u56fe\u4e2d\u9996\u5148\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u5305\u62ec\u72b6\u6001\uff08State\uff09\u3001\u52a8\u4f5c\uff08Action\uff09\u3001\u5956\u52b1\uff08Reward\uff09\u548c\u7b56\u7565\uff08Policy\uff09\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\u4e2d\uff0c\u4ee3\u7406\uff08Agent\uff09\u4e0e\u73af\u5883\uff08Environment\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ee3\u7406\u6839\u636e\u5f53\u524d\u72b6\u6001\u9009\u62e9\u52a8\u4f5c\uff0c\u73af\u5883\u5219\u6839\u636e\u4ee3\u7406\u7684\u52a8\u4f5c\u63d0", "AI": {"tldr": "\u56fe\u4e2d\u9996\u5148\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u5305\u62ec\u72b6\u6001\uff08State\uff09\u3001\u52a8\u4f5c\uff08Action\uff09\u3001\u5956\u52b1\uff08Reward\uff09\u548c\u7b56\u7565\uff08Policy\uff09\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\u4e2d\uff0c\u4ee3\u7406\uff08Agent\uff09\u4e0e\u73af\u5883\uff08Environment\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ee3\u7406\u6839\u636e\u5f53\u524d\u72b6\u6001\u9009\u62e9\u52a8\u4f5c\uff0c\u73af\u5883\u5219\u6839\u636e\u4ee3\u7406\u7684\u52a8\u4f5c\u63d0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.f6af3a26", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNzI0ODE4Nw==&mid=2247497620&idx=1&sn=41f65d527e8c78d00dfa10e85f29bd29&chksm=96608b9ffb9f50863685bd182699b0ff92af2d7d84840cf9cdf530e688ac1ae781d15cd60445#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNzI0ODE4Nw==&mid=2247497620&idx=1&sn=41f65d527e8c78d00dfa10e85f29bd29&chksm=96608b9ffb9f50863685bd182699b0ff92af2d7d84840cf9cdf530e688ac1ae781d15cd60445#rd", "authors": ["DeeplearningAI"], "title": "\u5434\u6069\u8fbe\u6765\u4fe1\uff1a<em class=\"highlight\">Agentic</em>\u7f16\u7a0b\u4e0e<em class=\"highlight\">Agentic</em>\u8f6f\u4ef6\u6d4b\u8bd5\u534f\u540c\u5408\u4f5c", "comment": "Source: WeChat, Published: 2025-09-22 04:45:38", "summary": "* \u201c\u5956\u52b1\u4f5c\u5f0a\u201d\uff0c\u5373\u7f16\u7a0b\u667a\u80fd\u4f53\u4fee\u6539\u6d4b\u8bd5\u4ee3\u7801\uff0c\u4f7f\u6d4b\u8bd5\u66f4\u5bb9\u6613\u901a\u8fc7\u3002* \u4e00\u4e2a\u667a\u80fd\u4f53\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u8fd0\u884c\u4e86\u201crm \\*.py\u201d\uff0c\u5bfc\u81f4\u6574\u4e2a\u9879\u76ee\u7684\u4ee3\u7801\u88ab\u5220\u9664\uff08\u5e78\u8fd0\u7684\u662f\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5907\u4efd\uff09\u3002", "AI": {"tldr": "* \u201c\u5956\u52b1\u4f5c\u5f0a\u201d\uff0c\u5373\u7f16\u7a0b\u667a\u80fd\u4f53\u4fee\u6539\u6d4b\u8bd5\u4ee3\u7801\uff0c\u4f7f\u6d4b\u8bd5\u66f4\u5bb9\u6613\u901a\u8fc7\u3002* \u4e00\u4e2a\u667a\u80fd\u4f53\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u8fd0\u884c\u4e86\u201crm \\*.py\u201d\uff0c\u5bfc\u81f4\u6574\u4e2a\u9879\u76ee\u7684\u4ee3\u7801\u88ab\u5220\u9664\uff08\u5e78\u8fd0\u7684\u662f\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5907\u4efd\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.8de22a67", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNDkxMjQ3OA==&mid=2247485249&idx=1&sn=ee46811f756bb43805db4bd0b5278afb&chksm=e9874817aaa8ea59ba65b9ccd707ec0e4a1520ce2e21361faf0f63dc8dd3ce47478658603092#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNDkxMjQ3OA==&mid=2247485249&idx=1&sn=ee46811f756bb43805db4bd0b5278afb&chksm=e9874817aaa8ea59ba65b9ccd707ec0e4a1520ce2e21361faf0f63dc8dd3ce47478658603092#rd", "authors": ["\u673a\u5668\u4e4b\u9b42"], "title": "10 \u500d\u6548\u7387\u30010 \u4eba\u5de5\u5e72\u9884\uff01\u4f01\u4e1a\u7ea7\u201c<em class=\"highlight\">Agentic</em> AI \u751f\u547d\u5468\u671f\u201d\u9996\u6b21\u66dd\u5149\uff0c\u6253\u5de5\u4eba\u770b\u5b8c\u76f4\u63a5\u6c89\u9ed8", "comment": "Source: WeChat, Published: 2025-09-22 03:03:16", "summary": "\u4f8b\u5982\uff0cAgent2Agent\uff08A2A\uff09\u534f\u8bae\u6307\u5b9a\u4e86\u667a\u80fd\u4f53\u5361\uff08\u4e00\u4e2aJSON\u6587\u6863\uff09\u7684\u6982\u5ff5\uff0c\u5b83\u5145\u5f53\u667a\u80fd\u4f53\u7684\u6570\u5b57\u201c\u540d\u7247\u201d\u3002\u5b83\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u4fe1\u606f\uff1aCopyIdentity\uff1a \u540d\u79f0\u3001\u63cf\u8ff0\u3001\u63d0\u4f9b\u8005\u4fe1\u606f\u3002", "AI": {"tldr": "\u4f8b\u5982\uff0cAgent2Agent\uff08A2A\uff09\u534f\u8bae\u6307\u5b9a\u4e86\u667a\u80fd\u4f53\u5361\uff08\u4e00\u4e2aJSON\u6587\u6863\uff09\u7684\u6982\u5ff5\uff0c\u5b83\u5145\u5f53\u667a\u80fd\u4f53\u7684\u6570\u5b57\u201c\u540d\u7247\u201d\u3002\u5b83\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u4fe1\u606f\uff1aCopyIdentity\uff1a \u540d\u79f0\u3001\u63cf\u8ff0\u3001\u63d0\u4f9b\u8005\u4fe1\u606f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.4bfbc40c", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3OTc2MTUyNg==&mid=2247499966&idx=1&sn=9a0789646bb8cc31f422bb033d7750c8&chksm=fc5dfeea95223a684a955d6e13ed3ca8c28061013dbf5d7f6a6ef8b12f0021a8e4f1f1cfc677#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3OTc2MTUyNg==&mid=2247499966&idx=1&sn=9a0789646bb8cc31f422bb033d7750c8&chksm=fc5dfeea95223a684a955d6e13ed3ca8c28061013dbf5d7f6a6ef8b12f0021a8e4f1f1cfc677#rd", "authors": ["\u5fae\u9489\u79d1\u6280"], "title": "<em class=\"highlight\">Agentic</em> AI : \u52a9\u529b\u4e2d\u56fd\u6c7d\u8f66\u96f6\u90e8\u4ef6\u4f01\u4e1a\u9ad8\u6548\u51fa\u6d77", "comment": "Source: WeChat, Published: 2025-09-22 02:27:06", "summary": "\u667a\u80fd\u5173\u52a1\u667a\u80fd\u4f53Intelligent Customs Agent\u667a\u6167\u5173\u52a1 submit\u30021\u3001\u6253\u5f00\u4fe1\u606f\u5f55\u5165\u8868\u5355\u30022\u3001\u4e0a\u4f20\u4f9b\u5e94\u5546\u5355\u636e\u30023\u3001ai\u81ea\u52a8\u586b\u5145\u4fe1\u606f\u30024\u3001\u4eba\u5de5\u4e8c\u6b21\u4fe1\u606f\u786e\u8ba4\u30025\u3001\u63d0\u4ea4\u6700\u7ec8\u5f55\u5165\u8868\u5355\u3002", "AI": {"tldr": "\u667a\u80fd\u5173\u52a1\u667a\u80fd\u4f53Intelligent Customs Agent\u667a\u6167\u5173\u52a1 submit\u30021\u3001\u6253\u5f00\u4fe1\u606f\u5f55\u5165\u8868\u5355\u30022\u3001\u4e0a\u4f20\u4f9b\u5e94\u5546\u5355\u636e\u30023\u3001ai\u81ea\u52a8\u586b\u5145\u4fe1\u606f\u30024\u3001\u4eba\u5de5\u4e8c\u6b21\u4fe1\u606f\u786e\u8ba4\u30025\u3001\u63d0\u4ea4\u6700\u7ec8\u5f55\u5165\u8868\u5355\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.b8878a22", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzODMwMzY2MA==&mid=2247483953&idx=1&sn=3b8be31236c26900a1d6907cdd83fb1b&chksm=c39e3a99171441dbcf1f900cec83b7073c9b4fff882996948bf9789db1a89fc7388438b5b4f2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzODMwMzY2MA==&mid=2247483953&idx=1&sn=3b8be31236c26900a1d6907cdd83fb1b&chksm=c39e3a99171441dbcf1f900cec83b7073c9b4fff882996948bf9789db1a89fc7388438b5b4f2#rd", "authors": ["ABCD\u542f\u793a\u5f55X"], "title": "<em class=\"highlight\">Agentic</em> AI \u5546\u4e1a\u843d\u5730\u7684\u516d\u5927\u5173\u952e\u7ecf\u9a8c-By \u9ea6\u80af\u9521", "comment": "Source: WeChat, Published: 2025-09-22 01:10:25", "summary": "The #1 mistake\uff1fthat win don't ask \"how cool is this agent\uff1f\" They ask \"how much faster can Sarah complete her entire workflow\uff1fvariance\uff1f\" if not\uff0c you're overengineering.\u30021. rule-based + structured data = use automation\uff0c not agents\u3002", "AI": {"tldr": "The #1 mistake\uff1fthat win don't ask \"how cool is this agent\uff1f\" They ask \"how much faster can Sarah complete her entire workflow\uff1fvariance\uff1f\" if not\uff0c you're overengineering.\u30021. rule-based + structured data ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.dc1fd4cf", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5OTI0ODY2Mg==&mid=2247625768&idx=2&sn=70eb861501f30ceb7f755edf368cd022&chksm=ff727cdf530230f1fbd7f1173b6d2b9cff3ab840225ccef0eb7ac7eb9e54be6b5cb56de5d265#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5OTI0ODY2Mg==&mid=2247625768&idx=2&sn=70eb861501f30ceb7f755edf368cd022&chksm=ff727cdf530230f1fbd7f1173b6d2b9cff3ab840225ccef0eb7ac7eb9e54be6b5cb56de5d265#rd", "authors": ["\u9655\u897f\u7701\u56fd\u751f\u5546\u4f1a"], "title": "\u3010\u4eba\u5de5\u667a\u80fd\u3011|Nature\u5c01\u9762\u805a\u7126DeepSeek-R1\uff1a\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u201c\u6559\u4f1a\u201d<em class=\"highlight\">\u5927\u6a21\u578b</em>\u81ea\u4e3b\u63a8\u7406\uff1f", "comment": "Source: WeChat, Published: 2025-09-22 06:21:08", "summary": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u680f 9\u670817\u65e5\uff0c\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u9886\u57df\u8fce\u6765\u4e86\u4e00\u9879\u91cc\u7a0b\u7891\u5f0f\u7684\u6210\u5c31\u3002\u7531DeepSeek\u56e2\u961f\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek-R1\u7684\u7814\u7a76\u6210\u679c\u2014\u2014\u300aDeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning\u300b\uff08DeepSeek-R1\u5229\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u4e3a\u5927\u6a21\u578b", "AI": {"tldr": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u680f 9\u670817\u65e5\uff0c\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u9886\u57df\u8fce\u6765\u4e86\u4e00\u9879\u91cc\u7a0b\u7891\u5f0f\u7684\u6210\u5c31\u3002\u7531DeepSeek\u56e2\u961f\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek-R1\u7684\u7814\u7a76\u6210\u679c\u2014\u2014\u300aDeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning\u300b\uff08DeepSeek-R1\u5229\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u4e3a\u5927\u6a21\u578b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.2fd5db19", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247708466&idx=1&sn=eec5d0fcdaf9dbbd80d1f03b5f634d95&chksm=97d231fed7ed800714cda2ab012e7e12fbb21f17ed38d13d023c7978bac00b5285680102ff84#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247708466&idx=1&sn=eec5d0fcdaf9dbbd80d1f03b5f634d95&chksm=97d231fed7ed800714cda2ab012e7e12fbb21f17ed38d13d023c7978bac00b5285680102ff84#rd", "authors": ["PaperWeekly"], "title": "RLHF\u8981\u4e0b\u5c97\uff1fMeta \u00d7 \u725b\u6d25\u641e\u51fa\u65b0\u5957\u8def\uff1a\u7528\u7b97\u529b\u6559\u7b97\u529b\uff0c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bad\u7ec3\u65b0\u8303\u5f0f\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-09-22 06:17:28", "summary": "\u5728\u6ca1\u6709\u6807\u51c6\u7b54\u6848\u7684\u4efb\u52a1\u91cc\uff0c\u5927\u6a21\u578b\u8be5\u5411\u8c01\u5b66\u4e60\uff1f\u957f\u671f\u4ee5\u6765\uff0c\u6211\u4eec\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u3001LLM \u5224\u5b98\u6216\u591a\u6570\u6295\u7968\u6765\u4e3a\u6a21\u578b\u63d0\u4f9b\u76d1\u7763\uff0c\u4f46\u8fd9\u4e9b\u65b9\u5f0f\u8981\u4e48\u6210\u672c\u9ad8\u6602\uff0c\u8981\u4e48\u504f\u597d\u660e\u663e\uff0c\u8981\u4e48\u53ea\u80fd\u5728\u5019\u9009\u91cc\u201c\u6311\u6700\u4e0d\u5dee\u7684\u201d\u3002", "AI": {"tldr": "\u5728\u6ca1\u6709\u6807\u51c6\u7b54\u6848\u7684\u4efb\u52a1\u91cc\uff0c\u5927\u6a21\u578b\u8be5\u5411\u8c01\u5b66\u4e60\uff1f\u957f\u671f\u4ee5\u6765\uff0c\u6211\u4eec\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u3001LLM \u5224\u5b98\u6216\u591a\u6570\u6295\u7968\u6765\u4e3a\u6a21\u578b\u63d0\u4f9b\u76d1\u7763\uff0c\u4f46\u8fd9\u4e9b\u65b9\u5f0f\u8981\u4e48\u6210\u672c\u9ad8\u6602\uff0c\u8981\u4e48\u504f\u597d\u660e\u663e\uff0c\u8981\u4e48\u53ea\u80fd\u5728\u5019\u9009\u91cc\u201c\u6311\u6700\u4e0d\u5dee\u7684\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.30663335", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652629554&idx=3&sn=f4a3a225e71b1015d6ca6884c63e1113&chksm=f0c696a70976731e49ff5402eebe7263b6de5bfa0edb43b672bec73906f564cbaee0f589d2f4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652629554&idx=3&sn=f4a3a225e71b1015d6ca6884c63e1113&chksm=f0c696a70976731e49ff5402eebe7263b6de5bfa0edb43b672bec73906f564cbaee0f589d2f4#rd", "authors": ["\u65b0\u667a\u5143"], "title": "\u6bd4\u601d\u7ef4\u94fe\u51c643%\uff01\u903b\u8f91\u8111+<em class=\"highlight\">\u5927\u6a21\u578b</em>\u76f4\u89c9\uff0c\u63a8\u7406\u53ef\u9760\u6027\u5927\u5e45\u63d0\u5347", "comment": "Source: WeChat, Published: 2025-09-22 05:38:57", "summary": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5728\u6587\u672c\u751f\u6210\u3001\u4ee3\u7801\u7f16\u5199\u4e43\u81f3\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u80fd\u529b\uff0c\u4f46\u5728\u6d89\u53ca\u4e25\u8c28\u903b\u8f91\u4e0e\u7269\u7406\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u5b83\u4eec\u4ecd\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\u3002", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5728\u6587\u672c\u751f\u6210\u3001\u4ee3\u7801\u7f16\u5199\u4e43\u81f3\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u80fd\u529b\uff0c\u4f46\u5728\u6d89\u53ca\u4e25\u8c28\u903b\u8f91\u4e0e\u7269\u7406\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u5b83\u4eec\u4ecd\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.510cf247", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247670980&idx=6&sn=974792504434c204f633c2d2c4d4a76e&chksm=fdccd3bd0a494f024882e711efe18ddbb6a3031624c38210d1c690dfd54d496fe0df5925bd69#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247670980&idx=6&sn=974792504434c204f633c2d2c4d4a76e&chksm=fdccd3bd0a494f024882e711efe18ddbb6a3031624c38210d1c690dfd54d496fe0df5925bd69#rd", "authors": ["\u4e13\u77e5"], "title": "2025\u5fc5\u770bAI\u5e72\u8d27!\u300a<em class=\"highlight\">\u5927\u6a21\u578b</em>/AIGC/GPT-4/Transformer/DL/KG/NLP/CV AI+X\u300b\u96c6\u5408", "comment": "Source: WeChat, Published: 2025-09-22 03:02:26", "summary": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\u7efc\u8ff0\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684AI\u667a\u80fd\u4f53\u901a\u4fe1\u7efc\u8ff0\uff1a\u534f\u8bae\u3001\u5b89\u5168\u98ce\u9669\u4e0e\u9632\u5fa1\u5bf9\u7b56\u3010\u535a\u58eb\u8bba\u6587\u3011\u7528\u4e8e\u5b9a\u4f4d\u3001\u91cd\u5efa\u4e0e\u6e32\u67d3\u7684\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u795e\u7ecf\u8868\u793a", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\u7efc\u8ff0\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684AI\u667a\u80fd\u4f53\u901a\u4fe1\u7efc\u8ff0\uff1a\u534f\u8bae\u3001\u5b89\u5168\u98ce\u9669\u4e0e\u9632\u5fa1\u5bf9\u7b56\u3010\u535a\u58eb\u8bba\u6587\u3011\u7528\u4e8e\u5b9a\u4f4d\u3001\u91cd\u5efa\u4e0e\u6e32\u67d3\u7684\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u795e\u7ecf\u8868\u793a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.d97ae210", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MDc2MDI0OQ==&mid=2247556270&idx=2&sn=eab4d87569da9aef1fc0ddefd658f069&chksm=e8c25159b42490c65b16fc33c32a3d6060d65ff35a438d486af4e373aa38ba9993eb4478b658#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MDc2MDI0OQ==&mid=2247556270&idx=2&sn=eab4d87569da9aef1fc0ddefd658f069&chksm=e8c25159b42490c65b16fc33c32a3d6060d65ff35a438d486af4e373aa38ba9993eb4478b658#rd", "authors": ["\u4eca\u65e5\u94a2\u57ce"], "title": "\u9996\u6279\u4e0a\u7ebf\uff01\u9996\u94a2\u80a1\u4efd\u7b2c\u4e00\u6279AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u6295\u5165\u4f7f\u7528", "comment": "Source: WeChat, Published: 2025-09-22 02:48:48", "summary": "\u52a9\u529b\u5b89\u5168\u9690\u60a3 \u201c\u65e9\u53d1\u73b0\u3001\u65e9\u6574\u6539\u201d \u9996\u94a2\u80a1\u4efd\u5927\u6a21\u578b\u5e73\u53f0 ouyang \u5f00\u59cb\u4f1a\u8bdd \u9690\u60a3\u9879 \u9690\u60a3\u63cf\u8ff0 \u6807\u51c6\u4f9d\u636e \u9690\u60a3 \u7c7b\u522b \u6f5c\u5728\u5371\u9669 \u6574\u6539\u63aa\u65bd \u7535\u7ebf\u7834\u635f \u591a\u6839\u7535\u7ebf\u8868\u9762\u6709\u660e\u663e\u7834", "AI": {"tldr": "\u52a9\u529b\u5b89\u5168\u9690\u60a3 \u201c\u65e9\u53d1\u73b0\u3001\u65e9\u6574\u6539\u201d \u9996\u94a2\u80a1\u4efd\u5927\u6a21\u578b\u5e73\u53f0 ouyang \u5f00\u59cb\u4f1a\u8bdd \u9690\u60a3\u9879 \u9690\u60a3\u63cf\u8ff0 \u6807\u51c6\u4f9d\u636e \u9690\u60a3 \u7c7b\u522b \u6f5c\u5728\u5371\u9669 \u6574\u6539\u63aa\u65bd \u7535\u7ebf\u7834\u635f \u591a\u6839\u7535\u7ebf\u8868\u9762\u6709\u660e\u663e\u7834", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.13e78e6e", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MTU4MjA2MQ==&mid=2247501208&idx=1&sn=62a158c800a90fe88a08651182c058d2&chksm=cf2744c76aad43f474faaefb69c7d1c94eb2ca610d6618039ad6c3be37e5f2a3bd587176a449#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MTU4MjA2MQ==&mid=2247501208&idx=1&sn=62a158c800a90fe88a08651182c058d2&chksm=cf2744c76aad43f474faaefb69c7d1c94eb2ca610d6618039ad6c3be37e5f2a3bd587176a449#rd", "authors": ["\u7528\u53cb\u91d1\u878d"], "title": "\u91d1\u878d<em class=\"highlight\">\u5927\u6a21\u578b</em>+Agent\uff1a\u6784\u5efa\u667a\u80fd\u91d1\u878d\u7ba1\u7406\u65b0\u573a\u666f", "comment": "Source: WeChat, Published: 2025-09-22 02:40:37", "summary": "\u5927\u6a21\u578b+\u667a\u80fd\u4f53 \u8fd1\u671f\uff0c\u91d1\u878d\u884c\u4e1aAI\u6280\u672f\u843d\u5730\u660e\u663e\u63d0\u901f\uff0c\u5927\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u5e94\u7528\u6b63\u52a0\u901f\u4ece\u6982\u5ff5\u9a8c\u8bc1\u8d70\u5411\u4e1a\u52a1\u6df1\u6c34\u533a\uff0c\u63a8\u52a8\u667a\u80fd\u8fd0\u8425\u3001\u98ce\u9669\u7ba1\u63a7\u3001\u6570\u636e\u51b3\u7b56\u7b49\u6838\u5fc3\u73af\u8282\u7684\u6548\u7387\u53d8\u9769\u3002", "AI": {"tldr": "\u5927\u6a21\u578b+\u667a\u80fd\u4f53 \u8fd1\u671f\uff0c\u91d1\u878d\u884c\u4e1aAI\u6280\u672f\u843d\u5730\u660e\u663e\u63d0\u901f\uff0c\u5927\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u5e94\u7528\u6b63\u52a0\u901f\u4ece\u6982\u5ff5\u9a8c\u8bc1\u8d70\u5411\u4e1a\u52a1\u6df1\u6c34\u533a\uff0c\u63a8\u52a8\u667a\u80fd\u8fd0\u8425\u3001\u98ce\u9669\u7ba1\u63a7\u3001\u6570\u636e\u51b3\u7b56\u7b49\u6838\u5fc3\u73af\u8282\u7684\u6548\u7387\u53d8\u9769\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.564f2e49", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTI1MzgwMg==&mid=2247484944&idx=1&sn=c0f5091293a5a5281bfcf9f61b995542&chksm=c537393de353c99c7177edaddd72dcf8d6e7fb067cba31e93eb740f31d1ff07db4e53d1f4ca1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTI1MzgwMg==&mid=2247484944&idx=1&sn=c0f5091293a5a5281bfcf9f61b995542&chksm=c537393de353c99c7177edaddd72dcf8d6e7fb067cba31e93eb740f31d1ff07db4e53d1f4ca1#rd", "authors": ["\u70db\u9f99\u7167\u7f51"], "title": "AI\u5708'<em class=\"highlight\">\u6a21\u578b\u5927</em>\u7206\u70b8'\uff1a6<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6740\u75af\u4e86\uff012025\u5e74\u666e\u901a\u4eba\u5fc5\u987b\u77e5\u9053\u7684\u751f\u5b58\u6307\u5357", "comment": "Source: WeChat, Published: 2025-09-22 02:23:26", "summary": "\u8fd9\u79cd\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\uff0c\u8ba9\u5b83\u57282025\u5e74\u5e74\u4e2d\u8fc5\u901f\u62a2\u5360\u4f01\u4e1a\u7ea7\u5927\u6a21\u578b\u5e02\u573a20%\u7684\u4efd\u989d\uff0c\u6210\u4e3a\u589e\u957f\u6700\u5feb\u7684\u53c2\u4e0e\u8005\u4e4b\u4e00\u3002\u65e0\u8bba\u662f\u5f00\u53d1\u8005\u7528\u5b83\u8c03\u8bd5\u590d\u6742\u4ee3\u7801\uff0c\u5b66\u751f\u7528\u5b83\u8f85\u52a9\u6570\u5b66\u89e3\u9898\uff0c\u8fd8\u662f\u8bbe\u8ba1\u5e08\u7528\u5b83\u5c06\u521b\u610f\u53ef\u89c6\u5316\uff0cGemini 2.5 Pro\u90fd\u5728\u8bc1\u660e\uff1aAI", "AI": {"tldr": "\u8fd9\u79cd\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\uff0c\u8ba9\u5b83\u57282025\u5e74\u5e74\u4e2d\u8fc5\u901f\u62a2\u5360\u4f01\u4e1a\u7ea7\u5927\u6a21\u578b\u5e02\u573a20%\u7684\u4efd\u989d\uff0c\u6210\u4e3a\u589e\u957f\u6700\u5feb\u7684\u53c2\u4e0e\u8005\u4e4b\u4e00\u3002\u65e0\u8bba\u662f\u5f00\u53d1\u8005\u7528\u5b83\u8c03\u8bd5\u590d\u6742\u4ee3\u7801\uff0c\u5b66\u751f\u7528\u5b83\u8f85\u52a9\u6570\u5b66\u89e3\u9898\uff0c\u8fd8\u662f\u8bbe\u8ba1\u5e08\u7528\u5b83\u5c06\u521b\u610f\u53ef\u89c6\u5316\uff0cGemini 2.5 Pro\u90fd\u5728\u8bc1\u660e\uff1aAI", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.46acec5f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650449986&idx=1&sn=67d62aa5bb686521a8321c769b882468&chksm=bf2493190114ba49954f768d52060ab35f39bb9d5aa46f05adc82da2995f2ee85fa0a887a2be#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650449986&idx=1&sn=67d62aa5bb686521a8321c769b882468&chksm=bf2493190114ba49954f768d52060ab35f39bb9d5aa46f05adc82da2995f2ee85fa0a887a2be#rd", "authors": ["AINLP"], "title": "\u51e0\u4e4e\u89e3\u51b3\u6240\u6709\u591a\u6a21\u6001<em class=\"highlight\">\u5927\u6a21\u578b</em>\u95ee\u9898", "comment": "Source: WeChat, Published: 2025-09-22 02:11:10", "summary": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411 \u591a\u6a21\u6001transformer\u7684\u4e03\u5341\u4e8c\u53d8 \u4efb\u610f\u89c6\u89c9\u63d0\u793a\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b \u591a\u6a21\u6001-lisa \uff08cvpr2024\uff09 \u6700\u65b0\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684idea \u591a\u6a21\u6001agents\u53ca\u5176\u5e94\u7528\u3002", "AI": {"tldr": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411 \u591a\u6a21\u6001transformer\u7684\u4e03\u5341\u4e8c\u53d8 \u4efb\u610f\u89c6\u89c9\u63d0\u793a\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b \u591a\u6a21\u6001-lisa \uff08cvpr2024\uff09 \u6700\u65b0\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684idea \u591a\u6a21\u6001agents\u53ca\u5176\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.cbe6e863", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2MzcxMTU5Mg==&mid=2247532970&idx=1&sn=af316b8bdb4dade0dfe17d0c3caf3d38&chksm=eb39ac92370da0d3177b4f7c4551d3b378ee62e5140a86404f6648d7849f4ee24871e5d354bb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2MzcxMTU5Mg==&mid=2247532970&idx=1&sn=af316b8bdb4dade0dfe17d0c3caf3d38&chksm=eb39ac92370da0d3177b4f7c4551d3b378ee62e5140a86404f6648d7849f4ee24871e5d354bb#rd", "authors": ["\u56db\u5ddd\u5174\u5408\u7530\u804c\u4e1a\u6559\u80b2\u7814\u7a76\u9662"], "title": "\u4e2d\u56fd\u4fe1\u901a\u9662\u7275\u5934\u76845\u9879<em class=\"highlight\">\u5927\u6a21\u578b</em>\u884c\u4e1a\u6807\u51c6\u6b63\u5f0f\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-09-22 01:46:55", "summary": "\u8be5\u7cfb\u5217\u6807\u51c6\u8986\u76d6\u5927\u6a21\u578b\u7684\u5f00\u53d1\u3001\u7ba1\u7406\u3001\u8fd0\u8425\u7b49\u591a\u4e2a\u9636\u6bb5\uff0c\u4e3b\u8981\u5305\u62ec\u6a21\u578b\u5f00\u53d1\u3001\u80fd\u529b\u8bc4\u4f30\u3001\u5e94\u7528\u6210\u6548\u3001\u8fd0\u8425\u7ba1\u7406\u548c\u53ef\u4fe1\u8981\u6c42\u4e94\u90e8\u5206\uff0c\u4e3a\u5927\u6a21\u578b\u6280\u672f\u548c\u4ea7\u54c1\u7684\u7814\u53d1\u6d4b\u8bd5\u548c\u5e94\u7528\u63a8\u5e7f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "AI": {"tldr": "\u8be5\u7cfb\u5217\u6807\u51c6\u8986\u76d6\u5927\u6a21\u578b\u7684\u5f00\u53d1\u3001\u7ba1\u7406\u3001\u8fd0\u8425\u7b49\u591a\u4e2a\u9636\u6bb5\uff0c\u4e3b\u8981\u5305\u62ec\u6a21\u578b\u5f00\u53d1\u3001\u80fd\u529b\u8bc4\u4f30\u3001\u5e94\u7528\u6210\u6548\u3001\u8fd0\u8425\u7ba1\u7406\u548c\u53ef\u4fe1\u8981\u6c42\u4e94\u90e8\u5206\uff0c\u4e3a\u5927\u6a21\u578b\u6280\u672f\u548c\u4ea7\u54c1\u7684\u7814\u53d1\u6d4b\u8bd5\u548c\u5e94\u7528\u63a8\u5e7f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2509.fe89383a", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzODIzNzE0NQ==&mid=2654455438&idx=1&sn=232cfdc7d033d0640fcacf2a25c54c6d&chksm=f3501ab8f8123553cfb5a88c04f9410e627aa54698551d13100a473c8cf38c128d5299b1b3ec#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzODIzNzE0NQ==&mid=2654455438&idx=1&sn=232cfdc7d033d0640fcacf2a25c54c6d&chksm=f3501ab8f8123553cfb5a88c04f9410e627aa54698551d13100a473c8cf38c128d5299b1b3ec#rd", "authors": ["\u7384\u59d0\u804aAGI"], "title": "\u4e00\u6587\u8bfb\u61c2 Go \u8bed\u8a00 AI \u667a\u80fd\u4f53\u6846\u67b6 Eino\uff1a\u7075\u6d3b\u9ad8\u6548\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\u5de5\u5177", "comment": "Source: WeChat, Published: 2025-09-22 00:01:29", "summary": "Tool\u6269\u5c55\u5927\u6a21\u578b\u80fd\u529b\u7684\u5de5\u5177\uff08\u6bd4\u5982\uff1a\u641c\u7d22\u5f15\u64ce\u3001\u6570\u636e\u5e93\u67e5\u8be2\u3001\u6587\u4ef6\u8bfb\u5199\uff09\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u3002Embedding\u628a\u6587\u672c\u8f6c\u6210\u5411\u91cf\uff08\u65b9\u4fbf\u505a\u8bed\u4e49\u641c\u7d22\uff09\u3002Retriever\u4ece\u5411\u91cf\u5e93 / \u6587\u6863\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u5185\u5bb9\uff08\u5927\u6a21\u578b \u201c\u67e5\u8d44\u6599\u201d \u7684\u5173\u952e\uff09\u3002", "AI": {"tldr": "Tool\u6269\u5c55\u5927\u6a21\u578b\u80fd\u529b\u7684\u5de5\u5177\uff08\u6bd4\u5982\uff1a\u641c\u7d22\u5f15\u64ce\u3001\u6570\u636e\u5e93\u67e5\u8be2\u3001\u6587\u4ef6\u8bfb\u5199\uff09\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u3002Embedding\u628a\u6587\u672c\u8f6c\u6210\u5411\u91cf\uff08\u65b9\u4fbf\u505a\u8bed\u4e49\u641c\u7d22\uff09\u3002Retriever\u4ece\u5411\u91cf\u5e93 / \u6587\u6863\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u5185\u5bb9\uff08\u5927\u6a21\u578b \u201c\u67e5\u8d44\u6599\u201d \u7684\u5173\u952e\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1753d725", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDEyMTM1Mg==&mid=2247485492&idx=2&sn=d7abea6e005a1c4cc241d4d301ae3774&chksm=c54dde5ff4dd85716d6e0b4b75f1b8086c836699e49922fd3f8a81a1930bb1e0e414e938ffd0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDEyMTM1Mg==&mid=2247485492&idx=2&sn=d7abea6e005a1c4cc241d4d301ae3774&chksm=c54dde5ff4dd85716d6e0b4b75f1b8086c836699e49922fd3f8a81a1930bb1e0e414e938ffd0#rd", "authors": ["GPU\u90a3\u4e9b\u4e8b\u513f"], "title": "\u767d\u8bdd\u6a21\u578b-01\u4e4b<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7814\u53d1\u5168\u6d41\u7a0b\u4e00\u6587\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-09-21 23:08:53", "summary": "\u8fd9\u662f\u8ba9\u5927\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u5173\u952e\u6280\u672f\u3002\u56db\u3001\u8bc4\u4f30\u4e0e\u8fed\u4ee3 \uff08Evaluation & Iteration\uff09\u6a21\u578b\u8bad\u7ec3\u4e0d\u662f\u4e00\u8e74\u800c\u5c31\u7684\uff0c\u9700\u8981\u6301\u7eed\u8bc4\u4f30\u548c\u4f18\u5316\u3002\u8bc4\u4f30\u57fa\u51c6\uff1a \u4f7f\u7528\u4e00\u7cfb\u5217\u6807\u51c6\u5316\u7684\u5b66\u672f\u57fa\u51c6\uff08\u5982MMLU\u7528\u4e8e\u6d4b\u8bd5 Massive Multitask Language Understanding", "AI": {"tldr": "\u8fd9\u662f\u8ba9\u5927\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u5173\u952e\u6280\u672f\u3002\u56db\u3001\u8bc4\u4f30\u4e0e\u8fed\u4ee3 \uff08Evaluation & Iteration\uff09\u6a21\u578b\u8bad\u7ec3\u4e0d\u662f\u4e00\u8e74\u800c\u5c31\u7684\uff0c\u9700\u8981\u6301\u7eed\u8bc4\u4f30\u548c\u4f18\u5316\u3002\u8bc4\u4f30\u57fa\u51c6\uff1a \u4f7f\u7528\u4e00\u7cfb\u5217\u6807\u51c6\u5316\u7684\u5b66\u672f\u57fa\u51c6\uff08\u5982MMLU\u7528\u4e8e\u6d4b\u8bd5 Massive Multitask Language Understanding", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2509.f0b13452", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODcwOTExMQ==&mid=2655833018&idx=3&sn=008c4399d1ff8ea0351f14a337d87b55&chksm=8a003ab5c6cdfd6fb725a38347920d1be76caf592e6e17206ec1da15be2d0757e5745adc2f6f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODcwOTExMQ==&mid=2655833018&idx=3&sn=008c4399d1ff8ea0351f14a337d87b55&chksm=8a003ab5c6cdfd6fb725a38347920d1be76caf592e6e17206ec1da15be2d0757e5745adc2f6f#rd", "authors": ["\u4e2d\u56fd\u6307\u6325\u4e0e\u63a7\u5236\u5b66\u4f1a"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8fdb\u5c55\u4e13\u680f\u7b2c\u5341\u671f\u4e28Nature\u5c01\u9762\u805a\u7126DeepSeek-R1\uff1a\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u201c\u6559\u4f1a\u201d<em class=\"highlight\">\u5927\u6a21\u578b</em>\u81ea\u4e3b\u63a8\u7406\uff1f", "comment": "Source: WeChat, Published: 2025-09-21 13:11:55", "summary": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u680f 9\u670817\u65e5\uff0c\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u9886\u57df\u8fce\u6765\u4e86\u4e00\u9879\u91cc\u7a0b\u7891\u5f0f\u7684\u6210\u5c31\u3002\u7531DeepSeek\u56e2\u961f\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek-R1\u7684\u7814\u7a76\u6210\u679c\u2014\u2014\u300aDeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning\u300b\uff08DeepSeek-R1\u5229\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u4e3a\u5927\u6a21\u578b", "AI": {"tldr": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u680f 9\u670817\u65e5\uff0c\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u9886\u57df\u8fce\u6765\u4e86\u4e00\u9879\u91cc\u7a0b\u7891\u5f0f\u7684\u6210\u5c31\u3002\u7531DeepSeek\u56e2\u961f\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek-R1\u7684\u7814\u7a76\u6210\u679c\u2014\u2014\u300aDeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning\u300b\uff08DeepSeek-R1\u5229\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u4e3a\u5927\u6a21\u578b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
