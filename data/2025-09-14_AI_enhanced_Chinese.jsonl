{"id": "2509.08863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08863", "abs": "https://arxiv.org/abs/2509.08863", "authors": ["Qianqian Luo", "Liuchang Xu", "Qingming Lin", "Sensen Wu", "Ruichen Mao", "Chao Wang", "Hailin Feng", "Bo Huang", "Zhenhong Du"], "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation", "comment": null, "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding.However,without expertise in GIS,they continue to encounter\nlimitations.To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture.This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques:Function Calling and Code\nGeneration.The architecture consists of three components-task parsing,agent\ncollaboration,and result integration-aimed at enhancing both the performance\nand scalability of GIS automation.The Planner agent interprets natural language\ntasks into structured GeoJSON commands.Then,specialized Worker agents\ncollaborate according to assigned roles to perform spatial data processing and\nanalysis,either by invoking predefined function APIs or by dynamically\ngenerating and executing Python-based spatial analysis code.Finally,the system\nintegrates the outputs from multiple execution rounds into\nreusable,standards-compliant GeoJSON files.To systematically evaluate the\nperformance of the two approaches,we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model.Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%,while the Code Generation-based agent\nreached 97.14%,both significantly outperforming the best-performing\ngeneral-purpose model (48.57%).Further analysis reveals that the Code\nGeneration provides greater flexibility,whereas the Function Calling approach\noffers more stable execution.This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods,offering new perspectives\nfor improving GeoAI system performance.", "AI": {"tldr": "GeoJSON Agents\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u901a\u8fc7Function Calling\u548cCode Generation\u4e24\u79cd\u65b9\u6cd5\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u8f6c\u6362\u4e3aGeoJSON\u64cd\u4f5c\u547d\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u4e86GIS\u81ea\u52a8\u5316\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4efb\u52a1\u81ea\u52a8\u5316\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4fGIS\u4e13\u4e1a\u77e5\u8bc6\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u9700\u8981\u89e3\u51b3LLMs\u5728\u7a7a\u95f4\u6570\u636e\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u67b6\u6784\uff1a\u4efb\u52a1\u89e3\u6790\u3001\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7ed3\u679c\u96c6\u6210\u3002Planner\u667a\u80fd\u4f53\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u4e3aGeoJSON\u547d\u4ee4\uff0cWorker\u667a\u80fd\u4f53\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u6216\u4ee3\u7801\u751f\u6210\u6267\u884c\u7a7a\u95f4\u6570\u636e\u5904\u7406\uff0c\u6700\u7ec8\u6574\u5408\u4e3a\u6807\u51c6GeoJSON\u6587\u4ef6\u3002", "result": "\u57fa\u4e8eFunction Calling\u7684\u65b9\u6cd5\u8fbe\u523085.71%\u51c6\u786e\u7387\uff0c\u57fa\u4e8eCode Generation\u7684\u65b9\u6cd5\u8fbe\u523097.14%\u51c6\u786e\u7387\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u7684\u6700\u4f73\u8868\u73b0(48.57%)\u3002\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u66f4\u7075\u6d3b\uff0c\u51fd\u6570\u8c03\u7528\u65b9\u6cd5\u6267\u884c\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9GeoJSON\u6570\u636e\u7684LLM\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e3b\u6d41LLM\u589e\u5f3a\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u4e3a\u6539\u8fdbGeoAI\u7cfb\u7edf\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.08865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08865", "abs": "https://arxiv.org/abs/2509.08865", "authors": ["Guangyu Zhang", "Xixuan Wang", "Shiyu Sun", "Peiyan Xiao", "Kun Sun", "Yanhai Xiong"], "title": "TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis", "comment": null, "summary": "Sophisticated evasion tactics in malicious Android applications, combined\nwith their intricate behavioral semantics, enable attackers to conceal\nmalicious logic within legitimate functions, underscoring the critical need for\nrobust and in-depth analysis frameworks. However, traditional analysis\ntechniques often fail to recover deeply hidden behaviors or provide\nhuman-readable justifications for their decisions. Inspired by advances in\nlarge language models (LLMs), we introduce TraceRAG, a retrieval-augmented\ngeneration (RAG) framework that bridges natural language queries and Java code\nto deliver explainable malware detection and analysis. First, TraceRAG\ngenerates summaries of method-level code snippets, which are indexed in a\nvector database. At query time, behavior-focused questions retrieve the most\nsemantically relevant snippets for deeper inspection. Finally, based on the\nmulti-turn analysis results, TraceRAG produces human-readable reports that\npresent the identified malicious behaviors and their corresponding code\nimplementations. Experimental results demonstrate that our method achieves 96\\%\nmalware detection accuracy and 83.81\\% behavior identification accuracy based\non updated VirusTotal (VT) scans and manual verification. Furthermore, expert\nevaluation confirms the practical utility of the reports generated by TraceRAG.", "AI": {"tldr": "TraceRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684Android\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548cJava\u4ee3\u7801\u5206\u6790\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6076\u610f\u884c\u4e3a\u68c0\u6d4b\uff0c\u8fbe\u523096%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c83.81%\u7684\u884c\u4e3a\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "Android\u6076\u610f\u5e94\u7528\u91c7\u7528\u590d\u6742\u7684\u89c4\u907f\u7b56\u7565\uff0c\u5c06\u6076\u610f\u903b\u8f91\u9690\u85cf\u5728\u5408\u6cd5\u529f\u80fd\u4e2d\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u6df1\u5c42\u9690\u85cf\u884c\u4e3a\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\uff0c\u9996\u5148\u751f\u6210\u65b9\u6cd5\u7ea7\u4ee3\u7801\u7247\u6bb5\u7684\u6458\u8981\u5e76\u5efa\u7acb\u5411\u91cf\u7d22\u5f15\uff0c\u7136\u540e\u901a\u8fc7\u884c\u4e3a\u5bfc\u5411\u7684\u95ee\u9898\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u6700\u7ec8\u751f\u6210\u5305\u542b\u6076\u610f\u884c\u4e3a\u53ca\u5176\u4ee3\u7801\u5b9e\u73b0\u7684\u53ef\u8bfb\u62a5\u544a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8fbe\u523096%\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u51c6\u786e\u7387\u548c83.81%\u7684\u884c\u4e3a\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u751f\u6210\u62a5\u544a\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "TraceRAG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfAndroid\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u6076\u610f\u884c\u4e3a\u68c0\u6d4b\u548c\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2509.08867", "categories": ["cs.SE", "cs.AI", "68T01", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08867", "abs": "https://arxiv.org/abs/2509.08867", "authors": ["K. Pronk", "Q. Zhao"], "title": "Benchmarking Energy Efficiency of Large Language Models Using vLLM", "comment": "6 pages, 6 figures", "summary": "The prevalence of Large Language Models (LLMs) is having an growing impact on\nthe climate due to the substantial energy required for their deployment and\nuse. To create awareness for developers who are implementing LLMs in their\nproducts, there is a strong need to collect more information about the energy\nefficiency of LLMs. While existing research has evaluated the energy efficiency\nof various models, these benchmarks often fall short of representing realistic\nproduction scenarios. In this paper, we introduce the LLM Efficiency Benchmark,\ndesigned to simulate real-world usage conditions. Our benchmark utilizes vLLM,\na high-throughput, production-ready LLM serving backend that optimizes model\nperformance and efficiency. We examine how factors such as model size,\narchitecture, and concurrent request volume affect inference energy efficiency.\nOur findings demonstrate that it is possible to create energy efficiency\nbenchmarks that better reflect practical deployment conditions, providing\nvaluable insights for developers aiming to build more sustainable AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM\u80fd\u6548\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528vLLM\u6a21\u62df\u771f\u5b9e\u751f\u4ea7\u573a\u666f\uff0c\u8bc4\u4f30\u6a21\u578b\u5927\u5c0f\u3001\u67b6\u6784\u548c\u5e76\u53d1\u8bf7\u6c42\u91cf\u5bf9\u63a8\u7406\u80fd\u6548\u7684\u5f71\u54cd\u3002", "motivation": "LLM\u7684\u5e7f\u6cdb\u90e8\u7f72\u548c\u4f7f\u7528\u5bf9\u6c14\u5019\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u6536\u96c6\u66f4\u591a\u5173\u4e8eLLM\u80fd\u6548\u7684\u4fe1\u606f\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u65e0\u6cd5\u4ee3\u8868\u771f\u5b9e\u751f\u4ea7\u573a\u666f\u3002", "method": "\u4f7f\u7528vLLM\uff08\u9ad8\u541e\u5410\u91cf\u3001\u751f\u4ea7\u5c31\u7eea\u7684LLM\u670d\u52a1\u540e\u7aef\uff09\u6784\u5efa\u80fd\u6548\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4f7f\u7528\u6761\u4ef6\uff0c\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001\u67b6\u6784\u548c\u5e76\u53d1\u8bf7\u6c42\u91cf\u7b49\u56e0\u7d20\u3002", "result": "\u8bc1\u660e\u4e86\u53ef\u4ee5\u521b\u5efa\u66f4\u597d\u5730\u53cd\u6620\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u7684\u80fd\u6548\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u8005\u6784\u5efa\u66f4\u53ef\u6301\u7eed\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u80fd\u6e90\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u53ef\u6301\u7eed\u7684AI\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2509.09072", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09072", "abs": "https://arxiv.org/abs/2509.09072", "authors": ["Ahmed Adnan", "Mushfiqur Rahman", "Saad Sakib Noor", "Kazi Sakib"], "title": "CLARA: A Developer's Companion for Code Comprehension and Analysis", "comment": "In proceedings at the 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "Code comprehension and analysis of open-source project codebases is a task\nfrequently performed by developers and researchers. However, existing tools\nthat practitioners use for assistance with such tasks often require prior\nproject setup, lack context-awareness, and involve significant manual effort.\nTo address this, we present CLARA, a browser extension that utilizes a\nstate-of-the-art inference model to assist developers and researchers in: (i)\ncomprehending code files and code fragments, (ii) code refactoring, and (iii)\ncode quality attribute detection. We qualitatively evaluated CLARA's inference\nmodel using existing datasets and methodology, and performed a comprehensive\nuser study with 10 developers and academic researchers to assess its usability\nand usefulness. The results show that CLARA is useful, accurate, and practical\nin code comprehension and analysis tasks. CLARA is an open-source tool\navailable at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing\nthe full capabilities of CLARA can be found at\nhttps://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.", "AI": {"tldr": "CLARA\u662f\u4e00\u4e2a\u6d4f\u89c8\u5668\u6269\u5c55\u5de5\u5177\uff0c\u5229\u7528\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u4ee3\u7801\u7406\u89e3\u3001\u91cd\u6784\u548c\u8d28\u91cf\u68c0\u6d4b\uff0c\u65e0\u9700\u9879\u76ee\u8bbe\u7f6e\uff0c\u7ecf\u8bc4\u4f30\u8bc1\u660e\u5b9e\u7528\u51c6\u786e\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5206\u6790\u5de5\u5177\u9700\u8981\u9879\u76ee\u8bbe\u7f6e\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u624b\u52a8\u5de5\u4f5c\u91cf\u5927\uff0cCLARA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u4ee3\u7801\u7406\u89e3\u8f85\u52a9\u3002", "method": "\u5f00\u53d1\u6d4f\u89c8\u5668\u6269\u5c55\u5de5\u5177CLARA\uff0c\u4f7f\u7528\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\uff0c\u652f\u6301\u4ee3\u7801\u6587\u4ef6\u7406\u89e3\u3001\u4ee3\u7801\u91cd\u6784\u548c\u4ee3\u7801\u8d28\u91cf\u5c5e\u6027\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u73b0\u6709\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793aCLARA\u5728\u4ee3\u7801\u7406\u89e3\u548c\u5206\u6790\u4efb\u52a1\u4e2d\u5b9e\u7528\u3001\u51c6\u786e\u4e14\u5b9e\u7528\uff0c\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "CLARA\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7801\u5206\u6790\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4ee3\u7801\u7406\u89e3\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08933", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.08933", "abs": "https://arxiv.org/abs/2509.08933", "authors": ["Sreejeet Maity", "Aritra Mitra"], "title": "Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates", "comment": null, "summary": "We consider the problem of learning the optimal policy in a discounted,\ninfinite-horizon reinforcement learning (RL) setting where the reward signal is\nsubject to adversarial corruption. Such corruption, which may arise from\nextreme noise, sensor faults, or malicious attacks, can severely degrade the\nperformance of classical algorithms such as Q-learning. To address this\nchallenge, we propose a new provably robust variant of the Q-learning algorithm\nthat operates effectively even when a fraction of the observed rewards are\narbitrarily perturbed by an adversary. Under the asynchronous sampling model\nwith time-correlated data, we establish that despite adversarial corruption,\nthe finite-time convergence rate of our algorithm matches that of existing\nresults for the non-adversarial case, up to an additive term proportional to\nthe fraction of corrupted samples. Moreover, we derive an information-theoretic\nlower bound revealing that the additive corruption term in our upper bounds is\nunavoidable.\n  Next, we propose a variant of our algorithm that requires no prior knowledge\nof the statistics of the true reward distributions. The analysis of this\nsetting is particularly challenging and is enabled by carefully exploiting a\nrefined Azuma-Hoeffding inequality for almost-martingales, a technical tool\nthat might be of independent interest. Collectively, our contributions provide\nthe first finite-time robustness guarantees for asynchronous Q-learning,\nbridging a significant gap in robust RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5956\u52b1\u4fe1\u53f7\u906d\u53d7\u5bf9\u6297\u6027\u7834\u574f\u7684\u9c81\u68d2Q\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5f02\u6b65\u91c7\u6837\u6a21\u578b\u4e0b\u8bc1\u660e\u4e86\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\uff0c\u6536\u655b\u901f\u7387\u4e0e\u975e\u5bf9\u6297\u60c5\u51b5\u76f8\u8fd1\uff0c\u4ec5\u589e\u52a0\u4e00\u4e2a\u4e0e\u7834\u574f\u6837\u672c\u6bd4\u4f8b\u6210\u6b63\u6bd4\u7684\u9644\u52a0\u9879\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u53ef\u80fd\u53d7\u5230\u6781\u7aef\u566a\u58f0\u3001\u4f20\u611f\u5668\u6545\u969c\u6216\u6076\u610f\u653b\u51fb\u7684\u5bf9\u6297\u6027\u7834\u574f\uff0c\u8fd9\u4f1a\u4e25\u91cd\u964d\u4f4e\u4f20\u7edf\u7b97\u6cd5\uff08\u5982Q\u5b66\u4e60\uff09\u7684\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u90e8\u5206\u5956\u52b1\u88ab\u4efb\u610f\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u8fd0\u884c\u7684\u9c81\u68d2\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u8bc1\u660e\u9c81\u68d2\u7684Q\u5b66\u4e60\u7b97\u6cd5\u53d8\u4f53\uff0c\u5728\u5f02\u6b65\u91c7\u6837\u6a21\u578b\u4e0b\u8fd0\u884c\u3002\u5bf9\u4e8e\u672a\u77e5\u771f\u5b9e\u5956\u52b1\u5206\u5e03\u7edf\u8ba1\u91cf\u7684\u60c5\u51b5\uff0c\u4f7f\u7528\u4e86\u6539\u8fdb\u7684Azuma-Hoeffding\u4e0d\u7b49\u5f0f\u6765\u5904\u7406\u51e0\u4e4e\u9785\u7684\u6280\u672f\u5de5\u5177\u3002", "result": "\u7b97\u6cd5\u5728\u5bf9\u6297\u6027\u7834\u574f\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u9650\u65f6\u95f4\u6536\u655b\uff0c\u6536\u655b\u901f\u7387\u4e0e\u975e\u5bf9\u6297\u60c5\u51b5\u5339\u914d\uff0c\u4ec5\u589e\u52a0\u4e00\u4e2a\u4e0e\u7834\u574f\u6837\u672c\u6bd4\u4f8b\u6210\u6b63\u6bd4\u7684\u4e0d\u53ef\u907f\u514d\u7684\u9644\u52a0\u9879\u3002\u63a8\u5bfc\u7684\u4fe1\u606f\u7406\u8bba\u4e0b\u754c\u8868\u660e\u8be5\u9644\u52a0\u9879\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f02\u6b65Q\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u6709\u9650\u65f6\u95f4\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u586b\u8865\u4e86\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u6240\u63d0\u51fa\u7684\u6280\u672f\u5de5\u5177\u53ef\u80fd\u5177\u6709\u72ec\u7acb\u7684\u7814\u7a76\u4ef7\u503c\u3002"}}
{"id": "2509.09192", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09192", "abs": "https://arxiv.org/abs/2509.09192", "authors": ["Doha Nam", "Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset", "comment": "An anonymous link containing the dataset, construction scripts, and\n  experimental code is publicly available for reproducibility:\n  https://figshare.com/s/4f202bc0921e26b41dc2", "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReDef\u6570\u636e\u96c6\uff0c\u901a\u8fc7revert commits\u6784\u5efa\u9ad8\u8d28\u91cf\u7f3a\u9677\u6807\u6ce8\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4fee\u6539\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3", "motivation": "\u73b0\u6709JIT-SDP\u6570\u636e\u96c6\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u7f3a\u9677\u6807\u6ce8\u57fa\u51c6\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u63a8\u7406\u4ee3\u7801\u4fee\u6539", "method": "\u57fa\u4e8erevert commits\u6784\u5efaReDef\u6570\u636e\u96c6\uff0c\u4f7f\u7528GPT\u8f85\u52a9\u7b5b\u9009\u8fc7\u7a0b\uff1b\u5bf9CodeBERT\u3001CodeT5+\u548cUniXcoder\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u4e94\u79cd\u7f16\u7801\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u6d4b\u8bd5\u6a21\u578b\u654f\u611f\u6027", "result": "\u7d27\u51d1\u7684diff\u98ce\u683c\u7f16\u7801\u5728\u6240\u6709PLM\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5b8c\u6574\u51fd\u6570\u683c\u5f0f\uff1b\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u663e\u793a\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\uff0c\u8868\u660e\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3", "conclusion": "\u4e0e\u57fa\u4e8e\u5feb\u7167\u7684\u4efb\u52a1\u4e0d\u540c\uff0c\u5f53\u524dPLM\u5728\u771f\u6b63\u7406\u89e3\u4ee3\u7801\u4fee\u6539\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u7814\u7a76\u6765\u63d0\u5347\u6a21\u578b\u5bf9\u4ee3\u7801\u4fee\u6539\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b"}}
{"id": "2509.09194", "categories": ["cs.SE", "cs.AI", "68N19"], "pdf": "https://arxiv.org/pdf/2509.09194", "abs": "https://arxiv.org/abs/2509.09194", "authors": ["Ayelet Berzack", "Guy Katz"], "title": "On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability", "comment": null, "summary": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u6280\u672f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u7684\u7f16\u7a0b\u8303\u5f0f\u6765\u63d0\u9ad8\u8f6f\u4ef6\u5f00\u53d1\u53ef\u9760\u6027\uff0c\u5e76\u5728Connect4\u6e38\u620f\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136LLMs\u80fd\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u4f1a\u4ea7\u751f\u4e25\u91cd\u9519\u8bef\u5e76\u4ee5\u81ea\u4fe1\u7684\u65b9\u5f0f\u5448\u73b0\u9519\u8bef\u4ee3\u7801\uff0c\u8bef\u5bfc\u5f00\u53d1\u8005\u63a5\u53d7\u6709\u7f3a\u9677\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u5f0f\u5c06LLMs\u6574\u5408\u5230\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u4e2d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u7f16\u7a0b\uff08SBP\uff09\u8303\u5f0f\uff0c\u8fd9\u662f\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u3001\u57fa\u4e8e\u573a\u666f\u7684\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u5c06\u4e13\u4e1a\u77e5\u8bc6\u6ce8\u5165LLM\uff0c\u5e76\u68c0\u67e5\u548c\u9a8c\u8bc1\u5176\u8f93\u51fa\u3002", "result": "\u901a\u8fc7Connect4\u6e38\u620f\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u7ed3\u5408LLMs\u548cSBP\u521b\u5efa\u4e86\u9ad8\u6027\u80fd\u7684\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u51fb\u8d25\u5404\u79cd\u73b0\u6709\u5f3a\u667a\u80fd\u4f53\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u591f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u667a\u80fd\u4f53\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u3001\u51cf\u5c11\u9519\u8bef\uff0c\u5e76\u63d0\u9ad8\u7528\u6237\u5bf9\u5173\u952e\u7a0b\u5e8f\u5c5e\u6027\u9a8c\u8bc1\u7684\u4fe1\u5fc3\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u6613\u7528\u6027\u65b9\u9762\u7684\u6709\u8da3\u89c1\u89e3\u3002"}}
{"id": "2509.09294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09294", "abs": "https://arxiv.org/abs/2509.09294", "authors": ["Solal Rapaport", "Laurent Pautet", "Samuel Tardieu", "Stefano Zacchiroli"], "title": "Altered Histories in Version Control System Repositories: Evidence from the Trenches", "comment": null, "summary": "Version Control Systems (VCS) like Git allow developers to locally rewrite\nrecorded history, e.g., to reorder and suppress commits or specific data in\nthem. These alterations have legitimate use cases, but become problematic when\nperformed on public branches that have downstream users: they break push/pull\nworkflows, challenge the integrity and reproducibility of repositories, and\ncreate opportunities for supply chain attackers to sneak into them nefarious\nchanges. We conduct the first large-scale investigation of Git history\nalterations in public code repositories. We analyze 111 M (millions)\nrepositories archived by Software Heritage, which preserves VCS histories even\nacross alterations. We find history alterations in 1.22 M repositories, for a\ntotal of 8.7 M rewritten histories. We categorize changes by where they happen\n(which repositories, which branches) and what is changed in them (files or\ncommit metadata). Conducting two targeted case studies we show that altered\nhistories recurrently change licenses retroactively, or are used to remove\n''secrets'' (e.g., private keys) committed by mistake. As these behaviors\ncorrespond to bad practices-in terms of project governance or security\nmanagement, respectively-that software recipients might want to avoid, we\nintroduce GitHistorian, an automated tool, that developers can use to spot and\ndescribe history alterations in public Git repositories.", "AI": {"tldr": "\u5bf91.11\u4ebf\u4e2aGit\u4ed3\u5e93\u7684\u5927\u89c4\u6a21\u7814\u7a76\u53d1\u73b0\uff0c\u6709122\u4e07\u4e2a\u4ed3\u5e93\u5b58\u5728\u5386\u53f2\u8bb0\u5f55\u7be1\u6539\u884c\u4e3a\uff0c\u603b\u8ba1870\u4e07\u6b21\u5386\u53f2\u91cd\u5199\u3002\u8fd9\u4e9b\u7be1\u6539\u5305\u62ec\u8bb8\u53ef\u8bc1\u53d8\u66f4\u548c\u5bc6\u94a5\u5220\u9664\u7b49\uff0c\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u548c\u6cbb\u7406\u98ce\u9669\u3002", "motivation": "Git\u7b49\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u5141\u8bb8\u5f00\u53d1\u8005\u5728\u672c\u5730\u91cd\u5199\u5386\u53f2\u8bb0\u5f55\uff0c\u4f46\u5f53\u8fd9\u4e9b\u7be1\u6539\u53d1\u751f\u5728\u516c\u5171\u5206\u652f\u4e0a\u65f6\uff0c\u4f1a\u7834\u574f\u63a8\u9001/\u62c9\u53d6\u5de5\u4f5c\u6d41\uff0c\u5a01\u80c1\u4ed3\u5e93\u7684\u5b8c\u6574\u6027\u548c\u53ef\u91cd\u73b0\u6027\uff0c\u5e76\u4e3a\u4f9b\u5e94\u94fe\u653b\u51fb\u521b\u9020\u673a\u4f1a\u3002", "method": "\u5206\u6790\u4e86Software Heritage\u5b58\u6863\u76841.11\u4ebf\u4e2a\u4ee3\u7801\u4ed3\u5e93\uff0c\u8bc6\u522b\u5386\u53f2\u7be1\u6539\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u9488\u5bf9\u6027\u6848\u4f8b\u7814\u7a76\uff08\u8bb8\u53ef\u8bc1\u53d8\u66f4\u548c\u5bc6\u94a5\u5220\u9664\uff09\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u53d1\u73b0122\u4e07\u4e2a\u4ed3\u5e93\u5b58\u5728\u5386\u53f2\u7be1\u6539\uff0c\u603b\u8ba1870\u4e07\u6b21\u91cd\u5199\u64cd\u4f5c\u3002\u7be1\u6539\u884c\u4e3a\u4e3b\u8981\u96c6\u4e2d\u5728\u7279\u5b9a\u4ed3\u5e93\u548c\u5206\u652f\uff0c\u6d89\u53ca\u6587\u4ef6\u5185\u5bb9\u6216\u63d0\u4ea4\u5143\u6570\u636e\u7684\u4fee\u6539\u3002", "conclusion": "\u5386\u53f2\u7be1\u6539\u884c\u4e3a\u53cd\u6620\u4e86\u9879\u76ee\u6cbb\u7406\u548c\u5b89\u5168\u7ba1\u7406\u7684\u574f\u5b9e\u8df5\uff0c\u4e3a\u6b64\u5f00\u53d1\u4e86GitHistorian\u5de5\u5177\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u63cf\u8ff0\u516c\u5171Git\u4ed3\u5e93\u4e2d\u7684\u5386\u53f2\u7be1\u6539\u3002"}}
{"id": "2509.09313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09313", "abs": "https://arxiv.org/abs/2509.09313", "authors": ["Moritz Mock", "Thomas Forrer", "Barbara Russo"], "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data", "comment": "Accepted to the 26th International Conference on Product-Focused\n  Software Process Improvement (PROFES 2025)", "summary": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86CodeBERT\u5728\u4ea7\u4e1a\u548c\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u68c0\u6d4b\u6f0f\u6d1e\u7684\u6027\u80fd\uff0c\u5f00\u53d1\u4e86\u96c6\u6210\u5230CI/CD\u6d41\u7a0b\u7684AI-DO\u5de5\u5177\uff0c\u901a\u8fc7\u8c03\u67e5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u6280\u672f\u5728\u4ea7\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9047\u5230\u4e86\u4fe1\u4efb\u6027\u3001\u7ee7\u627f\u7cfb\u7edf\u3001\u6570\u5b57\u8bed\u8bc6\u7b49\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u5b66\u672f\u7814\u7a76\u8f6c\u5316\u4e3a\u5b9e\u9645\u5de5\u5177", "method": "\u9996\u5148\u8bc4\u4f30CodeBERT\u5728\u4ea7\u4e1a\u548c\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u68c0\u6d4b\u6f0f\u6d1e\u7684\u6027\u80fd\uff0c\u5206\u6790\u8de8\u57df\u6c47\u603b\u80fd\u529b\uff0c\u63a2\u7d22\u7c7b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\uff0c\u7136\u540e\u5f00\u53d1\u96c6\u6210\u5230CI/CD\u6d41\u7a0b\u7684AI-DO\u63a8\u8350\u7cfb\u7edf", "result": "\u57f9\u8bad\u5728\u4ea7\u4e1a\u6570\u636e\u4e0a\u7684\u6a21\u578b\u5728\u540c\u57df\u68c0\u6d4b\u51c6\u786e\uff0c\u4f46\u5728\u5f00\u6e90\u4ee3\u7801\u4e0a\u6027\u80fd\u4e0b\u964d\uff1b\u4f7f\u7528\u9002\u5f53\u4e0b\u91c7\u6837\u6280\u672f\u5728\u5f00\u6e90\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u80fd\u591f\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684AI-DO\u5de5\u5177\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6570\u636e\u5904\u7406\u7b56\u7565\u548c\u57f9\u8bad\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u9645\u4ea7\u4e1a\u73af\u5883\u4e2d\u7684\u6f0f\u6d1e\u68c0\u6d4b\u6548\u679c"}}
{"id": "2509.09630", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09630", "abs": "https://arxiv.org/abs/2509.09630", "authors": ["Zhenguang Liu", "Lixun Ma", "Zhongzheng Mu", "Chengkun Wei", "Xiaojun Xu", "Yingying Jiao", "Kui Ren"], "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection", "comment": null, "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%.", "AI": {"tldr": "SmartDetector\u662f\u4e00\u79cd\u65b0\u9896\u7684\u667a\u80fd\u5408\u7ea6\u51fd\u6570\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06AST\u5206\u89e3\u4e3a\u8bed\u53e5\u6811\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6bd4\u8f83\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5206\u6570\u8fbe\u523095.88%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534714.01%\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u4e2d\u5f00\u6e90\u4ee3\u7801\u7684\u5e7f\u6cdb\u91cd\u7528\u867d\u7136\u63d0\u9ad8\u4e86\u7f16\u7a0b\u6548\u7387\uff0c\u4f46\u663e\u8457\u52a0\u5267\u4e86\u6f0f\u6d1e\u4f20\u64ad\u3002\u73b0\u6709\u7684\u57fa\u4e8eAST\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u6811\u7ed3\u6784\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4ee3\u7801\u8bed\u6cd5\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "SmartDetector\u5c06\u667a\u80fd\u5408\u7ea6\u51fd\u6570\u7684AST\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u8f83\u5c0f\u7684\u8bed\u53e5\u6811\uff0c\u6bcf\u4e2a\u8bed\u53e5\u6811\u53cd\u6620\u6e90\u4ee3\u7801\u7684\u7ed3\u6784\u5143\u7d20\u3002\u7136\u540e\u4f7f\u7528\u5206\u7c7b\u5668\u901a\u8fc7\u6bd4\u8f83\u6bcf\u5bf9\u8bed\u53e5\u6811\u6765\u8ba1\u7b97\u4e24\u4e2a\u51fd\u6570\u7684\u76f8\u4f3c\u6027\u5f97\u5206\u3002\u4e3a\u4e86\u89e3\u51b3\u5206\u7c7b\u5668\u65e0\u9650\u8d85\u53c2\u6570\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u6570\u5b66\u63a8\u5bfc\u4e86\u4f59\u5f26\u6269\u6563\u8fc7\u7a0b\u6765\u9ad8\u6548\u641c\u7d22\u6700\u4f18\u8d85\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u578b\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSmartDetector\u5728F1\u5206\u6570\u4e0a\u5e73\u5747\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e8614.01%\uff0c\u603b\u4f53\u5e73\u5747F1\u5206\u6570\u8fbe\u523095.88%\u3002", "conclusion": "SmartDetector\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u7ec6\u7c92\u5ea6\u8bed\u53e5\u7ea7\u522b\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u5408\u7ea6\u51fd\u6570\u76f8\u4f3c\u6027\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6811\u7ed3\u6784\u548c\u4fdd\u6301\u68c0\u6d4b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.09009", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09009", "abs": "https://arxiv.org/abs/2509.09009", "authors": ["Marianna Nezhurina", "Taishi Nakamura", "Timur Carstensen", "Niccol\u00f2 Ajroldi", "Ville Komulainen", "David Salinas", "Jenia Jitsev"], "title": "Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison", "comment": "Model weights and intermediate checkpoints are available at\n  \\url{https://huggingface.co/collections/open-sci/open-sci-ref-001-685905e598be658fbcebff4f};\n  code for reproducing training, evaluation and raw experiments data at\n  \\url{https://github.com/LAION-AI/open-sci-ref-0.01}", "summary": "We introduce open-sci-ref, a family of dense transformer models trained as\nresearch baselines across multiple model (0.13B to 1.7B parameters) and token\nscales (up to 1T) on 8 recent open reference datasets. Evaluating the models on\nvarious standardized benchmarks, our training runs set establishes reference\npoints that enable researchers to assess the sanity and quality of alternative\ntraining approaches across scales and datasets. Intermediate checkpoints allow\ncomparison and studying of the training dynamics. The established reference\nbaselines allow training procedures to be compared through their scaling\ntrends, aligning them on a common compute axis. Comparison of open reference\ndatasets reveals that training on NemoTron-CC HQ consistently outperforms other\nreference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to\nintermediate training checkpoints, the release includes logs, code, and\ndownstream evaluations to simplify reproduction, standardize comparison, and\nfacilitate future research.", "AI": {"tldr": "open-sci-ref\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u7814\u7a76\u57fa\u7ebf\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u542b0.13B\u52301.7B\u53c2\u6570\u89c4\u6a21\uff0c\u57288\u4e2a\u5f00\u653e\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e2d\u95f4\u68c0\u67e5\u70b9\u548c\u5b8c\u6574\u8bad\u7ec3\u65e5\u5fd7\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u53c2\u8003\u57fa\u51c6\u3002", "motivation": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u53c2\u8003\u57fa\u7ebf\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u8d28\u91cf\uff0c\u4fc3\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6bd4\u8f83\u548c\u590d\u73b0\u3002", "method": "\u4f7f\u7528\u5bc6\u96c6transformer\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u53c2\u6570\u89c4\u6a21(0.13B-1.7B)\u548ctoken\u89c4\u6a21(\u6700\u9ad81T)\u4e0b\uff0c\u57288\u4e2a\u5f00\u653e\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u63d0\u4f9b\u4e2d\u95f4\u68c0\u67e5\u70b9\u3002", "result": "NemoTron-CC HQ\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f73\uff0c\u5176\u6b21\u662fDCLM-baseline\u548cFineWeb-Edu\uff1b\u5efa\u7acb\u4e86\u53ef\u6bd4\u8f83\u7684\u7f29\u653e\u8d8b\u52bf\u57fa\u51c6\uff0c\u4fbf\u4e8e\u8bad\u7ec3\u65b9\u6cd5\u7684\u5bf9\u6bd4\u5206\u6790\u3002", "conclusion": "open-sci-ref\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u7814\u7a76\u57fa\u7ebf\uff0c\u5305\u542b\u5b8c\u6574\u8bad\u7ec3\u8fc7\u7a0b\u8bb0\u5f55\u548c\u8bc4\u4f30\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u7b80\u5316\u590d\u73b0\u3001\u6807\u51c6\u5316\u6bd4\u8f83\u548c\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.09135", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.09135", "abs": "https://arxiv.org/abs/2509.09135", "authors": ["Xuefeng Wang", "Lei Zhang", "Henglin Pu", "Ahmed H. Qureshi", "Husheng Li"], "title": "Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning", "comment": "19 pages, 10 figures", "summary": "Existing reinforcement learning (RL) methods struggle with complex dynamical\nsystems that demand interactions at high frequencies or irregular time\nintervals. Continuous-time RL (CTRL) has emerged as a promising alternative by\nreplacing discrete-time Bellman recursion with differential value functions\ndefined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.\nWhile CTRL has shown promise, its applications have been largely limited to the\nsingle-agent domain. This limitation stems from two key challenges: (i)\nconventional solution methods for HJB equations suffer from the curse of\ndimensionality (CoD), making them intractable in high-dimensional systems; and\n(ii) even with HJB-based learning approaches, accurately approximating\ncentralized value functions in multi-agent settings remains difficult, which in\nturn destabilizes policy training. In this paper, we propose a CT-MARL\nframework that uses physics-informed neural networks (PINNs) to approximate\nHJB-based value functions at scale. To ensure the value is consistent with its\ndifferential structure, we align value learning with value-gradient learning by\nintroducing a Value Gradient Iteration (VGI) module that iteratively refines\nvalue gradients along trajectories. This improves gradient fidelity, in turn\nyielding more accurate values and stronger policy learning. We evaluate our\nmethod using continuous-time variants of standard benchmarks, including\nmulti-agent particle environment (MPE) and multi-agent MuJoCo. Our results\ndemonstrate that our approach consistently outperforms existing continuous-time\nRL baselines and scales to complex multi-agent dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u8fde\u7eed\u65f6\u95f4\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u503c\u68af\u5ea6\u8fed\u4ee3\u6a21\u5757\u89e3\u51b3\u9ad8\u7ef4HJB\u65b9\u7a0b\u548c\u591a\u667a\u80fd\u4f53\u4ef7\u503c\u51fd\u6570\u8fd1\u4f3c\u96be\u9898", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u9891\u6216\u975e\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u4ea4\u4e92\u7684\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u8fde\u7eed\u65f6\u95f4RL\u867d\u7136\u524d\u666f\u5e7f\u9614\u4f46\u4e3b\u8981\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u9886\u57df\uff0c\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u548c\u591a\u667a\u80fd\u4f53\u4ef7\u503c\u51fd\u6570\u8fd1\u4f3c\u4e0d\u51c6\u786e\u7684\u6311\u6218", "method": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u8fd1\u4f3cHJB\u65b9\u7a0b\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5f15\u5165\u503c\u68af\u5ea6\u8fed\u4ee3(VGI)\u6a21\u5757\u8fed\u4ee3\u4f18\u5316\u8f68\u8ff9\u4e0a\u7684\u4ef7\u503c\u68af\u5ea6\uff0c\u786e\u4fdd\u4ef7\u503c\u5b66\u4e60\u4e0e\u4ef7\u503c\u68af\u5ea6\u5b66\u4e60\u7684\u4e00\u81f4\u6027", "result": "\u5728\u8fde\u7eed\u65f6\u95f4\u7248\u672c\u7684\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u73af\u5883\u548c\u591a\u667a\u80fd\u4f53MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8fde\u7eed\u65f6\u95f4RL\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u590d\u6742\u591a\u667a\u80fd\u4f53\u52a8\u529b\u5b66\u7cfb\u7edf", "conclusion": "\u6240\u63d0\u51fa\u7684CT-MARL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7PINNs\u548cVGI\u6a21\u5757\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u7ef4HJB\u65b9\u7a0b\u7684\u6709\u6548\u6c42\u89e3\u548c\u7a33\u5b9a\u7b56\u7565\u5b66\u4e60"}}
{"id": "2509.09176", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.09176", "abs": "https://arxiv.org/abs/2509.09176", "authors": ["Jun-Hao Chen", "Yu-Chien Huang", "Yun-Cheng Tsai", "Samuel Yen-Chi Chen"], "title": "Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning", "comment": null, "summary": "The convergence of quantum-inspired neural networks and deep reinforcement\nlearning offers a promising avenue for financial trading. We implemented a\ntrading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)\nfor short-term trend prediction with Quantum Asynchronous Advantage\nActor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained\non data from 2000-01-01 to 2025-04-30 (80\\% training, 20\\% testing), the\nlong-only agent achieves 11.87\\% return over around 5 years with 0.92\\% max\ndrawdown, outperforming several currency ETFs. We detail state design (QLSTM\nfeatures and indicators), reward function for trend-following/risk control, and\nmulti-core training. Results show hybrid models yield competitive FX trading\nperformance. Implications include QLSTM's effectiveness for small-profit trades\nwith tight risk and future enhancements. Key hyperparameters: QLSTM sequence\nlength$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and\nsimplified strategy. \\footnote{The views expressed in this article are those of\nthe authors and do not represent the views of Wells Fargo. This article is for\ninformational purposes only. Nothing contained in this article should be\nconstrued as investment advice. Wells Fargo makes no express or implied\nwarranties and expressly disclaims all legal, tax, and accounting implications\nrelated to this article.", "AI": {"tldr": "\u91cf\u5b50\u542f\u53d1\u7684QLSTM\u4e0eQA3C\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728USD/TWD\u4ea4\u6613\u4e2d\u5b9e\u73b011.87%\u6536\u76ca\uff0c\u6700\u5927\u56de\u64a4\u4ec50.92%\uff0c\u4f18\u4e8e\u4f20\u7edf\u8d27\u5e01ETF", "motivation": "\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u91d1\u878d\u4ea4\u6613\u63d0\u4f9b\u65b0\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u5916\u6c47\u5e02\u573a\u5bfb\u6c42\u66f4\u597d\u7684\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca", "method": "\u96c6\u6210\u91cf\u5b50\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(QLSTM)\u8fdb\u884c\u77ed\u671f\u8d8b\u52bf\u9884\u6d4b\uff0c\u7ed3\u5408\u91cf\u5b50\u5f02\u6b65\u4f18\u52bf\u6f14\u5458\u8bc4\u8bba\u5bb6(QA3C)\u7b97\u6cd5\uff0c\u4f7f\u7528\u591a\u6838\u8bad\u7ec3\uff0c\u72b6\u6001\u8bbe\u8ba1\u5305\u542bQLSTM\u7279\u5f81\u548c\u6280\u672f\u6307\u6807", "result": "\u57282000-2025\u5e74\u6570\u636e\u4e0a\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u591a\u5934\u7b56\u7565\u5b9e\u73b011.87%\u76845\u5e74\u56de\u62a5\u7387\uff0c\u6700\u5927\u56de\u64a4\u4ec50.92%\uff0c\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u8d27\u5e01ETF", "conclusion": "\u6df7\u5408\u91cf\u5b50\u542f\u53d1\u6a21\u578b\u5728\u5916\u6c47\u4ea4\u6613\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0cQLSTM\u7279\u522b\u9002\u5408\u5c0f\u5229\u6da6\u7d27\u98ce\u9669\u4ea4\u6613\uff0c\u4f46\u5b58\u5728\u7ecf\u5178\u91cf\u5b50\u6a21\u62df\u548c\u7b56\u7565\u7b80\u5316\u7b49\u9650\u5236"}}
{"id": "2509.09177", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09177", "abs": "https://arxiv.org/abs/2509.09177", "authors": ["Hanyi Mao", "Quanjia Xiao", "Lei Pang", "Haixiao Liu"], "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL", "comment": null, "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping\ndirectly in the importance-sampling (IS) weight space. We revisit\nsequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping\nis transplanted to sequences: a fixed clip range systematically reweights short\nvs. long responses, distorting the effective objective. Theoretically, we\nformalize length fairness via a Length Reweighting Error (LRE) and prove that\nsmall LRE yields a directional cosine guarantee between the clipped and true\nupdates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the\nsequence log-IS ratio with a band that applies a KL-corrected drift term and\nscales as $\\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,\nstabilizes training, and outperforms all baselines across multiple evaluation\ndatasets.", "AI": {"tldr": "FSPO\u662f\u4e00\u79cd\u5e8f\u5217\u7ea7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u7a7a\u95f4\u5b9e\u65bd\u957f\u5ea6\u516c\u5e73\u88c1\u526a\u6765\u89e3\u51b3\u4f20\u7edfPPO/GRPO\u65b9\u6cd5\u5728\u5e8f\u5217\u957f\u5ea6\u5904\u7406\u4e0a\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u7ea7RL\u65b9\u6cd5\u5728\u79fb\u690dPPO/GRPO\u5f0f\u88c1\u526a\u65f6\u5b58\u5728\u56fa\u5b9a\u88c1\u526a\u8303\u56f4\u7cfb\u7edf\u6027\u5730\u91cd\u52a0\u6743\u77ed\u54cd\u5e94\u4e0e\u957f\u54cd\u5e94\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6709\u6548\u76ee\u6807\u5931\u771f\u3002", "method": "\u63d0\u51faFSPO\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u52a8\u673a\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u7528KL\u6821\u6b63\u6f02\u79fb\u9879\u548c\u221aL\u7f29\u653e\u5e26\u88c1\u526a\u5e8f\u5217\u5bf9\u6570IS\u6bd4\u7387\u3002", "result": "FSPO\u5728\u4e0d\u540c\u957f\u5ea6\u533a\u95f4\u5185\u5e73\u5766\u5316\u88c1\u526a\u7387\uff0c\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FSPO\u901a\u8fc7\u7406\u8bba\u5f62\u5f0f\u5316\u7684\u957f\u5ea6\u516c\u5e73\u6027\uff08LRE\uff09\u548c\u5b9e\u9645\u6709\u6548\u7684\u88c1\u526a\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u7ea7RL\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.09208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09208", "abs": "https://arxiv.org/abs/2509.09208", "authors": ["Somnath Hazra", "Pallab Dasgupta", "Soumyajit Dey"], "title": "Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning", "comment": "11 pages, Accepted to the 34th International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025, Main Track", "summary": "Constrained Reinforcement Learning (RL) aims to maximize the return while\nadhering to predefined constraint limits, which represent domain-specific\nsafety requirements. In continuous control settings, where learning agents\ngovern system actions, balancing the trade-off between reward maximization and\nconstraint satisfaction remains a significant challenge. Policy optimization\nmethods often exhibit instability near constraint boundaries, resulting in\nsuboptimal training performance. To address this issue, we introduce a novel\napproach that integrates an adaptive incentive mechanism in addition to the\nreward structure to stay within the constraint bound before approaching the\nconstraint boundary. Building on this insight, we propose Incrementally\nPenalized Proximal Policy Optimization (IP3O), a practical algorithm that\nenforces a progressively increasing penalty to stabilize training dynamics.\nThrough empirical evaluation on benchmark environments, we demonstrate the\nefficacy of IP3O compared to the performance of state-of-the-art Safe RL\nalgorithms. Furthermore, we provide theoretical guarantees by deriving a bound\non the worst-case error of the optimality achieved by our algorithm.", "AI": {"tldr": "\u63d0\u51faIP3O\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6fc0\u52b1\u673a\u5236\u548c\u6e10\u8fdb\u60e9\u7f5a\u7b56\u7565\u6765\u89e3\u51b3\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ea6\u675f\u8fb9\u754c\u7684\u540c\u65f6\u4f18\u5316\u6027\u80fd", "motivation": "\u8fde\u7eed\u63a7\u5236\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u5728\u6700\u5927\u5316\u56de\u62a5\u548c\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u96be\u9898\uff0c\u4f20\u7edf\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u7ea6\u675f\u8fb9\u754c\u9644\u8fd1\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6027\u80fd\u4e0d\u4f73", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u6fc0\u52b1\u673a\u5236\uff0c\u5728\u63a5\u8fd1\u7ea6\u675f\u8fb9\u754c\u524d\u4fdd\u6301\u7ea6\u675f\u754c\u9650\uff1b\u63d0\u51faIP3O\u7b97\u6cd5\uff0c\u91c7\u7528\u6e10\u8fdb\u589e\u52a0\u7684\u60e9\u7f5a\u673a\u5236\u6765\u7a33\u5b9a\u8bad\u7ec3\u52a8\u6001", "result": "\u5728\u57fa\u51c6\u73af\u5883\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cIP3O\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5b89\u5168RL\u7b97\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u6700\u4f18\u6027\u8bef\u5dee\u754c\u9650\u7684\u4fdd\u8bc1", "conclusion": "IP3O\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u60e9\u7f5a\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u548c\u6027\u80fd\u4f18\u5316\u5e73\u8861"}}
{"id": "2509.09226", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09226", "abs": "https://arxiv.org/abs/2509.09226", "authors": ["Haipeng Liu", "Ting Long", "Jing Fu"], "title": "Constructing a Question-Answering Simulator through the Distillation of LLMs", "comment": null, "summary": "The question-answering (QA) simulator is a model that mimics real student\nlearning behaviors and predicts their correctness of their responses to\nquestions. QA simulators enable educational recommender systems (ERS) to\ncollect large amounts of training data without interacting with real students,\nthereby preventing harmful recommendations made by an undertrained ERS from\nundermining actual student learning. Given the QA history, there are two\ncategories of solutions to predict the correctness, conducting the simulation:\n(1) LLM-free methods, which apply a traditional sequential model to transfer\nthe QA history into a vector representation first, and make predictions based\non the representation; (2) LLM-based methods, which leverage the domain\nknowledge and reasoning capability of LLM to enhence the prediction. LLM-free\nmethods offer fast inference but generally yield suboptimal performance. In\ncontrast, most LLM-based methods achieve better results, but at the cost of\nslower inference speed and higher GPU memory consumption. In this paper, we\npropose a method named LLM Distillation based Simulator (LDSim), which distills\ndomain knowledge and reasoning capability from an LLM to better assist\nprediction, thereby improving simulation performance. Extensive experiments\ndemonstrate that our LDSim achieves strong results on both the simulation task\nand the knowledge tracing (KT) task. Our code is publicly available at\nhttps://anonymous.4open.science/r/LDSim-05A9.", "AI": {"tldr": "\u63d0\u51faLDSim\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eceLLM\u84b8\u998f\u9886\u57df\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u63d0\u5347\u95ee\u7b54\u6a21\u62df\u5668\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u8fbe\u5230\u66f4\u597d\u7684\u6a21\u62df\u6548\u679c", "motivation": "\u73b0\u6709\u95ee\u7b54\u6a21\u62df\u5668\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6743\u8861\uff1aLLM-free\u65b9\u6cd5\u63a8\u7406\u5feb\u4f46\u6027\u80fd\u6b21\u4f18\uff0cLLM-based\u65b9\u6cd5\u6027\u80fd\u597d\u4f46\u63a8\u7406\u6162\u4e14\u8d44\u6e90\u6d88\u8017\u5927", "method": "\u63d0\u51faLLM\u84b8\u998f\u6a21\u62df\u5668(LDSim)\uff0c\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u9886\u57df\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u8f85\u52a9\u9884\u6d4b\uff0c\u63d0\u5347\u6a21\u62df\u6027\u80fd", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eLDSim\u5728\u6a21\u62df\u4efb\u52a1\u548c\u77e5\u8bc6\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u7ed3\u679c", "conclusion": "LDSim\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u95ee\u7b54\u6a21\u62df\u5668\u4e2d\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86\u65e2\u9ad8\u6548\u53c8\u51c6\u786e\u7684\u6a21\u62df\u6027\u80fd"}}
{"id": "2509.09265", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09265", "abs": "https://arxiv.org/abs/2509.09265", "authors": ["Jiawei Wang", "Jiacai Liu", "Yuqian Fu", "Yingru Li", "Xintao Wang", "Yuan Lin", "Yu Yue", "Lin Zhang", "Yang Wang", "Ke Wang"], "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents", "comment": "ICLR 2026 Under review", "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEntropy-Modulated Policy Gradients (EMPG)\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u548c\u4efb\u52a1\u7ed3\u679c\u91cd\u65b0\u6821\u51c6\u5b66\u4e60\u4fe1\u53f7\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u56e0\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u9762\u4e34\u7a00\u758f\u5956\u52b1\u96be\u4ee5\u5206\u914d\u4fe1\u7528\u7684\u95ee\u9898\uff0c\u4e14\u7b56\u7565\u68af\u5ea6\u5927\u5c0f\u4e0e\u71b5\u56fa\u6709\u8026\u5408\uff0c\u5bfc\u81f4\u5bf9\u81ea\u4fe1\u6b63\u786e\u52a8\u4f5c\u7684\u66f4\u65b0\u6548\u7387\u4f4e\u4e0b\uff0c\u5bf9\u4e0d\u786e\u5b9a\u52a8\u4f5c\u7684\u66f4\u65b0\u53ef\u80fd\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faEMPG\u6846\u67b6\uff0c\u57fa\u4e8e\u6b65\u9aa4\u4e0d\u786e\u5b9a\u6027\u548c\u6700\u7ec8\u4efb\u52a1\u7ed3\u679c\u91cd\u65b0\u6821\u51c6\u5b66\u4e60\u4fe1\u53f7\uff1a\u653e\u5927\u81ea\u4fe1\u6b63\u786e\u52a8\u4f5c\u7684\u66f4\u65b0\uff0c\u60e9\u7f5a\u81ea\u4fe1\u9519\u8bef\uff0c\u8870\u51cf\u4e0d\u786e\u5b9a\u6b65\u9aa4\u7684\u66f4\u65b0\u4ee5\u7a33\u5b9a\u63a2\u7d22\uff0c\u5e76\u5f15\u5165\u672a\u6765\u6e05\u6670\u5ea6\u5956\u52b1\u9879\u9f13\u52b1\u5bfb\u627e\u53ef\u9884\u6d4b\u89e3\u8def\u5f84\u3002", "result": "\u5728WebShop\u3001ALFWorld\u548cDeep Search\u4e09\u4e2a\u6311\u6218\u6027\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cEMPG\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u7b56\u7565\u68af\u5ea6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EMPG\u901a\u8fc7\u8c03\u8282\u71b5\u6765\u4f18\u5316\u7b56\u7565\u68af\u5ea6\u5b66\u4e60\u52a8\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.09387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09387", "abs": "https://arxiv.org/abs/2509.09387", "authors": ["Mohammed Tiouti", "Mohamed Bal-Ghaoui"], "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization", "comment": null, "summary": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines.", "AI": {"tldr": "MetaLLMiX\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u8d85\u53c2\u6570\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u3001\u53ef\u89e3\u91caAI\u548c\u9ad8\u6548LLM\u63a8\u7406\uff0c\u65e0\u9700\u989d\u5916\u8bd5\u9a8c\u5373\u53ef\u63a8\u8350\u6700\u4f18\u8d85\u53c2\u6570\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u8d85\u53c2\u6570\u9009\u62e9\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4f9d\u8d56\u8bd5\u9519\u548c\u6602\u8d35API\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u5229\u7528\u5386\u53f2\u5b9e\u9a8c\u7ed3\u679c\u7684SHAP\u89e3\u91ca\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548cLLM\u63a8\u7406\u8fdb\u884c\u8d85\u53c2\u6570\u63a8\u8350\uff0c\u91c7\u7528LLM-as-judge\u8bc4\u4f30\u63a7\u5236\u8f93\u51fa\u683c\u5f0f\u3001\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "result": "\u57288\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cMetaLLMiX\u6027\u80fd\u4e0e\u4f20\u7edfHPO\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u672c\u5730\u90e8\u7f72\u57285/8\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f18\u7ed3\u679c\uff0c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1199.6-99.9%\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.4-15.7\u500d\u3002", "conclusion": "MetaLLMiX\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u8d85\u53c2\u6570\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.09396", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09396", "abs": "https://arxiv.org/abs/2509.09396", "authors": ["Harry Mayne", "Ryan Othniel Kearns", "Yushi Yang", "Andrew M. Bean", "Eoin Delaney", "Chris Russell", "Adam Mahdi"], "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations", "comment": "Accepted to EMNLP 2025 Main", "summary": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u6211\u53cd\u4e8b\u5b9e\u89e3\u91ca(SCEs)\u5b58\u5728\u6709\u6548\u6027-\u6700\u5c0f\u6027\u6743\u8861\u95ee\u9898\uff0c\u8981\u4e48\u4fee\u6539\u8fc7\u591a\u7f3a\u4e4f\u6d1e\u5bdf\u529b\uff0c\u8981\u4e48\u4fee\u6539\u8fc7\u5c11\u65e0\u6cd5\u6539\u53d8\u9884\u6d4b\u7ed3\u679c\uff0c\u8868\u660eSCEs\u4f5c\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u6548\u679c\u6709\u9650\u751a\u81f3\u53ef\u80fd\u8bef\u5bfc", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6765\u5411\u4eba\u7c7b\u89e3\u91ca\u5176\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u6709\u6548\u534f\u4f5c", "method": "\u8bc4\u4f30LLMs\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u6709\u6548\u6027\u548c\u6700\u5c0f\u6027\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cdLLMs\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u8bbe\u7f6e", "result": "LLMs\u901a\u5e38\u80fd\u751f\u6210\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4f46\u8fdc\u975e\u6700\u5c0f\u5316\u4fee\u6539\uff0c\u5f53\u8981\u6c42\u6700\u5c0f\u5316\u4fee\u6539\u65f6\u53c8\u5f80\u5f80\u4fee\u6539\u8fc7\u5c11\u800c\u65e0\u6cd5\u6539\u53d8\u9884\u6d4b\u7ed3\u679c", "conclusion": "\u81ea\u6211\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4f5c\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u6548\u679c\u6709\u9650\u751a\u81f3\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\uff0c\u5728\u5173\u952e\u5e94\u7528\u573a\u666f\u90e8\u7f72LLMs\u65f6\u5fc5\u987b\u8003\u8651\u4e0d\u53ef\u9760\u81ea\u6211\u89e3\u91ca\u5bf9\u4e0b\u6e38\u51b3\u7b56\u7684\u5f71\u54cd"}}
{"id": "2509.09470", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09470", "abs": "https://arxiv.org/abs/2509.09470", "authors": ["Om Vishesh", "Harshad Khadilkar", "Deepak Akkil"], "title": "AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings", "comment": "5 pages, 2 figures", "summary": "Keeping pace with the rapid growth of academia literature presents a\nsignificant challenge for researchers, funding bodies, and academic societies.\nTo address the time-consuming manual effort required for scholarly discovery,\nwe present a novel, fully automated system that transitions from data discovery\nto direct action. Our pipeline demonstrates how a specialized AI agent,\n'Agent-E', can be tasked with identifying papers from specific geographic\nregions within conference proceedings and then executing a Robotic Process\nAutomation (RPA) to complete a predefined action, such as submitting a\nnomination form. We validated our system on 586 papers from five different\nconferences, where it successfully identified every target paper with a recall\nof 100% and a near perfect accuracy of 99.4%. This demonstration highlights the\npotential of task-oriented AI agents to not only filter information but also to\nactively participate in and accelerate the workflows of the academic community.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAgent-E\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u4ece\u4f1a\u8bae\u8bba\u6587\u4e2d\u8bc6\u522b\u7279\u5b9a\u5730\u533a\u7684\u8bba\u6587\uff0c\u5e76\u901a\u8fc7RPA\u6280\u672f\u5b8c\u6210\u9884\u5b9a\u64cd\u4f5c\uff08\u5982\u63d0\u4ea4\u63d0\u540d\u8868\uff09\uff0c\u5728586\u7bc7\u8bba\u6587\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u53ec\u56de\u7387\u548c99.4%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u6587\u732e\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u53d1\u73b0\u548c\u7ba1\u7406\u6311\u6218\uff0c\u51cf\u5c11\u5b66\u672f\u53d1\u73b0\u6240\u9700\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u5b66\u672f\u793e\u533a\u7684\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7cfb\u7edf\u7ba1\u9053\uff0c\u4f7f\u7528\u4e13\u95e8\u7684AI\u4ee3\u7406Agent-E\u6765\u8bc6\u522b\u7279\u5b9a\u5730\u7406\u533a\u57df\u7684\u4f1a\u8bae\u8bba\u6587\uff0c\u7136\u540e\u901a\u8fc7\u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08RPA\uff09\u6267\u884c\u9884\u5b9a\u64cd\u4f5c\u3002", "result": "\u57285\u4e2a\u4e0d\u540c\u4f1a\u8bae\u7684586\u7bc7\u8bba\u6587\u4e0a\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u6210\u529f\u8bc6\u522b\u6240\u6709\u76ee\u6807\u8bba\u6587\uff0c\u53ec\u56de\u7387\u8fbe\u5230100%\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0899.4%\uff09\u3002", "conclusion": "\u9762\u5411\u4efb\u52a1\u7684AI\u4ee3\u7406\u4e0d\u4ec5\u80fd\u591f\u8fc7\u6ee4\u4fe1\u606f\uff0c\u8fd8\u80fd\u79ef\u6781\u53c2\u4e0e\u5e76\u52a0\u901f\u5b66\u672f\u793e\u533a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b66\u672f\u81ea\u52a8\u5316\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.09485", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09485", "abs": "https://arxiv.org/abs/2509.09485", "authors": ["Zhanhong Jiang", "Md Zahid Hasan", "Nastaran Saadati", "Aditya Balu", "Chao Liu", "Soumik Sarkar"], "title": "Balancing Utility and Privacy: Dynamically Private SGD with Random Projection", "comment": "27 pages, 13 figures", "summary": "Stochastic optimization is a pivotal enabler in modern machine learning,\nproducing effective models for various tasks. However, several existing works\nhave shown that model parameters and gradient information are susceptible to\nprivacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy\nconcerns, its static noise mechanism impacts the error bounds for model\nperformance. Additionally, with the exponential increase in model parameters,\nefficient learning of these models using stochastic optimizers has become more\nchallenging. To address these concerns, we introduce the Dynamically\nDifferentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we\ncombine two important ideas: (i) dynamic differential privacy (DDP) with\nautomatic gradient clipping and (ii) random projection with SGD, allowing\ndynamic adjustment of the tradeoff between utility and privacy of the model. It\nexhibits provably sub-linear convergence rates across different objective\nfunctions, matching the best available rate. The theoretical analysis further\nsuggests that DDP leads to better utility at the cost of privacy, while random\nprojection enables more efficient model learning. Extensive experiments across\ndiverse datasets show that D2P2-SGD remarkably enhances accuracy while\nmaintaining privacy. Our code is available here.", "AI": {"tldr": "\u63d0\u51fa\u4e86D2P2-SGD\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u52a8\u6001\u5dee\u5206\u9690\u79c1\u548c\u968f\u673a\u6295\u5f71\u6280\u672f\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709DPSGD\u7684\u9759\u6001\u566a\u58f0\u673a\u5236\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u968f\u7740\u6a21\u578b\u53c2\u6570\u6307\u6570\u589e\u957f\uff0c\u968f\u673a\u4f18\u5316\u5668\u7684\u5b66\u4e60\u6548\u7387\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898", "method": "\u7ed3\u5408\u52a8\u6001\u5dee\u5206\u9690\u79c1\uff08\u81ea\u52a8\u68af\u5ea6\u88c1\u526a\uff09\u548c\u968f\u673a\u6295\u5f71SGD\uff0c\u52a8\u6001\u8c03\u6574\u6548\u7528\u4e0e\u9690\u79c1\u4e4b\u95f4\u7684\u6743\u8861", "result": "\u5728\u4e0d\u540c\u76ee\u6807\u51fd\u6570\u4e0a\u8868\u73b0\u51fa\u53ef\u8bc1\u660e\u7684\u6b21\u7ebf\u6027\u6536\u655b\u7387\uff0c\u5339\u914d\u6700\u4f73\u53ef\u7528\u901f\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027", "conclusion": "D2P2-SGD\u901a\u8fc7\u52a8\u6001\u9690\u79c1\u673a\u5236\u548c\u968f\u673a\u6295\u5f71\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u4f18\u5316\u65b9\u6848"}}
{"id": "2509.09512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09512", "abs": "https://arxiv.org/abs/2509.09512", "authors": ["Cynthia Moreira Maia", "Lucas B. V. de Amorim", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "PIPES: A Meta-dataset of Machine Learning Pipelines", "comment": null, "summary": "Solutions to the Algorithm Selection Problem (ASP) in machine learning face\nthe challenge of high computational costs associated with evaluating various\nalgorithms' performances on a given dataset. To mitigate this cost, the\nmeta-learning field can leverage previously executed experiments shared in\nonline repositories such as OpenML. OpenML provides an extensive collection of\nmachine learning experiments. However, an analysis of OpenML's records reveals\nlimitations. It lacks diversity in pipelines, specifically when exploring data\npreprocessing steps/blocks, such as scaling or imputation, resulting in limited\nrepresentation. Its experiments are often focused on a few popular techniques\nwithin each pipeline block, leading to an imbalanced sample. To overcome the\nobserved limitations of OpenML, we propose PIPES, a collection of experiments\ninvolving multiple pipelines designed to represent all combinations of the\nselected sets of techniques, aiming at diversity and completeness. PIPES stores\nthe results of experiments performed applying 9,408 pipelines to 300 datasets.\nIt includes detailed information on the pipeline blocks, training and testing\ntimes, predictions, performances, and the eventual error messages. This\ncomprehensive collection of results allows researchers to perform analyses\nacross diverse and representative pipelines and datasets. PIPES also offers\npotential for expansion, as additional data and experiments can be incorporated\nto support the meta-learning community further. The data, code, supplementary\nmaterial, and all experiments can be found at\nhttps://github.com/cynthiamaia/PIPES.git.", "AI": {"tldr": "PIPES\u662f\u4e00\u4e2a\u65b0\u7684\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3OpenML\u5728\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u4e2d\u5b58\u5728\u7684\u7ba1\u9053\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e869,408\u4e2a\u7ba1\u9053\u5728300\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "OpenML\u7b49\u73b0\u6709\u5b9e\u9a8c\u5e93\u5728\u6570\u636e\u9884\u5904\u7406\u6b65\u9aa4\u7684\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5c11\u6570\u6d41\u884c\u6280\u672f\uff0c\u5bfc\u81f4\u6837\u672c\u4e0d\u5e73\u8861\uff0c\u65e0\u6cd5\u5145\u5206\u652f\u6301\u5143\u5b66\u4e60\u7814\u7a76\u3002", "method": "\u6784\u5efaPIPES\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7ec4\u5408\u9009\u5b9a\u6280\u672f\u96c6\u5408\u7684\u6240\u6709\u53ef\u80fd\u7ec4\u5408\uff0c\u521b\u5efa\u591a\u6837\u5316\u548c\u5b8c\u6574\u7684\u7ba1\u9053\u5b9e\u9a8c\u96c6\u5408\uff0c\u5305\u542b\u8be6\u7ec6\u7684\u7ba1\u9053\u5757\u4fe1\u606f\u3001\u8bad\u7ec3\u6d4b\u8bd5\u65f6\u95f4\u3001\u9884\u6d4b\u7ed3\u679c\u3001\u6027\u80fd\u6307\u6807\u548c\u9519\u8bef\u4fe1\u606f\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u5305\u542b9,408\u4e2a\u7ba1\u9053\u5728300\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u7ed3\u679c\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u6bd4OpenML\u66f4\u5168\u9762\u548c\u5e73\u8861\u7684\u7ba1\u9053\u8868\u793a\u3002", "conclusion": "PIPES\u4e3a\u5143\u5b66\u4e60\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u3001\u4ee3\u8868\u6027\u5f3a\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u652f\u6301\u66f4\u5168\u9762\u7684\u7b97\u6cd5\u9009\u62e9\u5206\u6790\uff0c\u5e76\u5177\u6709\u8fdb\u4e00\u6b65\u6269\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.09655", "categories": ["cs.LG", "cs.AI", "cs.LO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09655", "abs": "https://arxiv.org/abs/2509.09655", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management", "comment": "12 pages, 5 figures, 3 tables", "summary": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning\n(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds\nto reduce harm while equalizing a chosen fairness target (coverage or harm)\nacross protected subgroups. Using de-identified longitudinal trajectories from\na Medicaid population health management program, we evaluate FG-FARL against\nbehavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global\nconformal safety baseline). We report off-policy value estimates with bootstrap\n95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL\nachieves comparable value to baselines while improving fairness metrics,\ndemonstrating a practical path to safer and more equitable decision support.", "AI": {"tldr": "FG-FARL\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u4e0d\u540c\u4fdd\u62a4\u5b50\u7ec4\u7684\u5b89\u5168\u9608\u503c\u6765\u51cf\u5c11\u4f24\u5bb3\u5e76\u5b9e\u73b0\u516c\u5e73\u6027\u76ee\u6807\uff08\u8986\u76d6\u7387\u6216\u4f24\u5bb3\u5747\u7b49\u5316\uff09\uff0c\u5728\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u5065\u5eb7\u7ba1\u7406\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5bf9\u4fdd\u62a4\u5b50\u7ec4\u9020\u6210\u4e0d\u516c\u5e73\u4f24\u5bb3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u53c8\u80fd\u63d0\u5347\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53ef\u884c\u6027\u5f15\u5bfc\u7684\u516c\u5e73\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\uff08FG-FARL\uff09\uff0c\u6821\u51c6\u6bcf\u4e2a\u5b50\u7ec4\u7684\u5b89\u5168\u9608\u503c\uff0c\u4f7f\u7528\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u5065\u5eb7\u7ba1\u7406\u7684\u53bb\u6807\u8bc6\u5316\u7eb5\u5411\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0e\u884c\u4e3a\u514b\u9686\u548cHACO\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "FG-FARL\u5728\u4fdd\u6301\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u7684\u4ef7\u503c\u4f30\u8ba1\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u5584\u4e86\u516c\u5e73\u6027\u6307\u6807\uff0c\u901a\u8fc7bootstrap 95%\u7f6e\u4fe1\u533a\u95f4\u548c\u5b50\u7ec4\u5dee\u5f02\u5206\u6790\uff08p\u503c\uff09\u9a8c\u8bc1\u4e86\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "FG-FARL\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u51b3\u7b56\u652f\u6301\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u533b\u7597\u5065\u5eb7\u7ba1\u7406\u573a\u666f\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
