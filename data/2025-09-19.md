<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures](https://arxiv.org/abs/2509.14252)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.CL

TL;DR: 本文提出了LLM-JEPA，一种基于联合嵌入预测架构的语言模型训练方法，在多个数据集和模型上都显著优于标准LLM训练目标。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的预训练、微调和评估主要依赖于输入空间重构和生成能力，而视觉领域已经证明嵌入空间训练目标（如JEPA）远优于输入空间方法。这种训练方法的不匹配促使研究者探索语言训练能否从视觉方法中借鉴经验。

Method: 开发了LLM-JEPA，这是一种基于联合嵌入预测架构的解决方案，适用于语言模型的微调和预训练。该方法采用嵌入空间训练目标而非传统的输入空间重构方法。

Result: LLM-JEPA在多个模型（Llama3、OpenELM、Gemma2、Olmo）和数据集（NL-RX、GSM8K、Spider、RottenTomatoes）上都显著优于标准LLM训练目标，同时表现出对过拟合的鲁棒性。

Conclusion: 嵌入空间训练目标在语言模型训练中具有显著优势，LLM-JEPA为语言训练方法从视觉领域借鉴经验提供了可行的第一步，展示了在保持鲁棒性的同时提升性能的潜力。

Abstract: Large Language Model (LLM) pretraining, finetuning, and evaluation rely on
input-space reconstruction and generative capabilities. Yet, it has been
observed in vision that embedding-space training objectives, e.g., with Joint
Embedding Predictive Architectures (JEPAs), are far superior to their
input-space counterpart. That mismatch in how training is achieved between
language and vision opens up a natural question: {\em can language training
methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is
a testimony of the challenge in designing such objectives for language. In this
work, we propose a first step in that direction where we develop LLM-JEPA, a
JEPA based solution for LLMs applicable both to finetuning and pretraining.
Thus far, LLM-JEPA is able to outperform the standard LLM training objectives
by a significant margin across models, all while being robust to overfiting.
Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider,
RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo
families. Code: https://github.com/rbalestr-lab/llm-jepa.

</details>


### [2] [From Correction to Mastery: Reinforced Distillation of Large Language Model Agents](https://arxiv.org/abs/2509.14257)
*Yuanjie Lyu,Chengyu Wang,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: SCoRe是一个学生中心的蒸馏框架，通过教师只在关键错误时干预来训练小型语言模型，使7B参数学生模型在12个基准测试中达到72B教师模型的代理性能


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法训练小型学生模型模仿完整教师轨迹，但师生间的推理和知识差距会导致错误累积。需要更有效的蒸馏方法来缩小模型规模差距

Method: 学生生成轨迹，教师只在第一个关键错误时干预，生成匹配学生能力的数据。先微调修正轨迹，然后从验证前缀开始短视距强化学习，在关键错误步骤分配目标奖励

Result: 在12个具有挑战性的基准测试中，使用SCoRe蒸馏的7B参数学生模型匹配了72B参数教师模型的代理性能

Conclusion: SCoRe框架通过学生中心的干预策略和强化学习设计，有效提升了小型模型的代理能力，实现了显著的模型压缩效果

Abstract: Large Language Model agents excel at solving complex tasks through iterative
reasoning and tool use, but typically depend on ultra-large, costly backbones.
Existing distillation approaches train smaller students to imitate full teacher
trajectories, yet reasoning and knowledge gaps between the teacher and student
often lead to compounding errors. We propose SCoRe, a student-centered
framework in which the student generates trajectories and the teacher
intervenes only at the first critical error, producing training data matched to
the student's ability and exposing specific weaknesses. The student is first
fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement
learning starts from the verified prefix before the first critical error, with
target rewards assigned at that step. This design encourages autonomous
problem-solving beyond imitation and improves training stability. Particularly,
on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe
matches the agentic performance of a 72B-parameter teacher.

</details>


### [3] [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.14635)
*Weihan Peng,Yuling Shi,Yuhang Wang,Xinyun Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: SWE-QA是一个仓库级代码问答基准测试，包含576个高质量问题-答案对，覆盖意图理解、跨文件推理和多跳依赖分析等类别，旨在推动真实代码环境中的自动化问答系统研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注小型自包含代码片段，无法捕捉真实软件仓库的复杂性，需要理解多文件、软件架构和长距离代码依赖关系。

Method: 从11个热门仓库的77,100个GitHub问题中提取开发者问题，构建两级分类法，手动策划和验证问题并收集答案，开发SWE-QA-Agent代理框架进行自动回答。

Result: 实验评估了6个先进LLM在不同上下文增强策略下的表现，结果显示LLMs特别是SWE-QA-Agent框架在仓库级问答方面具有潜力。

Conclusion: 该研究为仓库级代码问答提供了新基准，揭示了现有挑战并指明了未来研究方向，LLM代理框架展现出良好前景。

Abstract: Understanding and reasoning about entire software repositories is an
essential capability for intelligent software engineering tools. While existing
benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly
focus on small, self-contained code snippets. These setups fail to capture the
complexity of real-world repositories, where effective understanding and
reasoning often require navigating multiple files, understanding software
architecture, and grounding answers in long-range code dependencies. In this
paper, we present SWE-QA, a repository-level code question answering (QA)
benchmark designed to facilitate research on automated QA systems in realistic
code environments. SWE-QA involves 576 high-quality question-answer pairs
spanning diverse categories, including intention understanding, cross-file
reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first
crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis
of naturally occurring developer questions extracted from these issues, we
developed a two-level taxonomy of repository-level questions and constructed a
set of seed questions for each category. For each category, we manually curated
and validated questions and collected their corresponding answers. As a
prototype application, we further develop SWE-QA-Agent, an agentic framework in
which LLM agents reason and act to find answers automatically. We evaluate six
advanced LLMs on SWE-QA under various context augmentation strategies.
Experimental results highlight the promise of LLMs, particularly our
SWE-QA-Agent framework, in addressing repository-level QA, while also revealing
open challenges and pointing to future research directions.

</details>


### [4] [Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation](https://arxiv.org/abs/2509.14477)
*Thales Sales Almeida,João Guilherme Alves Santos,Thiago Laitz,Giovana Kerche Bonás*

Main category: cs.CL

TL;DR: Ticket-Bench是一个多语言代理评估基准，专注于足球票购买场景，涵盖6种主要语言，用于评估LLM在跨语言环境下的函数调用准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有代理评估主要忽视文化和语言多样性，通常依赖单语或简单翻译的基准，无法真实反映多语言环境下的任务执行能力。

Method: 创建Ticket-Bench基准，模拟足球票购买场景，涵盖葡萄牙语、英语、西班牙语、德语、意大利语和法语六种语言，使用本地化的球队、城市和用户配置文件提高真实性。

Result: 推理导向模型（如GPT-5、Qwen3-235B）表现最佳，但仍存在显著的跨语言差异，不同语言间的性能表现不一致。

Conclusion: 需要开发文化感知的多语言基准来指导稳健LLM代理的发展，当前模型在多语言环境下仍存在性能差距。

Abstract: Large language models (LLMs) are increasingly deployed as task-oriented
agents, where success depends on their ability to generate accurate function
calls under realistic, multilingual conditions. However, existing agent
evaluations largely overlook cultural and linguistic diversity, often relying
on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a
benchmark for multilingual agent evaluation in task-oriented scenarios.
Ticket-Bench simulates the domain of soccer ticket purchases across six major
languages: Portuguese, English, Spanish, German, Italian, and French. Using
localized teams, cities, and user profiles to provide a higher level of
realism. We evaluate a wide range of commercial and open-source LLMs, measuring
function-calling accuracy and consistency across languages. Results show that
reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but
still exhibit notable cross-lingual disparities. These findings underscore the
need for culturally aware, multilingual benchmarks to guide the development of
robust LLM agents.

</details>


### [5] [Estimating Semantic Alphabet Size for LLM Uncertainty Quantification](https://arxiv.org/abs/2509.14478)
*Lucas H. McCabe,Rimon Melamed,Thomas Hartvigsen,H. Howie Huang*

Main category: cs.CL

TL;DR: 提出改进的语义字母表大小估计器来调整离散语义熵，提高LLM不确定性估计准确性，同时保持高可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的LLM不确定性量化方法计算成本高，需要从少量样本中可靠估计。语义熵虽然流行但会低估真实语义熵，且近期扩展方法可解释性差且引入额外超参数

Method: 重新审视经典离散语义熵估计器，提出改进的语义字母表大小估计器，用于调整离散语义熵的样本覆盖度

Result: 改进方法在语义熵估计上更准确，在错误LLM响应检测方面表现与顶级方法相当或更好，同时保持高度可解释性

Conclusion: 提出的语义字母表大小估计器既能提高不确定性估计准确性，又能保持方法的可解释性优势

Abstract: Many black-box techniques for quantifying the uncertainty of large language
models (LLMs) rely on repeated LLM sampling, which can be computationally
expensive. Therefore, practical applicability demands reliable estimation from
few samples. Semantic entropy (SE) is a popular sample-based uncertainty
estimator with a discrete formulation attractive for the black-box setting.
Recent extensions of semantic entropy exhibit improved LLM hallucination
detection, but do so with less interpretable methods that admit additional
hyperparameters. For this reason, we revisit the canonical discrete semantic
entropy estimator, finding that it underestimates the "true" semantic entropy,
as expected from theory. We propose a modified semantic alphabet size
estimator, and illustrate that using it to adjust discrete semantic entropy for
sample coverage results in more accurate semantic entropy estimation in our
setting of interest. Furthermore, our proposed alphabet size estimator flags
incorrect LLM responses as well or better than recent top-performing
approaches, with the added benefit of remaining highly interpretable.

</details>


### [6] [Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents](https://arxiv.org/abs/2509.14480)
*Weiting Tan,Xinghua Qu,Ming Tu,Meng Ge,Andy T. Liu,Philipp Koehn,Lu Lu*

Main category: cs.CL

TL;DR: 本文提出了TARL框架，使用LLM作为评判者进行回合级评估，解决长时程任务中的信用分配问题，在文本基准上提升任务通过率6%以上，并展示了多模态基础模型的适用性。


<details>
  <summary>Details</summary>
Motivation: 有效的交互式工具使用需要代理掌握工具集成推理(TIR)，这是一个涉及多轮规划和长上下文对话管理的复杂过程。为了在多模态环境中训练代理应对这种动态过程，需要新的强化学习方法。

Method: 提出了Turn-level Adjudicated Reinforcement Learning (TARL)策略，使用大型语言模型(LLM)作为评判者提供回合级评估，解决长时程任务中的信用分配问题。集成混合任务训练课程与数学推理问题来增强探索。

Result: 在文本基准τ-bench上的任务通过率相比强RL基线提升了超过6%。成功训练了一个基础多模态LLM，使其具备工具使用能力。

Conclusion: 该框架适用于微调多模态基础模型用于代理任务，为更自然的语音驱动交互代理铺平了道路。

Abstract: Effective interactive tool use requires agents to master Tool Integrated
Reasoning (TIR): a complex process involving multi-turn planning and
long-context dialogue management. To train agents for this dynamic process,
particularly in multi-modal contexts, we introduce a sandbox environment for
reinforcement learning (RL) that supports interleaved speech-text rollouts. Our
core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses
the challenge of credit assignment in long-horizon tasks by employing a Large
Language Model (LLM) as a judge to provide turn-level evaluation. To enhance
exploration, we integrate a mixed-task training curriculum with mathematical
reasoning problems. This unified approach boosts the task pass rate on the
text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially,
we demonstrate our framework's suitability for fine-tuning a multi-modal
foundation model for agentic tasks. By training a base multi-modal LLM on
interleaved speech-text rollouts, we equip it with tool-use abilities, paving
the way for more natural, voice-driven interactive agents.

</details>


### [7] [Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors](https://arxiv.org/abs/2509.14543)
*Zhengxiang Wang,Nafis Irtiza Tripto,Solha Park,Zhenzhen Li,Jiawei Zhou*

Main category: cs.CL

TL;DR: LLMs在模仿个人写作风格方面表现有限，在结构化格式中表现较好，但在非正式博客和论坛中难以捕捉细微风格差异


<details>
  <summary>Details</summary>
Motivation: 评估LLMs能否通过少量示例忠实模仿个人写作风格，这对于个性化写作工具的用户对齐生成至关重要

Method: 使用上下文学习从少量用户写作样本中模仿风格，采用作者归属、验证、风格匹配和AI检测等综合指标，在新闻、邮件、论坛和博客等领域的400多名真实作者数据上进行评估

Result: LLMs在新闻和邮件等结构化格式中可以近似用户风格，但在博客和论坛等非正式写作中表现不佳，提示策略分析显示个性化存在关键限制

Conclusion: 当前LLM个性化适配存在根本性差距，需要改进技术来支持隐式的风格一致性生成

Abstract: As large language models (LLMs) become increasingly integrated into personal
writing tools, a critical question arises: can LLMs faithfully imitate an
individual's writing style from just a few examples? Personal style is often
subtle and implicit, making it difficult to specify through prompts yet
essential for user-aligned generation. This work presents a comprehensive
evaluation of state-of-the-art LLMs' ability to mimic personal writing styles
via in-context learning from a small number of user-authored samples. We
introduce an ensemble of complementary metrics-including authorship
attribution, authorship verification, style matching, and AI detection-to
robustly assess style imitation. Our evaluation spans over 40000 generations
per model across domains such as news, email, forums, and blogs, covering
writing samples from more than 400 real-world authors. Results show that while
LLMs can approximate user styles in structured formats like news and email,
they struggle with nuanced, informal writing in blogs and forums. Further
analysis on various prompting strategies such as number of demonstrations
reveal key limitations in effective personalization. Our findings highlight a
fundamental gap in personalized LLM adaptation and the need for improved
techniques to support implicit, style-consistent generation. To aid future
research and for reproducibility, we open-source our data and code.

</details>


### [8] [MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models](https://arxiv.org/abs/2509.14651)
*Siyu Yan,Long Zeng,Xuecheng Wu,Chengcheng Han,Kongcheng Zhang,Chong Peng,Xuezhi Cao,Xunliang Cai,Chenjuan Guo*

Main category: cs.CL

TL;DR: MUSE框架从攻击和防御两个角度解决多轮对话中的LLM越狱问题，提出了基于框架语义和启发式树搜索的攻击方法MUSE-A，以及细粒度安全对齐防御方法MUSE-D


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛采用，确保其与人类价值观对齐至关重要。现有防御主要针对单轮攻击，但现实使用中多轮对话容易让攻击者利用对话上下文绕过安全措施

Method: 提出MUSE框架：攻击方面使用框架语义和启发式树搜索探索多样化语义轨迹(MUSE-A)；防御方面采用细粒度安全对齐方法，在对话早期进行干预(MUSE-D)

Result: 在各种模型上的广泛实验表明，MUSE能有效识别和缓解多轮漏洞

Conclusion: MUSE框架为多轮对话中的LLM安全提供了全面的攻击和防御解决方案，证明了其有效性

Abstract: As large language models~(LLMs) become widely adopted, ensuring their
alignment with human values is crucial to prevent jailbreaks where adversaries
manipulate models to produce harmful content. While most defenses target
single-turn attacks, real-world usage often involves multi-turn dialogues,
exposing models to attacks that exploit conversational context to bypass safety
measures. We introduce MUSE, a comprehensive framework tackling multi-turn
jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A,
a method that uses frame semantics and heuristic tree search to explore diverse
semantic trajectories. For defense, we present MUSE-D, a fine-grained safety
alignment approach that intervenes early in dialogues to reduce
vulnerabilities. Extensive experiments on various models show that MUSE
effectively identifies and mitigates multi-turn vulnerabilities. Code is
available at
\href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.

</details>


### [9] [LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring](https://arxiv.org/abs/2509.14834)
*Jinhee Jang,Ayoung Moon,Minkyoung Jung,YoungBin Kim. Seung Jin Lee*

Main category: cs.CL

TL;DR: RES是一个多智能体评估框架，通过模拟圆桌讨论实现零样本论文自动评分，相比传统方法在QWK指标上提升34.86%


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在自动论文评分中难以达到人类水平的多视角理解和判断的问题

Method: 构建基于LLM的评估器智能体，每个智能体针对特定提示和主题生成评分标准，进行多视角独立评估，然后通过辩证推理整合得到最终分数

Result: 在ASAP数据集上使用ChatGPT和Claude，RES相比直接提示方法在平均QWK指标上提升高达34.86%

Conclusion: 通过多智能体协作和共识机制，RES能够产生更接近人类评估的精确评分

Abstract: The emergence of large language models (LLMs) has brought a new paradigm to
automated essay scoring (AES), a long-standing and practical application of
natural language processing in education. However, achieving human-level
multi-perspective understanding and judgment remains a challenge. In this work,
we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework
designed to perform precise and human-aligned scoring under a zero-shot
setting. RES constructs evaluator agents based on LLMs, each tailored to a
specific prompt and topic context. Each agent independently generates a
trait-based rubric and conducts a multi-perspective evaluation. Then, by
simulating a roundtable-style discussion, RES consolidates individual
evaluations through a dialectical reasoning process to produce a final holistic
score that more closely aligns with human evaluation. By enabling collaboration
and consensus among agents with diverse evaluation perspectives, RES
outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset
using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in
average QWK over straightforward prompting (Vanilla) methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: EoK是一个基于LLM的进化程序搜索框架，通过从成熟内核库开发历史中挖掘可重用优化理念，为RISC-V等参考稀缺领域自动化内核设计，实现了1.27倍中值加速，超越人类专家和现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在RISC-V等参考稀缺硬件平台上自动化内核设计的挑战，因为现有LLM方法在CUDA等文档丰富的领域表现良好，但在参考稀缺领域效果未经验证。

Method: 提出EoK框架：1）从成熟内核库开发历史中挖掘可重用优化理念（通用设计原则+可操作思路）；2）使用RAG增强RISC-V特定上下文；3）基于历史有效技术指导并行LLM探索的进化程序搜索。

Result: 在80个内核设计任务上实现中值1.27倍加速，在所有任务上超越人类专家，比现有基于LLM的自动化内核设计方法提升20%。

Conclusion: 证明了将人类经验融入新兴领域的可行性，凸显了基于LLM的自动化内核优化的巨大潜力。

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [11] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: 该研究构建了一个上下文感知的Javadoc生成数据集，评估了5个开源LLM在零样本、少样本和微调设置下的性能，发现LLaMA 3.1在自动化Javadoc生成方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有自动化文档生成方法主要关注代码摘要，缺乏针对模板化文档生成（如Javadoc）的研究，且缺乏包含现代语言特性和必要上下文信息的专用数据集。

Method: 开发了一个包含现代Java代码库结构和语义信息的上下文感知数据集，评估了5个开源LLM（LLaMA-3.1、Gemma-2、Phi-3、Mistral、Qwen-2.5）在零样本、少样本和微调设置下的性能。

Result: LLaMA 3.1在所有设置下表现一致优秀，是自动化Javadoc生成的可靠选择，为专有系统提供了可行的替代方案。

Conclusion: 开源LLM特别是LLaMA 3.1在模板化代码文档生成方面具有实用价值，能够有效支持软件维护和理解。

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [12] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: 提出了robust-kbench基准测试和自动化CUDA内核发现框架，通过LLM将PyTorch代码转换为CUDA内核并进行迭代优化，在多种场景下实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注高层软件工程任务，缺乏对底层CUDA内核优化的关注，且现有基准测试存在可被利用的漏洞和测试条件不足的问题。

Method: 开发了robust-kbench基准测试，构建了自动化代理框架，包括PyTorch到CUDA的转换、基于进化元生成的运行时优化、以及LLM验证器的正确性验证。

Result: 在robust-kbench上评估显示，生成的CUDA内核性能优于torch实现，能够融合操作并部署多种运行时优化策略，验证器能准确分类错误内核。

Conclusion: 该框架成功实现了CUDA内核的自动化发现和优化，为硬件验证提供了高效解决方案，展示了LLM在底层代码优化方面的潜力。

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [13] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: 提出了一种基于真实编程数据集（Stack Overflow和Kaggle）合成代码问题的新框架，通过整合领域知识、领域技能和编程技能来模拟真实世界编程场景。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型的进一步发展受到真实世界编程问题稀缺性的限制，需要创建能够反映实际挑战的多样化代码问题。

Method: 从真实编程数据集中提取领域知识、领域技能和编程技能，构建场景中心图，并设计图采样策略来控制问题的复杂性和多样性。

Result: 实验结果表明，该方法在各种真实世界基准测试中始终优于不同规模和功能的最先进开源大语言模型。

Conclusion: 该框架能够有效生成反映真实世界挑战的代码问题，为代码大语言模型的训练和评估提供了有价值的资源。

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [14] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 对142,387个工业项目CI作业的实证研究发现，11%的成功作业被重新运行，其中35%在24小时后重跑，揭示了静默失败（成功标记但实际失败）的严重问题。


<details>
  <summary>Details</summary>
Motivation: CI构建结果的可靠性对软件开发至关重要，但静默失败（作业标记成功但实际未完成所有任务）往往被忽视，导致bug进入生产环境，需要系统研究。

Method: 通过对81个工业项目的142,387个作业进行分析，使用混合效应模型评估32个自变量（AUC 85%），并进一步分析92个公开问题来识别静默失败类别。

Result: 发现11%的成功作业被重新运行，识别出测试和静态分析任务、Shell脚本语言以及开发者重跑倾向是关键因素，总结出11类静默失败，最常见的是工件操作错误、缓存错误和忽略退出码。

Conclusion: 研究揭示了CI中静默失败的普遍性和严重性，为提高CI可靠性提供了重要见解和解决方案方向。

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [15] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI是一个结合低秩优化和领域特定指令调优的框架，用于在私有基础设施上高效生成高质量、领域特定的代码，避免依赖第三方API。


<details>
  <summary>Details</summary>
Motivation: 解决基于基础模型的自动化代码生成在领域特异性、成本效益和安全性方面的挑战，特别是在依赖第三方API时的安全问题。

Method: 应用低秩适应技术降低模型预训练和微调的计算成本，采用领域特定指令调优使代码生成与组织需求对齐，在真实JavaScript编码任务上进行测试。

Result: CodeLSI生成高质量、上下文感知的代码，在相关性、准确性和领域适应性方面优于基线模型，低秩优化显著降低了资源需求。

Conclusion: 低秩优化与领域特定调优相结合可以增强基础模型在自动化代码生成中的实用性和性能，提供安全、经济高效的替代方案。

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [16] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: FlashFuzz是首个针对深度学习库的覆盖率引导模糊测试技术，利用LLM自动合成API级测试harness，显著提高了PyTorch和TensorFlow的代码覆盖率和bug检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习库模糊测试方法缺乏覆盖率引导，限制了测试效果和效率。研究探索能否将覆盖率引导模糊测试(CGF)有效应用于DL库，并相比现有方法在代码覆盖率、bug检测和可扩展性方面提供有意义的改进。

Method: 提出FlashFuzz技术，利用大型语言模型(LLMs)自动合成API级测试harness，通过结合模板、辅助函数和API文档，采用反馈驱动的策略迭代合成和修复harness。

Result: 为1,151个PyTorch和662个TensorFlow API合成harness，相比现有方法(ACETest、PathFinder、TitanFuzz)实现101.13-212.88%更高的覆盖率，1.0x-5.4x更高的有效性率，1x-1182x的输入生成加速。发现42个未知bug，其中8个已修复。

Conclusion: 研究证实CGF可以有效地应用于DL库，并为未来的测试方法提供了强有力的基线。

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [17] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 分析253个Claude.md文件，发现智能体清单通常具有浅层层次结构，主要包含操作命令、技术实现说明和高级架构内容


<details>
  <summary>Details</summary>
Motivation: 智能体编码工具需要清单文件提供项目上下文和操作规则，但缺乏创建这些清单的全面文档，给开发者带来挑战

Method: 分析242个代码仓库中的253个Claude.md文件，识别结构模式和常见内容

Result: 清单通常具有一个主标题和若干子部分的浅层层次结构，内容以操作命令、技术实现说明和高级架构为主

Conclusion: 研究揭示了智能体清单的典型结构模式，为开发者创建更好的清单文档提供了参考

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [18] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对567个Claude Code生成的GitHub PR进行实证研究，发现83.8%的AI辅助PR被接受合并，其中54.9%无需修改直接集成，但仍有45.1%需要人工修订。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的集成度提高，需要了解AI代理生成的PR在实际项目中的实用性和接受程度。

Method: 实证研究分析157个开源项目中的567个由Claude Code生成的GitHub PR，研究其任务类型、接受率和修改需求。

Result: 83.8%的AI辅助PR被接受合并，54.9%无需修改直接集成，开发者主要依赖AI进行重构、文档和测试任务。

Conclusion: AI辅助PR大部分可被接受，但仍需要人工监督和精炼，特别是在bug修复、文档和项目标准遵循方面。

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [19] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER是一种基于规则的代码翻译调试方法，通过从LLM生成的正确翻译中自动推导代码翻译规则，显著提高了错误定位和修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的代码翻译调试方法缺乏可靠的参考来构建代码对齐和修复补丁模板，影响了定位准确性和修复效果。

Method: 从LLM生成的正确定译中自动推导代码翻译规则，动态组合规则以适应更多语句的对齐，利用规则进行代码对齐和修复模板设计。

Result: 在Java到C++和Python到C++翻译中，RulER的错误定位率和修复成功率分别比最佳基线方法高出20%和272%。

Conclusion: RulER展示了从LLM中提取和利用编码知识的有前景的方法论，在修复性能上优于直接提示LLM生成补丁。

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [20] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: 提出了CodeFuse-CR-Bench，首个面向代码仓库级别代码审查的综合性基准测试，包含601个高质量实例，覆盖9个PR问题领域，并提供丰富的多维度上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在孤立子任务上使用简化、上下文贫乏的数据评估模型，无法反映真实世界代码审查的整体性和上下文丰富性，存在"现实差距"问题。

Method: 构建包含601个实例的基准测试，每个实例提供issue、PR详情和仓库状态等丰富上下文；提出结合基于规则的定位语法检查和基于模型的审查质量评估的新评估框架。

Result: 进行了大规模评估，发现：(1)没有单一LLM在所有CR方面都占优；(2)Gemini 2.5 Pro综合性能最高；(3)不同LLM对冗余上下文的鲁棒性不同。

Conclusion: 研究强调了进行全面、多维度评估的必要性，为推进真正智能且实用的代码审查助手提供了可行见解。

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [21] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO是一个轻量级的LLM路由框架，通过嵌入回归器和可选分类器实现动态模型选择，无需人工标注，在四个主流LLM上达到76.4%的top-1路由准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在规模、专业化和延迟特性上的多样化，如何将用户提示路由到最合适的模型以平衡性能和成本变得至关重要。

Method: 使用基于嵌入的回归器训练LLM判断的成对比较来预测模型性能，不确定时调用可选二元分类器，支持五个任务类别的特定回归器。

Result: 在GPT-4o、Claude 3.5 Sonnet、DeepSeek V3和Perplexity Sonar四个模型上，CARGO达到76.4%的top-1路由准确率，对抗单个专家的胜率为72%-89%。

Conclusion: 置信度引导的轻量级路由能够以最小开销实现专家级性能，为现实世界多模型LLM部署提供实用解决方案。

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [22] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion是一个自动化模糊测试框架，通过整合LLM推理与传统工具，大幅减少人工工作量，在clib库中发现两个未知漏洞


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试虽然能自动生成输入和监控执行，但从代码库分析、配置测试工具到结果分类的整个工作流程仍需要大量人工操作，现有研究只关注单个阶段而缺乏完整自动化方案

Method: Orion框架结合LLM的代码推理和语义指导能力，同时依赖确定性工具进行验证、迭代优化和需要精确性的任务，实现端到端的模糊测试自动化

Result: 在基准测试套件中，Orion将人工工作量减少了46-204倍（取决于工作流程阶段），并在广泛使用的开源clib库中发现了两个先前未知的漏洞

Conclusion: Orion通过LLM与传统工具的集成，成功解决了模糊测试中的人工瓶颈问题，证明了在人类单独努力不切实际的场景下实现规模化模糊测试活动的可行性

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 对多种LLM代理在渗透测试中的性能评估，通过五种核心功能增强显著提升了模块化代理在复杂任务中的表现


<details>
  <summary>Details</summary>
Motivation: 评估LLM在自动化渗透测试中的有效性和可靠性，明确不同攻击阶段的性能表现和常见失败模式

Method: 在真实渗透测试场景中评估多种LLM代理架构，从单代理到模块化设计，并通过五种针对性增强功能（GCM、IAM、CCI、AP、RTM）来隔离各功能的影响

Result: 虽然某些架构天然具备部分功能特性，但针对性增强显著提升了模块化代理在复杂、多步骤和实时渗透测试任务中的性能

Conclusion: 通过核心功能增强可以有效提升LLM代理在渗透测试中的表现，特别是在复杂任务中模块化设计配合功能增强效果显著

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [24] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 提出了一个模块化评估框架，将Web智能体分解为可解释的阶段进行详细错误分析，揭示了标准指标遗漏的可操作弱点。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注整体成功率而忽略中间错误，限制了故障模式洞察并阻碍系统改进，缺乏细粒度诊断工具。

Method: 使用模块化评估框架，将智能体流水线分解为可解释阶段，以SeeAct框架和Mind2Web数据集为案例进行研究。

Result: 该方法揭示了标准指标遗漏的可操作弱点，为更鲁棒和可泛化的Web智能体铺平了道路。

Conclusion: 模块化错误分析框架能够提供更深入的故障模式洞察，有助于Web智能体的系统性改进。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [25] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 提出了一种先验动态框架用于自动化工作流构建，通过Q-table学习优化决策空间，结合任务进度评估和先验决策，相比现有方法平均提升4.05%性能，同时将成本降低至30.68%-48.31%。


<details>
  <summary>Details</summary>
Motivation: 现有工作流构建方法过度依赖历史经验，在效率和适应性方面存在局限，需要能够灵活响应每个任务独特特性的解决方案。

Method: 使用Q-table学习优化决策空间，结合任务进度评估和先验决策机制，并引入冷启动初始化、早停和剪枝等技术提升系统效率。

Result: 在四个基准数据集上验证了方法的可行性，相比最先进基线平均提升4.05%，同时将工作流构建和推理成本降至现有方法的30.68%-48.31%。

Conclusion: 该框架通过结合历史经验和任务特性响应，实现了更高效和自适应的自动化工作流构建。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [26] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass是首个专门为多智能体工作流设计的后部署监控和调试评估框架，通过多阶段分析流程和双记忆系统实现持续学习，在TRAIL基准测试中达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在自动化复杂多智能体工作流中的广泛应用，当前评估方法无法有效捕捉错误、涌现行为和系统性故障带来的风险，需要专门的监控调试工具。

Method: 采用结构化多阶段分析流程：错误识别分类、主题聚类、量化评分和策略总结，并配备情景记忆和语义记忆的双记忆系统实现持续学习。

Result: 在真实部署中验证实用性，在TRAIL基准测试中达到最先进指标，发现人工标注遗漏的关键问题。

Conclusion: AgentCompass是一个强大的开发者中心工具，能够可靠地监控和改进生产环境中的智能体系统。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [27] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 提出一个包含哨兵代理和协调代理的双层安全架构，用于增强多智能体系统的安全性和可靠性，通过模拟攻击测试验证了有效性


<details>
  <summary>Details</summary>
Motivation: 多智能体系统面临各种安全威胁（如提示注入、幻觉、数据泄露等），需要动态自适应的防御机制来保障系统安全和完整性

Method: 设计分布式安全层：哨兵代理网络负责语义分析、行为分析、异常检测；协调代理负责策略管理、威胁响应和系统治理

Result: 在模拟环境中成功检测了162个不同类型的合成攻击（提示注入、幻觉、数据泄露），证实了监控方法的可行性

Conclusion: 该框架提供了有效的动态防御机制，增强了系统可观测性，支持法规合规，并能够实现策略的持续演进

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [28] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC是一个知识驱动的自适应多智能体协作框架，通过动态组建专家团队来解决医疗诊断中的复杂问题，显著优于传统单智能体和多智能体方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体协作框架采用静态预分配角色，限制了适应性和动态知识整合能力，无法有效处理需要跨专业知识的复杂临床场景。

Method: 提出KAMAC框架，从初始专家智能体开始，通过知识驱动讨论识别知识缺口，动态招募额外专家，支持灵活可扩展的协作，最终通过审查更新的智能体评论来做出决策。

Result: 在两个真实医疗基准测试中，KAMAC显著优于单智能体和先进多智能体方法，特别是在需要动态跨专业知识的复杂临床场景（如癌症预后）中表现突出。

Conclusion: KAMAC框架通过动态团队组建和知识驱动协作，有效解决了医疗决策中跨专业知识整合的挑战，为复杂临床场景提供了更灵活的解决方案。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Online reinforcement learning via sparse Gaussian mixture model Q-functions](https://arxiv.org/abs/2509.14585)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 提出基于稀疏高斯混合模型Q函数(S-GMM-QFs)的结构化在线策略迭代框架，通过Hadamard过参数化实现稀疏化，在Riemannian流形上进行参数更新，性能媲美深度RL方法但参数更少。


<details>
  <summary>Details</summary>
Motivation: 开发结构化且可解释的在线强化学习框架，解决深度RL方法参数过多、泛化能力差的问题，特别是在低参数数量情况下保持良好性能。

Method: 使用稀疏高斯混合模型Q函数，通过Hadamard过参数化进行稀疏化控制模型复杂度，在Riemannian流形结构上进行在线梯度下降优化。

Result: 在标准基准测试中性能与密集深度RL方法相当，但使用参数显著减少，在低参数数量情况下仍保持强性能，而稀疏化深度RL方法无法泛化。

Conclusion: S-GMM-QFs提供了一种参数高效、可解释的在线强化学习替代方案，在保持性能的同时显著减少模型复杂度。

Abstract: This paper introduces a structured and interpretable online policy-iteration
framework for reinforcement learning (RL), built around the novel class of
sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work
that trained GMM-QFs offline, the proposed framework develops an online scheme
that leverages streaming data to encourage exploration. Model complexity is
regulated through sparsification by Hadamard overparametrization, which
mitigates overfitting while preserving expressiveness. The parameter space of
S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing
for principled parameter updates via online gradient descent on a smooth
objective. Numerical tests show that S-GMM-QFs match the performance of dense
deep RL (DeepRL) methods on standard benchmarks while using significantly fewer
parameters, and maintain strong performance even in low-parameter-count regimes
where sparsified DeepRL methods fail to generalize.

</details>


### [30] [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang*

Main category: cs.LG

TL;DR: DSCL框架通过奖励动态采样和任务课程学习，针对工具学习中样本效率低的问题，显著提升训练效率和模型性能


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在工具学习中面临简单样本过多、学习价值递减的问题，现有动态采样技术不适合工具学习的多任务结构和细粒度奖励机制

Method: 提出DSCL框架，包含基于奖励的动态采样（使用多维奖励统计）和基于任务的动态课程学习（自适应关注未掌握子任务）

Result: 在BFCLv3基准上实现3.29%的性能提升，显著优于强基线方法

Conclusion: DSCL为工具学习提供了定制化解决方案，有效利用复杂奖励信号和子任务动态实现优异结果

Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool
learning, its efficiency is often hampered by an overabundance of simple
samples that provide diminishing learning value as training progresses.
Existing dynamic sampling techniques are ill-suited for the multi-task
structure and fine-grained reward mechanisms inherent to tool learning. This
paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework
specifically designed to address this challenge by targeting the unique
characteristics of tool learning: its multiple interdependent sub-tasks and
multi-valued reward functions. DSCL features two core components: Reward-Based
Dynamic Sampling, which uses multi-dimensional reward statistics (mean and
variance) to prioritize valuable data, and Task-Based Dynamic Curriculum
Learning, which adaptively focuses training on less-mastered sub-tasks. Through
extensive experiments, we demonstrate that DSCL significantly improves training
efficiency and model performance over strong baselines, achieving a 3.29\%
improvement on the BFCLv3 benchmark. Our method provides a tailored solution
that effectively leverages the complex reward signals and sub-task dynamics
within tool learning to achieve superior results.

</details>


### [31] [Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization](https://arxiv.org/abs/2509.14848)
*Houssem Sifaou,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出MF-HRL-IGM算法，通过信息增益最大化在多保真度模拟器中选择最优模拟器，在固定成本预算下优化强化学习策略


<details>
  <summary>Details</summary>
Motivation: 解决传统RL需要与高保真模拟器大量交互成本高的问题，以及离线RL受限于数据集质量和规模的问题，利用多保真度模拟器的优势

Method: 基于信息增益最大化的多保真度混合RL算法，使用自举方法进行保真度选择，结合离线数据和在线模拟器交互

Result: 理论分析证明算法具有无后悔性质，实证评估显示其性能优于现有基准方法

Conclusion: MF-HRL-IGM算法能有效利用多保真度模拟器资源，在固定成本预算下实现最优策略优化

Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive
interactions with a high-fidelity simulator of the environment, which are often
costly or impractical. Offline RL addresses this problem by allowing training
from pre-collected data, but its effectiveness is strongly constrained by the
size and quality of the dataset. Hybrid offline-online RL leverages both
offline data and interactions with a single simulator of the environment. In
many real-world scenarios, however, multiple simulators with varying levels of
fidelity and computational cost are available. In this work, we study
multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We
introduce multi-fidelity hybrid RL via information gain maximization
(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity
selection based on information gain maximization through a bootstrapping
approach. Theoretical analysis establishes the no-regret property of
MF-HRL-IGM, while empirical evaluations demonstrate its superior performance
compared to existing benchmarks.

</details>


### [32] [Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation](https://arxiv.org/abs/2509.14925)
*Konrad Nowosadko,Franco Ruggeri,Ahmad Terra*

Main category: cs.LG

TL;DR: 提出基于自解释神经网络(SENNs)的强化学习方法，在保持预测精度的同时增强模型可解释性，特别针对低维问题生成局部和全局解释。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习的黑盒特性阻碍了可解释性和可信度，特别是在关键领域需要透明决策。

Method: 采用自解释神经网络(SENNs)结合解释提取方法，针对低维问题设计可解释的强化学习解决方案。

Result: 在移动网络资源分配问题上验证，SENNs能够提供具有竞争力的性能表现，同时生成鲁棒的解释。

Conclusion: SENNs有潜力提高低维任务中AI驱动决策的透明度和信任度，性能与现有最先进方法相当。

Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks
(DNN), though powerful, often lack transparency. Their black-box characteristic
hinders interpretability and reduces trustworthiness, particularly in critical
domains. To address this challenge in RL tasks, we propose a solution based on
Self-Explaining Neural Networks (SENNs) along with explanation extraction
methods to enhance interpretability while maintaining predictive accuracy. Our
approach targets low-dimensionality problems to generate robust local and
global explanations of the model's behaviour. We evaluate the proposed method
on the resource allocation problem in mobile networks, demonstrating that SENNs
can constitute interpretable solutions with competitive performance. This work
highlights the potential of SENNs to improve transparency and trust in
AI-driven decision-making for low-dimensional tasks. Our approach strong
performance on par with the existing state-of-the-art methods, while providing
robust explanations.

</details>


### [33] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: 提出了DEER方法，通过环境动态差异度量来区分策略更新和环境变化的影响，在非平稳环境中显著提升了离线强化学习算法的性能


<details>
  <summary>Details</summary>
Motivation: 传统经验回放方法在非平稳环境中难以区分策略变化和环境变化的影响，导致学习效率低下

Method: 提出环境动态差异度量(DoE)，并基于此开发DEER框架，使用二元分类器检测环境变化，在不同阶段应用不同的优先级策略

Result: 在四个非平稳基准测试中，DEER相比最先进的ER方法进一步提升了11.54%的性能

Conclusion: DEER能够有效处理非平稳环境中的强化学习问题，通过区分策略和环境变化实现更高效的样本利用

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [34] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 提出了一种结合离线模仿学习和在线强化学习的混合训练方法，用于2D射击游戏智能体，通过多头神经网络架构实现知识迁移和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂游戏环境中强化学习智能体面临的稀疏奖励、训练不稳定和样本效率低的问题。

Method: 使用多头神经网络，共享特征提取层但分离行为克隆和Q学习输出，先通过基于规则智能体的演示数据进行行为克隆，再过渡到强化学习。

Result: 混合方法实现了对基于规则对手70%以上的胜率，显著优于纯强化学习方法，后者表现出高方差和频繁的性能退化。

Conclusion: 结合演示数据初始化和强化学习优化的混合方法为复杂多智能体环境中的游戏AI开发提供了稳健解决方案。

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [35] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: TDRM通过时间差分正则化训练更平滑可靠的奖励模型，提升强化学习训练的稳定性和效果，在多个模型和任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型缺乏时间一致性，导致策略更新无效和强化学习训练不稳定，需要开发更平滑可靠的奖励模型。

Method: 提出TDRM方法，通过在训练过程中最小化时间差异来学习平滑的奖励模型，结合时间差分正则化产生平滑奖励并改善与长期目标的对齐。

Result: TD训练的过程奖励模型在Best-of-N设置中提升6.6%，在树搜索设置中提升23.7%。与RLVR结合时，仅需2.5k数据即可达到基线方法50.1k数据的性能，在8个模型变体上产生更高质量的语言模型策略。

Conclusion: TDRM作为可验证奖励方法的补充，能够显著提升强化学习的数据效率和策略质量，是奖励模型训练的有效改进方法。

Abstract: Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

</details>


### [36] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin*

Main category: cs.LG

TL;DR: 本文提出了一种数据重写框架，通过主动缩小策略差距来稳定监督微调中的离策略学习问题，在数学推理基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调中的离策略学习存在分布不匹配问题，传统重要性采样方法在大策略差距下会导致高方差和训练不稳定。现有方法通过KL惩罚或裁剪被动约束更新，而不是主动减少策略差距。

Method: 提出数据重写框架：保持正确解决方案作为同策略数据，对有错误的解决方案通过引导重写进行修正，仅在需要时回退到专家演示。这样在优化前就将训练分布与目标策略对齐。

Result: 在五个数学推理基准测试中，相比普通SFT和最先进的动态微调方法都取得了持续且显著的性能提升。

Conclusion: 主动缩小策略差距的数据重写方法能有效减少重要性采样方差，稳定离策略微调过程。

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an
off-policy learning problem, where expert demonstrations come from a fixed
behavior policy while training aims to optimize a target policy. Importance
sampling is the standard tool for correcting this distribution mismatch, but
large policy gaps lead to high variance and training instability. Existing
approaches mitigate this issue using KL penalties or clipping, which passively
constrain updates rather than actively reducing the gap. We propose a simple
yet effective data rewriting framework that proactively shrinks the policy gap
by keeping correct solutions as on-policy data and rewriting incorrect ones
with guided re-solving, falling back to expert demonstrations only when needed.
This aligns the training distribution with the target policy before
optimization, reducing importance sampling variance and stabilizing off-policy
fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate
consistent and significant gains over both vanilla SFT and the state-of-the-art
Dynamic Fine-Tuning (DFT) approach. The data and code will be released at
https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [37] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin*

Main category: cs.LG

TL;DR: FlowRL通过流平衡匹配完整奖励分布，而不是最大化奖励，在数学和代码推理任务中显著优于GRPO和PPO方法


<details>
  <summary>Details</summary>
Motivation: 现有的奖励最大化方法（如PPO和GRPO）倾向于过度优化主导奖励信号，忽视较少出现但有效的推理路径，从而降低多样性

Method: 将标量奖励转换为使用可学习配分函数的归一化目标分布，然后最小化策略与目标分布之间的反向KL散度，实现为流平衡优化方法

Result: 在数学基准测试中平均比GRPO提高10.0%，比PPO提高5.1%，在代码推理任务中表现一致更好

Conclusion: 奖励分布匹配是实现LLM强化学习中高效探索和多样化推理的关键步骤

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing
instead of maximizing rewards in large language model (LLM) reinforcement
learning (RL). Recent advanced reasoning models adopt reward-maximizing methods
(\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while
neglecting less frequent but valid reasoning paths, thus reducing diversity. In
contrast, we transform scalar rewards into a normalized target distribution
using a learnable partition function, and then minimize the reverse KL
divergence between the policy and the target distribution. We implement this
idea as a flow-balanced optimization method that promotes diverse exploration
and generalizable reasoning trajectories. We conduct experiments on math and
code reasoning tasks: FlowRL achieves a significant average improvement of
$10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs
consistently better on code reasoning tasks. These results highlight reward
distribution-matching as a key step toward efficient exploration and diverse
reasoning in LLM reinforcement learning.

</details>
