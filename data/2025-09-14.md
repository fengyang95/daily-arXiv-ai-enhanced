<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.LG](#cs.LG) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: GeoJSON Agents是一个多智能体LLM架构，通过Function Calling和Code Generation两种方法将自然语言任务转换为GeoJSON操作命令，显著提升了GIS自动化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在任务自动化和自然语言理解方面取得进展，但缺乏GIS专业知识限制了其应用。需要解决LLMs在空间数据处理方面的局限性。

Method: 提出三组件架构：任务解析、智能体协作和结果集成。Planner智能体解析自然语言为GeoJSON命令，Worker智能体通过函数调用或代码生成执行空间数据处理，最终整合为标准GeoJSON文件。

Result: 基于Function Calling的方法达到85.71%准确率，基于Code Generation的方法达到97.14%准确率，均显著优于通用模型的最佳表现(48.57%)。代码生成方法更灵活，函数调用方法执行更稳定。

Conclusion: 这是首个针对GeoJSON数据的LLM多智能体框架，比较了两种主流LLM增强方法的优劣，为改进GeoAI系统性能提供了新视角。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [2] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成(RAG)的Android恶意软件分析框架，通过自然语言查询和Java代码分析提供可解释的恶意行为检测，达到96%的检测准确率和83.81%的行为识别准确率。


<details>
  <summary>Details</summary>
Motivation: Android恶意应用采用复杂的规避策略，将恶意逻辑隐藏在合法功能中，传统分析方法难以发现深层隐藏行为且缺乏可解释性。

Method: 使用检索增强生成(RAG)框架，首先生成方法级代码片段的摘要并建立向量索引，然后通过行为导向的问题检索相关代码片段进行深入分析，最终生成包含恶意行为及其代码实现的可读报告。

Result: 实验结果显示达到96%的恶意软件检测准确率和83.81%的行为识别准确率，专家评估确认生成报告具有实际应用价值。

Conclusion: TraceRAG框架成功解决了传统Android恶意软件分析方法的局限性，提供了可解释且高效的恶意行为检测和分析能力。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [3] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 提出了LLM能效基准测试，使用vLLM模拟真实生产场景，评估模型大小、架构和并发请求量对推理能效的影响。


<details>
  <summary>Details</summary>
Motivation: LLM的广泛部署和使用对气候产生重大影响，需要收集更多关于LLM能效的信息，现有基准测试往往无法代表真实生产场景。

Method: 使用vLLM（高吞吐量、生产就绪的LLM服务后端）构建能效基准测试，模拟真实使用条件，分析模型大小、架构和并发请求量等因素。

Result: 证明了可以创建更好地反映实际部署条件的能效基准测试，为开发者构建更可持续的AI系统提供有价值的见解。

Conclusion: 该基准测试能够有效评估LLM在实际生产环境中的能源效率，有助于推动更可持续的AI系统开发。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [4] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个浏览器扩展工具，利用先进的推理模型帮助开发者和研究人员进行代码理解、重构和质量检测，无需项目设置，经评估证明实用准确。


<details>
  <summary>Details</summary>
Motivation: 现有代码分析工具需要项目设置、缺乏上下文感知且手动工作量大，CLARA旨在解决这些问题，提供更便捷的代码理解辅助。

Method: 开发浏览器扩展工具CLARA，使用先进的推理模型，支持代码文件理解、代码重构和代码质量属性检测，并通过现有数据集和用户研究进行定性评估。

Result: 评估结果显示CLARA在代码理解和分析任务中实用、准确且实用，用户研究证实了其可用性和有效性。

Conclusion: CLARA是一个开源工具，成功解决了现有代码分析工具的局限性，为开发者和研究人员提供了高效的代码理解辅助解决方案。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [5] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 提出了ReDef数据集，通过revert commits构建高质量缺陷标注，并系统评估了预训练语言模型在代码修改理解任务中的表现，发现当前模型主要依赖表面线索而非真正的语义理解


<details>
  <summary>Details</summary>
Motivation: 现有JIT-SDP数据集存在标签噪声和低精度问题，需要构建高置信度的缺陷标注基准，并深入理解预训练语言模型如何推理代码修改

Method: 基于revert commits构建ReDef数据集，使用GPT辅助筛选过程；对CodeBERT、CodeT5+和UniXcoder进行微调，采用五种编码策略，并通过反事实扰动测试模型敏感性

Result: 紧凑的diff风格编码在所有PLM中始终优于完整函数格式；反事实测试显示性能下降很小，表明模型依赖表面线索而非真正的语义理解

Conclusion: 与基于快照的任务不同，当前PLM在真正理解代码修改方面的能力仍然有限，需要更深入的研究来提升模型对代码修改的语义理解能力

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [6] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 本文提出了一种将大语言模型与传统软件工程技术相结合的方法，通过基于场景的编程范式来提高软件开发可靠性，并在Connect4游戏案例中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs能显著提高开发效率，但它们经常会产生严重错误并以自信的方式呈现错误代码，误导开发者接受有缺陷的解决方案。需要一种更可靠的方式将LLMs整合到软件开发周期中。

Method: 采用基于场景的编程（SBP）范式，这是一种事件驱动、基于场景的软件工程方法，允许开发者将专业知识注入LLM，并检查和验证其输出。

Result: 通过Connect4游戏的案例研究，结合LLMs和SBP创建了高性能的智能体，能够击败各种现有强智能体，并且在某些情况下能够形式化验证智能体的正确性。

Conclusion: 该方法能够简化开发流程、减少错误，并提高用户对关键程序属性验证的信心，同时揭示了该方法在易用性方面的有趣见解。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [7] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 对1.11亿个Git仓库的大规模研究发现，有122万个仓库存在历史记录篡改行为，总计870万次历史重写。这些篡改包括许可证变更和密钥删除等，可能带来安全和治理风险。


<details>
  <summary>Details</summary>
Motivation: Git等版本控制系统允许开发者在本地重写历史记录，但当这些篡改发生在公共分支上时，会破坏推送/拉取工作流，威胁仓库的完整性和可重现性，并为供应链攻击创造机会。

Method: 分析了Software Heritage存档的1.11亿个代码仓库，识别历史篡改行为，并通过两个针对性案例研究（许可证变更和密钥删除）进行深入分析。

Result: 发现122万个仓库存在历史篡改，总计870万次重写操作。篡改行为主要集中在特定仓库和分支，涉及文件内容或提交元数据的修改。

Conclusion: 历史篡改行为反映了项目治理和安全管理的坏实践，为此开发了GitHistorian工具来自动检测和描述公共Git仓库中的历史篡改。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [8] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 研究评估了CodeBERT在产业和开源软件中检测漏洞的性能，开发了集成到CI/CD流程的AI-DO工具，通过调查验证了其实用性


<details>
  <summary>Details</summary>
Motivation: 深度学习漏洞检测技术在产业环境中的应用遇到了信任性、继承系统、数字语识等挑战，需要研究如何将学术研究转化为实际工具

Method: 首先评估CodeBERT在产业和开源软件中检测漏洞的性能，分析跨域汇总能力，探索类不平衡处理策略，然后开发集成到CI/CD流程的AI-DO推荐系统

Result: 培训在产业数据上的模型在同域检测准确，但在开源代码上性能下降；使用适当下采样技术在开源数据上微调的深度学习模型提高了漏洞检测能力

Conclusion: 研究成功开发了能够集成到开发者工作流程中的AI-DO工具，通过适当的数据处理策略和培训方法，可以提高深度学习模型在实际产业环境中的漏洞检测效果

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [9] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的智能合约函数相似性检测方法，通过将AST分解为语句树并进行细粒度比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中开源代码的广泛重用虽然提高了编程效率，但显著加剧了漏洞传播。现有的基于AST的方法难以处理复杂树结构，而深度学习方法往往忽略代码语法和可解释性，导致性能不佳。

Method: SmartDetector将智能合约函数的AST分解为一系列较小的语句树，每个语句树反映源代码的结构元素。然后使用分类器通过比较每对语句树来计算两个函数的相似性得分。为了解决分类器无限超参数空间的问题，数学推导了余弦扩散过程来高效搜索最优超参数。

Result: 在三个大型真实数据集上的广泛实验表明，SmartDetector在F1分数上平均比当前最先进方法提高了14.01%，总体平均F1分数达到95.88%。

Conclusion: SmartDetector提供了一种在细粒度语句级别可解释的智能合约函数相似性计算方法，有效解决了现有方法在处理复杂树结构和保持检测可解释性方面的挑战，显著提升了检测性能。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 本文提出了一种针对奖励信号遭受对抗性破坏的鲁棒Q学习算法，在异步采样模型下证明了有限时间收敛性，收敛速率与非对抗情况相近，仅增加一个与破坏样本比例成正比的附加项。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励信号可能受到极端噪声、传感器故障或恶意攻击的对抗性破坏，这会严重降低传统算法（如Q学习）的性能，需要开发能够在部分奖励被任意扰动的情况下仍能有效运行的鲁棒算法。

Method: 提出了一种新的可证明鲁棒的Q学习算法变体，在异步采样模型下运行。对于未知真实奖励分布统计量的情况，使用了改进的Azuma-Hoeffding不等式来处理几乎鞅的技术工具。

Result: 算法在对抗性破坏下仍能保持有限时间收敛，收敛速率与非对抗情况匹配，仅增加一个与破坏样本比例成正比的不可避免的附加项。推导的信息理论下界表明该附加项是不可避免的。

Conclusion: 这项工作为异步Q学习提供了首个有限时间鲁棒性保证，填补了鲁棒强化学习领域的重要空白，所提出的技术工具可能具有独立的研究价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [11] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个开源的研究基线模型家族，包含0.13B到1.7B参数规模，在8个开放参考数据集上训练，提供中间检查点和完整训练日志，为研究人员提供标准化的参考基准。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供标准化的参考基线，使研究人员能够评估不同训练方法的有效性和质量，促进训练过程的比较和复现。

Method: 使用密集transformer架构，在多个参数规模(0.13B-1.7B)和token规模(最高1T)下，在8个开放参考数据集上进行训练，并提供中间检查点。

Result: NemoTron-CC HQ数据集表现最佳，其次是DCLM-baseline和FineWeb-Edu；建立了可比较的缩放趋势基准，便于训练方法的对比分析。

Conclusion: open-sci-ref提供了标准化的研究基线，包含完整训练过程记录和评估结果，有助于简化复现、标准化比较和促进未来研究。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [12] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 提出了一个基于物理信息神经网络的连续时间多智能体强化学习框架，通过值梯度迭代模块解决高维HJB方程和多智能体价值函数近似难题


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以处理高频或非规则时间间隔交互的复杂动力学系统，连续时间RL虽然前景广阔但主要限于单智能体领域，面临维度灾难和多智能体价值函数近似不准确的挑战

Method: 使用物理信息神经网络(PINNs)近似HJB方程的价值函数，引入值梯度迭代(VGI)模块迭代优化轨迹上的价值梯度，确保价值学习与价值梯度学习的一致性

Result: 在连续时间版本的多智能体粒子环境和多智能体MuJoCo基准测试中，该方法持续优于现有连续时间RL基线方法，并能扩展到复杂多智能体动力学系统

Conclusion: 所提出的CT-MARL框架成功解决了连续时间多智能体强化学习中的关键挑战，通过PINNs和VGI模块实现了对高维HJB方程的有效求解和稳定策略学习

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [13] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 量子启发的QLSTM与QA3C强化学习模型在USD/TWD交易中实现11.87%收益，最大回撤仅0.92%，优于传统货币ETF


<details>
  <summary>Details</summary>
Motivation: 结合量子启发神经网络与深度强化学习，为金融交易提供新途径，特别是在外汇市场寻求更好的风险调整后收益

Method: 集成量子长短期记忆网络(QLSTM)进行短期趋势预测，结合量子异步优势演员评论家(QA3C)算法，使用多核训练，状态设计包含QLSTM特征和技术指标

Result: 在2000-2025年数据上训练测试，多头策略实现11.87%的5年回报率，最大回撤仅0.92%，表现优于多个货币ETF

Conclusion: 混合量子启发模型在外汇交易中具有竞争力，QLSTM特别适合小利润紧风险交易，但存在经典量子模拟和策略简化等限制

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [14] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间实施长度公平裁剪来解决传统PPO/GRPO方法在序列长度处理上的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在固定裁剪范围系统性地重加权短响应与长响应的问题，导致有效目标失真。

Method: 提出FSPO方法，使用高斯动机的解决方案：用KL校正漂移项和√L缩放带裁剪序列对数IS比率。

Result: FSPO在不同长度区间内平坦化裁剪率，稳定训练过程，在多个评估数据集上优于所有基线方法。

Conclusion: FSPO通过理论形式化的长度公平性（LRE）和实际有效的裁剪策略，解决了序列级RL中的长度偏差问题，提供了更好的训练稳定性和性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [15] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚策略来解决约束强化学习中的训练不稳定问题，在保持约束边界的同时优化性能


<details>
  <summary>Details</summary>
Motivation: 连续控制场景中，智能体在最大化回报和满足安全约束之间存在平衡难题，传统策略优化方法在约束边界附近表现不稳定，导致训练性能不佳

Method: 引入自适应激励机制，在接近约束边界前保持约束界限；提出IP3O算法，采用渐进增加的惩罚机制来稳定训练动态

Result: 在基准环境上的实证评估表明，IP3O相比最先进的安全RL算法表现出更好的性能，同时提供了理论最优性误差界限的保证

Conclusion: IP3O算法有效解决了约束强化学习中的训练稳定性问题，通过渐进惩罚机制实现了更好的约束满足和性能优化平衡

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [16] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过从LLM蒸馏领域知识和推理能力来提升问答模拟器的性能，在保持高效推理的同时达到更好的模拟效果


<details>
  <summary>Details</summary>
Motivation: 现有问答模拟器方法存在性能与效率的权衡：LLM-free方法推理快但性能次优，LLM-based方法性能好但推理慢且资源消耗大

Method: 提出LLM蒸馏模拟器(LDSim)，从大型语言模型蒸馏领域知识和推理能力来辅助预测，提升模拟性能

Result: 大量实验表明LDSim在模拟任务和知识追踪任务上都取得了强劲的结果

Conclusion: LDSim方法成功解决了问答模拟器中性能与效率的权衡问题，通过知识蒸馏实现了既高效又准确的模拟性能

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [17] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 本文提出Entropy-Modulated Policy Gradients (EMPG)框架，通过基于不确定性和任务结果重新校准学习信号，解决LLM智能体在长时程任务中因稀疏奖励导致的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 长时程任务中，基于大语言模型的智能体面临稀疏奖励难以分配信用的问题，且策略梯度大小与熵固有耦合，导致对自信正确动作的更新效率低下，对不确定动作的更新可能不稳定。

Method: 提出EMPG框架，基于步骤不确定性和最终任务结果重新校准学习信号：放大自信正确动作的更新，惩罚自信错误，衰减不确定步骤的更新以稳定探索，并引入未来清晰度奖励项鼓励寻找可预测解路径。

Result: 在WebShop、ALFWorld和Deep Search三个挑战性智能体任务上的综合实验表明，EMPG实现了显著的性能提升，显著优于强策略梯度基线方法。

Conclusion: EMPG通过调节熵来优化策略梯度学习动态，有效解决了长时程任务中的信用分配问题，提升了基于LLM智能体的学习效率和稳定性。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [18] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型和超参数选择需要大量专业知识和计算资源的问题，当前基于LLM的方法依赖试错和昂贵API，缺乏可解释性和泛化性。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理进行超参数推荐，采用LLM-as-judge评估控制输出格式、准确性和完整性。

Result: 在8个医学影像数据集上测试，MetaLLMiX性能与传统HPO方法相当或更优，计算成本大幅降低，本地部署在5/8任务上达到最优结果，响应时间减少99.6-99.9%，训练速度提升2.4-15.7倍。

Conclusion: MetaLLMiX提供了一种高效、低成本的超参数优化解决方案，在保持性能的同时显著提升效率，为自动化机器学习提供了新的可行路径。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [19] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的自我反事实解释(SCEs)存在有效性-最小性权衡问题，要么修改过多缺乏洞察力，要么修改过少无法改变预测结果，表明SCEs作为可解释性工具效果有限甚至可能误导


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自我生成的反事实解释来向人类解释其决策过程，以促进人机有效协作

Method: 评估LLMs生成的反事实解释在有效性和最小性两个维度上的表现，测试了多种LLMs、数据集和评估设置

Result: LLMs通常能生成有效的反事实解释但远非最小化修改，当要求最小化修改时又往往修改过少而无法改变预测结果

Conclusion: 自我反事实解释作为可解释性工具效果有限甚至可能产生误导，在关键应用场景部署LLMs时必须考虑不可靠自我解释对下游决策的影响

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [20] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 开发了一个名为Agent-E的AI代理系统，能够自动从会议论文中识别特定地区的论文，并通过RPA技术完成预定操作（如提交提名表），在586篇论文测试中实现了100%召回率和99.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献快速增长带来的发现和管理挑战，减少学术发现所需的手动工作量，提高学术社区的工作效率。

Method: 构建了一个完全自动化的系统管道，使用专门的AI代理Agent-E来识别特定地理区域的会议论文，然后通过机器人流程自动化（RPA）执行预定操作。

Result: 在5个不同会议的586篇论文上验证，系统成功识别所有目标论文，召回率达到100%，准确率接近完美（99.4%）。

Conclusion: 面向任务的AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程，展示了其在学术自动化方面的巨大潜力。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [21] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出了D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保护隐私的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能，且随着模型参数指数增长，随机优化器的学习效率面临挑战，需要解决隐私保护与模型效用之间的平衡问题

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影SGD，动态调整效用与隐私之间的权衡

Result: 在不同目标函数上表现出可证明的次线性收敛率，匹配最佳可用速率，实验显示在保持隐私的同时显著提高准确性

Conclusion: D2P2-SGD通过动态隐私机制和随机投影技术，有效解决了隐私保护与模型性能之间的权衡问题，为大规模机器学习提供了高效的隐私保护优化方案

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [22] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个新的机器学习实验数据集，旨在解决OpenML在算法选择问题中存在的管道多样性不足问题，提供了9,408个管道在300个数据集上的完整实验结果。


<details>
  <summary>Details</summary>
Motivation: OpenML等现有实验库在数据预处理步骤的多样性方面存在局限，主要关注少数流行技术，导致样本不平衡，无法充分支持元学习研究。

Method: 构建PIPES数据集，通过系统性地组合选定技术集合的所有可能组合，创建多样化和完整的管道实验集合，包含详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息。

Result: 成功创建了包含9,408个管道在300个数据集上实验结果的综合数据集，提供了比OpenML更全面和平衡的管道表示。

Conclusion: PIPES为元学习社区提供了一个多样化、代表性强的实验数据集，支持更全面的算法选择分析，并具有进一步扩展的潜力。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [23] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准不同保护子组的安全阈值来减少伤害并实现公平性目标（覆盖率或伤害均等化），在医疗补助人群健康管理数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习在决策支持系统中可能对保护子组造成不公平伤害的问题，需要开发一种既能保证安全性又能提升公平性的方法。

Method: 提出可行性引导的公平自适应强化学习（FG-FARL），校准每个子组的安全阈值，使用医疗补助人群健康管理的去标识化纵向轨迹数据进行评估，与行为克隆和HACO基线方法进行比较。

Result: FG-FARL在保持与基线方法相当的价值估计的同时，显著改善了公平性指标，通过bootstrap 95%置信区间和子组差异分析（p值）验证了统计显著性。

Conclusion: FG-FARL提供了一条实现更安全、更公平决策支持的实用路径，在医疗健康管理场景中证明了其有效性。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>
