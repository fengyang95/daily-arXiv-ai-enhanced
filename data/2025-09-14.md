<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 14]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出了一种对抗性腐败奖励信号下的鲁棒Q学习算法，在异步采样模型中实现了与非对抗情况相近的有限时间收敛率，并建立了信息理论下界证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在奖励信号受到对抗性腐败（如极端噪声、传感器故障或恶意攻击）时性能严重下降，需要开发能够有效处理腐败奖励的鲁棒算法。

Method: 提出了新的可证明鲁棒的Q学习变体算法，使用精炼的Azuma-Hoeffding不等式分析几乎鞅，无需真实奖励分布统计信息先验知识。

Result: 在异步采样模型下，算法收敛率与非对抗情况匹配，仅增加与腐败样本比例成正比的附加项，且该附加项被证明是不可避免的最优下界。

Conclusion: 这是首个为异步Q学习提供有限时间鲁棒性保证的工作，填补了鲁棒强化学习领域的重要空白，所提出的技术工具可能具有独立研究价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [2] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个开源研究基线模型家族，包含0.13B到1.7B参数规模，在8个开放参考数据集上训练，提供中间检查点和完整训练资源以便复现和比较。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供标准化的参考基线，用于评估不同训练方法的质量和有效性，特别是在不同模型规模和数据集上的表现比较。

Method: 使用密集transformer架构，在多个参数规模(0.13B-1.7B)和token规模(最高1T)下，在8个开放参考数据集上进行训练，并提供中间检查点、训练日志和评估代码。

Result: NemoTron-CC HQ数据集表现最佳，其次是DCLM-baseline和FineWeb-Edu；建立了可比较的参考基线，使不同训练方法能在统一计算轴上进行比较。

Conclusion: open-sci-ref提供了标准化的研究基线，有助于复现性研究、训练方法比较和未来研究发展，特别是在模型缩放趋势分析方面具有重要价值。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [3] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 提出了一个基于物理信息神经网络(PINNs)的连续时间多智能体强化学习框架(CT-MARL)，通过值梯度迭代(VGI)模块解决高维HJB方程和多智能体价值函数近似难题


<details>
  <summary>Details</summary>
Motivation: 现有连续时间RL方法主要局限于单智能体领域，因为HJB方程存在维度灾难问题，且多智能体设置中集中式价值函数近似困难，导致策略训练不稳定

Method: 使用物理信息神经网络(PINNs)近似HJB基价值函数，引入值梯度迭代(VGI)模块通过沿轨迹迭代细化值梯度来确保价值与微分结构的一致性

Result: 在连续时间版多智能体粒子环境和MuJoCo基准测试中，该方法持续优于现有连续时间RL基线，并能扩展到复杂多智能体动力学系统

Conclusion: 该框架成功解决了连续时间多智能体RL的关键挑战，通过结合PINNs和VGI实现了在复杂动力学系统中的可扩展和高性能学习

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [4] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 量子启发的QLSTM和QA3C算法在USD/TWD交易中实现11.87%的5年回报率，最大回撤仅0.92%，优于传统货币ETF


<details>
  <summary>Details</summary>
Motivation: 结合量子启发神经网络和深度强化学习，为金融交易提供新的解决方案，特别是在外汇市场的小利润交易和严格风险控制方面

Method: 集成量子长短期记忆网络(QLSTM)进行短期趋势预测，结合量子异步优势演员评论家算法(QA3C)，使用多核训练，状态设计包含QLSTM特征和技术指标

Result: 在2000-2025年数据上训练测试，纯多头策略实现11.87%的5年回报率，最大回撤0.92%，表现优于多个货币ETF

Conclusion: 混合量子启发模型在外汇交易中具有竞争力，QLSTM特别适合小利润交易和严格风险控制，未来可进一步优化量子模拟和策略复杂度

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [5] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过重要性采样权重空间的长度公平裁剪来解决PPO/GRPO方法在序列长度处理上的不匹配问题，提出基于高斯理论的KL校正漂移项和√L缩放策略，实现跨长度区间的公平训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法（如PPO/GRPO）在序列裁剪时存在固定裁剪范围对长短响应重新加权不一致的问题，导致有效目标失真，需要解决长度公平性问题。

Method: 提出FSPO方法：在重要性采样权重空间实施长度公平裁剪，引入高斯理论启发的解决方案——使用KL校正漂移项和√L缩放来裁剪序列对数IS比率。

Result: FSPO在多个评估数据集上均优于所有基线方法，能够平坦化不同长度区间的裁剪率，稳定训练过程。

Conclusion: FSPO通过理论形式化的长度重加权误差和方向余弦保证，有效解决了序列级RL中的长度公平性问题，为LLM的序列优化提供了更公平稳定的训练方法。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [6] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚策略来解决约束强化学习中的训练不稳定问题，在约束边界附近实现更好的性能平衡


<details>
  <summary>Details</summary>
Motivation: 连续控制环境中，智能体在奖励最大化和约束满足之间难以平衡，传统策略优化方法在约束边界附近表现不稳定，导致次优训练性能

Method: 引入自适应激励机制，在接近约束边界前保持约束范围内；提出IP3O算法，采用渐进增加的惩罚机制来稳定训练动态

Result: 在基准环境上的实证评估显示，IP3O相比最先进的安全RL算法表现出更好的性能

Conclusion: IP3O算法有效解决了约束强化学习的训练稳定性问题，同时提供了理论保证，推导了算法最优性的最坏情况误差界限

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [7] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过从LLM中蒸馏领域知识和推理能力来提升问答模拟器的性能，在保持推理速度的同时达到更好的效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM-free方法推理快但性能次优，LLM-based方法效果好但推理慢且GPU内存消耗高，需要平衡性能与效率

Method: LLM蒸馏基于模拟器(LDSim)，从大型语言模型中蒸馏领域知识和推理能力来辅助预测，提升模拟性能

Result: 大量实验表明LDSim在模拟任务和知识追踪任务上都取得了强劲的结果

Conclusion: LDSim方法有效解决了问答模拟器中性能与效率的平衡问题，通过知识蒸馏技术实现了更好的模拟效果

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [8] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 本文提出了熵调制策略梯度（EMPG）框架，解决LLM智能体在长时程任务中因稀疏奖励导致的信用分配问题，通过基于不确定性和任务结果重新校准学习信号，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 长时程任务中，基于大语言模型的智能体面临稀疏奖励难以分配给中间步骤的挑战，传统方法主要关注创建密集奖励信号，但存在学习动态中的根本问题：策略梯度幅度与熵耦合，导致置信正确动作更新效率低，不确定动作更新不稳定。

Method: 提出熵调制策略梯度（EMPG）框架，基于步骤不确定性和最终任务结果重新校准学习信号，放大置信正确动作的更新，惩罚置信错误，衰减不确定步骤的更新以稳定探索，并引入未来清晰度奖励项鼓励寻找更可预测的解决方案路径。

Result: 在WebShop、ALFWorld和DeepSearch三个具有挑战性的智能体任务上进行综合实验，EMPG实现了显著的性能提升，显著优于强策略梯度基线方法。

Conclusion: EMPG通过解耦策略梯度幅度与熵的耦合关系，有效解决了长时程任务中的信用分配问题，为基于LLM的智能体学习提供了更稳定和高效的训练框架。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [9] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中模型和超参数选择需要大量专业知识和计算资源的问题，克服现有LLM方法依赖试错和昂贵API、可解释性和泛化性有限的局限性。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理进行零样本超参数优化，采用LLM-as-judge评估控制输出格式、准确性和完整性。

Result: 在8个医学影像数据集上使用9个开源轻量级LLM，性能与传统HPO方法相当或更优，计算成本大幅降低。本地部署在5/8任务上达到最优结果，响应时间减少99.6-99.9%，6个数据集训练速度提升2.4-15.7倍，准确率与最佳基线相差1-5%以内。

Conclusion: MetaLLMiX框架有效解决了AutoML中的超参数优化问题，通过结合元学习和可解释AI实现了高效、低成本的零样本优化，在保持性能的同时显著提升了效率。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [10] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的自我反事实解释(SCEs)存在有效性-最小性权衡问题，虽然能产生有效解释但缺乏最小性，或者过度最小化导致预测不变，这使其成为不可靠的解释性工具。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自我生成的反事实解释来向人类解释其决策过程，这对于模型在关键场景中的可信部署至关重要。

Method: 评估LLMs生成的反事实解释的有效性(是否达到预期结果)和最小性(是否只做必要修改)，在多个LLMs、数据集和评估设置中进行测试。

Result: LLMs通常能产生有效的SCEs但远非最小化，当要求最小化时又往往过度编辑导致预测不变，这种权衡问题在不同模型和设置中普遍存在。

Conclusion: SCEs作为解释性工具效果有限甚至可能误导，在关键应用部署LLMs时必须考虑不可靠自我解释对下游决策的影响。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [11] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 本文提出了一种全自动系统，使用AI代理Agent-E从会议论文中识别特定地理区域的论文，并通过RPA执行预定义操作，实现了100%召回率和99.4%准确率。


<details>
  <summary>Details</summary>
Motivation: 学术文献快速增长给研究人员、资助机构和学术团体带来了巨大挑战，需要减少学术发现过程中耗时的手动工作。

Method: 开发了一个从数据发现到直接行动的自动化流水线，使用专门的AI代理Agent-E识别特定地理区域的会议论文，并通过机器人流程自动化（RPA）执行预定义操作（如提交提名表格）。

Result: 在5个不同会议的586篇论文上验证，系统成功识别所有目标论文，召回率达到100%，准确率接近完美的99.4%。

Conclusion: 面向任务的AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程，展示了巨大潜力。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [12] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出了D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保证隐私的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能边界，且随着模型参数指数增长，随机优化器的学习效率面临挑战

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影SGD，动态调整效用与隐私的权衡

Result: 在不同目标函数上表现出可证明的次线性收敛率，匹配最佳可用率，实验显示在保持隐私的同时显著提高准确性

Conclusion: DDP以隐私为代价带来更好的效用，随机投影实现更高效的模型学习，D2P2-SGD在隐私和性能间取得更好平衡

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [13] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的实验数据集，包含9,408个管道在300个数据集上的完整实验结果


<details>
  <summary>Details</summary>
Motivation: 解决机器学习算法选择问题中评估成本高的问题，同时克服OpenML在管道多样性和代表性方面的局限性

Method: 创建PIPES实验集合，通过系统组合不同的数据预处理技术构建多样化管道，在300个数据集上执行9,408个管道的实验

Result: 建立了包含详细管道信息、训练测试时间、预测结果、性能指标和错误信息的全面实验数据库

Conclusion: PIPES为元学习研究提供了多样化和完整的实验数据基础，支持算法选择问题的深入研究，并具有可扩展性

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [14] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准各受保护子组的安全阈值来减少伤害并实现公平性目标（覆盖率或伤害均等化），在医疗补助人群健康管理数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习在决策支持系统中可能对受保护子组造成不公平伤害的问题，需要开发一种既能保证安全性又能提升公平性的方法。

Method: 提出可行性引导的公平自适应强化学习（FG-FARL），通过校准每个子组的安全阈值，在减少总体伤害的同时实现跨子组的公平性目标（覆盖率或伤害均等化）。

Result: 在医疗补助人群健康管理数据上，FG-FARL与行为克隆和HACO基线相比，在保持可比价值的同时显著改善了公平性指标，bootstrap 95%置信区间和子组差异分析的p值证明了其统计显著性。

Conclusion: FG-FARL为构建更安全、更公平的决策支持系统提供了一条实用路径，能够在保持性能的同时有效减少跨子组的不公平现象。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: GeoJSON Agents是一个多智能体LLM架构，通过Function Calling和Code Generation两种方法将自然语言任务转换为GeoJSON操作命令，显著提升了GIS自动化性能，其中代码生成方法达到97.14%的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在任务自动化和自然语言理解方面取得进展，但缺乏GIS专业知识限制了其在空间数据处理中的应用。需要开发专门框架来解决LLM在GIS领域的局限性。

Method: 提出三组件架构：任务解析、智能体协作和结果集成。Planner智能体将自然语言解析为GeoJSON命令，Worker智能体通过函数调用或代码生成执行空间数据处理，最终集成输出为标准GeoJSON文件。

Result: 基于GPT-4o的实验显示：Function Calling方法准确率85.71%，Code Generation方法准确率97.14%，均显著优于通用模型（48.57%）。代码生成更灵活，函数调用更稳定。

Conclusion: 这是首个针对GeoJSON数据的LLM多智能体框架，系统比较了两种主流LLM增强方法的优劣，为提升GeoAI系统性能提供了新视角。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [16] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成(RAG)的框架，通过自然语言查询和Java代码分析，提供可解释的Android恶意软件检测和分析，准确率达到96%


<details>
  <summary>Details</summary>
Motivation: 传统分析技术难以恢复深度隐藏的恶意行为或提供人类可读的决策解释，需要更强大的深度分析框架来应对复杂的Android恶意应用规避策略

Method: 首先生成方法级代码片段的摘要并索引到向量数据库中，然后通过行为导向的问题检索语义最相关的代码片段进行深度检查，最后基于多轮分析结果生成包含恶意行为及其对应代码实现的人类可读报告

Result: 实验结果显示该方法在更新的VirusTotal扫描和手动验证基础上，实现了96%的恶意软件检测准确率和83.81%的行为识别准确率，专家评估确认了生成报告的实际效用

Conclusion: TraceRAG框架成功地将自然语言查询与代码分析相结合，为Android恶意软件检测提供了可解释且准确的分析解决方案

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [17] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 提出了LLM能效基准测试，使用vLLM模拟真实生产环境，分析模型大小、架构和并发请求量对推理能效的影响，为开发者构建可持续AI系统提供参考


<details>
  <summary>Details</summary>
Motivation: LLM的广泛部署和使用对气候产生日益增长的影响，需要收集更多关于LLM能效的信息，现有基准测试往往无法代表真实生产场景

Method: 引入LLM能效基准测试，使用vLLM（高吞吐量、生产就绪的LLM服务后端）来模拟真实使用条件，优化模型性能和效率

Result: 研究发现可以创建更好地反映实际部署条件的能效基准测试，分析了模型大小、架构和并发请求量等因素对推理能效的具体影响

Conclusion: 该基准测试为开发者构建更可持续的AI系统提供了有价值的见解，有助于提高LLM部署的能源效率

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [18] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个浏览器扩展工具，利用先进的推理模型帮助开发者和研究人员进行代码理解、重构和质量检测，无需项目设置，具有上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解工具需要项目设置、缺乏上下文感知且需要大量手动操作，CLARA旨在解决这些问题。

Method: 开发浏览器扩展工具，集成最先进的推理模型，支持代码文件理解、代码重构和代码质量属性检测。

Result: 通过定性评估和10名开发者和研究人员的用户研究，证明CLARA在代码理解和分析任务中实用、准确且有效。

Conclusion: CLARA是一个开源工具，能够有效辅助代码理解和分析，提高开发和研究效率。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [19] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 提出了ReDef数据集，基于revert commits构建高质量软件缺陷预测基准，并通过系统实验发现预训练语言模型在代码修改理解上仍依赖表面线索而非真正的语义理解


<details>
  <summary>Details</summary>
Motivation: 现有JIT-SDP数据集存在标签噪声和低精度问题，需要构建高置信度的缺陷预测基准，并系统评估预训练语言模型对代码修改的理解能力

Method: 从22个大型C/C++项目中构建ReDef数据集，使用revert commits锚定缺陷案例，GPT辅助筛选模糊实例；对CodeBERT、CodeT5+、UniXcoder进行微调，使用五种编码策略，并通过反事实扰动测试模型敏感性

Result: 构建了3,164个缺陷和10,268个清洁修改的高质量数据集；diff-style编码在所有PLMs中始终优于whole-function格式；反事实测试显示模型性能下降很小，表明模型依赖表面线索而非真正的语义理解

Conclusion: 当前预训练语言模型在代码修改理解任务中能力有限，主要依赖表面特征而非真正的语义理解，与快照式任务不同

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [20] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 本文提出了一种将大型语言模型（LLMs）与传统软件工程技术（特别是基于场景的编程范式）相结合的方法，以提高软件开发过程的可靠性，减少错误，并增强对关键程序属性的验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件开发中能显著减少开发时间、生成组织良好的代码并提供创新思路，但它们经常引入严重错误并以高度自信呈现错误代码，可能误导开发者接受有缺陷的解决方案。

Method: 采用基于场景的编程（SBP）范式，这是一种事件驱动、基于场景的软件工程方法，允许开发者将专业知识注入LLM，并检查和验证其输出。通过结合LLMs和SBP来设计系统。

Result: 通过一个重要的案例研究（Connect4游戏的设计与实现），成功创建了一个高性能的智能体，能够击败各种现有的强大智能体。在某些情况下，还能正式验证智能体的正确性。

Conclusion: 该方法展示了将LLMs与传统软件工程技术相结合的有效性，能够提高开发过程的可靠性，减少错误，并为关键程序属性提供验证。经验还揭示了该方法在易用性方面的有趣见解。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [21] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 首次大规模调查Git历史修改行为，分析了1.11亿个仓库，发现122万个仓库存在历史修改，总计870万次历史重写，并开发了GitHistorian工具来检测这些修改。


<details>
  <summary>Details</summary>
Motivation: Git等版本控制系统允许开发者本地重写历史记录，这在公共分支上会产生问题：破坏推送/拉取工作流、挑战仓库完整性和可复现性，并为供应链攻击创造机会。

Method: 分析Software Heritage存档的1.11亿个代码仓库，识别历史修改行为，按发生位置（仓库、分支）和修改内容（文件或提交元数据）进行分类，并通过两个案例研究验证发现。

Result: 发现122万个仓库存在历史修改，总计870万次历史重写。案例研究表明历史修改常用于追溯性更改许可证或移除错误提交的密钥等敏感信息。

Conclusion: Git历史修改行为普遍存在且可能带来治理和安全风险，为此开发了GitHistorian工具帮助开发者检测和描述公共Git仓库中的历史修改。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [22] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 本研究评估了CodeBERT在工业与开源软件中的漏洞检测性能，开发了CI/CD集成的AI-DO系统，并通过调查验证了其可用性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习漏洞检测技术从学术界向工业界转移的挑战，包括可信性、遗留系统、数字素养差距以及工作流集成等问题。

Method: 使用CodeBERT检测漏洞函数，分析跨领域泛化性能，采用欠采样技术处理类别不平衡，开发CI/CD集成的AI-DO推荐系统。

Result: 基于工业数据训练的模型在相同领域内检测准确，但在开源代码上性能下降；基于开源数据微调的深度学习模型通过欠采样技术能改进漏洞检测。

Conclusion: 研究证明了适当的数据处理和模型微调策略可以提升跨领域漏洞检测性能，AI-DO系统为工业实践提供了可行的集成解决方案。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [23] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的智能合约函数相似性检测方法，通过将AST分解为语句树并进行细粒度比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中广泛重用开源代码显著提高了编程效率，但也放大了漏洞传播风险。现有的基于AST的方法难以处理复杂树结构，而深度学习方法往往忽略代码语法和可解释性，导致性能不佳。

Method: SmartDetector将智能合约函数的AST分解为一系列较小的语句树，每个语句树反映源代码的结构元素。然后使用分类器通过比较每对语句树来计算两个函数的相似度得分。为了解决分类器无限超参数空间的问题，数学推导了余弦扩散过程来高效搜索最优超参数。

Result: 在三个大型真实数据集上的广泛实验表明，SmartDetector在F1分数上平均比当前最先进方法提升了14.01%，总体平均F1分数达到95.88%。

Conclusion: SmartDetector提供了一种在细粒度语句级别可解释的智能合约函数相似性计算方法，有效解决了现有方法在处理复杂树结构和保持可解释性方面的局限性，显著提升了检测性能。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>
