{"id": "2509.12395", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12395", "abs": "https://arxiv.org/abs/2509.12395", "authors": ["Yash Mundhra", "Max Valk", "Maliheh Izadi"], "title": "Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large language models have shown impressive performance in various domains,\nincluding code generation across diverse open-source domains. However, their\napplicability in proprietary industrial settings, where domain-specific\nconstraints and code interdependencies are prevalent, remains largely\nunexplored. We present a case study conducted in collaboration with the\nleveling department at ASML to investigate the performance of LLMs in\ngenerating functional, maintainable code within a closed, highly specialized\nsoftware environment.\n  We developed an evaluation framework tailored to ASML's proprietary codebase\nand introduced a new benchmark. Additionally, we proposed a new evaluation\nmetric, build@k, to assess whether LLM-generated code successfully compiles and\nintegrates within real industrial repositories. We investigate various\nprompting techniques, compare the performance of generic and code-specific\nLLMs, and examine the impact of model size on code generation capabilities,\nusing both match-based and execution-based metrics. The findings reveal that\nprompting techniques and model size have a significant impact on output\nquality, with few-shot and chain-of-thought prompting yielding the highest\nbuild success rates. The difference in performance between the code-specific\nLLMs and generic LLMs was less pronounced and varied substantially across\ndifferent model families.", "AI": {"tldr": "LLM\u5728\u5de5\u4e1a\u4e13\u6709\u4ee3\u7801\u5e93\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\u7814\u7a76\uff0c\u63d0\u51fa\u4e86build@k\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u63d0\u793a\u6280\u672f\u548c\u6a21\u578b\u5927\u5c0f\u5bf9\u8f93\u51fa\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd", "motivation": "\u63a2\u7d22LLM\u5728\u5de5\u4e1a\u4e13\u6709\u73af\u5883\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u548c\u4ee3\u7801\u4f9d\u8d56\u6027\u95ee\u9898", "method": "\u5f00\u53d1\u9488\u5bf9ASML\u4e13\u6709\u4ee3\u7801\u5e93\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65b0\u57fa\u51c6\uff0c\u63d0\u51fabuild@k\u8bc4\u4f30\u6307\u6807\uff0c\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u6280\u672f\u3001\u901a\u7528\u4e0e\u4ee3\u7801\u4e13\u7528LLM\u7684\u6027\u80fd", "result": "\u63d0\u793a\u6280\u672f\u548c\u6a21\u578b\u5927\u5c0f\u5bf9\u8f93\u51fa\u8d28\u91cf\u5f71\u54cd\u663e\u8457\uff0cfew-shot\u548cchain-of-thought\u63d0\u793a\u83b7\u5f97\u6700\u9ad8\u6784\u5efa\u6210\u529f\u7387\uff0c\u4ee3\u7801\u4e13\u7528\u4e0e\u901a\u7528LLM\u6027\u80fd\u5dee\u5f02\u4e0d\u660e\u663e", "conclusion": "LLM\u5728\u5de5\u4e1a\u4e13\u6709\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u63d0\u793a\u6280\u672f", "topic": "swe application"}}
{"id": "2509.12443", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12443", "abs": "https://arxiv.org/abs/2509.12443", "authors": ["Sparsh Gupta", "Kamalavasan Kamalakkannan", "Maxim Moraru", "Galen Shipman", "Patrick Diehl"], "title": "From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow", "comment": null, "summary": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware.\n  This paper presents an agentic AI workflow where specialized LLM \"agents\"\ncollaborate to translate, validate, compile, run, test, debug, and optimize\nFortran kernels into portable Kokkos C++ programs. Results show the pipeline\nmodernizes a range of benchmark kernels, producing performance-portable Kokkos\ncodes across hardware partitions. Paid OpenAI models such as GPT-5 and\no4-mini-high executed the workflow for only a few U.S. dollars, generating\noptimized codes that surpassed Fortran baselines, whereas open-source models\nlike Llama4-Maverick often failed to yield functional codes.\n  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684AI\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5c06\u4f20\u7edfFortran\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u4e3a\u6027\u80fd\u53ef\u79fb\u690d\u7684Kokkos C++\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u4e86\u8de8\u786c\u4ef6\u5e73\u53f0\u7684\u4ee3\u7801\u73b0\u4ee3\u5316\u3002", "motivation": "\u968f\u7740HPC\u5411GPU\u52a0\u901f\u67b6\u6784\u8f6c\u53d8\uff0c\u4f20\u7edfFortran\u4ee3\u7801\u7f3a\u4e4f\u539f\u751fGPU\u7ed1\u5b9a\uff0c\u9700\u8981\u73b0\u4ee3\u5316\u6539\u9020\u4ee5\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002\u624b\u52a8\u79fb\u690d\u5de5\u4f5c\u91cf\u5927\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u63a2\u7d22AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e13\u95e8\u7684LLM\u4ee3\u7406\u534f\u4f5c\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u7ffb\u8bd1\u3001\u9a8c\u8bc1\u3001\u7f16\u8bd1\u3001\u8fd0\u884c\u3001\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u4f18\u5316\u7b49\u591a\u4e2a\u9636\u6bb5\uff0c\u5c06Fortran\u5185\u6838\u8f6c\u6362\u4e3a\u53ef\u79fb\u690d\u7684Kokkos C++\u7a0b\u5e8f\u3002", "result": "\u5de5\u4f5c\u6d41\u6210\u529f\u73b0\u4ee3\u5316\u4e86\u591a\u4e2a\u57fa\u51c6\u5185\u6838\uff0c\u751f\u6210\u7684Kokkos\u4ee3\u7801\u5728\u4e0d\u540c\u786c\u4ef6\u5206\u533a\u4e0a\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u3002\u4ed8\u8d39OpenAI\u6a21\u578b\uff08\u5982GPT-5\uff09\u4ec5\u9700\u51e0\u7f8e\u5143\u5c31\u80fd\u751f\u6210\u4f18\u5316\u4ee3\u7801\u5e76\u8d85\u8d8aFortran\u57fa\u7ebf\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u7ecf\u5e38\u5931\u8d25\u3002", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684AI\u5728Fortran\u5230Kokkos\u8f6c\u6362\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u81ea\u4e3b\u73b0\u4ee3\u5316\u4f20\u7edf\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u5c55\u793a\u4e86LLM\u9a71\u52a8\u7cfb\u7edf\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u7ed3\u6784\u5316\u63a8\u7406\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "2509.12251", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.12251", "abs": "https://arxiv.org/abs/2509.12251", "authors": ["Duong Q. Nguyen", "Quy P. Nguyen", "Nguyen Van Nhon", "Quang-Thinh Bui", "H. Nguyen-Xuan"], "title": "V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams", "comment": null, "summary": "This paper develops an autonomous agentic framework called V-Math that aims\nto assist Vietnamese high school students in preparing for the National High\nSchool Graduation Mathematics Exams (NHSGMEs). The salient framework integrates\nthree specialized AI agents: a specification-matrix-conditioned question\ngenerator, a solver/explainer for detailed step-by-step reasoning, and a\npersonalized tutor that adapts to student performance. Beyond enabling\nself-paced student practice, V-Math supports teachers by generating innovative,\ncompliant exam questions and building diverse, high-quality question banks.\nThis reduces manual workload and enriches instructional resources. We describe\nthe system architecture, focusing on practice modes for learners and\nteacher-oriented features for question generation. Preliminary evaluations\ndemonstrate that V-Math produces matrix-aligned exams with high solution\naccuracy, delivers coherent explanations, and enhances the variety of practice\nmaterials. These results highlight its potential to support scalable, equitable\nmathematics preparation aligned with national standards while also empowering\nteachers through AI-assisted exam creation.", "AI": {"tldr": "V-Math\u662f\u4e00\u4e2a\u9762\u5411\u8d8a\u5357\u9ad8\u4e2d\u6570\u5b66\u8003\u8bd5\u7684\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\uff0c\u96c6\u6210\u4e09\u4e2aAI\u4ee3\u7406\uff1a\u9898\u76ee\u751f\u6210\u5668\u3001\u89e3\u9898\u89e3\u91ca\u5668\u548c\u4e2a\u6027\u5316\u5bfc\u5e08\uff0c\u652f\u6301\u5b66\u751f\u81ea\u4e3b\u7ec3\u4e60\u548c\u6559\u5e08\u751f\u6210\u6807\u51c6\u5316\u8bd5\u9898\u3002", "motivation": "\u5e2e\u52a9\u8d8a\u5357\u9ad8\u4e2d\u751f\u51c6\u5907\u56fd\u5bb6\u9ad8\u4e2d\u6570\u5b66\u6bd5\u4e1a\u8003\u8bd5\uff0c\u51cf\u8f7b\u6559\u5e08\u624b\u52a8\u51fa\u9898\u8d1f\u62c5\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u4e14\u591a\u6837\u5316\u7684\u7ec3\u4e60\u6750\u6599\u3002", "method": "\u5f00\u53d1\u5305\u542b\u4e09\u4e2a\u4e13\u95e8AI\u4ee3\u7406\u7684\u6846\u67b6\uff1a\u57fa\u4e8e\u89c4\u8303\u77e9\u9635\u7684\u9898\u76ee\u751f\u6210\u5668\u3001\u63d0\u4f9b\u8be6\u7ec6\u6b65\u9aa4\u63a8\u7406\u7684\u89e3\u9898\u5668/\u89e3\u91ca\u5668\u3001\u4ee5\u53ca\u6839\u636e\u5b66\u751f\u8868\u73b0\u81ea\u9002\u5e94\u7684\u4e2a\u6027\u5316\u5bfc\u5e08\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793aV-Math\u80fd\u751f\u6210\u7b26\u5408\u77e9\u9635\u6807\u51c6\u7684\u8003\u8bd5\u9898\u76ee\uff0c\u5177\u6709\u9ad8\u89e3\u9898\u51c6\u786e\u7387\uff0c\u63d0\u4f9b\u8fde\u8d2f\u7684\u89e3\u91ca\uff0c\u5e76\u4e30\u5bcc\u4e86\u7ec3\u4e60\u6750\u6599\u7684\u591a\u6837\u6027\u3002", "conclusion": "V-Math\u6709\u6f5c\u529b\u652f\u6301\u7b26\u5408\u56fd\u5bb6\u6807\u51c6\u7684\u53ef\u6269\u5c55\u3001\u516c\u5e73\u7684\u6570\u5b66\u5907\u8003\uff0c\u540c\u65f6\u901a\u8fc7AI\u8f85\u52a9\u7684\u8003\u8bd5\u521b\u5efa\u8d4b\u80fd\u6559\u5e08\u3002", "topic": "swe application"}}
{"id": "2509.12491", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12491", "abs": "https://arxiv.org/abs/2509.12491", "authors": ["Veronica Pimenova", "Sarah Fakhoury", "Christian Bird", "Margaret-Anne Storey", "Madeline Endres"], "title": "Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding", "comment": "19 pages, 2 figures", "summary": "Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly\nbecome a compelling and controversial natural language programming paradigm in\nAI-assisted software development. Centered on iterative co-design with an AI\nassistant, vibe coding emphasizes flow and experimentation over strict upfront\nspecification. While initial studies have begun to explore this paradigm, most\nfocus on analyzing code artifacts or proposing theories with limited empirical\nbacking. There remains a need for a grounded understanding of vibe coding as it\nis perceived and experienced by developers. We present the first systematic\nqualitative investigation of vibe coding perceptions and practice. Drawing on\nover 190,000 words from semi-structured interviews, Reddit threads, and\nLinkedIn posts, we characterize what vibe coding is, why and how developers use\nit, where it breaks down, and which emerging practices aim to support it. We\npropose a qualitatively grounded theory of vibe coding centered on\nconversational interaction with AI, co-creation, and developer flow and joy. We\nfind that AI trust regulates movement along a continuum from delegation to\nco-creation and supports the developer experience by sustaining flow. We\nsurface recurring pain points and risks in areas including specification,\nreliability, debugging, latency, code review burden, and collaboration. We also\npresent best practices that have been discovered and shared to mitigate these\nchallenges. We conclude with implications for the future of AI dev tools and\ndirections for researchers investigating vibe coding.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9vibe coding\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5b9a\u6027\u7814\u7a76\uff0c\u901a\u8fc7\u5206\u6790\u8bbf\u8c08\u548c\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u8fd9\u79cdAI\u8f85\u52a9\u7f16\u7a0b\u8303\u5f0f\u7684\u672c\u8d28\u3001\u4f7f\u7528\u52a8\u673a\u3001\u5b9e\u8df5\u65b9\u5f0f\u3001\u75db\u70b9\u53ca\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "vibe coding\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u8303\u5f0f\uff0c\u867d\u7136\u5df2\u6709\u521d\u6b65\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5f00\u53d1\u8005\u5b9e\u9645\u4f53\u9a8c\u548c\u8ba4\u77e5\u7684\u5b9e\u8bc1\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b9a\u6027\u8c03\u67e5\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u8d85\u8fc719\u4e07\u5b57\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3001Reddit\u8ba8\u8bba\u548cLinkedIn\u5e16\u5b50\u5185\u5bb9\uff0c\u6784\u5efa\u57fa\u4e8e\u5b9e\u8bc1\u7684vibe coding\u7406\u8bba\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4ee5\u5bf9\u8bdd\u5f0fAI\u4ea4\u4e92\u3001\u5171\u540c\u521b\u9020\u548c\u5f00\u53d1\u8005\u5fc3\u6d41\u4f53\u9a8c\u4e3a\u6838\u5fc3\u7684vibe coding\u7406\u8bba\uff1b\u53d1\u73b0AI\u4fe1\u4efb\u5728\u59d4\u6258\u5230\u5171\u540c\u521b\u9020\u7684\u8fde\u7eed\u4f53\u4e2d\u8d77\u8c03\u8282\u4f5c\u7528\uff1b\u8bc6\u522b\u4e86\u89c4\u8303\u3001\u53ef\u9760\u6027\u3001\u8c03\u8bd5\u7b49\u65b9\u9762\u7684\u75db\u70b9\u548c\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5f00\u53d1\u5de5\u5177\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u4e3avibe coding\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u8005\u4f53\u9a8c\u548c\u5fc3\u6d41\u7ef4\u6301\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.12382", "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.12382", "abs": "https://arxiv.org/abs/2509.12382", "authors": ["Anu Pradhan", "Alexandra Ortan", "Apurv Verma", "Madhavan Seshadri"], "title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "comment": "Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models at RecSys 2025", "summary": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u6765\u8bc4\u4f30\u6cd5\u5f8b\u9886\u57df\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5728AI\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u63d0\u51fa\u4e86\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u6307\u6807\u548c\u7edf\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5174\u8d77\uff0c\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u6307\u6807\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u6cd5\u5f8b\u7814\u7a76\uff09\u4e2d\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u7684\u8d28\u91cf\u7ef4\u5ea6\uff0c\u9700\u8981\u63a2\u7d22LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u7814\u7a76\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u54ea\u79cd\u8bc4\u4f30\u8005\u95f4\u53ef\u9760\u6027\u6307\u6807\u6700\u80fd\u6355\u6349LLM\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u7edf\u8ba1\u4e0a\u53ef\u9760\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u4f20\u7edf\u534f\u8bae\u6307\u6807\u5982Krippendorff's alpha\u5728AI\u7cfb\u7edf\u8bc4\u4f30\u7684\u504f\u659c\u5206\u5e03\u4e2d\u5177\u6709\u8bef\u5bfc\u6027\uff0cGwet's AC2\u548c\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u66f4\u9002\u5408\u8bc4\u4f30\u8005\u9009\u62e9\uff0cWilcoxon Signed-Rank Test\u4e0eBenjamini-Hochberg\u6821\u6b63\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7cfb\u7edf\u6bd4\u8f83\u7edf\u8ba1\u4e25\u8c28\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6cd5\u5f8b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u4eba\u529b\u5bc6\u96c6\u578b\u74f6\u9888\u8f6c\u53d8\u4e3a\u81ea\u52a8\u5316\u4e14\u7edf\u8ba1\u539f\u5219\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2509.12282", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12282", "abs": "https://arxiv.org/abs/2509.12282", "authors": ["Sasi Kiran Gaddipati", "Farhana Keya", "Gollam Rabby", "S\u00f6ren Auer"], "title": "AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning", "comment": null, "summary": "Advances in AI-assisted research have introduced powerful tools for\nliterature retrieval, hypothesis generation, experimentation, and manuscript\npreparation. However, systems remain fragmented and lack human-centred\nworkflows. To address these gaps, we introduce AIssistant, an agentic,\nopen-source Human-AI collaborative framework designed to simplify the\nend-to-end creation of scientific workflows. Since our development is still in\nan early stage, we present here the first experiments with AIssistant for\nperspective and review research papers in machine learning. Our system\nintegrates modular tools and agents for literature synthesis, section-wise\nexperimentation, citation management, and automatic LaTeX paper text\ngeneration, while maintaining human oversight at every stage to ensure\naccuracy, coherence, and scholarly rigour. We conducted a comprehensive\nevaluation across three layers: (1) Independent Human Review, following NeurIPS\ndouble-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable\nhuman review proxy; and (3) Program Chair Oversight, where the chair monitors\nthe entire review process and makes final validation and acceptance decisions.\nThe results demonstrate that AIssistant improves drafting efficiency and\nthematic consistency. Nonetheless, Human-AI collaboration remains essential for\nmaintaining factual correctness, methodological soundness, and ethical\ncompliance. Despite its effectiveness, we identify key limitations, including\nhallucinated citations, difficulty adapting to dynamic paper structures, and\nincomplete integration of multimodal content.", "AI": {"tldr": "AIssistant\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4eba\u7c7b-AI\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u7aef\u5230\u7aef\u521b\u5efa\uff0c\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7efc\u8ff0\u548c\u89c6\u89d2\u8bba\u6587\u5199\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u8d77\u8349\u6548\u7387\u548c\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4f46\u4ecd\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u6765\u786e\u4fdd\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dAI\u8f85\u52a9\u7814\u7a76\u5de5\u5177\u867d\u7136\u5f3a\u5927\u4f46\u5206\u6563\uff0c\u7f3a\u4e4f\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u96c6\u6210\u5316\u7684\u534f\u4f5c\u6846\u67b6\u6765\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u521b\u5efa\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1AIssistant\u6846\u67b6\uff0c\u96c6\u6210\u6a21\u5757\u5316\u5de5\u5177\u548c\u667a\u80fd\u4f53\u8fdb\u884c\u6587\u732e\u7efc\u5408\u3001\u5206\u6bb5\u5b9e\u9a8c\u3001\u5f15\u7528\u7ba1\u7406\u548c\u81ea\u52a8LaTeX\u8bba\u6587\u751f\u6210\uff0c\u540c\u65f6\u5728\u6bcf\u4e2a\u9636\u6bb5\u4fdd\u6301\u4eba\u7c7b\u76d1\u7763\u3002\u91c7\u7528\u4e09\u5c42\u8bc4\u4f30\uff1a\u72ec\u7acb\u4eba\u7c7b\u8bc4\u5ba1\u3001\u81ea\u52a8\u5316LLM\u8bc4\u5ba1\u548c\u7a0b\u5e8f\u4e3b\u5e2d\u76d1\u7763\u3002", "result": "AIssistant\u63d0\u9ad8\u4e86\u8d77\u8349\u6548\u7387\u548c\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4f46\u5b58\u5728\u5f15\u7528\u5e7b\u89c9\u3001\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u8bba\u6587\u7ed3\u6784\u4ee5\u53ca\u591a\u6a21\u6001\u5185\u5bb9\u6574\u5408\u4e0d\u5b8c\u5168\u7b49\u5c40\u9650\u6027\u3002\u4eba\u7c7b-AI\u534f\u4f5c\u5bf9\u4e8e\u4fdd\u6301\u4e8b\u5b9e\u6b63\u786e\u6027\u3001\u65b9\u6cd5\u5408\u7406\u6027\u548c\u4f26\u7406\u5408\u89c4\u6027\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "AIssistant\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u4eba\u7c7b-AI\u534f\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u521b\u5efa\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u51b3\u5f15\u7528\u51c6\u786e\u6027\u548c\u7ed3\u6784\u9002\u5e94\u6027\u7b49\u6311\u6218\u3002", "topic": "swe application"}}
{"id": "2509.12973", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12973", "abs": "https://arxiv.org/abs/2509.12973", "authors": ["Aamer Aljagthami", "Mohammed Banabila", "Musab Alshehri", "Mohammed Kabini", "Mohammad D. Alahmadi"], "title": "Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design", "comment": null, "summary": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728C++\u3001Java\u3001Python\u548cC#\u4e4b\u95f4\u7684\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\uff0c\u53d1\u73b0\u8be6\u7ec6\u63d0\u793a\u548c\u82f1\u8bed\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6240\u6709LLM\u5747\u4f18\u4e8e\u4f20\u7edfTransCoder\u57fa\u7ebf\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540cLLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u63d0\u793a\u8bbe\u8ba1\u548c\u63d0\u793a\u8bed\u8a00\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4e3a\u8f6f\u4ef6\u73b0\u4ee3\u5316\u548c\u8de8\u8bed\u8a00\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u4f7f\u7528BLEU\u548cCodeBLEU\u6307\u6807\uff0c\u5728C++\u3001Java\u3001Python\u3001C#\u56db\u79cd\u8bed\u8a00\u95f4\u8fdb\u884c\u53cc\u5411\u7ffb\u8bd1\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e24\u79cd\u63d0\u793a\u98ce\u683c\uff08\u7b80\u6d01\u6307\u4ee4\u548c\u8be6\u7ec6\u89c4\u8303\uff09\u548c\u4e24\u79cd\u63d0\u793a\u8bed\u8a00\uff08\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\uff09\u3002", "result": "\u8be6\u7ec6\u63d0\u793a\u5728\u6240\u6709\u6a21\u578b\u548c\u7ffb\u8bd1\u65b9\u5411\u4e0a\u5747\u5e26\u6765\u4e00\u81f4\u63d0\u5347\uff0c\u82f1\u8bed\u63d0\u793a\u6bd4\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\u6027\u80fd\u9ad813-15%\uff0c\u6700\u4f73\u6a21\u578b\u5728Java\u5230C#\u548cPython\u5230C++\u7b49\u6311\u6218\u6027\u8bed\u8a00\u5bf9\u4e0a\u83b7\u5f97\u6700\u9ad8CodeBLEU\u5206\u6570\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u63d0\u793a\u8bed\u8a00\u9009\u62e9\u5bf9\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0cLLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2509.12423", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12423", "abs": "https://arxiv.org/abs/2509.12423", "authors": ["Danielle Cohen", "Yoni Halpern", "Noam Kahlon", "Joel Oren", "Omri Berkovitch", "Sapir Caduri", "Ido Dagan", "Anatoly Efros"], "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition", "comment": null, "summary": "Understanding user intents from UI interaction trajectories remains a\nchallenging, yet crucial, frontier in intelligent agent development. While\nmassive, datacenter-based, multi-modal large language models (MLLMs) possess\ngreater capacity to handle the complexities of such sequences, smaller models\nwhich can run on-device to provide a privacy-preserving, low-cost, and\nlow-latency user experience, struggle with accurate intent inference. We\naddress these limitations by introducing a novel decomposed approach: first, we\nperform structured interaction summarization, capturing key information from\neach user action. Second, we perform intent extraction using a fine-tuned model\noperating on the aggregated summaries. This method improves intent\nunderstanding in resource-constrained models, even surpassing the base\nperformance of large MLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u89e3\u65b9\u6cd5\uff0c\u5148\u8fdb\u884c\u7ed3\u6784\u5316\u4ea4\u4e92\u6458\u8981\uff0c\u518d\u7528\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u610f\u56fe\u63d0\u53d6\uff0c\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u5728UI\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b", "motivation": "\u89e3\u51b3\u5c0f\u578b\u8bbe\u5907\u7aef\u6a21\u578b\u5728UI\u4ea4\u4e92\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u56f0\u96be\uff0c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u3001\u4f4e\u6210\u672c\u548c\u4f4e\u5ef6\u8fdf\u7684\u7528\u6237\u4f53\u9a8c", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u7ed3\u6784\u5316\u4ea4\u4e92\u6458\u8981\u6355\u83b7\u6bcf\u4e2a\u7528\u6237\u52a8\u4f5c\u7684\u5173\u952e\u4fe1\u606f\uff1b2) \u4f7f\u7528\u5fae\u8c03\u6a21\u578b\u5bf9\u805a\u5408\u6458\u8981\u8fdb\u884c\u610f\u56fe\u63d0\u53d6", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u6027\u80fd", "conclusion": "\u5206\u89e3\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u5728UI\u4ea4\u4e92\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e3a\u8bbe\u5907\u7aef\u667a\u80fd\u4ee3\u7406\u53d1\u5c55\u63d0\u4f9b\u53ef\u884c\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2509.13023", "categories": ["cs.SE", "cs.AI", "I.2.2;D.2.5;D.2.4;D.4.6"], "pdf": "https://arxiv.org/pdf/2509.13023", "abs": "https://arxiv.org/abs/2509.13023", "authors": ["\u015etefan-Claudiu Susan", "Andrei Arusoaie", "Dorel Lucanu"], "title": "Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Slither\u68c0\u6d4b\u5668\u3001LLMs\u3001Kontrol\u548cForge\u7684\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7ba1\u9053\uff0c\u80fd\u53ef\u9760\u68c0\u6d4b\u7f3a\u9677\u5e76\u751f\u6210\u8bc1\u660e\uff0c\u6709\u6548\u51cf\u5c11\u8bef\u62a5\u548c\u4eba\u5de5\u9a8c\u8bc1\u8d1f\u62c5\u3002", "motivation": "\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Solidity\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\uff0c\u9700\u8981\u80fd\u591f\u6b63\u5f0f\u6216\u7ecf\u9a8c\u8bc1\u660e\u7f3a\u9677\u5b58\u5728\u7684\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u5b9a\u5236Slither\u68c0\u6d4b\u5668\u3001LLMs\u3001Kontrol\u548cForge\u7684\u68c0\u6d4b\u7ba1\u9053\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u6216\u5177\u4f53\u6267\u884c\u6765\u6b63\u786e\u5206\u7c7b\u4ee3\u7801\u6545\u969c\u3002", "result": "\u5bf9\u4e03\u79cd\u5173\u952e\u7f3a\u9677\u7c7b\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u5728\u91cd\u5165\u3001\u590d\u6742\u56de\u8c03\u548c\u9519\u8bef\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7b49\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5c06\u542f\u53d1\u5f0f\u5206\u6790\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u5f3a\u5927\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u81ea\u52a8\u5316\u7684\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u3002", "topic": "swe application"}}
{"id": "2509.12476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12476", "abs": "https://arxiv.org/abs/2509.12476", "authors": ["Sumanta Bhattacharyya", "Sara Riaz", "Pedram Rooshenas"], "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "comment": null, "summary": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond.", "AI": {"tldr": "R2tA\u65b9\u6cd5\u901a\u8fc7\u63d0\u70bcLLM\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u8bad\u7ec3\u4efb\u52a1\u7279\u5b9a\u7684\u5c0f\u578b\u63a8\u7406\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u6807\u7b7e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5728\u6570\u636e\u5e93EERD\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u76f4\u63a5\u4eba\u5de5\u76d1\u7763\u6216\u9ad8\u8d28\u91cf\u6807\u7b7e\u7a00\u7f3a\u65f6\uff0c\u8bad\u7ec3\u4efb\u52a1\u7279\u5b9a\u7684\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002LLM\u4ea7\u751f\u7684\u4e30\u5bcc\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u53ef\u4ee5\u88ab\u7cfb\u7edf\u63d0\u70bc\uff0c\u4e3a\u8bad\u7ec3\u63d0\u4f9b\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51faReason-Refine-then-Align (R2tA)\u65b9\u6cd5\uff1a1) \u4ece\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u751f\u6210\u521d\u59cb\u63a8\u7406\u548c\u54cd\u5e94\uff1b2) \u63d0\u70bc\u8fd9\u4e9b\u8f68\u8ff9\uff0c\u4fee\u590d\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5f62\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b3) \u8fdb\u884c\u4e24\u9636\u6bb5\u5bf9\u9f50\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\uff0c\u6821\u51c6\u6a21\u578b\u7684\u4e2d\u95f4\u63a8\u7406\u3002", "result": "\u5728600\u4e2aEERD\u53d8\u4f53\u6570\u636e\u96c6\uff08450\u8bad\u7ec3/150\u6d4b\u8bd5\uff09\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d611\u4e2a\u9519\u8bef\u7c7b\u522b\u3002R2tA\u5728\u7ed3\u6784\u590d\u6742\u7684\u6570\u636e\u5e93\u7cfb\u7edf\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u4ec5\u63d0\u793a\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u68c0\u6d4b\u9519\u8bef\u800c\u4e0d\u4ea7\u751f\u5e7b\u89c9\u3002", "conclusion": "R2tA\u4e3a\u6570\u636e\u7a00\u7f3a\u9886\u57df\u7684\u53ef\u6269\u5c55LLM\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u4f7f\u6559\u80b2\u548c\u5176\u5b83\u9886\u57df\u7684\u53ef\u91cd\u73b0AI\u5de5\u5177\u6210\u4e3a\u53ef\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.13055", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13055", "abs": "https://arxiv.org/abs/2509.13055", "authors": ["Youngkyoung Kim", "Sanghyeok Park", "Misoo Kim", "Gangho Yoon", "Eunseok Lee", "Simon S. Woo"], "title": "Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs", "comment": null, "summary": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development.", "AI": {"tldr": "\u63d0\u51faProgressive Knowledge Enhancement (PKE)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u9010\u6b65\u63d0\u53d6LLM\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u534a\u5bfc\u4f53\u8bbe\u5907\u8bed\u8a00ALPG\u7684\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387", "motivation": "\u534a\u5bfc\u4f53\u8bbe\u5907\u7f16\u7a0b\u8bed\u8a00\u5982ALPG\u5177\u6709\u4f4e\u5c42\u8bed\u6cd5\u548c\u9661\u5ced\u5b66\u4e60\u66f2\u7ebf\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u8fd9\u7c7b\u4e13\u4e1a\u4f4e\u7ea7\u8bed\u8a00\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4ee3\u7801\u751f\u6210\u80fd\u529b", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u589e\u5f3a(PKE)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u7b56\u7565\uff0c\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u793a\u4f8b\u9010\u6b65\u6fc0\u6d3bLLM\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u65e0\u9700\u5927\u91cf\u5fae\u8c03", "result": "\u5728\u5de5\u4e1aALPG\u6570\u636e\u96c6\u4e0a\uff0cPKE\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\uff0c\u6bd4\u6b21\u4f18\u6280\u672f\u9ad8\u51fa11.1%\u548c15.2%\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\uff0c\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u63d0\u53d6\u6709\u6548\u63d0\u5347\u51c6\u786e\u6027", "conclusion": "PKE\u4e3a\u63d0\u5347LLM\u5728\u4e13\u4e1a\u4f4e\u7ea7\u7f16\u7a0b\u8bed\u8a00\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u652f\u6301\u534a\u5bfc\u4f53\u8f6f\u4ef6\u5f00\u53d1\u751f\u4ea7\u529b\u7684\u63d0\u5347", "topic": "code agent"}}
{"id": "2509.12235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12235", "abs": "https://arxiv.org/abs/2509.12235", "authors": ["Hangzhan Jin", "Sitao Luan", "Sicheng Lyu", "Guillaume Rabusseau", "Reihaneh Rabbany", "Doina Precup", "Mohammad Hamdaqa"], "title": "RL Fine-Tuning Heals OOD Forgetting in SFT", "comment": "10 pages, 15 figures", "summary": "The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed\nby Reinforcement Learning (RL) has empirically shown better reasoning\nperformance than one-stage SFT for the post-training of Large Language Models\n(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL\nare still under-explored and inconclusive. In our study, we find the well-known\nclaim \"SFT memorizes, RL generalizes\" is over-simplified, and discover that:\n(1) OOD performance peaks at the early stage of SFT and then declines (OOD\nforgetting), the best SFT checkpoint cannot be captured by training/test loss;\n(2) the subsequent RL stage does not generate fundamentally better OOD\ncapability, instead it plays an \\textbf{OOD restoration} role, recovering the\nlost reasoning ability during SFT; (3) The recovery ability has boundaries,\n\\ie{} \\textbf{if SFT trains for too short or too long, RL cannot recover the\nlost OOD ability;} (4) To uncover the underlying mechanisms behind the\nforgetting and restoration process, we employ SVD analysis on parameter\nmatrices, manually edit them, and observe their impacts on model performance.\nUnlike the common belief that the shift of model capacity mainly results from\nthe changes of singular values, we find that they are actually quite stable\nthroughout fine-tuning. Instead, the OOD behavior strongly correlates with the\n\\textbf{rotation of singular vectors}. Our findings re-identify the roles of\nSFT and RL in the two-stage fine-tuning and discover the rotation of singular\nvectors as the key mechanism. %reversing the rotations induced by SFT, which\nshows recovery from forgetting, whereas imposing the SFT parameter directions\nonto a RL-tuned model results in performance degradation. Code is available at\nhttps://github.com/xiaodanguoguo/RL_Heals_SFT", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0SFT+RL\u4e24\u9636\u6bb5\u5fae\u8c03\u4e2d\uff0cSFT\u65e9\u671fOOD\u6027\u80fd\u6700\u4f73\u4f46\u968f\u540e\u4e0b\u964d\uff0cRL\u4e3b\u8981\u8d77\u6062\u590d\u4f5c\u7528\u800c\u975e\u521b\u9020\u65b0\u80fd\u529b\uff0c\u5947\u5f02\u5411\u91cf\u65cb\u8f6c\u662f\u6027\u80fd\u53d8\u5316\u7684\u5173\u952e\u673a\u5236", "motivation": "\u63a2\u7d22SFT\u548cRL\u5728\u4e24\u9636\u6bb5\u5fae\u8c03\u4e2d\u7684\u534f\u540c\u673a\u5236\uff0c\u6311\u6218\"SFT\u8bb0\u5fc6\u3001RL\u6cdb\u5316\"\u7684\u7b80\u5355\u8bf4\u6cd5", "method": "\u4f7f\u7528SVD\u5206\u6790\u53c2\u6570\u77e9\u9635\uff0c\u624b\u52a8\u7f16\u8f91\u53c2\u6570\u5e76\u89c2\u5bdf\u6027\u80fd\u5f71\u54cd\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u7684OOD\u8868\u73b0", "result": "\u53d1\u73b0OOD\u6027\u80fd\u5728SFT\u65e9\u671f\u8fbe\u5230\u5cf0\u503c\u540e\u4e0b\u964d\uff0cRL\u4e3b\u8981\u6062\u590dSFT\u4e22\u5931\u7684\u80fd\u529b\u800c\u975e\u521b\u9020\u65b0\u80fd\u529b\uff0c\u5947\u5f02\u5411\u91cf\u65cb\u8f6c\u800c\u975e\u5947\u5f02\u503c\u53d8\u5316\u662f\u4e3b\u8981\u673a\u5236", "conclusion": "\u91cd\u65b0\u5b9a\u4e49\u4e86SFT\u548cRL\u5728\u4e24\u9636\u6bb5\u5fae\u8c03\u4e2d\u7684\u89d2\u8272\uff0c\u53d1\u73b0\u5947\u5f02\u5411\u91cf\u65cb\u8f6c\u662f\u5173\u952e\u673a\u5236\uff0cRL\u7684\u6062\u590d\u80fd\u529b\u6709\u9650\u5236", "topic": "agentic reinforcement learning"}}
{"id": "2509.12464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12464", "abs": "https://arxiv.org/abs/2509.12464", "authors": ["Ryan Lucas", "Kayhan Behdin", "Zhipeng Wang", "Qingquan Song", "Shao Tang", "Rahul Mazumder"], "title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction", "comment": null, "summary": "Reasoning language models such as DeepSeek-R1 produce long chain-of-thought\ntraces during inference time which make them costly to deploy at scale. We show\nthat using compression techniques such as neural network pruning produces\ngreater performance loss than in typical language modeling tasks, and in some\ncases can make the model slower since they cause the model to produce more\nthinking tokens but with worse performance. We show that this is partly due to\nthe fact that standard LLM pruning methods often focus on input reconstruction,\nwhereas reasoning is a decode-dominated task. We introduce a simple, drop-in\nfix: during pruning we jointly reconstruct activations from the input and the\nmodel's on-policy chain-of-thought traces. This \"Reasoning-Aware Compression\"\n(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,\nand boosts their performance significantly. Code reproducing the results in the\npaper can be found at: https://github.com/RyanLucas3/RAC", "AI": {"tldr": "\u63d0\u51faReasoning-Aware Compression (RAC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u526a\u679d\u8fc7\u7a0b\u4e2d\u540c\u65f6\u91cd\u6784\u8f93\u5165\u548c\u601d\u7ef4\u94fe\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6027\u80fd", "motivation": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4ea7\u751f\u957f\u601d\u7ef4\u94fe\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u6027\u80fd\u635f\u5931\u8f83\u5927\u4e14\u53ef\u80fd\u4f7f\u6a21\u578b\u66f4\u6162", "method": "\u5728\u526a\u679d\u8fc7\u7a0b\u4e2d\u8054\u5408\u91cd\u6784\u8f93\u5165\u548c\u6a21\u578b\u7b56\u7565\u5185\u601d\u7ef4\u94fe\u7684\u6fc0\u6d3b\uff0c\u4e0e\u73b0\u6709\u526a\u679d\u5de5\u4f5c\u6d41\uff08\u5982SparseGPT\uff09\u65e0\u7f1d\u96c6\u6210", "result": "RAC\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u635f\u5931\u95ee\u9898", "conclusion": "RAC\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684drop-in\u4fee\u590d\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u679c", "topic": "agent analysis"}}
{"id": "2509.12603", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12603", "abs": "https://arxiv.org/abs/2509.12603", "authors": ["Mukai Li", "Linfeng Song", "Zhenwen Liang", "Jiahao Xu", "Shansan Gong", "Qi Liu", "Haitao Mi", "Dong Yu"], "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving", "comment": null, "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEconProver\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001CoT\u5207\u6362\u673a\u5236\u548c\u591a\u6837\u5316\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4ec5\u752812%\u7684\u8ba1\u7b97\u5f00\u9500\u5c31\u80fd\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2d\u91c7\u7528\u53cd\u5c04\u5f0fCoT\u63a8\u7406\u548c\u589e\u52a0\u91c7\u6837\u6b21\u6570\u7b49\u7b56\u7565\uff0c\u867d\u7136\u6027\u80fd\u63d0\u5347\u4f46\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u4e14\u73b0\u6709\u6210\u672c\u5206\u6790\u5ffd\u7565\u4e86\u4e0d\u540c\u6269\u5c55\u7b56\u7565\u5e26\u6765\u7684\u91c7\u6837\u6210\u672c\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001CoT\u5207\u6362\u673a\u5236\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684token\u6d88\u8017\uff1b2\uff09\u591a\u6837\u5316\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u8bad\u7ec3\u524d\u7f00\uff0c\u5728\u53d7\u9650\u91c7\u6837\u6b21\u6570\u4e0b\u63d0\u9ad8\u901a\u8fc7\u7387\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u7edf\u4e00\u7684EconRL\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728miniF2F\u548cProofNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEconProver\u4ec5\u7528\u57fa\u7ebf\u65b9\u6cd512%\u7684\u8ba1\u7b97\u6210\u672c\u5c31\u8fbe\u5230\u4e86\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12249", "abs": "https://arxiv.org/abs/2509.12249", "authors": ["Jiacan Yu", "Siyi Chen", "Mingrui Liu", "Nono Horiuchi", "Vladimir Braverman", "Zicheng Xu", "Dan Haramati", "Randall Balestriero"], "title": "Why and How Auxiliary Tasks Improve JEPA Representations", "comment": null, "summary": "Joint-Embedding Predictive Architecture (JEPA) is increasingly used for\nvisual representation learning and as a component in model-based RL, but its\nbehavior remains poorly understood. We provide a theoretical characterization\nof a simple, practical JEPA variant that has an auxiliary regression head\ntrained jointly with latent dynamics. We prove a No Unhealthy Representation\nCollapse theorem: in deterministic MDPs, if training drives both the\nlatent-transition consistency loss and the auxiliary regression loss to zero,\nthen any pair of non-equivalent observations, i.e., those that do not have the\nsame transition dynamics or auxiliary label, must map to distinct latent\nrepresentations. Thus, the auxiliary task anchors which distinctions the\nrepresentation must preserve. Controlled ablations in a counting environment\ncorroborate the theory and show that training the JEPA model jointly with the\nauxiliary head generates a richer representation than training them separately.\nOur work indicates a path to improve JEPA encoders: training them with an\nauxiliary function that, together with the transition dynamics, encodes the\nright equivalence relations.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5e26\u6709\u8f85\u52a9\u56de\u5f52\u5934\u7684JEPA\u53d8\u4f53\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5728\u786e\u5b9a\u6027MDP\u4e2d\uff0c\u5f53\u8bad\u7ec3\u4f7f\u6f5c\u5728\u8f6c\u79fb\u4e00\u81f4\u6027\u635f\u5931\u548c\u8f85\u52a9\u56de\u5f52\u635f\u5931\u8d8b\u8fd1\u4e8e\u96f6\u65f6\uff0c\u975e\u7b49\u4ef7\u89c2\u6d4b\u5fc5\u987b\u6620\u5c04\u5230\u4e0d\u540c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u8f85\u52a9\u4efb\u52a1\u951a\u5b9a\u4e86\u8868\u793a\u5fc5\u987b\u4fdd\u7559\u7684\u533a\u5206\u6027\u3002", "motivation": "Joint-Embedding Predictive Architecture (JEPA) \u5728\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u884c\u4e3a\u673a\u5236\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u6765\u6307\u5bfc\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u7684\u5b9e\u7528JEPA\u53d8\u4f53\uff0c\u5305\u542b\u4e0e\u6f5c\u5728\u52a8\u529b\u5b66\u8054\u5408\u8bad\u7ec3\u7684\u8f85\u52a9\u56de\u5f52\u5934\uff0c\u5728\u786e\u5b9a\u6027MDP\u4e2d\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u8ba1\u6570\u73af\u5883\u4e2d\u7684\u63a7\u5236\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\"\u65e0\u75c5\u6001\u8868\u793a\u574d\u7f29\"\u5b9a\u7406\uff1a\u5728\u8bad\u7ec3\u635f\u5931\u8d8b\u96f6\u7684\u6761\u4ef6\u4e0b\uff0c\u975e\u7b49\u4ef7\u89c2\u6d4b\u5fc5\u987b\u6620\u5c04\u5230\u4e0d\u540c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u8f85\u52a9\u4efb\u52a1\u51b3\u5b9a\u4e86\u8868\u793a\u9700\u8981\u4fdd\u6301\u7684\u533a\u5206\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8054\u5408\u8bad\u7ec3\u6bd4\u5355\u72ec\u8bad\u7ec3\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u3002", "conclusion": "\u901a\u8fc7\u8f85\u52a9\u51fd\u6570\u4e0e\u8f6c\u79fb\u52a8\u529b\u5b66\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u7f16\u7801\u6b63\u786e\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u8fd9\u4e3a\u63d0\u9ad8JEPA\u7f16\u7801\u5668\u6027\u80fd\u6307\u660e\u4e86\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12652", "abs": "https://arxiv.org/abs/2509.12652", "authors": ["Paul Kr\u00f6ger", "Emilio Barkett"], "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4bLLM\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u8f93\u51fa\u5728\u76f8\u5173\u4e3b\u9898\u63d0\u793a\u4e0b\u7684\u5206\u5e03\u53d8\u5316\u6765\u8bc6\u522b\u6f5c\u5728\u7684\u610f\u8bc6\u5f62\u6001\u64cd\u63a7\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2\u7cfb\u7edf\u5ba1\u8ba1\u3002", "motivation": "\u968f\u7740LLM\u5728\u6570\u767e\u4e07\u7528\u6237\u4ea7\u54c1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8f93\u51fa\u53ef\u80fd\u5f71\u54cd\u4e2a\u4eba\u4fe1\u5ff5\u548c\u516c\u4f17\u8206\u8bba\u3002\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u68c0\u6d4bLLM\u662f\u5426\u88ab\u6709\u610f\u5f15\u5bfc\u5411\u7279\u5b9a\u610f\u8bc6\u5f62\u6001\u7acb\u573a\uff0c\u4ee5\u9632\u6b62\u5bf9\u516c\u5171\u8bdd\u8bed\u7684\u8fc7\u5ea6\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5148\u524d\u63d0\u51fa\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u9002\u5e94\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u5ba1\u8ba1\u7684\u65b0\u80cc\u666f\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u4e0d\u9700\u8981\u8bbf\u95ee\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u6790\u4e3b\u9898\u76f8\u5173\u63d0\u793a\u4e0b\u6a21\u578b\u8f93\u51fa\u7684\u5206\u5e03\u53d8\u5316\u6765\u8bc6\u522b\u6f5c\u5728\u610f\u8bc6\u5f62\u6001\u64cd\u63a7\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u652f\u6301\u5bf9LLM\u884c\u4e3a\u7684\u72ec\u7acb\u4e8b\u540e\u5ba1\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4bLLM\u610f\u8bc6\u5f62\u6001\u64cd\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u7279\u522b\u9002\u5408\u5bf9\u4e13\u6709\u9ed1\u76d2\u7cfb\u7edf\u8fdb\u884c\u5ba1\u8ba1\uff0c\u6709\u52a9\u4e8e\u7ef4\u62a4\u516c\u5171\u8bdd\u8bed\u7684\u516c\u5e73\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.12589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12589", "abs": "https://arxiv.org/abs/2509.12589", "authors": ["Garima Agrawal", "Riccardo De Maria", "Kiran Davuluri", "Daniele Spera", "Charlie Read", "Cosimo Spera", "Jack Garrett", "Don Miller"], "title": "Redefining CX with Agentic AI: Minerva CQ Case Study", "comment": null, "summary": "Despite advances in AI for contact centers, customer experience (CX)\ncontinues to suffer from high average handling time (AHT), low first-call\nresolution, and poor customer satisfaction (CSAT). A key driver is the\ncognitive load on agents, who must navigate fragmented systems, troubleshoot\nmanually, and frequently place customers on hold. Existing AI-powered\nagent-assist tools are often reactive driven by static rules, simple prompting,\nor retrieval-augmented generation (RAG) without deeper contextual reasoning. We\nintroduce Agentic AI goal-driven, autonomous, tool-using systems that\nproactively support agents in real time. Unlike conventional approaches,\nAgentic AI identifies customer intent, triggers modular workflows, maintains\nevolving context, and adapts dynamically to conversation state. This paper\npresents a case study of Minerva CQ, a real-time Agent Assist product deployed\nin voice-based customer support. Minerva CQ integrates real-time transcription,\nintent and sentiment detection, entity recognition, contextual retrieval,\ndynamic customer profiling, and partial conversational summaries enabling\nproactive workflows and continuous context-building. Deployed in live\nproduction, Minerva CQ acts as an AI co-pilot, delivering measurable\nimprovements in agent efficiency and customer experience across multiple\ndeployments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Minerva CQ\uff0c\u4e00\u79cd\u57fa\u4e8eAgentic AI\u7684\u5b9e\u65f6\u5ba2\u670d\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u52a8\u5de5\u4f5c\u6d41\u548c\u6301\u7eed\u4e0a\u4e0b\u6587\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba2\u670d\u6548\u7387\u548c\u5ba2\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u5ba2\u670d\u7cfb\u7edf\u5b58\u5728\u5904\u7406\u65f6\u95f4\u957f\u3001\u9996\u6b21\u89e3\u51b3\u7387\u4f4e\u3001\u5ba2\u6237\u6ee1\u610f\u5ea6\u5dee\u7684\u95ee\u9898\uff0c\u73b0\u6709AI\u8f85\u52a9\u5de5\u5177\u591a\u4e3a\u88ab\u52a8\u54cd\u5e94\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528Agentic AI\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9e\u65f6\u8f6c\u5f55\u3001\u610f\u56fe\u60c5\u611f\u68c0\u6d4b\u3001\u5b9e\u4f53\u8bc6\u522b\u3001\u4e0a\u4e0b\u6587\u68c0\u7d22\u3001\u52a8\u6001\u5ba2\u6237\u753b\u50cf\u548c\u90e8\u5206\u5bf9\u8bdd\u6458\u8981\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5de5\u4f5c\u6d41\u89e6\u53d1\u548c\u52a8\u6001\u9002\u5e94\u3002", "result": "\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u7684Minerva CQ\u4f5c\u4e3aAI\u526f\u9a7e\u9a76\uff0c\u5728\u591a\u8f6e\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u5ba2\u670d\u6548\u7387\u548c\u5ba2\u6237\u4f53\u9a8c\u7684\u53ef\u8861\u91cf\u6539\u8fdb\u3002", "conclusion": "Agentic AI\u9a71\u52a8\u7684\u5b9e\u65f6\u5ba2\u670d\u52a9\u624b\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5ba2\u670d\u75db\u70b9\uff0c\u901a\u8fc7\u4e3b\u52a8\u667a\u80fd\u8f85\u52a9\u63d0\u5347\u6574\u4f53\u670d\u52a1\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2509.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12661", "abs": "https://arxiv.org/abs/2509.12661", "authors": ["Yougen Zhou", "Qin Chen", "Ningning Zhou", "Jie Zhou", "Xingjiao Wu", "Liang He"], "title": "Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations", "comment": null, "summary": "Emotional support conversation (ESC) aims to alleviate distress through\nempathetic dialogue, yet large language models (LLMs) face persistent\nchallenges in delivering effective ESC due to low accuracy in strategy\nplanning. Moreover, there is a considerable preference bias towards specific\nstrategies. Prior methods using fine-tuned strategy planners have shown\npotential in reducing such bias, while the underlying causes of the preference\nbias in LLMs have not well been studied. To address these issues, we first\nreveal the fundamental causes of the bias by identifying the knowledge\nboundaries of LLMs in strategy planning. Then, we propose an approach to\nmitigate the bias by reinforcement learning with a dual reward function, which\noptimizes strategy planning via both accuracy and entropy-based confidence for\neach region according to the knowledge boundaries. Experiments on the ESCov and\nExTES datasets with multiple LLM backbones show that our approach outperforms\nthe baselines, confirming the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53cc\u5956\u52b1\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522bLLM\u5728\u7b56\u7565\u89c4\u5212\u4e2d\u7684\u77e5\u8bc6\u8fb9\u754c\u6765\u7f13\u89e3\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u504f\u597d\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u5b58\u5728\u7b56\u7565\u89c4\u5212\u51c6\u786e\u6027\u4f4e\u548c\u7279\u5b9a\u7b56\u7565\u504f\u597d\u504f\u5dee\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6df1\u5165\u5206\u6790\u504f\u5dee\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u9996\u5148\u8bc6\u522bLLM\u5728\u7b56\u7565\u89c4\u5212\u4e2d\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53cc\u5956\u52b1\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u548c\u57fa\u4e8e\u71b5\u7684\u7f6e\u4fe1\u5ea6\u6765\u4f18\u5316\u7b56\u7565\u89c4\u5212\u3002", "result": "\u5728ESCov\u548cExTES\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86LLM\u5728\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u504f\u597d\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12612", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12612", "abs": "https://arxiv.org/abs/2509.12612", "authors": ["Daojun Chen", "Xi Wang", "Shenyuan Ren", "Qingzhi Ma", "Pengpeng Zhao", "An Liu"], "title": "GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL", "comment": null, "summary": "While Large Language Models have significantly advanced Text2SQL generation,\na critical semantic gap persists where syntactically valid queries often\nmisinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a\nnovel multi-agent framework that introduces Guided Generation with SQL2Text\nBack-translation Validation. This mechanism uses a specialized agent to\ntranslate the generated SQL back into natural language, which verifies its\nlogical alignment with the original question. Critically, our investigation\nreveals that current evaluation is undermined by a systemic issue: the poor\nquality of the benchmarks themselves. We introduce a formal typology for \"Gold\nErrors\", which are pervasive flaws in the ground-truth data, and demonstrate\nhow they obscure true model performance. On the challenging BIRD benchmark,\nGBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After\nremoving flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)\nexecution accuracy on the Spider benchmark. Our work offers both a robust\nframework for semantic validation and a critical perspective on benchmark\nintegrity, highlighting the need for more rigorous dataset curation.", "AI": {"tldr": "GBV-SQL\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7SQL2Text\u56de\u8bd1\u9a8c\u8bc1\u6765\u89e3\u51b3Text2SQL\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u9ec4\u91d1\u9519\u8bef\u95ee\u9898", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Text2SQL\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8bed\u6cd5\u6709\u6548\u7684\u67e5\u8be2\u7ecf\u5e38\u8bef\u89e3\u7528\u6237\u610f\u56fe\uff0c\u5b58\u5728\u5173\u952e\u8bed\u4e49\u9e3f\u6c9f\u3002\u6b64\u5916\uff0c\u5f53\u524d\u8bc4\u4f30\u53d7\u5230\u57fa\u51c6\u6d4b\u8bd5\u8d28\u91cf\u95ee\u9898\u7684\u4e25\u91cd\u5f71\u54cd", "method": "\u63d0\u51faGBV-SQL\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5f15\u5165\u5e26SQL2Text\u56de\u8bd1\u9a8c\u8bc1\u7684\u5f15\u5bfc\u751f\u6210\u673a\u5236\u3002\u4f7f\u7528\u4e13\u95e8\u4ee3\u7406\u5c06\u751f\u6210\u7684SQL\u7ffb\u8bd1\u56de\u81ea\u7136\u8bed\u8a00\uff0c\u9a8c\u8bc1\u5176\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002\u8fd8\u5efa\u7acb\u4e86\u9ec4\u91d1\u9519\u8bef\u7684\u6b63\u5f0f\u5206\u7c7b\u6cd5", "result": "\u5728BIRD\u57fa\u51c6\u4e0a\u8fbe\u523063.23%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u7edd\u5bf9\u63d0\u53475.8%\u3002\u5728\u53bb\u9664\u6709\u7f3a\u9677\u6837\u672c\u540e\uff0c\u5728Spider\u57fa\u51c6\u4e0a\u8fbe\u523096.5%\uff08\u5f00\u53d1\u96c6\uff09\u548c97.6%\uff08\u6d4b\u8bd5\u96c6\uff09\u7684\u6267\u884c\u51c6\u786e\u7387", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u8bed\u4e49\u9a8c\u8bc1\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u5e76\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u5b8c\u6574\u6027\u63d0\u51fa\u4e86\u6279\u5224\u6027\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u66f4\u4e25\u683c\u6570\u636e\u96c6\u7b56\u5c55\u7684\u5fc5\u8981\u6027", "topic": "code agent"}}
{"id": "2509.12886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12886", "abs": "https://arxiv.org/abs/2509.12886", "authors": ["Yubo Zhu", "Dongrui Liu", "Zecheng Lin", "Wei Tong", "Sheng Zhong", "Jing Shao"], "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations", "comment": null, "summary": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u9690\u85cf\u8868\u793a\u7684\u95ee\u9898\u96be\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u751f\u6210\u8f93\u51fatoken\u5373\u53ef\u51c6\u786e\u8bc4\u4f30\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u5728\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u4e2d\u63d0\u9ad8\u6548\u7387", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u91cd\u590d\u91c7\u6837\u3001\u8f85\u52a9\u6a21\u578b\u6216\u5fae\u8c03\u76ee\u6807\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u635f\u5bb3\u901a\u7528\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u96be\u5ea6\u4f30\u8ba1\u65b9\u6cd5", "method": "\u5c06token\u7ea7\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5b9a\u4e49\u4ef7\u503c\u51fd\u6570\u6765\u4f30\u8ba1\u7ed9\u5b9a\u9690\u85cf\u72b6\u6001\u7684\u9884\u671f\u8f93\u51fa\u8d28\u91cf\uff0c\u4ec5\u57fa\u4e8e\u521d\u59cb\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u96be\u5ea6\u4f30\u8ba1", "result": "\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96be\u5ea6\u4f30\u8ba1\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u4f7f\u7528\u66f4\u5c11\u7684\u751f\u6210token\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u96be\u5ea6\u4f30\u8ba1\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\uff0c\u53ef\u6709\u6548\u6307\u5bfc\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565", "topic": "agent analysis"}}
{"id": "2509.12810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12810", "abs": "https://arxiv.org/abs/2509.12810", "authors": ["Shicheng Ye", "Chao Yu", "Kaiqiang Ke", "Chengdong Xu", "Yinqi Wei"], "title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents", "comment": null, "summary": "Large language model (LLM)-based agents have shown strong potential in\nmulti-task scenarios, owing to their ability to transfer knowledge across\ndiverse tasks. However, existing approaches often treat prior experiences and\nknowledge as monolithic units, leading to inefficient and coarse-grained\nknowledge transfer. In this work, we propose a novel hierarchical memory\narchitecture that enables fine-grained knowledge transfer by decoupling\nhigh-level planning memory from low-level execution memory. To construct and\nrefine these hierarchical memories, we introduce Hierarchical Hindsight\nReflection (H$^2$R), a mechanism that distills reusable and hierarchical\nknowledge from past agent-environment interactions. At test time, H$^2$R\nperforms retrievals of high-level and low-level memories separately, allowing\nLLM-based agents to efficiently access and utilize task-relevant knowledge for\nnew tasks.Experimental results across two benchmarks demonstrate that H$^2$R\ncan improve generalization and decision-making performance, outperforming prior\nbaselines such as Expel.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784H\u00b2R\uff0c\u901a\u8fc7\u5c06\u9ad8\u7ea7\u89c4\u5212\u8bb0\u5fc6\u4e0e\u4f4e\u7ea7\u6267\u884c\u8bb0\u5fc6\u89e3\u8026\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u9ad8LLM\u667a\u80fd\u4f53\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5148\u9a8c\u7ecf\u9a8c\u548c\u77e5\u8bc6\u89c6\u4e3a\u5355\u4e00\u6574\u4f53\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u4f4e\u4e0b\u4e14\u7c92\u5ea6\u7c97\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u77e5\u8bc6\u8fc1\u79fb\u673a\u5236\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff0c\u5305\u542b\u9ad8\u7ea7\u89c4\u5212\u8bb0\u5fc6\u548c\u4f4e\u7ea7\u6267\u884c\u8bb0\u5fc6\uff1b\u5f15\u5165\u5206\u5c42\u540e\u89c1\u53cd\u601d\u673a\u5236(H\u00b2R)\uff0c\u4ece\u8fc7\u5f80\u4ea4\u4e92\u4e2d\u63d0\u70bc\u53ef\u91cd\u7528\u7684\u5206\u5c42\u77e5\u8bc6\uff1b\u6d4b\u8bd5\u65f6\u5206\u522b\u68c0\u7d22\u9ad8\u4f4e\u7ea7\u8bb0\u5fc6\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cH\u00b2R\u80fd\u591f\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u6027\u80fd\uff0c\u4f18\u4e8eExpel\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u548cH\u00b2R\u673a\u5236\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2509.12987", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12987", "abs": "https://arxiv.org/abs/2509.12987", "authors": ["Yarin Benyamin", "Argaman Mordoch", "Shahaf S. Shperberg", "Roni Stern"], "title": "Toward PDDL Planning Copilot", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks.", "AI": {"tldr": "Planning Copilot\u662f\u4e00\u4e2a\u96c6\u6210\u591a\u79cd\u89c4\u5212\u5de5\u5177\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u7528\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u751a\u81f3\u8d85\u8d8a\u5927\u578b\u5546\u4e1a\u6a21\u578bGPT-5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u7f3a\u4e4f\u53ef\u9760\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u9700\u8981\u5916\u90e8\u5de5\u5177\u652f\u6301\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u57fa\u4e8eModel Context Protocol (MCP)\u6807\u51c6\uff0c\u96c6\u6210\u591a\u79cd\u89c4\u5212\u5de5\u5177\uff08\u8bed\u6cd5\u68c0\u67e5\u3001\u89c4\u5212\u5668\u9009\u62e9\u3001\u8ba1\u5212\u9a8c\u8bc1\u3001\u6267\u884c\u6a21\u62df\u7b49\uff09\uff0c\u5141\u8bb8\u4efb\u4f55\u652f\u6301MCP\u7684LLM\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePlanning Copilot\u5728\u4f7f\u7528\u4e09\u4e2a\u5f00\u6e90LLM\u65f6\u663e\u8457\u4f18\u4e8e\u65e0\u5de5\u5177\u652f\u6301\u7684\u76f8\u540c\u6a21\u578b\uff0c\u5728\u6709\u9650\u5b9a\u6027\u6bd4\u8f83\u4e2d\u751a\u81f3\u663e\u8457\u8d85\u8d8aGPT-5\u3002", "conclusion": "\u4e13\u7528\u89c4\u5212\u5de5\u5177\u662f\u4f7fLLMs\u6709\u6548\u6267\u884c\u89c4\u5212\u4efb\u52a1\u7684\u6709\u6548\u9014\u5f84\uff0cMCP\u6807\u51c6\u4e3a\u5b9e\u73b0\u5de5\u5177\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.13127", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13127", "abs": "https://arxiv.org/abs/2509.13127", "authors": ["Sijia Cui", "Shuai Xu", "Aiyao He", "Yanna Wang", "Bo Xu"], "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning", "comment": "Accepted to IJCNN 2025", "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP.", "AI": {"tldr": "PLAP\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u89c4\u5212\u548c\u53c2\u6570\u5316\u6280\u80fd\u6267\u884c\uff0c\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u843d\u5730\u95ee\u9898\uff0c\u5728MicroRTS\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u591a\u4e2a\u57fa\u7ebf\u667a\u80fd\u4f53\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u76f4\u63a5\u751f\u6210\u5e95\u5c42\u52a8\u4f5c\u4e0d\u53ef\u9760\uff0c\u800c\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u53c8\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5728\u590d\u6742\u957f\u89c6\u91ce\u73af\u5883\u4e2d\u843d\u5730\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "\u63d0\u51faPLAP\uff08Plan with Language, Act with Parameter\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53c2\u6570\u5316\u6280\u80fd\u5e93\u3001LLM\u9a71\u52a8\u7684\u6280\u80fd\u89c4\u5212\u5668\u3001\u4ee5\u53ca\u5c06\u53c2\u6570\u5316\u6280\u80fd\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u7684\u6280\u80fd\u6267\u884c\u5668\u3002", "result": "\u5728MicroRTS\u6e38\u620f\u4e2d\uff0cGPT-4o\u9a71\u52a8\u7684PLAP\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e8680%\u7684\u57fa\u7ebf\u667a\u80fd\u4f53\uff0cQwen2-72B\u9a71\u52a8\u7684PLAP\u5728\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u9876\u7ea7\u811a\u672c\u667a\u80fd\u4f53CoacAI\u3002", "conclusion": "PLAP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u73af\u5883\u4e2d\u7684\u843d\u5730\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u5efa\u7acb\u4e86LLM\u957f\u89c6\u91ce\u6280\u80fd\u89c4\u5212\u80fd\u529b\u7684\u6392\u884c\u699c\u3002", "topic": "agent analysis"}}
{"id": "2509.13196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13196", "abs": "https://arxiv.org/abs/2509.13196", "authors": ["Yongjian Tang", "Doruk Tuncel", "Christian Koerner", "Thomas Runkler"], "title": "The Few-shot Dilemma: Over-prompting Large Language Models", "comment": "accepted for the main track of FLLM", "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0\u8fc7\u5ea6\u63d0\u793a\uff08over-prompting\uff09\u73b0\u8c61\uff1a\u5728\u63d0\u793a\u4e2d\u52a0\u5165\u8fc7\u591a\u9886\u57df\u7279\u5b9a\u793a\u4f8b\u53cd\u800c\u4f1a\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5c11\u6837\u672c\u5b66\u4e60\u7684\u8ba4\u77e5\u3002\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u4e86\u4e0d\u540cLLM\u7684\u6700\u4f73\u793a\u4f8b\u6570\u91cf\uff0c\u5728\u8f6f\u4ef6\u9700\u6c42\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8fc7\u5ea6\u63d0\u793a\u73b0\u8c61\uff0c\u5373\u8fc7\u591a\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6311\u6218\u4f20\u7edf\u5c11\u6837\u672c\u5b66\u4e60\u8ba4\u4e3a\u66f4\u591a\u76f8\u5173\u793a\u4f8b\u603b\u662f\u6709\u76ca\u7684\u8ba4\u77e5\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6807\u51c6\u5c11\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff08\u968f\u673a\u91c7\u6837\u3001\u8bed\u4e49\u5d4c\u5165\u3001TF-IDF\u5411\u91cf\uff09\u6784\u5efa\u63d0\u793a\u6846\u67b6\uff0c\u5728\u591a\u4e2aLLM\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0TF-IDF\u9009\u62e9\u548c\u5206\u5c42\u5c11\u6837\u672c\u793a\u4f8b\u6570\u91cf\u6765\u786e\u5b9a\u6bcf\u4e2aLLM\u7684\u6700\u4f73\u793a\u4f8b\u91cf\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u8fc7\u5ea6\u6dfb\u52a0\u9886\u57df\u7279\u5b9a\u793a\u4f8b\u4f1a\u964d\u4f4e\u67d0\u4e9bLLM\u6027\u80fd\uff0c\u786e\u5b9a\u4e86\u6bcf\u4e2aLLM\u7684\u6700\u4f73\u793a\u4f8b\u6570\u91cf\uff0c\u5728\u8f6f\u4ef6\u9700\u6c42\u5206\u7c7b\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53471%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fc7\u5ea6\u63d0\u793a\u786e\u5b9e\u5b58\u5728\uff0c\u9700\u8981\u4e3a\u4e0d\u540cLLM\u627e\u5230\u6700\u4f73\u793a\u4f8b\u6570\u91cf\uff0c\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002\u7ed3\u5408TF-IDF\u548c\u5206\u5c42\u9009\u62e9\u7684\u65b9\u6cd5\u80fd\u7528\u66f4\u5c11\u793a\u4f8b\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.13244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13244", "abs": "https://arxiv.org/abs/2509.13244", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data", "comment": "8 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning.", "AI": {"tldr": "LLM\u5728\u4eba\u683c\u7279\u8d28\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u4e0e\u771f\u5b9e\u4eba\u683c\u7279\u8d28\u7684Pearson\u76f8\u5173\u7cfb\u6570\u4f4e\u4e8e0.26\uff0c\u63d0\u793a\u5f53\u524d\u6a21\u578b\u5728\u5fc3\u7406\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6311\u6218", "motivation": "\u63a2\u7d22LLM\u5728\u81ea\u7136\u5bf9\u8bdd\u73af\u5883\u4e2d\u5bf9\u4eba\u7c7b\u4eba\u683c\u7279\u8d28\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u8fde\u7eed\u771f\u5b9e\u4eba\u683c\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d", "method": "\u4f7f\u7528\u5305\u542b\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8bb0\u5f55\u548c\u8fde\u7eedBig Five\u7279\u8d28\u5206\u6570\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e09\u79cd\u65b9\u6cd5\uff1a\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u3001LoRA\u5fae\u8c03\u3001\u9884\u8bad\u7ec3\u5d4c\u5165\u56de\u5f52", "result": "\u6240\u6709\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u4eba\u683c\u7279\u8d28\u7684\u76f8\u5173\u6027\u90fd\u4f4e\u4e8e0.26\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u76f8\u6bd4\u96f6\u6837\u672c\u63d0\u793a\u63d0\u5347\u6709\u9650", "conclusion": "\u5f53\u524dLLM\u4e0e\u9a8c\u8bc1\u5fc3\u7406\u6784\u5ff5\u7684\u5bf9\u9f50\u7a0b\u5ea6\u6709\u9650\uff0c\u4eba\u683c\u63a8\u65ad\u66f4\u591a\u4f9d\u8d56\u6f5c\u5728\u8bed\u4e49\u8868\u793a\u800c\u975e\u663e\u5f0f\u63a8\u7406\uff0c\u9700\u8981\u7279\u8d28\u7279\u5b9a\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5efa\u6a21\u7684\u8fdb\u4e00\u6b65\u7814\u7a76", "topic": "agent analysis"}}
{"id": "2509.13137", "categories": ["cs.AI", "cs.HC", "cs.MA", "K.4.4; K.6.5; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.13137", "abs": "https://arxiv.org/abs/2509.13137", "authors": ["Henrik Axelsen", "Valdemar Licht", "Jan Damsgaard"], "title": "Agentic AI for Financial Crime Compliance", "comment": "Accepted for presentation at HICSS-59 (2026), forthcoming in\n  Proceedings", "summary": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u91d1\u878d\u72af\u7f6a\u5408\u89c4\u7684\u667a\u80fdAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u884c\u52a8\u8bbe\u8ba1\u7814\u7a76\u65b9\u6cd5\u5f00\u53d1\uff0c\u5f3a\u8c03\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u5408\u89c4\u6027\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u91d1\u878d\u72af\u7f6a\u5408\u89c4\u6210\u672c\u4e0d\u65ad\u4e0a\u5347\u4f46\u6548\u679c\u6709\u9650\uff0c\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5408\u89c4\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u884c\u52a8\u8bbe\u8ba1\u7814\u7a76(ADR)\u65b9\u6cd5\uff0c\u4e0e\u91d1\u878d\u79d1\u6280\u516c\u53f8\u548c\u76d1\u7ba1\u673a\u6784\u5408\u4f5c\uff0c\u4f7f\u7528\u57fa\u4e8e\u5de5\u4ef6\u7684\u5efa\u6a21\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5206\u914d\u660e\u786e\u89d2\u8272\uff0c\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u8def\u7531\u548c\u5ba1\u8ba1\u65e5\u5fd7\u3002", "result": "\u5f00\u53d1\u4e86\u53c2\u8003\u67b6\u6784\u548c\u5b9e\u9645\u539f\u578b\uff0c\u5c55\u793a\u4e86\u667a\u80fdAI\u5982\u4f55\u91cd\u6784\u91d1\u878d\u72af\u7f6a\u5408\u89c4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u900f\u660e\u5ea6\u548c\u673a\u6784\u4fe1\u4efb\u3002", "conclusion": "\u5f53\u81ea\u52a8\u5316\u5d4c\u5165\u5230\u8d1f\u8d23\u4efb\u7684\u6cbb\u7406\u7ed3\u6784\u4e2d\u65f6\uff0c\u53ef\u4ee5\u5728\u9ad8\u98ce\u9669\u76d1\u7ba1\u73af\u5883\u4e2d\u652f\u6301\u900f\u660e\u5ea6\u548c\u673a\u6784\u4fe1\u4efb\uff0c\u6269\u5c55\u4e86\u4fe1\u606f\u7cfb\u7edf\u6587\u732e\u4e2d\u5173\u4e8eAI\u652f\u6301\u5408\u89c4\u7684\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.13310", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13310", "abs": "https://arxiv.org/abs/2509.13310", "authors": ["Liangcai Su", "Zhen Zhang", "Guangyu Li", "Zhuo Chen", "Chenxi Wang", "Maojia Song", "Xinyu Wang", "Kuan Li", "Jialong Wu", "Xuanzhong Chen", "Zile Qiao", "Zhongwang Zhang", "Huifeng Yin", "Shihao Cai", "Runnan Fang", "Zhengwei Tao", "Wenbiao Yin", "Chenxiong Qian", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "Scaling Agents via Continual Pre-training", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentic CPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u6784\u5efa\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578bAgentFounder-30B\uff0c\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578b\uff0c\u5bfc\u81f4\u6a21\u578b\u9700\u8981\u540c\u65f6\u5b66\u4e60\u591a\u79cd\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u4e0e\u4e13\u5bb6\u6f14\u793a\u5bf9\u9f50\uff0c\u4ea7\u751f\u4f18\u5316\u51b2\u7a81", "method": "\u63d0\u51faAgentic Continual Pre-training (Agentic CPT)\u65b9\u6cd5\uff0c\u5c06\u5176\u6574\u5408\u5230\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u6784\u5efa\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578bAgentFounder", "result": "AgentFounder-30B\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aBrowseComp-en 39.9%\u3001BrowseComp-zh 43.3%\u3001HLE Pass@1 31.5%\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b", "conclusion": "Agentic CPT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578b\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "topic": "agent analysis"}}
{"id": "2509.13311", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13311", "abs": "https://arxiv.org/abs/2509.13311", "authors": ["Runnan Fang", "Shihao Cai", "Baixuan Li", "Jialong Wu", "Guangyu Li", "Wenbiao Yin", "Xinyu Wang", "Xiaobin Wang", "Liangcai Su", "Zhen Zhang", "Shibin Wu", "Zhengwei Tao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "Towards General Agentic Intelligence via Environment Scaling", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u73af\u5883\u6784\u5efa\u6846\u67b6AgentScaler\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5f02\u6784\u6a21\u62df\u73af\u5883\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fd\u6570\u8c03\u7528\u80fd\u529b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754cAPI\u9700\u8981\u7cbe\u786e\u3001\u9c81\u68d2\u7684\u51fd\u6570\u8c03\u7528\u667a\u80fd\uff0c\u800c\u667a\u80fd\u4f53\u80fd\u529b\u7684\u5e7f\u5ea6\u4e0e\u5176\u8bad\u7ec3\u73af\u5883\u7684\u591a\u6837\u6027\u5bc6\u5207\u76f8\u5173\u3002\u4e3a\u4e86\u89e3\u51b3\u73af\u5883\u89c4\u6a21\u5316\u6784\u5efa\u548c\u6709\u6548\u8bad\u7ec3\u7684\u6838\u5fc3\u6311\u6218", "method": "\u8bbe\u8ba1\u53ef\u6269\u5c55\u6846\u67b6\u81ea\u52a8\u6784\u5efa\u5b8c\u5168\u6a21\u62df\u7684\u5f02\u6784\u73af\u5883\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff1a\u5148\u8d4b\u4e88\u57fa\u7840\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u518d\u8fdb\u884c\u9886\u57df\u4e13\u4e1a\u5316", "result": "\u5728tau-bench\u3001tau2-Bench\u548cACEBench\u7b49\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bad\u7ec3\u7684AgentScaler\u6a21\u578b\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u51fd\u6570\u8c03\u7528\u80fd\u529b", "conclusion": "\u901a\u8fc7\u89c4\u6a21\u5316\u73af\u5883\u6784\u5efa\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u667a\u80fd\u4f53\u667a\u80fd\u548c\u51fd\u6570\u8c03\u7528\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2509.13281", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13281", "abs": "https://arxiv.org/abs/2509.13281", "authors": ["Vincent Siu", "Nathan W. Henry", "Nicholas Crispino", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "RepIt: Representing Isolated Targets to Steer Language Models", "comment": null, "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.", "AI": {"tldr": "RepIt\u662f\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u6982\u5ff5\u5411\u91cf\u63d0\u53d6\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u5728LLM\u4e2d\u9694\u79bb\u7279\u5b9a\u6982\u5ff5\u8868\u793a\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u884c\u4e3a\u5e72\u9884\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u6a21\u578b\u5176\u4ed6\u529f\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u6bd4\u9884\u671f\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7eaf\u51c0\u7684\u6982\u5ff5\u5411\u91cf\u6765\u5b9e\u73b0\u9488\u5bf9\u6027\u5e72\u9884\uff0c\u4ece\u800c\u66f4\u7cbe\u7ec6\u5730\u7406\u89e3LLM\u884c\u4e3a\u3002", "method": "\u63d0\u51faRepIt\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u63d0\u53d6\u6982\u5ff5\u7279\u5b9a\u8868\u793a\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u793a\u4f8b\uff08\u4ec5\u9700\u5341\u51e0\u4e2a\uff09\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\uff08\u5355\u5f20A6000\uff09\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u5e72\u9884\u3002", "result": "\u5728\u4e94\u4e2a\u524d\u6cbfLLM\u4e0a\u9a8c\u8bc1\uff0cRepIt\u80fd\u591f\u9009\u62e9\u6027\u6291\u5236\u76ee\u6807\u6982\u5ff5\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u5b89\u5168\u57fa\u51c6\u5f97\u5206\uff1b\u7ea0\u6b63\u4fe1\u53f7\u4ec5\u5b9a\u4f4d\u5230100-200\u4e2a\u795e\u7ecf\u5143\u3002", "conclusion": "RepIt\u5c55\u793a\u4e86\u9488\u5bf9\u6027\u5e72\u9884\u53ef\u4ee5\u62b5\u6d88\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u66f4\u7cbe\u7ec6\u7684\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5173\u4e8e\u4f7f\u7528\u6709\u9650\u8ba1\u7b97\u548c\u6570\u636e\u5c31\u80fd\u64cd\u7eb5\u6a21\u578b\u7684\u62c5\u5fe7\u3002", "topic": "agent analysis"}}
{"id": "2509.13288", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13288", "abs": "https://arxiv.org/abs/2509.13288", "authors": ["Marjorie McShane", "Sergei Nirenburg", "Sanjay Oruganti", "Jesse English"], "title": "Shapes of Cognition for Computational Cognitive Modeling", "comment": null, "summary": "Shapes of cognition is a new conceptual paradigm for the computational\ncognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are\nremembered constellations of sensory, linguistic, conceptual, episodic, and\nprocedural knowledge that allow agents to cut through the complexity of real\nlife the same way as people do: by expecting things to be typical, recognizing\npatterns, acting by habit, reasoning by analogy, satisficing, and generally\nminimizing cognitive load to the degree situations permit. Atypical outcomes\nare treated using shapes-based recovery methods, such as learning on the fly,\nasking a human partner for help, or seeking an actionable, even if imperfect,\nsituational understanding. Although shapes is an umbrella term, it is not\nvague: shapes-based modeling involves particular objectives, hypotheses,\nmodeling strategies, knowledge bases, and actual models of wide-ranging\nphenomena, all implemented within a particular cognitive architecture. Such\nspecificity is needed both to vet our hypotheses and to achieve our practical\naims of building useful agent systems that are explainable, extensible, and\nworthy of our trust, even in critical domains. However, although the LEIA\nexample of shapes-based modeling is specific, the principles can be applied\nmore broadly, giving new life to knowledge-based and hybrid AI.", "AI": {"tldr": "Shapes of cognition\u662f\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u8ba4\u77e5\u5efa\u6a21\u8303\u5f0f\uff0c\u901a\u8fc7\u8bb0\u5fc6\u611f\u5b98\u3001\u8bed\u8a00\u3001\u6982\u5ff5\u3001\u60c5\u666f\u548c\u7a0b\u5e8f\u77e5\u8bc6\u7684\u661f\u5ea7\u6a21\u5f0f\uff0c\u8ba9\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u5904\u7406\u73b0\u5b9e\u751f\u6d3b\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u9ad8\u6548\u8ba4\u77e5\u5904\u7406\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u6765\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u540c\u65f6\u5904\u7406\u5f02\u5e38\u60c5\u51b5\u3002", "method": "\u57fa\u4e8e\u5f62\u72b6\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5305\u62ec\u7279\u5b9a\u76ee\u6807\u3001\u5047\u8bbe\u3001\u5efa\u6a21\u7b56\u7565\u3001\u77e5\u8bc6\u5e93\u548c\u5b9e\u9645\u6a21\u578b\uff0c\u5728\u7279\u5b9a\u8ba4\u77e5\u67b6\u6784\u4e2d\u5b9e\u73b0\u3002\u4f7f\u7528\u5f62\u72b6\u8bc6\u522b\u6a21\u5f0f\u3001\u4e60\u60ef\u884c\u52a8\u3001\u7c7b\u6bd4\u63a8\u7406\u548c\u6ee1\u610f\u5ea6\u539f\u5219\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u4f53\u7684LEIA\uff08\u8bed\u8a00\u8d4b\u80fd\u7684\u667a\u80fd\u4ee3\u7406\uff09\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "conclusion": "\u5f62\u72b6\u8ba4\u77e5\u8303\u5f0f\u4e0d\u4ec5\u9002\u7528\u4e8eLEIA\u5efa\u6a21\uff0c\u5176\u539f\u5219\u53ef\u4ee5\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u77e5\u8bc6\u578b\u548c\u6df7\u5408\u578bAI\uff0c\u4e3a\u8fd9\u4e9b\u9886\u57df\u6ce8\u5165\u65b0\u7684\u6d3b\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.13313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13313", "abs": "https://arxiv.org/abs/2509.13313", "authors": ["Xixi Wu", "Kuan Li", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Huifeng Yin", "Zhongwang Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Minhao Cheng", "Shuai Wang", "Hong Cheng", "Jingren Zhou"], "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.", "AI": {"tldr": "ReSum\u662f\u4e00\u79cd\u65b0\u7684web agent\u8303\u5f0f\uff0c\u901a\u8fc7\u5468\u671f\u6027\u4e0a\u4e0b\u6587\u6458\u8981\u514b\u670dLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u5b9e\u73b0\u65e0\u9650\u63a2\u7d22\uff0c\u6bd4ReAct\u5e73\u5747\u63d0\u53474.5%\u6027\u80fd", "motivation": "\u89e3\u51b3LLM-based web agent\u5728\u590d\u6742\u67e5\u8be2\u4efb\u52a1\u4e2d\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u800c\u65e0\u6cd5\u5b8c\u6210\u591a\u8f6e\u641c\u7d22\u7684\u95ee\u9898", "method": "\u63d0\u51faReSum\u8303\u5f0f\u8fdb\u884c\u5468\u671f\u6027\u4e0a\u4e0b\u6587\u6458\u8981\uff0c\u5e76\u5f00\u53d1ReSum-GRPO\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u6458\u8981\u6761\u4ef6\u63a8\u7406\u8bad\u7ec3", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6bd4ReAct\u63d0\u53474.5%\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u63d0\u53478.2%\uff0cWebResummer-30B\u5728BrowseComp\u57fa\u51c6\u4e0a\u8fbe\u523033.3%\u548c18.3%\u7684Pass@1", "conclusion": "ReSum\u901a\u8fc7\u6458\u8981\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86web agent\u7684\u6027\u80fd\u8868\u73b0", "topic": "agent analysis"}}
{"id": "2509.13316", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13316", "abs": "https://arxiv.org/abs/2509.13316", "authors": ["Millicent Li", "Alberto Mario Ceballos Arroyo", "Giordano Rogers", "Naomi Saphra", "Byron C. Wallace"], "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?", "comment": "34 pages, 6 figures", "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u5224\u6027\u8bc4\u4f30\u4e86LLM\u6fc0\u6d3b\u89e3\u91ca\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u7f3a\u9677\uff0c\u89e3\u91ca\u7ed3\u679c\u5f80\u5f80\u53cd\u6620\u4e86\u89e3\u91ca\u5668LLM\u7684\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u76ee\u6807\u6a21\u578b\u7684\u771f\u5b9e\u6fc0\u6d3b\u6a21\u5f0f\u3002", "motivation": "\u9a8c\u8bc1LLM\u5185\u90e8\u8868\u793a\u89e3\u91ca\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63a2\u7a76\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u63ed\u793a\u4e86\u76ee\u6807\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u8fd8\u662f\u4ec5\u4ec5\u53cd\u6620\u4e86\u8f93\u5165\u4fe1\u606f\u6216\u89e3\u91ca\u5668\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u8bc4\u4f30\u6d41\u884c\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8bbe\u8ba1\u63a7\u5236\u5b9e\u9a8c\u6765\u5206\u6790\u89e3\u91ca\u7ed3\u679c\u7684\u4fe1\u606f\u6765\u6e90\u3002", "result": "\u53d1\u73b0\u89e3\u91ca\u65b9\u6cd5\u5728\u65e0\u9700\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u5185\u90e8\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6210\u529f\uff0c\u4e14\u89e3\u91ca\u7ed3\u679c\u5f80\u5f80\u53cd\u6620\u4e86\u89e3\u91ca\u5668LLM\u7684\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u76ee\u6807\u6a21\u578b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u6709\u9488\u5bf9\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u63a7\u5236\u6765\u4e25\u683c\u8bc4\u4f30\u89e3\u91ca\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u63d0\u4f9b\u4e86\u5bf9LLM\u64cd\u4f5c\u7684\u6709\u610f\u4e49\u6d1e\u5bdf\u3002", "topic": "agent analysis"}}
{"id": "2509.12936", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12936", "abs": "https://arxiv.org/abs/2509.12936", "authors": ["Denis Janiak", "Julia Moska", "Dawid Motyka", "Karolina Seweryn", "Pawe\u0142 Walkowiak", "Bartosz \u017buk", "Arkadiusz Janz"], "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety", "comment": null, "summary": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83PPO\u3001DPO\u3001ORPO\u3001KTO\u7b49LLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6027\u3001\u5b89\u5168\u6027\u3001\u7b80\u6d01\u6027\u3001\u4e3b\u52a8\u6027\u548c\u591a\u6837\u6027\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u7ef4\u5ea6\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6280\u672f\u6216\u7279\u5b9a\u7ef4\u5ea6\uff0c\u7f3a\u4e4f\u5bf9LLM\u5bf9\u9f50\u65b9\u6cd5\u5185\u5728\u6743\u8861\u7684\u6574\u4f53\u8bc4\u4f30\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6307\u5bfc\u66f4\u5e73\u8861\u53ef\u9760\u7684LLM\u5f00\u53d1\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u56db\u79cd\u4e3b\u6d41\u5bf9\u9f50\u65b9\u6cd5\uff08PPO\u3001DPO\u3001ORPO\u3001KTO\uff09\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u7528\u7ecf\u8fc7\u4eba\u5de5\u7814\u7a76\u9a8c\u8bc1\u7684LLM-as-Judge\u63d0\u793a\u65b9\u6cd5\u3002", "result": "DPO\u548cKTO\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cPPO\u548cDPO\u5728\u5b89\u5168\u6027\u65b9\u9762\u9886\u5148\uff0cPPO\u5728\u7b80\u6d01\u6027\u548c\u4e3b\u52a8\u6027\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5e38\u89c1\u5bf9\u9f50\u65b9\u6cd5\u7684\u5185\u5728\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u5f00\u53d1\u66f4\u5e73\u8861\u53ef\u9760\u7684LLM\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2509.13305", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13305", "abs": "https://arxiv.org/abs/2509.13305", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Dingchu Zhang", "Xixi Wu", "Jialong Wu", "Xinyu Wang", "Zile Qiao", "Zhen Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "AI": {"tldr": "WebSailor\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u4e0d\u786e\u5b9a\u6027\u4efb\u52a1\u3001RFT\u51b7\u542f\u52a8\u548cDUPO\u7b97\u6cd5\uff0c\u4f7f\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e13\u6709\u4ee3\u7406\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3LLM\u8bad\u7ec3\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u8ba4\u77e5\u9650\u5236\u7684\u95ee\u9898\uff0c\u4e13\u6709\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8d85\u4eba\u7c7b\u80fd\u529b\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u5316\u5904\u7406\u6781\u7aef\u4e0d\u786e\u5b9a\u6027\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u91c7\u6837\u548c\u4fe1\u606f\u6a21\u7cca\u5316\u751f\u6210\u65b0\u9896\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u4efb\u52a1\uff0c\u91c7\u7528RFT\u51b7\u542f\u52a8\u548c\u9ad8\u6548\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5DUPO\uff08\u91cd\u590d\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff09\u3002", "result": "WebSailor\u5728\u590d\u6742\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u4ee3\u7406\uff0c\u5339\u914d\u4e13\u6709\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u4e13\u6709\u4ee3\u7406\u7684\u5173\u952e\u80fd\u529b\u79fb\u690d\u5230\u5f00\u6e90\u6a21\u578b\u4e2d\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u5316\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12833", "abs": "https://arxiv.org/abs/2509.12833", "authors": ["Hannah Markgraf", "Shamburaj Sawant", "Hanna Krasowski", "Lukas Sch\u00e4fer", "Sebastien Gros", "Matthias Althoff"], "title": "Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?", "comment": null, "summary": "Projection-based safety filters, which modify unsafe actions by mapping them\nto the closest safe alternative, are widely used to enforce safety constraints\nin reinforcement learning (RL). Two integration strategies are commonly\nconsidered: Safe environment RL (SE-RL), where the safeguard is treated as part\nof the environment, and safe policy RL (SP-RL), where it is embedded within the\npolicy through differentiable optimization layers. Despite their practical\nrelevance in safety-critical settings, a formal understanding of their\ndifferences is lacking. In this work, we present a theoretical comparison of\nSE-RL and SP-RL. We identify a key distinction in how each approach is affected\nby action aliasing, a phenomenon in which multiple unsafe actions are projected\nto the same safe action, causing information loss in the policy gradients. In\nSE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it\nmanifests directly as rank-deficient Jacobians during backpropagation through\nthe safeguard. Our contributions are threefold: (i) a unified formalization of\nSE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical\nanalysis of their respective policy gradient estimates, highlighting the role\nof action aliasing, and (iii) a comparative study of mitigation strategies,\nincluding a novel penalty-based improvement for SP-RL that aligns with\nestablished SE-RL practices. Empirical results support our theoretical\npredictions, showing that action aliasing is more detrimental for SP-RL than\nfor SE-RL. However, with appropriate improvement strategies, SP-RL can match or\noutperform improved SE-RL across a range of environments. These findings\nprovide actionable insights for choosing and refining projection-based safe RL\nmethods based on task characteristics.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e24\u79cd\u57fa\u4e8e\u6295\u5f71\u7684\u5b89\u5168RL\u65b9\u6cd5\uff08SE-RL\u548cSP-RL\uff09\u8fdb\u884c\u4e86\u7406\u8bba\u6bd4\u8f83\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u52a8\u4f5c\u522b\u540d\u73b0\u8c61\u5bf9\u7b56\u7565\u68af\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6295\u5f71\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u5b89\u5168\u5173\u952eRL\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u5176\u4e24\u79cd\u4e3b\u8981\u96c6\u6210\u7b56\u7565\uff08SE-RL\u548cSP-RL\uff09\u7684\u7406\u8bba\u5dee\u5f02\u7f3a\u4e4f\u6b63\u5f0f\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5b83\u4eec\u7684\u884c\u4e3a\u548c\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7406\u8bba\u5206\u6790\u4e86SE-RL\u548cSP-RL\u7684\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u52a8\u4f5c\u522b\u540d\u73b0\u8c61\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u5305\u62ec\u4e3aSP-RL\u63d0\u51fa\u7684\u65b0\u578b\u57fa\u4e8e\u60e9\u7f5a\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u7406\u8bba\u9884\u6d4b\uff0c\u663e\u793a\u52a8\u4f5c\u522b\u540d\u5bf9SP-RL\u7684\u635f\u5bb3\u66f4\u5927\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u7684\u6539\u8fdb\u7b56\u7565\uff0cSP-RL\u53ef\u4ee5\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u6539\u8fdb\u7684SE-RL\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u548c\u4f18\u5316\u57fa\u4e8e\u6295\u5f71\u7684\u5b89\u5168RL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u8868\u660e\u4e24\u79cd\u65b9\u6cd5\u5728\u9002\u5f53\u6539\u8fdb\u540e\u90fd\u80fd\u53d6\u5f97\u826f\u597d\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12867", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12867", "abs": "https://arxiv.org/abs/2509.12867", "authors": ["Yabo Zhang", "Yihan Zeng", "Qingyun Li", "Zhen Hu", "Kavin Han", "Wangmeng Zuo"], "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.", "AI": {"tldr": "Tool-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884cPython\u4ee3\u7801\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u901a\u7528\u3001\u7ec4\u5408\u5f0f\u548c\u591a\u6b65\u9aa4\u7684\u5de5\u5177\u4f7f\u7528\uff0c\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u7ea610%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5728\u5904\u7406\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u3001\u7cbe\u786e\u64cd\u4f5c\u6216\u4e13\u7528\u5de5\u5177\u4f7f\u7528\u7684\u73b0\u5b9e\u4efb\u52a1\u65f6\u4ecd\u6709\u9650\u5236\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Tool-R1\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5de5\u5177\u548c\u6807\u51c6\u5e93\u96c6\u6210\uff0c\u901a\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u51fd\u6570\uff08\u7ed3\u5408LLM\u7b54\u6848\u5224\u65ad\u548c\u4ee3\u7801\u6267\u884c\u6210\u529f\uff09\u6307\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6837\u672c\u961f\u5217\u7f13\u5b58\u548c\u91cd\u7528\u9ad8\u8d28\u91cf\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTool-R1\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u7ea610%\uff0c\u5728\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u4e0a\u63d0\u5347\u66f4\u5927\u3002", "conclusion": "Tool-R1\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u73b0\u53ef\u9760\u9ad8\u6548\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.13160", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13160", "abs": "https://arxiv.org/abs/2509.13160", "authors": ["Liang Hu", "Jianpeng Jiao", "Jiashuo Liu", "Yanle Ren", "Zhoufutu Wen", "Kaiyuan Zhang", "Xuanliang Zhang", "Xiang Gao", "Tianci He", "Fei Hu", "Yali Liao", "Zaiyuan Wang", "Chenghao Yang", "Qianyu Yang", "Mingren Yin", "Zhiyuan Zeng", "Ge Zhang", "Xinyi Zhang", "Xiying Zhao", "Zhenwei Zhu", "Hongseok Namkoong", "Wenhao Huang", "Yuwen Tang"], "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning", "comment": "29 pages", "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.", "AI": {"tldr": "FinSearchComp\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u91d1\u878d\u641c\u7d22\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u4e2a\u771f\u5b9e\u91d1\u878d\u5206\u6790\u5e08\u5de5\u4f5c\u6d41\u7a0b\u4efb\u52a1\uff0c\u753170\u4f4d\u91d1\u878d\u4e13\u5bb6\u6807\u6ce8\uff0c\u8bc4\u4f30\u4e8621\u4e2a\u6a21\u578b\u5728\u590d\u6742\u91d1\u878d\u641c\u7d22\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u6570\u636e\u641c\u7d22\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u91d1\u878d\u9886\u57df\u9700\u8981\u5904\u7406\u65f6\u6548\u6027\u5f3a\u3001\u9886\u57df\u7279\u5b9a\u7684\u590d\u6742\u591a\u6b65\u641c\u7d22\u4efb\u52a1\uff0c\u662f\u8bc4\u4f30\u641c\u7d22\u80fd\u529b\u548c\u77e5\u8bc6\u63a8\u7406\u7684\u7406\u60f3\u573a\u666f\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u4efb\u52a1\uff08\u65f6\u6548\u6570\u636e\u83b7\u53d6\u3001\u7b80\u5355\u5386\u53f2\u67e5\u8be2\u3001\u590d\u6742\u5386\u53f2\u8c03\u67e5\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u5168\u7403\u548c\u5927\u4e2d\u534e\u533a\u5e02\u573a635\u4e2a\u95ee\u9898\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\uff0c\u753170\u4f4d\u91d1\u878d\u4e13\u5bb6\u8fdb\u884c\u6807\u6ce8\u3002", "result": "Grok 4\u5728\u5168\u7403\u5316\u5b50\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u63a5\u8fd1\u4e13\u5bb6\u7ea7\u51c6\u786e\u7387\uff1bDouBao\u5728\u5927\u4e2d\u534e\u533a\u5b50\u96c6\u9886\u5148\uff1b\u5b9e\u9a8c\u663e\u793a\u914d\u5907\u7f51\u7edc\u641c\u7d22\u548c\u91d1\u878d\u63d2\u4ef6\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u548c\u5de5\u5177\u7684\u56fd\u5bb6\u6765\u6e90\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "FinSearchComp\u901a\u8fc7\u4e0e\u771f\u5b9e\u5206\u6790\u5e08\u4efb\u52a1\u5bf9\u9f50\u5e76\u63d0\u4f9b\u7aef\u5230\u7aef\u8bc4\u4f30\uff0c\u4e3a\u590d\u6742\u91d1\u878d\u641c\u7d22\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e13\u4e1a\u3001\u9ad8\u96be\u5ea6\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "swe benchmark"}}
{"id": "2509.13232", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.13232", "abs": "https://arxiv.org/abs/2509.13232", "authors": ["Zhongwen Xu", "Zihan Ding"], "title": "Single-stream Policy Optimization", "comment": null, "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.", "AI": {"tldr": "SPO\u662f\u4e00\u79cd\u5355\u6d41\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u4e45\u5316KL\u81ea\u9002\u5e94\u503c\u8ddf\u8e2a\u5668\u548c\u5168\u5c40\u4f18\u52bf\u5f52\u4e00\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u7ec4\u65b9\u6cd5GRPO\u7684\u9000\u5316\u7ec4\u95ee\u9898\u548c\u540c\u6b65\u969c\u788d\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5206\u7ec4\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5b58\u5728\u9000\u5316\u7ec4\u6d88\u9664\u5b66\u4e60\u4fe1\u53f7\u548c\u540c\u6b65\u969c\u788d\u9650\u5236\u53ef\u6269\u5c55\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5355\u6d41\u7b56\u7565\u4f18\u5316\uff08SPO\uff09\uff0c\u4f7f\u7528\u6301\u4e45\u5316\u7684KL\u81ea\u9002\u5e94\u503c\u8ddf\u8e2a\u5668\u66ff\u4ee3\u6bcf\u7ec4\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u6574\u4e2a\u6279\u6b21\u4e2d\u5168\u5c40\u5f52\u4e00\u5316\u4f18\u52bf\uff0c\u5b9e\u73b0\u65e0\u5206\u7ec4\u7684\u9ad8\u541e\u5410\u91cf\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPO\u6bd4GRPO\u5e73\u5747maj@32\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u5728BRUMO 25\u4e0a\u63d0\u53477.3pp\uff0cAIME 25\u4e0a\u63d0\u53474.4pp\uff0cHMMT 25\u4e0a\u63d0\u53473.3pp\u3002", "conclusion": "SPO\u901a\u8fc7\u57fa\u672c\u539f\u7406\u800c\u975e\u67b6\u6784\u53d8\u901a\u6765\u63a8\u52a8LLM\u63a8\u7406\u8fdb\u6b65\uff0c\u8bc1\u660e\u4e86\u7b80\u5316\u590d\u6742\u6027\u53ef\u4ee5\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.13237", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13237", "abs": "https://arxiv.org/abs/2509.13237", "authors": ["Aniket Didolkar", "Nicolas Ballas", "Sanjeev Arora", "Anirudh Goyal"], "title": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors", "comment": "18 pages, 9 Figures, 5 Tables", "summary": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u91cd\u590d\u63a8\u7406\u7247\u6bb5\u8f6c\u6362\u4e3a\u53ef\u91cd\u7528\"\u884c\u4e3a\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u7684\u5143\u8ba4\u77e5\u5206\u6790\u521b\u5efa\"\u884c\u4e3a\u624b\u518c\"\uff0c\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u76f8\u5173\u884c\u4e3a\u6307\u5bfc\uff0c\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "LLMs\u5728\u89e3\u51b3\u591a\u6b65\u95ee\u9898\u65f6\u7ecf\u5e38\u91cd\u590d\u63a8\u5bfc\u76f8\u540c\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5bfc\u81f4token\u4f7f\u7528\u548c\u5ef6\u8fdf\u589e\u52a0\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9971\u548c\u9650\u5236\u4e86\u63a2\u7d22\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u907f\u514d\u91cd\u590d\u63a8\u7406\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7LLM\u7684\u5143\u8ba4\u77e5\u5206\u6790\u5c06\u91cd\u590d\u63a8\u7406\u7247\u6bb5\u8f6c\u6362\u4e3a\u7b80\u6d01\u53ef\u91cd\u7528\u7684\"\u884c\u4e3a\"\uff08\u540d\u79f0+\u6307\u4ee4\uff09\uff0c\u5b58\u50a8\u5728\"\u884c\u4e3a\u624b\u518c\"\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6307\u5bfc\u6216\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u84b8\u998f\u5230\u53c2\u6570\u4e2d\u3002", "result": "1) \u884c\u4e3a\u6761\u4ef6\u63a8\u7406\uff1a\u51cf\u5c11\u63a8\u7406token\u8fbe46%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u7387\uff1b2) \u884c\u4e3a\u5f15\u5bfc\u81ea\u6539\u8fdb\uff1a\u65e0\u53c2\u6570\u66f4\u65b0\u4e0b\u51c6\u786e\u7387\u63d0\u9ad810%\uff1b3) \u884c\u4e3a\u6761\u4ef6SFT\uff1a\u6bd4\u666e\u901aSFT\u66f4\u6709\u6548\u5730\u5c06\u975e\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "\u5c06\u7f13\u6162\u63a8\u5bfc\u8f6c\u6362\u4e3a\u5feb\u901f\u7a0b\u5e8f\u63d0\u793a\u4f7fLLMs\u80fd\u591f\u8bb0\u4f4f\u5982\u4f55\u63a8\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u8bb0\u4f4f\u7ed3\u8bba\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u6548\u679c\u3002", "topic": "agent analysis"}}
