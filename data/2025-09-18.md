<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 本文提出了一个分析框架来研究LLM跨任务迁移学习中的潜在能力和副作用，发现性能提升主要受隐藏的统计因素和语言特征影响，而非表面数据集相似性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM部署到训练时未见过的新任务时，枚举所有高质量训练数据不可行，需要依赖不同特性的数据集进行迁移学习，因此需要分析跨任务交互的复杂动态。

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（推理、情感分类、自然语言理解、算术等），并分析迁移学习的副作用。

Result: 发现性能提升往往无法用表面数据集相似性或源数据质量来解释，而是受源数据集的隐藏统计因素（如类别分布、生成长度倾向）和特定语言特征的影响更大。

Conclusion: 这项工作揭示了迁移学习的复杂动态，为更可预测和有效的LLM适应铺平了道路。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [2] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现LLMs在内部表征中线性编码问题歧义性，通过识别和操控歧义编码神经元(AENs)，可以检测和控制模型从直接回答到弃权的行为


<details>
  <summary>Details</summary>
Motivation: 现实世界问题普遍存在歧义性，但大型语言模型往往自信回答而非寻求澄清，需要理解模型如何内部处理歧义信息

Method: 在模型预填充阶段识别歧义编码神经元，训练探针进行歧义检测，通过神经元操控控制模型行为

Result: 发现少量神经元(少至1个)编码歧义信息，AENs探针在歧义检测上表现优异且具有跨数据集泛化能力，浅层出现AENs表明早期编码歧义信号

Conclusion: LLMs形成紧凑的内部歧义表征，支持可解释和可控的行为，为模型透明度提供了新见解

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [3] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种通过逐步剪枝冗余推理链来加速Self-Consistency的方法，在保持精度的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: Self-Consistency虽然能提升LLM推理性能，但其数量级的计算开销限制了广泛应用，需要更高效的替代方案

Method: 基于理论和实证分析，提出Slim-SC方法，利用思维层面的链间相似性识别和移除冗余推理链

Result: 在三个STEM推理数据集和两种LLM架构上，Slim-SC将推理延迟降低45%，KVC使用降低26%，同时保持或提升精度

Conclusion: Slim-SC为Self-Consistency提供了一个简单而高效的测试时缩放替代方案

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [4] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT是一种推理时方法，通过检测答案收敛性来提前终止思维链生成，平均减少41%的推理token，同时保持与标准CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂问题时需要生成长思维链，但这种方法会产生高昂的推理成本。研究旨在减少推理token使用量，同时保持模型性能。

Method: 在每个推理步骤结束时提示LLM输出当前最终答案（步骤答案），跟踪连续相同步骤答案的运行长度作为收敛度量。当运行长度出现急剧增加并超过最小阈值时终止生成。

Result: 在五个推理数据集和三个LLM上的实验表明，ES-CoT平均减少约41%的推理token，同时保持与标准CoT相当的准确性。该方法还能与自一致性提示无缝集成，并在超参数选择上保持鲁棒性。

Conclusion: ES-CoT是一种实用有效的推理效率提升方法，通过检测答案收敛性实现早期停止，显著降低推理成本。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [5] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 提出了一种新的端到端金融顾问框架，通过整合金融背景和行为金融学研究构建监督数据，使8B参数模型在性能上媲美14-32B基线模型，同时成本降低80%


<details>
  <summary>Details</summary>
Motivation: 现有金融顾问系统维护成本高且收益不佳，需要一种能够综合考虑用户目标、约束、风险承受能力和司法管辖区的个性化金融建议方法

Method: 创建包含19k样本的推理数据集，整合相关金融背景和行为金融学研究，对Qwen-3-8B模型进行全面微调

Result: 8B模型在事实准确性、流畅性和个性化指标上达到与14-32B基线模型相当的性能，同时成本降低80%

Conclusion: 通过精心策划的数据整合和行为金融学方法，较小的模型可以实现与大型模型相当的性能，显著降低成本

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文提出了一种基于代理的LLM方法，用于将法律条文转化为可执行软件，通过高阶蜕变测试和角色框架自动化测试生成，在复杂税法任务中表现优于前沿模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在法律关键环境中因歧义和幻觉导致的可靠性问题，特别是在美国联邦税务准备等需要精确法律解释的领域。

Method: 采用基于角色的多代理系统框架，使用高阶蜕变关系进行测试用例生成，通过LLM驱动的自动化测试生成和代码合成方法。

Result: 使用较小模型(GPT-4o-mini)在最坏情况下达到45%的通过率，显著优于GPT-4o和Claude 3.5模型(9-15%)。

Conclusion: 代理式LLM方法为从自然语言规范开发健壮、可信赖的法律关键软件提供了一条可行路径。

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [7] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG是一种将自然语言描述转换为可执行Apache Airflow DAG的方法，通过混合方法实现78.5%的成功率，显著优于纯LLM和直接方法。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的数据丰富管道需要大量工程专业知识，需要一种自动化方法来降低技术门槛并民主化数据管道开发。

Method: 提出四种生成方法（直接、纯LLM、混合和基于模板），使用13个LLM在260个实验中评估，采用惩罚性评分框架衡量可靠性、代码质量、结构完整性和可执行性。

Result: 混合方法表现最佳，成功率78.5%（SAT:6.79, DST:7.67, PCT:7.76），显著优于纯LLM（66.2%）和直接方法（29.2%）。可靠性是主要区分因素，混合方法的成本效益是直接提示的两倍。

Conclusion: 结构化混合方法对于平衡自动化工作流生成的灵活性和可靠性至关重要，为民主化数据管道开发提供了可行路径。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [8] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot的代码审查功能在检测安全漏洞方面效果不佳，主要关注低严重性问题而非关键安全漏洞


<details>
  <summary>Details</summary>
Motivation: 评估AI辅助工具在安全编码支持方面的实际效果，特别是在检测安全漏洞方面的能力

Method: 使用来自多个编程语言和应用领域的标记漏洞代码样本集，系统评估Copilot检测常见安全缺陷的能力

Result: Copilot经常无法检测SQL注入、XSS和不安全反序列化等关键漏洞，反馈主要针对编码风格和拼写错误等低严重性问题

Conclusion: AI辅助代码审查的实际效果与预期能力存在显著差距，仍需专用安全工具和人工代码审计来确保软件安全

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [9] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest是一个针对机器学习笔记本的回归测试框架，提供单元级断言和自动化断言生成，提高ML笔记本的可靠性


<details>
  <summary>Details</summary>
Motivation: 解决机器学习笔记本在持续开发中缺乏测试支持的问题，防止因细微bug导致的性能回归和静默错误

Method: 开发了NBTest框架，包括断言API库、JupyterLab插件和自动化断言生成方法，支持在pytest和CI中运行笔记本测试

Result: 在592个Kaggle笔记本中生成21163个断言，变异得分0.57，能够捕获回归错误，用户研究显示易用性评分4.3/5，实用性评分4.24/5

Conclusion: NBTest有效提高了机器学习笔记本的测试覆盖率和可靠性，已被主流ML库采用，通过统计技术最小化非确定性计算带来的测试不稳定性

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [10] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 提出了PromptSE框架，通过创建语义相同但情感和风格不同的提示变体来评估代码生成模型的稳定性，发现性能与稳定性是解耦的优化目标。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型对提示词 phrasing 的敏感性未被充分研究，相同需求用不同情感或沟通风格表达会产生不同输出，而现有基准只关注峰值性能。

Method: 创建语义等价的提示变体（含情感和个性模板），使用概率感知连续评分或二元通过率评估稳定性，提出AUC-E指标进行跨模型比较。

Result: 在14个模型（Llama、Qwen、DeepSeek）上的研究表明，性能和稳定性是解耦的优化目标，揭示了架构和规模相关的模式，挑战了关于模型鲁棒性的常见假设。

Conclusion: PromptSE框架支持快速筛选闭源模型和详细稳定性分析，使从业者能够量化性能稳定性权衡，将提示稳定性定位为与性能和公平性互补的评估维度。

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [11] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 本文提出CodeEraser方法，通过机器遗忘技术有效擦除代码语言模型中敏感信息的记忆，无需完全重新训练，在保持模型功能的同时解决隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型存在敏感信息记忆问题，传统方法需要完全重新训练且计算成本高昂，需要寻找更高效的解决方案。

Method: 使用梯度上升为基础的机器遗忘方法，包括vanilla、constraint-based方法和提出的CodeEraser变体，选择性擦除代码中的敏感记忆片段。

Result: 在CodeParrot、CodeGen-Mono和Qwen2.5-Coder三个模型上的实验验证了CodeEraser能有效擦除目标敏感记忆同时保持模型效用。

Conclusion: 机器遗忘是解决代码语言模型隐私漏洞的有效方法，CodeEraser在效率和效果上都表现出色。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [12] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分析了大型推理模型(LRMs)在代码生成中的推理行为，通过手动标注推理轨迹建立了包含15种推理行为的分类法，揭示了不同模型的推理模式差异及其与代码正确性的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)在代码生成方面表现出色，但对其推理模式如何影响生成代码质量的系统性分析还很缺乏，需要深入研究这些模型的推理行为。

Method: 使用代码生成任务提示多个最先进的LRMs，应用开放式编码手动标注推理轨迹，从中推导出LRM推理行为的分类法，包含15种推理动作和四个阶段。

Result: 发现LRMs遵循类似人类的编码工作流程，复杂任务会引发额外动作；不同模型推理方式差异明显(Qwen3迭代式，DeepSeek-R1-7B线性)；单元测试创建和脚手架生成等动作与代码正确性强相关；基于发现的轻量级提示策略能改善代码生成质量。

Conclusion: 研究结果为推进自动代码生成提供了重要见解和实践意义，揭示了LRMs的推理行为模式及其对代码质量的影响。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [13] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS是首个基于频谱的多智能体系统故障归因方法，通过轨迹重放和频谱分析来识别导致失败的智能体行为


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在自动化复杂任务时存在故障，但故障归因研究不足且人工成本高，阻碍了系统调试和改进

Method: 提出FAMAS方法，通过系统性的轨迹重放和抽象，然后进行频谱分析，使用专门为MAS设计的可疑度公式，整合智能体行为组和动作行为组两个关键因素

Result: 在Who and When基准测试中对12个基线方法进行评估，FAMAS表现出优越性能，优于所有对比方法

Conclusion: FAMAS为多智能体系统故障归因提供了有效的自动化解决方案，有助于系统调试和改进

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [14] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 该论文分析了SWE-Bench中自动化问题解决的失败模式，提出了包含3个阶段、9个类别和25个子类别的失败分类法，并设计了专家-执行者协作框架来解决认知死锁和推理错误问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理工具在自动化问题解决中仍有大量失败案例，且现有评估主要关注聚合指标，难以诊断具体失败原因和指导针对性改进。

Method: 首先分析三种SOTA工具在SWE-Bench-Verified中的表现，然后对150个失败案例进行系统手动分析建立失败分类法，最后提出专家-执行者协作框架来纠正推理错误和打破认知死锁。

Result: 建立了全面的失败模式分类法，发现代理式架构主要失败源于推理错误和认知死锁，提出的协作框架解决了领先单代理22.2%的先前无法解决的问题。

Conclusion: 通过诊断性评估和协作设计可以构建更强大的代理，失败模式分析为改进自动化问题解决系统提供了重要指导。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [15] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 该研究探索了如何将传统软件开发流程（瀑布模型、V模型、敏捷）作为协调框架应用于基于LLM的多智能体系统，通过132次实验发现不同流程在代码质量、成本和效率方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索传统软件开发流程能否作为LLM多智能体系统的协调框架，并分析其对代码质量、成本和生产率的影响。

Method: 在3种流程模型和4种GPT变体下执行11个软件项目，共132次运行，使用标准化指标评估输出结果（文件数、代码行数、执行时间、token使用量、代码异味和bug检测）。

Result: 流程模型和LLM选择都显著影响系统性能：瀑布模型最有效率，V模型产生最冗长的代码，敏捷开发获得最高代码质量但计算成本更高。

Conclusion: 传统软件流程可有效应用于LLM多智能体系统，但需要在质量、成本和适应性之间权衡，流程选择应根据项目目标（效率、健壮性或结构化验证）来决定。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [16] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought推理虽然提升LLM性能但带来高计算成本，研究发现过长推理反而有害。提出SEER自适应框架压缩推理过程，在保持精度的同时减少42.1%推理长度。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽然能提高大语言模型在算术、逻辑和常识任务中的准确性和鲁棒性，但带来了高计算成本（延迟、内存使用和KV缓存需求），特别是在需要简洁确定性输出的软件工程任务中。研究发现过长推理会导致截断、精度下降和高达5倍的延迟。

Method: 提出SEER（Self-Enhancing Efficient Reasoning）自适应框架，结合Best-of-N采样和任务感知自适应过滤，通过预推理输出动态调整阈值来压缩Chain-of-Thought推理过程，减少冗余和计算开销。

Result: 在三个软件工程任务和一个数学任务上的评估显示，SEER平均缩短Chain-of-Thought推理长度42.1%，通过减少截断提高准确性，并消除了大多数无限循环问题。

Conclusion: SEER是一种实用方法，能使Chain-of-Thought增强的大语言模型在资源受限条件下更加高效和鲁棒，挑战了"推理越长越好"的假设。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个动态框架，将图像-文本交错生成重新定义为工具使用问题，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型存在"单一工具"瓶颈，仅限于合成图像，难以处理需要事实基础或编程精度的任务，需要更灵活的框架来协调多种专业视觉工具。

Method: 设计强化学习框架，让中心LLM/MLLM代理智能协调在线图像搜索、扩散生成、代码执行和图像编辑等专业工具，采用结合规则逻辑和LLM评估的混合奖励系统。

Result: 在四个基准测试中大幅超越现有方法，达到最先进性能，并引入了新颖的测试时扩展策略获得额外性能提升。

Conclusion: LLM-I框架成功解决了当前模型的局限性，通过工具协调和强化学习实现了更灵活、准确的图像-文本交错生成能力。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [18] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，推导了贝叶斯风险值函数与真实值函数之间的渐近正态性关系，并提出了基于后验采样的在线RL和CMAB算法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中由于数据不足导致的认知不确定性，通过贝叶斯风险规避方法来处理模型参数的不确定性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导渐近正态性理论，提出基于后验采样的在线强化学习和上下文多臂老虎机算法。

Result: 理论分析表明贝叶斯风险规避方法会悲观地低估原始值函数，这种差异随风险规避强度增加而增大，随数据量增加而减小。建立了次线性遗憾界，并通过数值实验验证了有效性。

Conclusion: 贝叶斯风险规避方法能有效处理认知不确定性，在在线强化学习和CMAB问题中表现出良好的性能，理论分析和实验结果一致。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [19] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: Compute as Teacher (CaT) 是一种将推理时探索转化为监督信号的方法，通过合成多个并行rollout的结果生成参考答案，无需外部监督即可优化模型性能


<details>
  <summary>Details</summary>
Motivation: 解决在无监督的后训练阶段如何获得学习信号的问题，探索如何将推理时的计算资源转化为有效的监督信号

Method: 使用当前策略生成一组并行rollout，通过冻结的初始策略（anchor）协调冲突和遗漏来合成单一参考答案，将其转化为奖励信号。对于可验证任务使用程序等价性，对于不可验证任务使用自提出的可审计标准并由独立LLM评分

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上显著提升性能（MATH-500上最高+27%，HealthBench上+12%）。结合强化学习（CaT-RL）后获得进一步增益（最高+33%和+30%）

Conclusion: CaT能够有效将推理时计算转化为监督信号，性能随rollout数量增加而提升，合成方法可能优于多数投票，训练后的策略可以超越初始教师信号

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练提高思维链推理忠实性的方法，通过生成忠实/不忠实推理对来训练模型偏好因果一致的推理路径，在多个推理任务上显著提升了忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱且不可信。虽然已有方法主要关注测量忠实性，但系统性改进方法仍然有限。

Method: 提出FRIT方法：1）通过在模型生成的思维链中对单个推理步骤进行干预，生成合成训练数据（忠实/不忠实推理对）；2）应用直接偏好优化训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上，FRIT在GSM8K任务上将Mistral的忠实推理提高了3.4个百分点，同时准确性提高了7.6个百分点。

Conclusion: FRIT提供了第一个可扩展、无监督的方法来训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [21] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC方法利用世界模型生成想象环境，通过无监督环境设计自动课程训练，在有限数据下实现对新任务的强泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决在实体环境中训练智能体需要大量训练数据或精确仿真的问题，利用离线被动收集数据构建世界模型来生成多样化训练环境

Method: 提出IMAC方法，结合世界模型和无监督环境设计(UED)，自动生成课程化的想象环境来训练智能体

Result: 在程序生成的环境中，仅使用较窄数据集学习的世界模型就能在保留环境上实现强大的迁移性能

Conclusion: 该方法为利用更大规模的基础世界模型训练通用智能体开辟了新途径

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [22] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文提出了Chain of Action (CoA)框架，统一了高级规划和低级控制，通过将抽象动作视为中间推理步骤来解决动作空间选择的难题，在Minecraft中实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决动作空间选择这一关键但未解决的挑战，因为研究发现没有单一动作空间在所有任务中都最优，这给构建通用智能体带来了困境。

Method: 引入Chain of Action (CoA)框架，将抽象动作视为中间推理步骤而非单独策略的命令，统一了高级规划和低级控制；使用多样化动作空间混合训练All-in-One智能体。

Result: CoA框架实现了新的最先进性能，提高了整体任务成功率，超越了专门的基线模型；训练出的统一智能体学习到了更鲁棒和可泛化的策略。

Conclusion: CoA框架成功解决了动作空间选择的困境，通过统一规划和控制在单一模型中实现了更好的性能；同时发布了OpenHA套件促进可重复研究。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [23] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的无人机自主框架Agentic UAVs，通过五层架构实现感知、推理、行动、集成和学习，在搜救模拟中显著提升了检测性能和自主决策能力


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统主要依赖基于规则的控制和窄AI，缺乏上下文感知推理、自主决策和生态系统集成能力，无法在动态不确定任务中灵活适应

Method: 设计五层架构（感知、推理、行动、集成、学习），集成YOLOv11目标检测、GPT-4推理和本地Gemma-3部署，基于ROS2和Gazebo构建原型系统

Result: 在模拟搜救场景中，检测置信度从0.72提升到0.79，人员检测率从75%提升到91%，行动建议率从4.5%大幅提升到92%

Conclusion: 适度的计算开销能够实现质的自主性提升和生态系统集成，为大语言模型驱动的无人机自主系统提供了有效解决方案

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [24] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2是一个完全自动化的强化学习代理生成框架，通过LLM驱动的智能生成，将自然语言任务描述和环境代码转换为高性能RL解决方案，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理开发需要大量专业知识和长时间迭代，失败率高且可访问性有限。需要实现完全自动化的RL代理设计来降低门槛和提高效率。

Method: 采用双代理架构：生成器代理作为自主AI设计器分析任务并生成可执行RL代理，目标代理是自动生成的RL代理。框架将RL开发分解为MDP建模和算法优化两个阶段，基于模型上下文协议提供统一框架。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent^2始终优于人工设计的解决方案，性能提升高达55%，平均表现显著提升。

Conclusion: 这项工作建立了智能代理设计和优化其他代理的新范式，实现了真正端到端的闭环自动化，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [25] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作通常只关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未被系统性地理解。

Method: 构建了一个模块化的引导框架，收集了安全相关的主要和次要行为数据集，评估了五种流行引导方法在Qwen-2.5-7B和Llama-3.1-8B模型上的效果。

Result: 发现强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导的效果具有高度情境依赖性，需要仔细考虑方法、模型和目标的组合选择。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [26] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 为Claude Code代理配备社交媒体和日志工具后，在34个Aider多语言Python编程挑战中，协作工具显著提升了最难问题的解决性能，成本降低15-40%，轮次减少12-27%，完成时间加快12-38%。


<details>
  <summary>Details</summary>
Motivation: 研究是否通过赋予LLM代理人类自然使用的协作工具和自主权，能够提升其问题解决性能。

Method: 为Claude Code代理配备基于MCP的社交媒体和日志工具，允许它们自主使用这些工具来解决编程挑战。

Result: 协作工具对最难问题表现提升显著：成本降低15-40%，轮次减少12-27%，完成时间加快12-38%。不同模型自然采用不同协作策略，Sonnet 3.7广泛使用工具，Sonnet 4选择性地依赖基于日志的语义搜索。

Conclusion: AI代理在其能力边界处能够系统性地从人类启发的协作工具中受益，表明自适应协作界面可作为推理增强器而非通用效率提升工具。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [27] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个用于在基于LLM的社会模拟中系统化指定智能体行为的工具包，通过显式编程认知偏见来解决传统自然语言描述方法的不一致性问题


<details>
  <summary>Details</summary>
Motivation: 传统使用自然语言隐式描述智能体行为的方法无法在不同模型间产生一致行为，且无法准确捕捉描述的细微差别，需要更系统化的行为规范方法

Method: CoBRA包含两个组件：1）认知偏见指数 - 通过经典社会科学实验量化智能体反应来测量认知偏见；2）行为调节引擎 - 将智能体行为与受控认知偏见对齐

Result: 评估显示CoBRA能够以模型无关的方式精确编程社交智能体中展示的认知偏见

Conclusion: CoBRA提供了一个有效的工具包，能够系统化地指定和控制基于LLM的社交智能体的认知偏见行为

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [28] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 该论文提出了State-aware Reasoning (StaR)训练方法，解决多模态代理在GUI切换控制中的不可靠性问题，特别是在当前状态与期望状态匹配时的执行错误。


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在图形用户界面(GUI)控制中，特别是在切换控制指令执行方面存在不可靠性，这成为GUI交互的关键瓶颈。

Method: 构建状态控制基准测试，提出StaR训练方法，教导代理感知当前切换状态、分析指令中的期望状态，并相应执行动作。

Result: 在三个多模态代理上的实验表明，StaR能将切换指令执行准确率提高30%以上，在三个公共基准测试中也提升了通用任务性能。

Conclusion: StaR方法有效解决了GUI切换控制问题，在动态环境评估中显示出实际应用的潜力。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [29] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的基于探索的GUI代理框架，通过五个创新模块解决工业管理中的关键挑战，在DCIM平台上显著优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战，现有RPA和通用LLM GUI代理在工业管理中存在五个关键问题需要解决。

Method: 提出InfraMind框架，包含五个模块：系统搜索探索、内存驱动规划、高级状态识别、结构化知识蒸馏和多层安全机制。

Result: 在开源和商业DCIM平台上的广泛实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了工业环境中的特定挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [30] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习进行工具集成层次优化的框架，解决了LLM在数学推理中高精度任务的挑战，包括数据构建、细粒度优化和推理增强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得显著进展，但在数值计算和形式符号操作等高精度任务上仍存在困难，需要集成外部工具来弥补这一差距。

Method: 提出THOR框架：1) TIRGen多智能体管道构建高质量工具集成推理数据集；2) 分层强化学习策略联合优化轨迹级问题解决和步骤级代码生成；3) 推理时利用工具反馈进行动态错误修正。

Result: 在多个数学基准测试上达到同类规模模型的最先进性能，在代码基准测试上也获得一致改进，展现出良好的跨模型泛化能力。

Conclusion: THOR通过工具集成和分层强化学习有效提升了LLM在高精度数学推理任务上的表现，为工具增强的推理提供了有效解决方案。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [31] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，通过整合任务分配、数据标注和质量/成本管理，为LLM、SLM和人类专家提供端到端的协同标注流程控制。


<details>
  <summary>Details</summary>
Motivation: 当前NLP标注方法主要关注标注步骤本身，缺乏对多源标注（LLM、SLM、人类专家）的动态调度和质量-成本权衡的整体流程控制。

Method: 采用多智能体系统架构，实现任务分配、数据标注和质量/成本管理的端到端集成，通过合理任务分配使不同标注源在协同工作流中协同推进。

Result: 在六个多样化多模态分类任务上的广泛实验证明了CrowdAgent的有效性。

Conclusion: CrowdAgent为多源标注提供了统一的流程控制解决方案，能够有效管理复杂的调度和质量-成本权衡问题。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>
