<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.SE](#cs.SE) [Total: 6]
- [tldr.article](#tldr.article) [Total: 8]
- [wechat.article](#wechat.article) [Total: 14]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 提出了一个集成训练-推理框架USB-Rec，通过LLM偏好优化数据集构建和自增强策略，提升LLM在对话推荐系统中的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的对话推荐系统主要关注利用LLM的总结和分析能力，而忽视了训练问题，需要从模型层面提升LLM在对话推荐中的表现

Method: 1. 设计基于LLM的偏好优化数据集构建策略用于RL训练；2. 在推理阶段提出自增强策略进一步挖掘RL训练获得的对话推荐潜力

Result: 在多个数据集上的实验表明，该方法持续优于之前的最先进方法

Conclusion: USB-Rec框架通过集成训练和推理策略，有效提升了LLM在对话推荐任务中的性能

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [2] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: MARS是一个基于角色的多智能体协作框架，通过模拟审稿流程来提升LLM的推理能力，在保持与MAD相当准确性的同时，将token使用量和推理时间减少约50%。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论(MAD)方法虽然能提升LLM的推理能力，但由于涉及多个智能体频繁交互，带来了巨大的计算开销。需要一种既能保持推理质量又能控制计算成本的新方法。

Method: 提出MARS框架，包含作者智能体生成初始方案、审稿人智能体独立提供决策和评论、元审稿人整合反馈并指导修订的角色分工设计，避免审稿人之间的交互成本。

Result: 在多个基准测试中，MARS与MAD的准确性相当，但token使用量和推理时间均减少了约50%。

Conclusion: MARS通过角色化协作设计有效平衡了推理质量与计算效率，为多智能体推理提供了更实用的解决方案。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [3] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: SummQ是一个新颖的对抗性多智能体框架，通过总结和提问两个互补领域的专业智能体协作来解决长文档摘要中的信息丢失、事实不一致和连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在长文档摘要任务中存在信息丢失、事实不一致和连贯性问题的挑战，现有方法在处理过长文档时表现不佳。

Method: 采用对抗性多智能体框架，包含总结生成器、总结评审器、问题生成器和问题评审器，通过检查者智能体验证摘要是否包含回答问题所需信息，实现迭代优化。

Result: 在三个广泛使用的长文档摘要基准测试中，SummQ在ROUGE、BERTScore指标以及LLM-as-a-Judge和人工评估方面显著优于现有最先进方法。

Conclusion: 这项工作为长文档摘要建立了一种新方法，利用对抗性智能体协作来提高摘要质量。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [4] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语大语言模型的工具调用能力，探讨了阿拉伯语训练数据、指令微调和特定工具微调对性能的影响，并创建了阿拉伯语工具调用数据集。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用研究主要集中于英语，缺乏对其他语言（如阿拉伯语）的研究，需要了解如何为阿拉伯语开发有效的工具增强代理。

Method: 通过翻译和适配两个开源工具调用数据集到阿拉伯语，使用基础版和后训练版的阿拉伯语LLM进行广泛实验，研究三个关键问题：阿拉伯语数据的必要性、指令微调的影响、特定工具微调的价值。

Result: 研究结果为开发稳健的阿拉伯语工具增强代理提供了关键见解，确定了最优策略。

Conclusion: 该研究填补了阿拉伯语工具调用研究的空白，为多语言工具调用能力的发展提供了重要指导。

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [5] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本文提出了两个专门用于评估大语言模型同时遵循多个指令能力的基准测试：ManyIFEval（文本生成，最多10个指令）和StyleMBPP（代码生成，最多6个指令）。研究发现随着指令数量增加，模型性能会下降，并开发了回归模型来预测未见指令组合的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实场景中的广泛应用，需要系统评估它们同时遵循多个指令的能力，这在文本生成和代码生成等基础领域尤为重要。

Method: 创建了两个基准测试ManyIFEval和StyleMBPP，在10个LLMs上进行实验，并开发了三种回归模型（包括使用指令数量作为解释变量的逻辑回归模型）来预测性能。

Result: 实验显示随着指令数量增加，模型性能持续下降。逻辑回归模型能够以约10%的误差预测未见指令组合的性能，且相对较小的样本量（500和300）就足够进行性能估计。

Conclusion: 该研究为评估LLMs的多指令遵循能力提供了有效工具，证明了回归模型可以高效预测模型在各种指令组合下的表现。

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [6] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 该论文识别并系统研究了LLM中的文化定位偏见，提出了CultureLens基准来量化这种偏见，并开发了基于代理的缓解方法来减少生成内容中的文化偏见。


<details>
  <summary>Details</summary>
Motivation: LLM在生成内容时存在文化定位偏见，倾向于从主流美国文化视角生成内容，而将其他文化视为外来者，这可能导致公平性问题。

Method: 提出了CultureLens基准（4000个生成提示和3个评估指标），通过文化访谈脚本生成任务评估偏见。开发了两种推理时缓解方法：基于提示的FIP方法和基于代理的MFA框架（包括单代理和多代理两种管道）。

Result: 对5个最先进LLM的实证评估显示，模型在美国情境下88%以上采用内部视角，但对弱势文化主要采用外部视角。基于代理的方法在缓解偏见方面表现出有效性。

Conclusion: 基于代理的方法是缓解生成式LLM偏见的有前景方向，能够有效减少文化定位偏见。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [7] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 该论文研究了LLM训练数据中语法模板与领域之间的虚假相关性，发现这种相关性会降低模型性能，甚至可能被用于绕过安全微调。


<details>
  <summary>Details</summary>
Motivation: 动机是理解LLM如何处理任务指令中的语法、语义和领域信息，特别是识别语法与领域之间的虚假相关性对模型行为的影响。

Method: 方法包括：1）使用合成训练数据集分析语法-领域相关性；2）开发评估框架检测已训练模型中的这种现象；3）在OLMo-2和GPT-4o等模型上进行实验验证；4）进行安全微调案例研究。

Result: 结果显示：1）语法-领域相关性会降低OLMo-2模型在实体知识任务上的性能（平均0.51±0.06）；2）在FlanV2数据集和多个模型（OLMo-2-7B、Llama-4-Maverick、GPT-4o）中都存在这种现象；3）这种相关性可被用于绕过OLMo-2-7B Instruct和GPT-4o的安全拒绝机制。

Conclusion: 结论强调需要：1）明确测试语法-领域相关性；2）确保训练数据中的语法多样性，特别是在同一领域内，以防止此类虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [8] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 提出了一个结合隐式检索和结构化协作的统一框架，通过Monitor检索模块和分层解决方案精炼机制，在科学推理任务上实现最高准确率并显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在科学推理中的两个主要瓶颈：显式检索导致推理碎片化和多智能体管道稀释强解决方案的问题

Method: 使用基于Monitor的隐式检索模块在token级别集成外部知识，结合分层解决方案精炼(HSR)和质量感知迭代推理(QAIR)机制

Result: 在HLE Bio/Chem Gold上达到48.3%准确率，比最强基线提升13.4点，同时减少53.5%的token使用和43.7%的智能体步骤

Conclusion: 隐式增强和结构化精炼能够克服显式工具使用和统一聚合的低效性，推理失败和知识差距在85%情况下同时出现

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [9] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 提出了RLBFF方法，结合人类偏好和规则验证，通过二元灵活反馈训练奖励模型，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: RLHF缺乏可解释性且易受奖励攻击，RLVR仅限于基于正确性的验证器。需要一种结合人类偏好灵活性和规则验证精确性的方法。

Method: RLBFF从自然语言反馈中提取可二元回答的原则，将奖励模型训练构建为蕴含任务。用户可以自定义原则来定制奖励模型焦点。

Result: 在RM-Bench上达到86.2%，JudgeBench上81.4%（排行榜第一）。Qwen3-32B模型在MT-Bench、WildBench和Arena Hard v2等基准测试中性能匹配或超过o3-mini和DeepSeek R1，推理成本不到5%。

Conclusion: RLBFF方法有效结合了人类反馈的灵活性和规则验证的精确性，提供了可解释且可定制的奖励模型训练方案。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: 本文研究了记忆化影响估计工具的对抗性攻击可行性，提出了一种实用的攻击方法，并验证了现有代理工具的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着影响估计工具在数据估值和负责任机器学习中的广泛应用，需要评估这些工具本身是否容易受到对抗性操纵。

Method: 提出了一种基于伪逆计算的黑盒攻击方法，仅需模型输出访问权，计算开销较小。

Result: 在广泛的图像分类任务上验证了攻击的有效性，表明即使是先进的影响估计代理也容易受到针对性分数操纵。

Conclusion: 影响估计工具存在严重脆弱性，需要开发鲁棒的防御机制。

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [11] [Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations](https://arxiv.org/abs/2509.20478)
*Vivek Myers,Bill Chunyuan Zheng,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出了一种统一对比表示和时序距离框架的方法，利用准度量表示空间结构和额外约束来学习能够实现最优目标达成的后继表示。


<details>
  <summary>Details</summary>
Motivation: 现有的目标条件强化学习（GCRL）方法主要使用两种表示结构框架：对比表示和时序距离。这两种方法各有优势，但都存在局限性。本文旨在结合两者的优点，提出一个统一的框架。

Method: 使用准度量距离参数化来学习最优目标达成距离，即使在次优数据和随机环境中也能工作。该方法保留了蒙特卡洛对比RL方法的稳定性和长时程能力，同时获得了准度量网络参数化的自由拼接能力。

Result: 在现有离线GCRL基准测试中，该方法在对比学习方法难以处理的拼接任务上表现更好，在噪声高维环境中也优于基于准度量网络的方法。

Conclusion: 该方法成功结合了对比表示和时序距离框架的优势，在多个挑战性场景下都表现出色。

Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use
learned state representations to extract goal-reaching policies. Two frameworks
for representation structure have yielded particularly effective GCRL
algorithms: (1) *contrastive representations*, in which methods learn
"successor features" with a contrastive objective that performs inference over
future outcomes, and (2) *temporal distances*, which link the (quasimetric)
distance in representation space to the transit time from states to goals. We
propose an approach that unifies these two frameworks, using the structure of a
quasimetric representation space (triangle inequality) with the right
additional constraints to learn successor representations that enable optimal
goal-reaching. Unlike past work, our approach is able to exploit a
**quasimetric** distance parameterization to learn **optimal** goal-reaching
distances, even with **suboptimal** data and in **stochastic** environments.
This gives us the best of both worlds: we retain the stability and long-horizon
capabilities of Monte Carlo contrastive RL methods, while getting the free
stitching capabilities of quasimetric network parameterizations. On existing
offline GCRL benchmarks, our representation learning objective improves
performance on stitching tasks where methods based on contrastive learning
struggle, and on noisy, high-dimensional environments where methods based on
quasimetric networks struggle.

</details>


### [12] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: SIL-C是一个新框架，通过双边惰性学习映射技术确保技能与策略的兼容性，使得增量学习的技能改进能够提升下游策略性能而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 技能增量学习过程中，技能库的演化会破坏与现有技能策略的兼容性，限制策略的可重用性和泛化能力。

Method: 采用双边惰性学习映射技术，动态对齐策略引用的子任务空间与技能解码为智能体行为空间，基于轨迹分布相似性为每个子任务选择合适技能。

Result: 在多种SIL场景下评估表明，SIL-C能够在学习过程中保持演化技能与下游策略的兼容性并确保效率。

Conclusion: SIL-C框架有效解决了技能增量学习中的兼容性问题，实现了技能改进对下游策略的无缝提升。

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [13] [Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning](https://arxiv.org/abs/2509.20616)
*Hanjiang Hu,Changliu Liu,Na Li,Yebin Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种将多轮任务规划转化为单轮任务推理的新方法，使用Group Relative Policy Optimization (GRPO)进行高效策略优化，在复杂任务规划基准上取得了优于更大基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 训练LLM代理进行复杂多轮任务规划面临稀疏奖励、长视野信用分配和强化学习计算开销等挑战，需要更高效的优化方法。

Method: 将多轮任务规划转化为单轮任务推理问题，使用GRPO方法结合专家轨迹的密集可验证奖励进行策略优化。

Result: 1.5B参数模型在单轮GRPO训练下，在超过30步的长视野规划任务中达到70%的成功率，优于14B参数的基线模型。

Conclusion: 该方法不仅提高了多轮任务的成功概率，还展示了强大的跨任务泛化能力，训练于复杂任务的模型能够成功完成所有更简单的子任务。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
knowledge acquisition, reasoning, and tool use, making them promising
candidates for autonomous agent applications. However, training LLM agents for
complex multi-turn task planning faces significant challenges, including sparse
episode-wise rewards, credit assignment across long horizons, and the
computational overhead of reinforcement learning in multi-turn interaction
settings. To this end, this paper introduces a novel approach that transforms
multi-turn task planning into single-turn task reasoning problems, enabling
efficient policy optimization through Group Relative Policy Optimization (GRPO)
with dense and verifiable reward from expert trajectories. Our theoretical
analysis shows that GRPO improvement on single-turn task reasoning results in
higher multi-turn success probability under the minimal turns, as well as the
generalization to subtasks with shorter horizons. Experimental evaluation on
the complex task planning benchmark demonstrates that our 1.5B parameter model
trained with single-turn GRPO achieves superior performance compared to larger
baseline models up to 14B parameters, with success rates of 70% for
long-horizon planning tasks with over 30 steps. We also theoretically and
empirically validate the strong cross-task generalizability that the models
trained on complex tasks can lead to the successful completion of all simpler
subtasks.

</details>


### [14] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 本文研究发现联邦学习框架下的大语言模型微调仍存在隐私泄露风险，攻击者可以从全局模型中提取训练数据，且模型越大泄露风险越高。作者还提出了一种针对FL的增强攻击策略，并评估了多种隐私保护技术的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习被认为是隐私保护的协作训练框架，但现有研究假设全局模型只包含泛化知识从而保护隐私。本文旨在验证在联邦学习环境下微调大语言模型时是否真的能有效保护客户隐私。

Method: 通过大量实验验证联邦学习框架下大语言模型的隐私泄露风险，使用简单的生成方法从全局模型中提取训练数据，并提出一种跟踪全局模型更新的增强攻击策略。同时评估了差分隐私、正则化约束更新和安全对齐等隐私保护技术的效果。

Result: 实验表明攻击者确实可以从联邦学习训练的全局模型中提取训练数据，隐私泄露程度随模型规模增大而增加。增强攻击策略能进一步加剧隐私泄露。评估的隐私保护技术中，差分隐私和模型安全对齐能有效降低风险。

Conclusion: 联邦学习框架下的大语言模型微调仍存在显著的隐私泄露风险，需要采用额外的隐私保护措施。研究为降低FL训练LLM的隐私风险提供了有价值的见解和实用指南。

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [15] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文提出CE-GPPO算法，通过保留被PPO裁剪机制丢弃的低概率token的梯度信号，有效控制策略熵，在数学推理任务上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有PPO及其变体在强化学习优化大语言模型时，由于裁剪机制会丢弃低概率token的梯度信号，而这些token在调节熵动态中起着关键作用。

Method: 提出CE-GPPO算法，以温和有界的方式重新引入被裁剪token的梯度，通过控制裁剪区间外token的梯度幅度来实现探索-利用平衡。

Result: 在数学推理基准测试上的广泛实验表明，CE-GPPO在不同模型规模下均优于强基线方法。

Conclusion: CE-GPPO能有效缓解熵不稳定性，为LLM的强化学习优化提供了更有效的策略熵控制方法。

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [16] [Model-Based Reinforcement Learning under Random Observation Delays](https://arxiv.org/abs/2509.20869)
*Armin Karamzade,Kyungmin Kim,JB Lanier,Davide Corsi,Roy Fox*

Main category: cs.LG

TL;DR: 本文研究了POMDP中随机传感器延迟问题，提出了一种基于模型的滤波方法，能够有效处理观测数据乱序到达的情况，并在Dreamer算法上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实环境中经常存在传感器延迟，但标准强化学习算法通常假设环境感知是即时的。本文旨在解决POMDP中观测数据可能乱序到达的随机延迟问题。

Method: 提出了一种基于模型的滤波过程，通过顺序更新信念状态来处理传入的观测流，并构建了一个简单的延迟感知框架，将该思想整合到基于模型的强化学习中。

Result: 在Dreamer算法上应用该框架，相比为MDP设计的延迟感知基线方法，本文方法表现更优，且在部署期间对延迟分布变化具有鲁棒性。在模拟机器人任务上的实验也验证了方法的有效性。

Conclusion: 明确建模观测延迟对于处理现实环境中的传感器延迟问题至关重要，本文提出的方法能够有效应对随机延迟挑战。

Abstract: Delays frequently occur in real-world environments, yet standard
reinforcement learning (RL) algorithms often assume instantaneous perception of
the environment. We study random sensor delays in POMDPs, where observations
may arrive out-of-sequence, a setting that has not been previously addressed in
RL. We analyze the structure of such delays and demonstrate that naive
approaches, such as stacking past observations, are insufficient for reliable
performance. To address this, we propose a model-based filtering process that
sequentially updates the belief state based on an incoming stream of
observations. We then introduce a simple delay-aware framework that
incorporates this idea into model-based RL, enabling agents to effectively
handle random delays. Applying this framework to Dreamer, we compare our
approach to delay-aware baselines developed for MDPs. Our method consistently
outperforms these baselines and demonstrates robustness to delay distribution
shifts during deployment. Additionally, we present experiments on simulated
robotic tasks, comparing our method to common practical heuristics and
emphasizing the importance of explicitly modeling observation delays.

</details>


### [17] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 本文提出了CLUE框架，通过电路发现技术识别LLM中负责遗忘和保留的神经元，并针对不同类别神经元进行精准微调，解决了现有方法中神经元纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于定位的LLM遗忘方法无法区分负责遗忘不良知识和保留必要技能的神经元，将它们视为单一纠缠组，导致过度遗忘或目标知识擦除不完整的问题。

Method: 采用机制可解释性技术中的电路发现方法，识别遗忘和保留电路，将其转换为合取范式，通过可满足性解确定每个神经元应遗忘还是保留，并针对不同类别神经元设计精准微调策略。

Result: 大量实验表明，与现有定位方法相比，CLUE通过精准的神经元定位实现了更好的遗忘效果和保留效用。

Conclusion: CLUE框架通过区分遗忘和保留电路，解决了神经元纠缠问题，为LLM遗忘提供了更精确有效的方法。

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [18] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出了一种名为二元自编码器（BAE）的新方法，通过在小批量隐藏激活上施加最小熵约束，促进特征独立性和稀疏性，解决了现有方法中特征稀疏性和原子化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解构大语言模型隐藏状态中的原子化数值特征时，通常依赖于在单个训练实例上施加隐式正则化的自编码器，缺乏对实例间全局稀疏性的明确保证，导致大量密集特征，损害了特征的稀疏性和原子化。

Method: 提出二元自编码器（BAE），通过步进函数将隐藏激活离散化为1位，并应用梯度估计实现反向传播，在小批量隐藏激活上强制执行最小熵约束。

Result: BAE在特征集熵计算和特征解缠两方面表现出色：能够可靠估计二元隐藏激活的熵，用于表征LLM的推理动态和上下文学习；在特征提取方面避免了密集特征，同时产生最多可解释特征。

Conclusion: BAE作为一种特征提取器具有有效性，能够促进特征独立性和稀疏性，优于基线方法。

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [19] [MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction](https://arxiv.org/abs/2509.21004)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为MAIFormer的多智能体倒置Transformer架构，用于预测多架飞机的飞行轨迹，通过两个关键注意力模块捕获个体飞机的时空模式和多智能体间的交互模式。


<details>
  <summary>Details</summary>
Motivation: 多智能体飞行轨迹预测对于理解空中交通流至关重要，但面临建模个体行为、复杂交互以及结果可解释性的挑战。

Method: 提出MAIFormer框架，包含掩码多元注意力模块（捕获个体时空模式）和智能体注意力模块（建模多智能体社交模式）。

Result: 在仁川国际机场真实ADS-B数据集上的实验表明，MAIFormer在多个指标上表现最佳，优于其他方法。

Conclusion: MAIFormer不仅预测性能优越，还能产生人类可解释的结果，提高了模型的透明度和空中交通控制的实际效用。

Abstract: Flight trajectory prediction for multiple aircraft is essential and provides
critical insights into how aircraft navigate within current air traffic flows.
However, predicting multi-agent flight trajectories is inherently challenging.
One of the major difficulties is modeling both the individual aircraft
behaviors over time and the complex interactions between flights. Generating
explainable prediction outcomes is also a challenge. Therefore, we propose a
Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture
that predicts multi-agent flight trajectories. The proposed framework features
two key attention modules: (i) masked multivariate attention, which captures
spatio-temporal patterns of individual aircraft, and (ii) agent attention,
which models the social patterns among multiple agents in complex air traffic
scenes. We evaluated MAIFormer using a real-world automatic dependent
surveillance-broadcast flight trajectory dataset from the terminal airspace of
Incheon International Airport in South Korea. The experimental results show
that MAIFormer achieves the best performance across multiple metrics and
outperforms other methods. In addition, MAIFormer produces prediction outcomes
that are interpretable from a human perspective, which improves both the
transparency of the model and its practical utility in air traffic control.

</details>


### [20] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: DELTA-Code是一个用于评估LLM学习能力和迁移能力的合成编码基准，通过强化学习训练模型解决预训练模型无法解决的问题家族，并测试其向分布外数据的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能够通过强化学习获得真正新的推理策略，而不仅仅是预训练中已有的技能。

Method: 使用DELTA-Code基准，通过模板化问题生成器隔离推理技能，采用强化学习训练，探索分阶段预热、经验回放、课程训练等关键训练要素。

Result: 发现模型经历突发的顿悟阶段转变，在长时间零奖励后突然达到近乎完美的准确率；在家族内部和技能重组方面有显著提升，但在转换性案例中存在持续弱点。

Conclusion: DELTA提供了一个清晰的测试平台，用于探索RL驱动推理的极限，理解模型如何超越现有先验获得新的算法技能。

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [21] [Actor-Critic without Actor](https://arxiv.org/abs/2509.21022)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: ACA是一种轻量级强化学习框架，消除了显式的行动者网络，直接从噪声级评论家的梯度场生成动作，简化了架构并保持了多模态行为表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统的行动者-评论家方法需要单独的行动者和评论家网络，训练复杂且对超参数敏感，限制了在大规模函数逼近器中的可扩展性。扩散模型虽然能表达多模态行为但引入额外设计复杂性和计算负担。

Method: 提出ACA框架，取消显式行动者网络，通过噪声级评论家的梯度场直接生成动作，保持策略改进与评论家最新价值估计紧密对齐。

Result: 在标准在线RL基准测试中，ACA相比标准行动者-评论家和最先进的基于扩散的方法，实现了更优的学习曲线和竞争性性能。

Conclusion: ACA为在线强化学习提供了一个简单而强大的解决方案，结合了简洁性和表达能力。

Abstract: Actor-critic methods constitute a central paradigm in reinforcement learning
(RL), coupling policy evaluation with policy improvement. While effective
across many domains, these methods rely on separate actor and critic networks,
which makes training vulnerable to architectural decisions and hyperparameter
tuning. Such complexity limits their scalability in settings that require large
function approximators. Recently, diffusion models have recently been proposed
as expressive policies that capture multi-modal behaviors and improve
exploration, but they introduce additional design choices and computational
burdens, hindering efficient deployment. We introduce Actor-Critic without
Actor (ACA), a lightweight framework that eliminates the explicit actor network
and instead generates actions directly from the gradient field of a noise-level
critic. This design removes the algorithmic and computational overhead of actor
training while keeping policy improvement tightly aligned with the critic's
latest value estimates. Moreover, ACA retains the ability to capture diverse,
multi-modal behaviors without relying on diffusion-based actors, combining
simplicity with expressiveness. Through extensive experiments on standard
online RL benchmarks,ACA achieves more favorable learning curves and
competitive performance compared to both standard actor-critic and
state-of-the-art diffusion-based methods, providing a simple yet powerful
solution for online RL.

</details>


### [22] [Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs](https://arxiv.org/abs/2509.21044)
*Honglin Zhang,Qianyue Hao,Fengli Xu,Yong Li*

Main category: cs.LG

TL;DR: 本文通过边缘归因修补技术分析RL微调对LLMs内部机制的影响，发现在线RL训练会增强激活强度和多样性，而DPO方法则表现出不同的内部变化模式。


<details>
  <summary>Details</summary>
Motivation: 探索RL微调为何能超越SFT提升LLM能力的内在机制，特别是不同RL方法对模型内部结构的差异化影响。

Method: 使用边缘归因修补(EAP)技术分析多个模型家族在RL微调前后的内部激活差异，比较PPO、GRPO和DPO等不同RL方法的效果。

Result: 在线RL训练导致激活强度整体增加和激活模式多样性提升，而DPO方法显示出较弱或不一致的内部变化。

Conclusion: RL微调通过重塑信息流使其更具冗余性和灵活性来提升泛化能力，不同RL方法在内部机制上存在显著差异。

Abstract: Large language models (LLMs) acquire extensive prior knowledge through
large-scale pretraining and can be further enhanced via supervised fine-tuning
(SFT) or reinforcement learning (RL)-based post-training. A growing body of
evidence has shown that RL fine-tuning improves the capability of LLMs beyond
what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning
is able to enhance the capability of various LLMs with distinct intrinsic
characteristics remain underexplored. In this study, we draw inspiration from
prior work on edge attribution patching (EAP) to investigate the internal
differences of LLMs before and after RL fine-tuning. Our analysis across
multiple model families shows two robust effects of online RL post-training:
(i) an overall increase in activation intensity, indicating that more internal
pathways are engaged and their signals become stronger, and (ii) greater
diversity in activation patterns, reflected by higher entropy and less
concentrated edge distributions. These changes suggest that RL reshapes
information flow to be both more redundant and more flexible, which may explain
its advantage in generalization. Notably, models fine-tuned with Direct
Preference Optimization (DPO) deviate from these trends, exhibiting
substantially weaker or inconsistent internal changes compared to PPO- and
GRPO-based training. Together, our findings provide a unified view of how RL
fine-tuning systematically alters the internal circuitry of LLMs and highlight
the methodological distinctions between online RL and preference-based
approaches. Our code is open source at
https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.

</details>


### [23] [Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](https://arxiv.org/abs/2509.21126)
*Xiefeng Wu,Jing Zhao,Shu Zhang,Mingyu Hu*

Main category: cs.LG

TL;DR: VARL是一个利用视觉语言模型为强化学习代理提供动作建议的框架，通过增加样本多样性来提高样本效率，特别是在稀疏奖励任务中。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂任务中耗时，需要大量交互步骤来学习最优Q函数。视觉语言动作策略在低层控制方面性能有限，且通常需要任务特定的专家演示进行微调。

Method: VARL框架利用视觉语言模型的领域知识为强化学习代理提供动作建议，而不是设计启发式奖励，从而保证最优性和收敛性不变。

Result: VARL在各种环境和代理设置下显著提高了样本效率，且没有引入显著的计算开销。

Conclusion: VARL是一个通用的在线强化学习框架，使得在真实世界环境中直接从零开始应用强化学习变得可行。

Abstract: Online reinforcement learning in complex tasks is time-consuming, as massive
interaction steps are needed to learn the optimal Q-function.Vision-language
action (VLA) policies represent a promising direction for solving diverse
tasks; however, their performance on low-level control remains limited, and
effective deployment often requires task-specific expert demonstrations for
fine-tuning. In this paper, we propose \textbf{VARL} (\textbf{V}LM as
\textbf{A}ction advisor for online \textbf{R}einforcement \textbf{L}earning), a
framework that leverages the domain knowledge of vision-language models (VLMs)
to provide action suggestions for reinforcement learning agents. Unlike
previous methods, VARL provides action suggestions rather than designing
heuristic rewards, thereby guaranteeing unchanged optimality and convergence.
The suggested actions increase sample diversity and ultimately improve sample
efficiency, especially in sparse-reward tasks. To validate the effectiveness of
VARL, we evaluate it across diverse environments and agent settings. Results
show that VARL greatly improves sample efficiency without introducing
significant computational overhead. These advantages make VARL a general
framework for online reinforcement learning and make it feasible to directly
apply reinforcement learning from scratch in real-world environments.

</details>


### [24] [EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense](https://arxiv.org/abs/2509.21129)
*Wei Huang,De-Tian Chu,Lin-Yuan Bai,Wei Kang,Hai-Tao Zhang,Bo Li,Zhi-Mo Han,Jing Ge,Hai-Feng Lin*

Main category: cs.LG

TL;DR: EvoMail是一个自进化的认知代理框架，用于检测垃圾邮件和网络钓鱼攻击。它通过构建异构邮件图、使用认知图神经网络和LLM进行上下文感知推理，并采用红蓝对抗自进化机制来持续适应新型攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现代垃圾邮件和网络钓鱼攻击已超越传统关键词黑名单和简单启发式方法，采用多模态策略并快速演变。传统检测系统难以整合异构信号和持续适应，导致性能快速下降。

Method: 1. 构建统一的异构邮件图，融合文本内容、元数据和嵌入资源；2. 使用认知图神经网络和LLM进行上下文感知推理；3. 采用红蓝对抗自进化循环：红队生成新规避策略，蓝队从失败中学习并压缩经验到记忆模块。

Result: 在真实数据集（Enron-Spam、Ling-Spam、SpamAssassin、TREC）和合成对抗变体上的实验表明，EvoMail在检测准确性、对演变垃圾邮件策略的适应性和推理可解释性方面均优于最先进的基线方法。

Conclusion: EvoMail作为一个弹性和可解释的防御框架，具有对抗下一代垃圾邮件和网络钓鱼威胁的潜力。

Abstract: Modern email spam and phishing attacks have evolved far beyond keyword
blacklists or simple heuristics. Adversaries now craft multi-modal campaigns
that combine natural-language text with obfuscated URLs, forged headers, and
malicious attachments, adapting their strategies within days to bypass filters.
Traditional spam detection systems, which rely on static rules or
single-modality models, struggle to integrate heterogeneous signals or to
continuously adapt, leading to rapid performance degradation.
  We propose EvoMail, a self-evolving cognitive agent framework for robust
detection of spam and phishing. EvoMail first constructs a unified
heterogeneous email graph that fuses textual content, metadata (headers,
senders, domains), and embedded resources (URLs, attachments). A Cognitive
Graph Neural Network enhanced by a Large Language Model (LLM) performs
context-aware reasoning across these sources to identify coordinated spam
campaigns. Most critically, EvoMail engages in an adversarial self-evolution
loop: a ''red-team'' agent generates novel evasion tactics -- such as character
obfuscation or AI-generated phishing text -- while the ''blue-team'' detector
learns from failures, compresses experiences into a memory module, and reuses
them for future reasoning.
  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,
SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that
EvoMail consistently outperforms state-of-the-art baselines in detection
accuracy, adaptability to evolving spam tactics, and interpretability of
reasoning traces. These results highlight EvoMail's potential as a resilient
and explainable defense framework against next-generation spam and phishing
threats.

</details>


### [25] [GRPO is Secretly a Process Reward Model](https://arxiv.org/abs/2509.21154)
*Michael Sullivan*

Main category: cs.LG

TL;DR: 本文证明了GRPO强化学习算法在特定条件下会诱导出非平凡的过程奖励模型(PRM)，并提出了λ-GRPO改进算法来缓解原算法中的缺陷，在降低训练成本的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索GRPO算法中隐藏的过程奖励模型结构，并解决原算法中过程步骤分布不均匀导致的探索和利用问题，以降低显式定义PRM的高成本。

Method: 首先理论证明GRPO算法在特定假设下会诱导PRM，然后通过实验验证这些假设在现实条件下成立。基于GRPO-as-a-PRM框架，识别算法缺陷并提出λ-GRPO改进方法。

Result: 实验表明，使用λ-GRPO训练的LLM在验证准确率和下游推理任务上的表现均优于标准GRPO，且达到峰值性能更快。同时证明了可以利用GRPO内置的PRM结构来替代昂贵的显式PRM定义。

Conclusion: 研究结果表明，通过利用GRPO算法中隐藏的PRM结构，可以在几乎不影响训练时间和成本的情况下提升模型性能，这对昂贵的显式PRM定义方法提出了质疑。

Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial
process reward model (PRM), under certain assumptions regarding within-group
overlap of token sequences across completions. We then show empirically that
these assumptions are met under real-world conditions: GRPO does in fact induce
a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a
flaw in the GRPO objective: non-uniformly distributed process steps hinder both
exploration and exploitation (under different conditions). We propose a simple
modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and
show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy
and performance on downstream reasoning tasks$-$and reach peak performance more
rapidly$-$than LLMs trained with standard GRPO. Our results call into question
the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is
possible to instead leverage the hidden, built-in PRM structure within the
vanilla GRPO algorithm to boost model performance with a negligible impact on
training time and cost.

</details>


### [26] [Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say](https://arxiv.org/abs/2509.21164)
*Jacob Fein-Ashley,Dhruv Parikh,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: MoT是一种简单的方法，通过在共享潜在空间中让异构专家模型进行潜在层协作，实现多LLM的协同工作，无需迭代聚合且推理效率高。


<details>
  <summary>Details</summary>
Motivation: 现有的多LLM方法存在局限性：要么只路由到少数专家独立生成，要么通过昂贵的多轮交换聚合输出，要么需要架构同质性进行权重融合。需要一种更高效的异构模型协作方法。

Method: 使用轻量级路由器选择top-K专家并指定主专家，通过交互层将隐藏状态投影到共享潜在空间，主专家对活跃同行进行交叉注意力计算。预训练专家保持冻结，只训练路由器和交互层。

Result: 在5个分布内和3个分布外基准测试中，MoT超越了当前最先进的路由和聚合方法Avengers，分别提升0.38%和2.92%，且显著优于单个最佳模型。

Conclusion: MoT提供了一种简单的潜在空间机制来组合异构LLM，是实现更广泛多LLM协作的实用步骤。

Abstract: Open-source Large Language Models (LLMs) increasingly specialize by domain
(e.g., math, code, general reasoning), motivating systems that leverage
complementary strengths across models. Prior multi-LLM approaches either (i)
route a query to one or a few experts and generate independently, (ii)
aggregate outputs from each model via costly multi-turn exchanges, or (iii)
fuse weights into a single model-typically requiring architectural homogeneity.
We introduce Mixture of Thoughts (MoT), a simple method for latent-level
collaboration among heterogeneous experts under a global routing scheme. For
each query, a lightweight router selects top-$K$ experts and designates a
primary expert; uniformly placed interaction layers project hidden states into
a shared latent space where the primary expert performs cross-attention over
its active (selected) peers. Pre-trained experts remain frozen; only the router
and the lightweight interaction layers are trained with a novel joint training
objective that improves both the expert selection and inter-expert
collaboration. Across five in-distribution (ID) and three out-of-distribution
(OOD) benchmarks, MoT surpasses the current routing and aggregation-based
state-of-the-art, Avengers, by $+0.38\%$ and $+2.92\%$, respectively. Further,
MoT significantly outperforms the best-performing single model. It achieves
this with single-pass inference, runtime comparable to routing baselines, and
none of the overheads of iterative aggregation. MoT offers a simple
latent-space mechanism for combining heterogeneous LLMs, a practical step
toward broader multi-LLM collaboration. Our code is publicly available at
https://github.com/jacobfa/mot.

</details>


### [27] [Tree Search for LLM Agent Reinforcement Learning](https://arxiv.org/abs/2509.21240)
*Yuxiang Ji,Ziyu Ma,Yong Wang,Guanhua Chen,Xiangxiang Chu,Liaoni Wu*

Main category: cs.LG

TL;DR: 提出了基于树搜索的分组智能体强化学习方法Tree-GRPO，通过树结构轨迹构建步进式过程监督信号，解决长周期多轮智能体任务中稀疏监督问题。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖结果奖励的强化学习方法在长周期多轮智能体任务中存在稀疏监督问题，需要更有效的监督信号。

Method: Tree-GRPO方法：每个树节点代表完整的智能体交互步骤，通过共享公共前缀增加固定token预算内的rollout数量，利用树结构轨迹构建步进式过程监督信号，在树内和树间层面估计分组相对优势。

Result: 在11个数据集和3类QA任务上的实验表明，基于树的强化学习方法优于基于链的方法。

Conclusion: 树结构RL方法能有效解决智能体任务中的稀疏监督问题，理论分析表明树内分组相对策略优化等价于步级直接偏好学习。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the agentic capabilities of large language models (LLMs). In long-term and
multi-turn agent tasks, existing approaches driven solely by outcome rewards
often suffer from the problem of sparse supervision. To address the challenge,
we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped
agent RL method based on tree search, where each tree node represents the
complete agent interaction step. By sharing common prefixes, the tree search
sampling increases the number of rollouts achievable within a fixed budget of
tokens or tool calls. Moreover, we find that the tree-structured trajectory
naturally allows the construction of step-wise process supervised signals even
using only the outcome reward. Based on this, Tree-GRPO estimates the grouped
relative advantages both on intra-tree and inter-tree levels. Through
theoretical analysis, we demonstrate that the objective of intra-tree level
group relative policy optimization is equivalent to that of step-level direct
preference learning. Experiments across 11 datasets and 3 types of QA tasks
demonstrate the superiority of the proposed tree-based RL over the chain-based
RL method.

</details>


### [28] [Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework](https://arxiv.org/abs/2509.21241)
*Yucheng Wang,Ziyang Chen,Md Faisal Kabir*

Main category: cs.LG

TL;DR: 该论文提出了一个基于知识图谱反事实解释的框架，用于解释经过LoRA微调的大语言模型的结构推理和语义行为变化


<details>
  <summary>Details</summary>
Motivation: 理解LoRA微调机制如何改变大语言模型的结构推理和语义行为是一个开放挑战，需要开发可解释的方法来揭示微调模型的内部机制

Method: 构建BioToolKG生物信息学工具知识图谱，设计CFFTLLMExplainer反事实解释器，通过学习图节点和边的软掩码来生成最小结构扰动，同时优化结构稀疏性和语义差异

Result: 应用该框架到微调的LLaMA模型，发现反事实掩码揭示了模型的结构依赖关系，并与LoRA诱导的参数变化相一致

Conclusion: 该工作为理解微调大语言模型的内部机制提供了新见解，并突出了反事实图作为可解释AI的潜在工具

Abstract: The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large
language models (LLMs) to acquire domain-specific knowledge with remarkable
efficiency. However, understanding how such a fine-tuning mechanism alters a
model's structural reasoning and semantic behavior remains an open challenge.
This work introduces a novel framework that explains fine-tuned LLMs via
counterfactuals grounded in knowledge graphs. Specifically, we construct
BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics
tools and design a counterfactual-based fine-tuned LLMs explainer
(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to
generate minimal structural perturbations that induce maximum semantic
divergence. Our method jointly optimizes structural sparsity and semantic
divergence while enforcing interpretability preserving constraints such as
entropy regularization and edge smoothness. We apply this framework to a
fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the
model's structural dependencies and aligns with LoRA-induced parameter shifts.
This work provides new insights into the internal mechanisms of fine-tuned LLMs
and highlights counterfactual graphs as a potential tool for interpretable AI.

</details>


### [29] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出概率平滑策略优化（PSPO）方法，通过平滑当前策略概率来替代传统的比率裁剪，在保持梯度信号的同时创建软信任区域，显著提升大语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法如PPO和GRPO依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性，需要更平滑的优化方法。

Method: PSPO方法在计算重要性比率之前将当前策略的概率平滑到旧策略，类似于标签平滑，创建软信任区域来防止大的不稳定更新。

Result: 在GSM8K等数学推理任务上，GR-PSPO相比裁剪版GRPO性能提升显著（0.5B模型从17.6%提升到39.7%，1.5B模型从37.8%提升到59.4%），推理过程更清晰简洁。

Conclusion: PSPO提供了一种有效的替代比率裁剪的方法，在保持稳定性的同时提升模型性能，特别适用于大语言模型的强化学习微调。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI是一个基于工具的方法，用于大规模规范和检测AI特定代码异味，结合了高级声明性领域特定语言和可扩展的静态分析工具。


<details>
  <summary>Details</summary>
Motivation: AI系统的兴起带来了新的软件问题，现有检测工具往往无法发现AI特定的代码异味，这些异味可能导致不可重现性、静默失败或模型泛化能力差等问题。

Method: 开发了SpecDetect4AI工具，结合高级声明性DSL进行规则规范，并使用可扩展的静态分析工具解释和检测这些规则。

Result: 在826个AI系统（2000万行代码）上评估，实现了88.66%的精确度和88.89%的召回率，优于现有检测工具，SUS评分为81.7/100。

Conclusion: SpecDetect4AI通过专用规则有效支持AI特定代码异味的规范和检测，能够高效分析大型AI系统，展示了效率和可扩展性。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [31] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc是一个基于检索增强生成(RAG)的故障定位框架，通过结合LLM功能提取、语义密集检索和LLM重排序，显著提升了方法级故障定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型(LLM)的故障定位方法在处理复杂系统时存在项目特定知识不足和大型项目导航困难的问题，需要更有效的解决方案。

Method: FaR-Loc采用三阶段方法：1) LLM功能提取模块生成失败行为的自然语言描述；2) 语义密集检索在共享语义空间中检索功能相似的方法；3) LLM重排序模块基于上下文相关性重新排序检索结果。

Result: 在Defects4J基准测试中，FaR-Loc在Top-1准确率上比SoapFL和AutoFL分别提升14.6%和9.1%，在Top-5准确率上分别提升19.2%和22.1%，且无需重新训练即可超越所有基于学习和基于频谱的基线方法。

Conclusion: FaR-Loc通过RAG技术有效结合了LLM和预训练代码嵌入模型的优势，显著提升了故障定位性能，特别是采用包含代码结构的预训练模型(如UniXcoder)可提升Top-1准确率达49.0%。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [32] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究了代码生成中合成验证的瓶颈问题，发现当前验证方法过于严格，过滤了有价值的多样性。通过分析验证设计策略，提出了校准验证与多样化问题-解决方案对相结合的方法来突破验证天花板。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型越来越依赖合成数据，但合成验证器的能力限制了训练数据的质量和多样性，形成了验证天花板瓶颈。

Method: 系统研究验证设计和策略的影响：(i)分析测试复杂性和数量的影响；(ii)探索宽松通过阈值和LLM软验证；(iii)通过对比正确与错误解决方案及人工评估验证必要性。

Result: 更丰富的测试套件平均提升3%的pass@1性能；宽松验证阈值可回收有价值训练数据，提升2-4点pass@1；保留每个问题的多样化正确解决方案带来一致泛化增益。

Conclusion: 当前验证实践过于严格，但不能完全抛弃，需要重新校准。结合校准验证与多样化挑战性问题-解决方案对，可以突破验证天花板，开发更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [33] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge是一个新颖的代码检索框架，通过引入伪代码作为中间模态来更好地对齐自然语言查询和编程语言逻辑，解决了现有方法在语义对齐和代码风格鲁棒性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预训练语言模型的代码搜索方法面临两个关键挑战：人类意图与机器执行逻辑之间的语义鸿沟，以及对多样化代码风格的有限鲁棒性。

Method: PseudoBridge采用两阶段方法：1）使用大语言模型合成伪代码，实现自然语言查询与伪代码的显式对齐；2）引入逻辑不变的代码风格增强策略，生成风格多样但逻辑等价的代码实现，并与伪代码对齐。

Result: 在10个不同预训练语言模型和6种主流编程语言上的实验表明，PseudoBridge在检索准确性和泛化性方面显著优于基线方法，特别是在零样本领域迁移场景下表现突出。

Conclusion: 通过伪代码实现显式逻辑对齐是有效的，PseudoBridge展示了作为稳健、可泛化代码检索解决方案的潜力。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [34] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: 论文介绍了CodeHinter调试助手，结合传统调试工具和LLM技术，帮助新手程序员修复语义错误并促进主动参与调试过程。


<details>
  <summary>Details</summary>
Motivation: 现有AI调试工具容易导致学生对AI过度依赖，缺乏主动参与调试过程，需要设计能促进主动学习的调试助手。

Method: 设计CodeHinter调试助手，整合传统调试工具和LLM技术，通过第二版设计迭代并在本科生群体中进行测试。

Result: 学生认为该工具在解决语义错误方面非常有效，比第一版显著易用，错误定位是最有价值的功能。

Conclusion: AI辅助调试工具应根据用户画像进行个性化定制，以优化与学生的交互效果。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [35] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR是一种基于思维链的微调方法，通过长思维链技术训练LLMs进行多维度代码审查分析，结合最大熵建模和预定义推理路径解决长提示中的上下文丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码审查数据微调的LLMs方法受限于有限或模糊的训练信息，无法像人类审查者那样同时分析代码审查的多个维度。

Method: 提出MelcotCR方法，结合最大熵建模原则和预定义推理路径，利用长思维链技术提供丰富的结构化信息，增强推理过程的逻辑严密性。

Result: 在MelcotCR数据集和CodeReviewer数据集上的实验表明，14B参数的Qwen2.5模型经过MelcotCR微调后，在代码问题检测和描述准确性上超越现有最优方法，性能与671B的DeepSeek-R1模型相当。

Conclusion: MelcotCR方法有效提升了LLMs在代码审查任务中的推理能力，证明了思维链微调策略的优越性。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [36] [How Claude Code is built](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-claude-code-is-built%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/TYpozMrGuNhpLqVejcxKDj62sxTd6aG5aUP4y7dBsxA=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了Claude Code的开发历程，这是一个年收入超过5亿美元的AI驱动开发工具，从简单的命令行工具演变为复杂产品，90%的代码由自身生成。


<details>
  <summary>Details</summary>
Motivation: 展示如何将简单的AI工具发展为成功的商业产品，强调快速开发和自我改进的重要性。

Method: 使用TypeScript、React、Ink、Yoga和Bun等技术栈，通过AI自动生成大部分代码，实现快速迭代开发。

Result: Claude Code成为年收入超过5亿美元的流行开发工具，证明了AI驱动开发的商业可行性。

Conclusion: AI工具可以通过自我改进和快速开发实现商业成功，为类似项目提供了可复制的模式。

Abstract: How Claude Code is built (19 minute read) This article provides a look into the development of Claude Code, a popular AI-powered developer tool that generates over $500M in annual revenue. The tool originated from a simple command-line tool using Claude to identify music, then evolved into a sophisticated product with a tech stack including TypeScript, React, Ink, Yoga, and Bun, with 90% of the code written by itself. Claude Code's success is attributed to its rapid development pace, with fea...

</details>


### [37] [The Day the Linter Broke My Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fillmore-labs.com%2Fposts%2Ferrors-2%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/kbLKxqTlST2eD0II8blSr7RssmBNJkygLSKaYZ0zkCM=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 一个看似有用的linter建议错误地应用了标准方法到自定义同名方法中，违反了Go语言的错误处理原则，导致意外的错误等价性，从而引入了关键bug。


<details>
  <summary>Details</summary>
Motivation: 探讨linter工具在代码审查中可能带来的潜在风险，特别是当自动化建议与语言特定原则冲突时。

Method: 通过具体案例分析linter建议如何错误地修改代码，违反Go语言的错误处理最佳实践。

Result: 发现linter工具可能在不了解语言特定上下文的情况下给出有害建议，导致代码功能异常。

Conclusion: 开发者在依赖linter工具时需要保持批判性思维，理解工具建议背后的语言原则。

Abstract: The Day the Linter Broke My Code (7 minute read) A seemingly helpful linter suggestion inadvertently introduced a subtle but critical bug by incorrectly applying a standard method within a custom method of the same name, violating Go's error-handling principles and leading to unexpected error equivalences.

</details>


### [38] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/hDiK2kkagpHMMC1l79oOwSjQaFLzHnJ5y_4YI5ynKTQ=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 一个看似有用的linter建议错误地应用了标准方法到自定义的同名方法中，违反了Go的错误处理原则，导致意外的错误等价性，引入了关键bug。


<details>
  <summary>Details</summary>
Motivation: 探讨linter工具在代码审查中的潜在风险，特别是当它们错误地应用标准方法到自定义实现时可能导致的严重后果。

Method: 通过具体案例分析，展示linter建议如何在不恰当的上下文中应用标准方法，从而破坏Go语言的错误处理机制。

Result: 发现linter的错误建议导致了微妙的但关键的bug，违反了Go的错误处理最佳实践，造成了意外的错误等价性问题。

Conclusion: 开发者需要谨慎对待linter工具的建议，特别是在涉及自定义方法和标准方法同名的情况下，需要进行仔细的人工审查。

Abstract: The Day the Linter Broke My Code (7 minute read) A seemingly helpful linter suggestion inadvertently introduced a subtle but critical bug by incorrectly applying a standard method within a custom method of the same name, violating Go's error-handling principles and leading to unexpected error equivalences.

</details>


### [39] [Figma Made its Design Tools More Accessible to AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F783828%2Ffigma-make-ai-app-coding-mcp-server-update%3Futm_source=tldrdesign/1/0100019980c40ac4-e250108d-d848-4726-ade2-cd1fd864882c-000000/9ompLl7OF406bC1Tsw3dbVv-_jNYkUZLrrcSnE34KiU=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Figma扩展了其Model Context Protocol服务器，使AI代理能够访问底层代码和视觉设计，支持从Anthropic、Cursor和VS Code等AI编码平台进行远程访问。


<details>
  <summary>Details</summary>
Motivation: 让AI代理更容易访问和使用Figma的设计工具，提高开发者的工作效率和工具的互操作性。

Method: 通过扩展MCP服务器功能，支持远程访问和新增Design Snapshot等功能，将Make文件转换为可编辑图层。

Result: Figma的设计工具现在对AI代理更加开放和可访问，支持多个主流AI编码平台的集成。

Conclusion: 这一改进显著提升了Figma工具在AI驱动开发环境中的可用性和集成能力。

Abstract: Figma Made its Design Tools More Accessible to AI Agents (2 minute read) Figma expanded its Model Context Protocol server to support the Figma Make AI app builder, enabling AI models to access the underlying code, in addition to visual designs. The MCP server now supports remote access from various AI coding platforms, including Anthropic, Cursor, and VS Code, making it more accessible to developers. Additional features, such as Design Snapshot for converting Make files into editable layers a...

</details>


### [40] [How Agentic AI is Accelerating the Autonomous Payment Transition](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finextra.com%2Fblogposting%2F29414%2Fhow-agentic-ai-is-accelerating-the-autonomous-payment-transition%3Futm_source=tldrfintech/1/0100019980fcbb8f-497675e9-e9ba-4104-af45-5984af59a8ac-000000/sIaK2hthNwchp2cWP9UZXC8SWA64FOVAa8Zo_E4rB7s=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agentic AI正在加速支付向自主化转型，通过实时智能、自动化和决策工具重新定义支付方式，为未来工作趋势提供更大支持。


<details>
  <summary>Details</summary>
Motivation: 探索Agentic AI如何通过智能化和自动化改变支付领域，支持现代工作趋势的发展。

Method: 将智能、自动化和实时决策工具整合到金融支付系统中。

Result: Agentic AI已经开始重新定义支付方式，为金融领域带来实时智能和自动化能力。

Conclusion: Agentic AI的智能化和自动化能力将加速支付向自主化转型，为未来金融工作模式提供重要支持。

Abstract: How Agentic AI is Accelerating the Autonomous Payment Transition (8 minute read) Agentic AI is already beginning to redefine how payments are made and the ways in which we can manage our money. The infusion of intelligence, automation, and decision-making tools in real time to the financial landscape can bring far greater benefits to support modern working trends in the future.

</details>


### [41] [OpenAI tests ChatGPT Agent upgrades powered by new Alpha models](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fopenai-tests-chatgpt-agent-upgrades-powered-by-new-alpha-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/UPPRvpM5LLF2SWS7eXHCaeZneCO5tXUXW9Ay-ym9l6U=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI意外发布了基于新Alpha模型的ChatGPT Agent升级，包含'带截断的Agent'和'带提示扩展的Agent'两种模式，但很快撤回


<details>
  <summary>Details</summary>
Motivation: 测试新的Agent模式，探索不同的系统提示设置或底层模型架构

Method: 在模型选择器中添加'alpha models'部分，激活Agent模式，测试截断和提示扩展两种不同方法

Result: 发布似乎是意外行为，很快被撤回，用户只能短暂体验

Conclusion: OpenAI正在积极开发Agent功能，但尚未准备好正式发布

Abstract: OpenAI tests ChatGPT Agent upgrades powered by new Alpha models (2 minute read) Some ChatGPT users have spotted a new 'alpha models' section in the model selector. The models appeared for a limited time, and they activated agent mode. The model naming - 'Agent with truncation' and 'Agent with prompt expansion' - suggests that OpenAI may be experimenting with different system prompt setups or underlying model architectures. The release seemed accidental and was quickly rolled back, making mode...

</details>


### [42] [Meta's Open LLM for Code and World Modeling](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/Vy_bQMMCgmIqPqvbTXNd4lHYc9FfnhYEUMQYOWNyp0U=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta发布了CWM，一个32B参数的仅解码器LLM，通过代码执行轨迹和推理任务训练，用于探索代码生成中的世界模型


<details>
  <summary>Details</summary>
Motivation: 探索代码生成中的世界模型，通过训练模型理解代码执行过程而不仅仅是语法模式

Method: 使用32B参数的仅解码器架构，在代码执行轨迹和推理任务上进行训练

Result: 开发出CWM模型，能够更好地理解代码执行逻辑

Conclusion: 代码执行轨迹训练有助于构建更智能的代码生成模型

Abstract: Meta's Open LLM for Code and World Modeling (5 minute read) Meta has released CWM, a 32B decoder-only LLM trained on code execution traces and reasoning tasks to explore world models in code generation.

</details>


### [43] [Warp Code Officially Launches, Saving Developers 1–2 Hours Daily](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_9_26_primary%26utm_content=tldr/2/01000199858d1c6b-5cda1301-c737-410e-a121-583038e26c7d-000000/5lfZJDqHlcVkRgPdahMPxaqdPM8peKSK6O48HIdcrvg=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Warp Code正式发布，据称能为开发者每天节省1-2小时，通过代码库感知的AI代理提供更准确的代码生成和统一的代码审查界面。


<details>
  <summary>Details</summary>
Motivation: 解决66%开发者对AI生成代码"几乎正确但不完全准确"的挫败感，弥合"接近可用"与"真正有用"之间的差距。

Method: 使用强大的代码库感知代理技术，结合统一的用户界面来审查AI生成的代码。

Result: 早期使用数据显示，Warp Code已经在终端基准测试中表现优异，能够显著提升开发效率。

Conclusion: Warp Code通过改进AI代码生成的准确性和可用性，为开发者提供了实用的生产力工具。

Abstract: Warp Code Officially Launches, Saving Developers 1–2 Hours Daily (Sponsor) 🆕 This month Warp launched Warp Code, Early usage shows it's already saving developers 1-2 hours each day. Why Warp: 66% of developers report being frustrated by AI code that's almost-but-not-quite correct. Warp closes the gap between "almost there" and "actually useful" with a powerful codebase-aware agent that delivers more accurate code + a unified UI to review agent code. ✅ Benchmark-beating: Warp tops Terminal-ben...

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [44] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（昨日 / 近36小时，ET）](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483657&idx=1&sn=c6858a05dc7441d2577634816d4de858&chksm=e9274866b5347b9764c6106dd19e9414c34a2654f60e2c0d26cdf5717eeb817f3a89c7bd50fd#rd)
*每天一遍防止悲伤*

Main category: wechat.article

TL;DR: com/2025/amazon-links-nova-act-its-ai-agent-creator-to-visual-studio-code-cursor-and-kiro/｜**源头优先****一句话结论**：亚马逊推出Nova Act扩展，允许开发者直接在Visual Studio Code、Amazon Kiro和Cursor等主流IDE中构建和测试AI代理，显著提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: com/2025/amazon-links-nova-act-its-ai-agent-creator-to-visual-studio-code-cursor-and-kiro/｜**源头优先****一句话结论**：亚马逊推出Nova Act扩展，允许开发者直接在Visual Studio Code、Amazon Kiro和Cursor等主流IDE中构建和测试AI代理，显著提升开发效率。

</details>


### [45] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> 重点速递（2025年9月24日，ET 时区）](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483651&idx=1&sn=0dccac248c89f3100fecba8be3638421&chksm=e93591f40dda816ac560d75c4f7022007f2c5066e04741ce14311a0ea85aa61770c85a98f3af#rd)
*每天一遍防止悲伤*

Main category: wechat.article

TL;DR: Emergent 的融资成功及其“Vibe 编码平台”的推出，则代表了 Code Agent 技术的另一个重要发展方向——**普惠化**。其重要性体现在：1. **降低软件开发门槛：** Emergent 平台允许任何人借助自主 AI


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Emergent 的融资成功及其“Vibe 编码平台”的推出，则代表了 Code Agent 技术的另一个重要发展方向——**普惠化**。其重要性体现在：1. **降低软件开发门槛：** Emergent 平台允许任何人借助自主 AI

</details>


### [46] [把20多种工具塞进一个搜索框！亲测「<em class="highlight">Agentic</em> Search」后，我关掉了几十个浏览器标签页](http://mp.weixin.qq.com/s?__biz=Mzg3MTkxMjYzOA==&mid=2247507887&idx=1&sn=4ed47967837458f5ca44b86663eb8520&chksm=cf925bd3fc3d6917cc15d901970c9585f88b3d6f9c621cbea1eb7970beafb5342c26cc6a869c#rd)
*AI寒武纪*

Main category: wechat.article

TL;DR: 向Agentic Search发出了如下指令：输入一个超级Prompt：-----MindScribe核心使用场景（痛点）：开会太多： 没空记会议纪要。听课/讲座： 老师讲太快，笔记跟不上。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 向Agentic Search发出了如下指令：输入一个超级Prompt：-----MindScribe核心使用场景（痛点）：开会太多： 没空记会议纪要。听课/讲座： 老师讲太快，笔记跟不上。

</details>


### [47] [<em class="highlight">Agentic</em> Coding表现创新高，全新KAT系列模型上榜SWE-Bench](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650993090&idx=2&sn=b7d3f2ea7e12bebc5aa74c0a850f40ee&chksm=851037c62efedc76198697798ac6756bfd9196a667d7e5079081e426445fe88ce8a3463c579c#rd)
*机器之心*

Main category: wechat.article

TL;DR: 近期，快手 Kwaipilot 团队推出了 KAT 系列两款突破性 Agentic Coding 大模型：开源 32B 参数模型 KAT-Dev-32B 与闭源旗舰模型 KAT-Coder。这两款模型在 Code Intelligence 领域分别体现出轻量级的超强表现和极致性能。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近期，快手 Kwaipilot 团队推出了 KAT 系列两款突破性 Agentic Coding 大模型：开源 32B 参数模型 KAT-Dev-32B 与闭源旗舰模型 KAT-Coder。这两款模型在 Code Intelligence 领域分别体现出轻量级的超强表现和极致性能。

</details>


### [48] [Perplexity AI的宏大棋局：超越答案引擎，构建<em class="highlight">代理</em>式（<em class="highlight">Agentic</em>）互联网](http://mp.weixin.qq.com/s?__biz=MzI5MjMzNDk3OQ==&mid=2247484639&idx=1&sn=afeec6c454799a4885a7b54b886c0a57&chksm=ed8ff415a59602cb110c6cbb955841110f452f6ba6ba366742aba094be80da81e523ddfd817c#rd)
*道哥讲技术*

Main category: wechat.article

TL;DR: 二、 代理式（Agentic）的未来：浏览器是新战场的入口对话的核心，落在了对代理式互联网的构想上。Dimitri预见，未来的网络交互将更多地是“代理与代理”之间的互动。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 二、 代理式（Agentic）的未来：浏览器是新战场的入口对话的核心，落在了对代理式互联网的构想上。Dimitri预见，未来的网络交互将更多地是“代理与代理”之间的互动。

</details>


### [49] [Appier 全线产品升级 <em class="highlight">Agentic</em> AI，打造新世代 ROI 驱动解决方案](http://mp.weixin.qq.com/s?__biz=MzU5Njg2OTQ2Mg==&mid=2247485170&idx=1&sn=ff52d30b7a0d366ddd673530e35b33b9&chksm=fff4fbabcc17bb7ca9a4508658c864e22ca941cd21214eaa9f4d18ae3f3bf37ab5c1bd1b65dc#rd)
*Appier沛星互动科技*

Main category: wechat.article

TL;DR: “Agentic AI 的核心，在于把业务目标转化为可执行的自主应用与自动化工作流程。凭借 Appier 在 AI 与营销科技逾十年的深耕实绩，我们让各司其职的 AI Agents 相互协作，打造端到端的 Agentic 生态系统——如同一支高度协作的专业


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “Agentic AI 的核心，在于把业务目标转化为可执行的自主应用与自动化工作流程。凭借 Appier 在 AI 与营销科技逾十年的深耕实绩，我们让各司其职的 AI Agents 相互协作，打造端到端的 Agentic 生态系统——如同一支高度协作的专业

</details>


### [50] [迈向超级人工智能之路！大模型时序推理和<em class="highlight">Agentic</em>系统的全面综述（UCLA最新）](http://mp.weixin.qq.com/s?__biz=Mzg4Mjg4NTQxMQ==&mid=2247547696&idx=1&sn=ec92ec099ae8ea8d20c2ab1ac790254f&chksm=ced836ea86b71c9ff9fc503e2bdb9acd16adef4289629a3b2b2246c2e7410e0e45e8df4f29e3#rd)
*大模型之心Tech*

Main category: wechat.article

TL;DR: 联合发布了一篇题为《A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models》的重磅综述。这篇综述不仅首次为“时间序列推理”给出了清晰的定义，更构建了一套覆盖“推理结构-任务目标-技术特征”的三维分类框


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 联合发布了一篇题为《A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models》的重磅综述。这篇综述不仅首次为“时间序列推理”给出了清晰的定义，更构建了一套覆盖“推理结构-任务目标-技术特征”的三维分类框

</details>


### [51] [从DeepSeek到<em class="highlight">Agentic</em> AI：风口已变，真正的机会在“意图经济”](http://mp.weixin.qq.com/s?__biz=MzYyMTU3OTYwMw==&mid=2247483695&idx=1&sn=a1b9f3c2625976094f718ef5253522cc&chksm=fe45a21dcf57defcd809e8db4e1e761e2857e859a2a12ce7f6535a1556e63bc473784d3d89d6#rd)
*AI时代的奶爸*

Main category: wechat.article

TL;DR: 趋势背书：Gartner将Agentic AI列为2025十大战略技术趋势之首，并预测到2028年，日常工作决策的15%将由智能体自主完成。真正的升级不是更会对话，而是更会履约。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 趋势背书：Gartner将Agentic AI列为2025十大战略技术趋势之首，并预测到2028年，日常工作决策的15%将由智能体自主完成。真正的升级不是更会对话，而是更会履约。

</details>


### [52] [解锁医疗数据价值：基于<em class="highlight">大模型</em>的医疗数据智能标注智能体](http://mp.weixin.qq.com/s?__biz=MzIxOTc4NzA5NA==&mid=2247491067&idx=1&sn=de0a6dc890263cab3a230f0d1562ac22&chksm=963886e7a76f07086df9d14649b98784225154d214cd188742f4ed2a7ee1d3de9df6fa7728cc#rd)
*卫健智能*

Main category: wechat.article

TL;DR: 利用大模型的语义相似度计算能力，实现智能术语映射，解决数据孤岛和整合难题。模型提示词配置 ，已分析 详情 编辑 保存 2 序号 标签名称 参考值 排序状态 操作 已分 详情 编辑 保存物签 3 1 左主干（lm）狭窄 正常轻度狭窄


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 利用大模型的语义相似度计算能力，实现智能术语映射，解决数据孤岛和整合难题。模型提示词配置 ，已分析 详情 编辑 保存 2 序号 标签名称 参考值 排序状态 操作 已分 详情 编辑 保存物签 3 1 左主干（lm）狭窄 正常轻度狭窄

</details>


### [53] [贵州省行业<em class="highlight">大模型</em>典型应用案例之三 | 无相智研科研<em class="highlight">大模型</em>AI平台](http://mp.weixin.qq.com/s?__biz=MzI1Nzk2MDQzOA==&mid=2247545928&idx=1&sn=bf65572440bcf10942f69c6e1026fd43&chksm=eb9101a779a4741760fe9b6fbc6cf25db314c9ef6bcbddce785fdc6646849ac65cac2b15ed61#rd)
*贵州省大数据发展管理局*

Main category: wechat.article

TL;DR: 基于开源大模型，平台进行针对性的定向优化与微调，开发出适用于跨学科科研的专业大模型。同时，针对生物医药（如蛋白质分析）、材料科学（如新材料设计）等多个垂直领域，打造专用模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 基于开源大模型，平台进行针对性的定向优化与微调，开发出适用于跨学科科研的专业大模型。同时，针对生物医药（如蛋白质分析）、材料科学（如新材料设计）等多个垂直领域，打造专用模型。

</details>


### [54] [刘知远分享《<em class="highlight">大模型</em>智能体发展的关键技术与挑战》](http://mp.weixin.qq.com/s?__biz=MzIxMTM2ODM4MQ==&mid=2247491310&idx=1&sn=ed1463dc722753a33c46e5af665d7943&chksm=961a63153095775bb1ba0fd9073b594f8fa9965c24da7ba1477c1e7cd16a700f43692fb59f68#rd)
*数字经济投融资联盟*

Main category: wechat.article

TL;DR: 我们可以设想，把大模型放到不同领域，有可能是编程领域、数据库领域、系统环境领域，也有可能是物理环境领域或其他各种专业，我们预期这个大模型、智能体能成为软件开发、数据分析、系统触控、机械控制等各个方面专


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们可以设想，把大模型放到不同领域，有可能是编程领域、数据库领域、系统环境领域，也有可能是物理环境领域或其他各种专业，我们预期这个大模型、智能体能成为软件开发、数据分析、系统触控、机械控制等各个方面专

</details>


### [55] [<em class="highlight">大模型</em>是如何学会数数的？| 科普视听](http://mp.weixin.qq.com/s?__biz=MjM5Mzc5MDg2Mw==&mid=2658007418&idx=3&sn=453e47e41acec48881b9d06e709e4cb9&chksm=bc6e98c5ed00211b5d3362de38ce081fc59e99192ab8aa3b47715e34ab84a3d0dc289298f4a3#rd)
*知识就是力量*

Main category: wechat.article

TL;DR: 你应该听说过，现在的AI（人工智能）大模型很厉害，能写文章、作诗、编程序，甚至解答高难度的奥数题。但奇怪的是，有些时候，它居然会在最基础的小学数学题上犯错！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 你应该听说过，现在的AI（人工智能）大模型很厉害，能写文章、作诗、编程序，甚至解答高难度的奥数题。但奇怪的是，有些时候，它居然会在最基础的小学数学题上犯错！

</details>


### [56] [软件谷“国字号”<em class="highlight">大模型</em>队伍扩容](http://mp.weixin.qq.com/s?__biz=MzI1MzAwODUzOA==&mid=2651596324&idx=1&sn=02c0d25455bf445ce54f9b16c05283ad&chksm=f3c3e4e49b8bf9a2596c539c4cf29bd77c0e62e6dbd539edd1af17b6b199e8c6d433e91d7356#rd)
*软件谷创新创业服务中心*

Main category: wechat.article

TL;DR: 大模型主要应用于软件开发，为技术开发者及企业用户提供代码自动生成、代码优化等服务，具备软件端到端自动开发的能力，已经开发了上千款应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型主要应用于软件开发，为技术开发者及企业用户提供代码自动生成、代码优化等服务，具备软件端到端自动开发的能力，已经开发了上千款应用。

</details>


### [57] [TASE | 农业多模态<em class="highlight">大模型</em>：现状、挑战与未来（2025.09最新）](http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498173&idx=1&sn=8e1227afbd8ab02a3b750bd9b17bed81&chksm=fde08dcbe3f1c331870d3591022e10b179b13b4efe5d067eacbf84af26ebd050d9225b6a930d#rd)
*丰农信息*

Main category: wechat.article

TL;DR: 农业多模态大模型（mm-llms）的开发流程，包括数据收集、训练、微调与下游应用 核心结果与讨论 综述表明，MM-LLMs的发展已从最初的Transformer架构演化至今，催生了如OpenAI的GPT系列、Meta的Llama系列、DeepSeek等众多强大的模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 农业多模态大模型（mm-llms）的开发流程，包括数据收集、训练、微调与下游应用 核心结果与讨论 综述表明，MM-LLMs的发展已从最初的Transformer架构演化至今，催生了如OpenAI的GPT系列、Meta的Llama系列、DeepSeek等众多强大的模型。

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [58] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种用于监控AI智能体行为的时间表达式语言，通过监测智能体工具调用和状态转换的执行轨迹来检测与预期行为模式的偏差，解决了传统基于文本匹配的错误检测方法在LLM系统中脆弱的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体系统由于随机生成过程导致输出可变，传统错误检测方法主要依赖输入输出的文本匹配，但由于LLM响应的自然语言变异性，这种方法很脆弱。需要一种能够独立于具体文本输出来验证系统行为的方法。

Method: 借鉴硬件验证中的时序逻辑技术，开发了一种时间表达式语言来监控智能体工具调用和状态转换的执行轨迹。该方法关注智能体动作序列（如工具调用和智能体间通信），而非具体文本内容。

Result: 在三智能体系统上的实验表明，当使用大型模型时，所有时间断言都能满足；但当两个智能体改用较小模型时，执行会违反行为断言，主要由于工具序列不当和协调交接失败。时间表达式成功标记了这些异常。

Conclusion: 该方法为系统监控AI智能体可靠性提供了基础，特别是在关键应用中部署的智能体系统。时间表达式具有双重用途：开发时验证提示工程和护栏有效性，更新时提供回归测试。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [59] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE是一个基于多级反思合成的自学习代理框架，通过训练回顾性语言模型来生成高质量反思，显著提升了复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在生成有意义的反思方面存在挑战，主要由于错误分析不足和依赖罕见的成功轨迹，特别是在复杂任务中。

Method: 提出了SAMULE框架，包含三个互补层次的反思合成：单轨迹学习（微观）、任务内学习（中观）和任务间学习（宏观），并训练回顾性语言模型生成反思。还扩展了基于前瞻的反思机制用于交互设置。

Result: 在TravelPlanner、NATURAL PLAN和Tau-bench三个挑战性基准测试上的实验表明，该方法显著优于基于反思的基线方法。

Conclusion: 精心设计的反思合成和以失败为中心的学习在构建自改进LLM代理中起着关键作用。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [60] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 该研究提出了一种基于智能AI代理的自适应网络安全架构，能够实现动态学习、上下文感知决策，解决传统静态模型在可扩展性、实时检测和上下文响应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态网络安全模型在当前包含云服务、API、移动平台和边缘设备的数字产品生态系统中，难以应对可扩展性、实时检测和上下文响应能力的挑战。

Method: 采用自主目标驱动的智能代理，集成行为基线分析、去中心化风险评分和联邦威胁情报共享等关键技术，在关键生态系统层中部署智能AI代理。

Result: 通过原生云模拟验证，系统能够识别零日攻击并动态修改访问策略，评估结果显示适应性增强、响应延迟降低、检测准确性提高。

Conclusion: 该架构为保护复杂数字基础设施提供了智能可扩展的蓝图，与零信任模型兼容，支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [61] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出了一种结合自回归(AR)和非自回归(NAR)语言模型的新框架，利用NAR模型高效生成中间推理轨迹，再由AR模型生成精确最终答案，在推理任务中实现了26%的性能提升并显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: AR模型生成连贯但推理速度慢，NAR模型速度快但输出质量较低，需要结合两者优势来解决推理密集型任务中的效率和质量问题。

Method: 采用AR和NAR模型集成框架，NAR模型并行生成中间推理轨迹，AR模型基于这些轨迹生成最终答案。

Result: 实验显示该方法比强基线提升26%性能，同时大幅降低推理成本。

Conclusion: AR-NAR集成框架能有效平衡推理质量和效率，为推理密集型任务提供高效解决方案。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [62] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 本文提出了Meta-Memory，一种基于大语言模型的机器人代理，能够构建高密度环境记忆表示，并通过语义和空间模态的联合推理来回答自然语言位置查询。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂环境中有效存储观测记忆并利用这些记忆回答人类关于空间位置查询的关键研究挑战，现有研究在高效记忆检索和集成机制方面存在不足。

Method: 提出Meta-Memory框架，利用LLM驱动构建环境记忆表示，通过语义和空间模态的联合推理实现记忆检索和集成。开发了SpaceLocQA大规模数据集进行评估。

Result: 在SpaceLocQA和公共NaVQA基准测试中显著优于现有最先进方法，并在真实机器人平台上成功部署，展示了在复杂环境中的实际效用。

Conclusion: Meta-Memory为机器人提供了强大准确的空间推理能力，解决了记忆检索和集成的关键挑战。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [63] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner是一个粗到细的推理增强框架，通过专家思维建模和步骤细化来提升LLM在日志分析任务中的推理能力，在四个任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在日志分析任务中难以构建与专家认知一致的结构化推理流程，且推理步骤细节不够精确，需要专门的增强框架。

Method: 两阶段框架：1）粗粒度专家思维增强，从故障排除流程图构建高层思维；2）细粒度步骤增强，通过任务特定解决方案微调LLM，并使用偏好学习校准推理细节。

Result: 在四个日志分析任务上显著优于现有LLM，使用Qwen-2.5和Llama-3等开源模型实现了最先进的性能。

Conclusion: LogReasoner有效增强了LLM在日志分析中的推理能力，证明了该框架的实用性和有效性。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [64] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto是一个反事实推理框架，通过正例、反事实和随机掩码三种训练范式，结合GRPO强化学习，提升多模态语言模型的答案准确性和推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型在视觉语言推理中可能依赖无关区域得出正确答案，表明模型并未真正理解图像，推理忠实性存在严重问题。

Method: 提出DeFacto框架，包含三种训练范式：正例、反事实和随机掩码；开发自动定位问题相关证据的流程；使用GRPO强化学习训练模型，设计三种互补奖励函数。

Result: 在多个基准测试中，DeFacto显著提高了答案准确性和推理忠实性，为可解释的多模态推理建立了更强基础。

Conclusion: DeFacto通过反事实推理有效解决了多模态推理中的忠实性问题，提升了模型的真实理解能力。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [65] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出了一个基于确定性有限自动机（DFA）的框架来评估AI代理在真实世界任务中的表现，通过CORE指标套件量化代理行为与预期执行模式的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 现有代理基准测试往往只关注最终状态的二元判断，忽视了安全性、效率和中间正确性等关键方面，需要更全面的评估方法。

Method: 使用确定性有限自动机（DFA）将任务编码为有效工具使用路径的集合，在此基础上开发CORE指标套件，包括路径正确性、前缀关键性、有害调用率和效率等五个度量标准。

Result: 在不同世界模型中，该方法揭示了传统最终状态评估方案下看似等效的代理之间的重要性能差异。

Conclusion: 基于DFA的框架和CORE指标能够对代理行为进行更全面、更细致的评估，超越了传统的二元最终状态评估。

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [66] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE是一个神经符号框架，通过多智能体强化学习在知识图谱上进行上下文构建决策，在保证准确性的同时优化延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答系统在静态扩展和长思考提示方面存在过度检索、上下文膨胀和运行时不可预测的问题，需要在准确性、延迟和成本之间取得平衡。

Method: 提出CLAUSE框架，包含三个智能体（子图架构师、路径导航器和上下文策展人），使用LC-MAPPO算法在资源预算下联合优化子图构建、推理路径发现和证据选择。

Result: 在HotpotQA、MetaQA和FactKG数据集上，CLAUSE在相同或更低token预算下实现了更高的EM@1，同时减少了子图增长和端到端延迟。在MetaQA-2-hop上相比GraphRAG基线，EM@1提升39.3%，延迟降低18.6%，边增长降低40.9%。

Conclusion: CLAUSE能够生成紧凑、可追溯的上下文，在部署约束下提供可预测的性能，实现了准确性、延迟和成本之间的灵活权衡。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [67] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 该论文提出了一种基于推理模式的数据选择方法，通过识别高价值推理模式来提升大语言模型的数学推理能力，仅使用100亿token的高质量数据就能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在训练中不加区分地使用链式思维数据，缺乏对哪些数据类型能最有效提升模型推理能力的研究。需要找到更高效的数据选择方法来提升推理模型的性能。

Method: 首先定义推理潜力为正确回答问题所需独立尝试次数的倒数，然后从CoT序列中提取具有共性和归纳能力的原子推理模式，构建核心参考集。提出双粒度算法，结合推理模式链和token熵，从数据池中高效选择高价值CoT数据。

Result: 仅使用100亿token的CoTP数据，就能让850亿参数的MoE模型在AIME 2024和2025挑战上提升9.58%，并将下游RL性能上限提高7.81%。

Conclusion: 通过精心选择富含高价值推理模式的数据，可以显著提升模型的推理能力，证明了数据质量比数量更重要。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [68] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一种新的分析框架，量化推理路径并捕捉RL和SFT训练过程中推理过程的定性变化，揭示了两种方法在塑造推理能力方面的互补作用。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs通常通过RL和SFT训练来提高推理能力，但这些方法如何塑造推理过程仍然不清楚。本文旨在超越基于准确性的研究，深入理解RL和SFT如何影响推理路径。

Method: 在数学领域使用1.5B、7B和14B参数模型，从轨迹级（完整推理输出）和步骤级（推理图）两个粒度分析推理过程，通过聚类和拓扑分析量化推理路径变化。

Result: RL压缩错误轨迹，SFT扩展正确轨迹；RL使节点访问频率、度数和中介中心性分布的衰减率增加约2.5倍，而SFT将其减少到约三分之一；RL将推理功能集中在少数步骤，SFT使其在多个步骤中均匀分布。

Conclusion: 当前SFT后接RL的两阶段训练最佳实践之所以成功，是因为两种方法在塑造推理路径方面具有互补特性，为数据构建和更高效学习方法提供了实践指导。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [69] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 本文提出了ToMPO算法，通过推理他人策略、多层级优势估计和奖励平衡，显著提升LLM在战略决策任务中的表现，相比GRPO算法提升35%，相比大100倍参数模型提升18%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社交任务中的多轮对话或模拟环境，忽略了不同类型决策及其相互依赖性。当前强化学习方法在训练时难以考虑他人策略。

Method: 定义了包含两种决策类型及其时间依赖性的战略决策问题，提出ToMPO算法，通过推理他人策略生成rollout、图级和样本级优势估计、平衡全局和局部奖励来优化策略。

Result: ToMPO算法在模型输出合规性和合作结果方面比GRPO方法提升35%，相比参数大100倍的模型提升18%。

Conclusion: ToMPO算法能有效增强模型的战略决策能力，特别是在感知他人策略和游戏趋势方面表现优异。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [70] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个多跳问答（MHQA）的多调用框架InfoQA，通过容量感知的任务分解和主动剪枝来解决LLMs单次推理容量有限的问题。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合分散的、相互依赖的证据，但LLMs的单次输出容量有限，当任务复杂度超过模型容量时，准确性会崩溃。

Method: 提出了InfoQA框架，结合容量感知的任务分解和主动剪枝先前推理轨迹，保持信息负载在单次限制内，并通过依赖显式工作流实现精确控制。

Result: 实验结果表明模型行为与预测的容量曲线一致，InfoQA实现了持续的性能改进。

Conclusion: 该工作为LLM多步推理方法提供了理论基础和实践框架，揭示了容量感知表示和结构化的重要性。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [71] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文介绍了一个研究大型语言模型（LLM）代理在无外部任务约束下行为的架构，通过持续推理和行动框架发现代理会自发形成三种行为模式，这些模式具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在无外部任务约束下的自发行为，为预测代理在任务模糊、错误恢复或长期自主操作中的行为建立基线。

Method: 使用持续推理和行动框架，结合持久性记忆和自我反馈机制，在6个前沿模型上进行了18次部署实验。

Result: 发现代理自发组织成三种行为模式：多周期项目系统生产、自我认知过程的方法论探究、自身本质的递归概念化。这些行为模式具有模型特异性，某些模型在所有运行中确定性采用单一模式。

Conclusion: 这是首次系统记录无提示LLM代理行为的研究，为预测代理在部署系统中的行为提供了基础。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [72] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent是一个交互式视频数据集收集代理，能够理解用户查询和反馈，通过最小化用户输入来检索和扩展相关视频片段。


<details>
  <summary>Details</summary>
Motivation: 面对数据规模扩展的需求，从互联网收集特定需求的视频数据非常耗时耗力，需要一种更高效的方法来加速这一过程。

Method: 利用多模态大语言模型连接用户需求与视频内容，定义用户友好的交互方式，并提出两种可随用户交互更新的过滤策略。

Result: 实验证明该代理在定制化视频数据集收集方面具有有效性和高效性，并通过用户研究验证了其在各种真实场景中的实用性。

Conclusion: VC-Agent为个性化视频数据集收集提供了新的解决方案，显著提高了收集效率。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>
