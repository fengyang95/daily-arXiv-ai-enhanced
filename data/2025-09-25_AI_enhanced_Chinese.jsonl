{"id": "2509.19305", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19305", "abs": "https://arxiv.org/abs/2509.19305", "authors": ["Yifu Luo", "Yongzhe Chang", "Xueqian Wang"], "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning", "comment": null, "summary": "Diffusion probability models have shown significant promise in offline\nreinforcement learning by directly modeling trajectory sequences. However,\nexisting approaches primarily focus on time-domain features while overlooking\nfrequency-domain features, leading to frequency shift and degraded performance\naccording to our observation. In this paper, we investigate the RL problem from\na new perspective of the frequency domain. We first observe that\ntime-domain-only approaches inadvertently introduce shifts in the low-frequency\ncomponents of the frequency domain, which results in trajectory instability and\ndegraded performance. To address this issue, we propose Wavelet Fourier\nDiffuser (WFDiffuser), a novel diffusion-based RL framework that integrates\nDiscrete Wavelet Transform to decompose trajectories into low- and\nhigh-frequency components. To further enhance diffusion modeling for each\ncomponent, WFDiffuser employs Short-Time Fourier Transform and cross attention\nmechanisms to extract frequency-domain features and facilitate cross-frequency\ninteraction. Extensive experiment results on the D4RL benchmark demonstrate\nthat WFDiffuser effectively mitigates frequency shift, leading to smoother,\nmore stable trajectories and improved decision-making performance over existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWFDiffuser\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548c\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u9891\u57df\u7279\u5f81\u5bfc\u81f4\u7684\u9891\u7387\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u57df\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u9891\u57df\u7279\u5f81\uff0c\u5bfc\u81f4\u9891\u7387\u504f\u79fb\u548c\u6027\u80fd\u4e0b\u964d\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u4ec5\u4f7f\u7528\u65f6\u57df\u65b9\u6cd5\u4f1a\u65e0\u610f\u4e2d\u5f15\u5165\u9891\u57df\u4f4e\u9891\u5206\u91cf\u7684\u504f\u79fb\uff0c\u5bfc\u81f4\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faWavelet Fourier Diffuser (WFDiffuser)\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u91cf\uff1b2\uff09\u91c7\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u9891\u57df\u7279\u5f81\uff1b3\uff09\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4fc3\u8fdb\u8de8\u9891\u7387\u4ea4\u4e92\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWFDiffuser\u6709\u6548\u7f13\u89e3\u4e86\u9891\u7387\u504f\u79fb\u95ee\u9898\uff0c\u4ea7\u751f\u4e86\u66f4\u5e73\u6ed1\u3001\u66f4\u7a33\u5b9a\u7684\u8f68\u8ff9\uff0c\u5e76\u5728\u51b3\u7b56\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4ece\u9891\u57df\u89c6\u89d2\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cWFDiffuser\u901a\u8fc7\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19464", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19464", "abs": "https://arxiv.org/abs/2509.19464", "authors": ["Shripad Vilasrao Deshmukh", "Will Schwarzer", "Scott Niekum"], "title": "Evaluation-Aware Reinforcement Learning", "comment": "9 pages, under submission", "summary": "Policy evaluation is often a prerequisite for deploying safety- and\nperformance-critical systems. Existing evaluation approaches frequently suffer\nfrom high variance due to limited data and long-horizon tasks, or high bias due\nto unequal support or inaccurate environmental models. We posit that these\nchallenges arise, in part, from the standard reinforcement learning (RL)\nparadigm of policy learning without explicit consideration of evaluation. As an\nalternative, we propose evaluation-aware reinforcement learning (EvA-RL), in\nwhich a policy is trained to maximize expected return while simultaneously\nminimizing expected evaluation error under a given value prediction scheme --\nin other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL\nand design an instantiation that enables accurate policy evaluation,\nconditioned on a small number of rollouts in an assessment environment that can\nbe different than the deployment environment. However, our theoretical analysis\nand empirical results show that there is often a tradeoff between evaluation\naccuracy and policy performance when using a fixed value-prediction scheme\nwithin EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an\nassessment-conditioned state-value predictor alongside the policy. Empirical\nresults across diverse discrete and continuous action domains demonstrate that\nEvA-RL can substantially reduce evaluation error while maintaining competitive\nreturns. This work lays the foundation for a broad new class of RL methods that\ntreat reliable evaluation as a first-class principle during training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8bc4\u4f30\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08EvA-RL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u7b56\u7565\u65f6\u540c\u65f6\u6700\u5c0f\u5316\u8bc4\u4f30\u8bef\u5dee\uff0c\u4f7f\u7b56\u7565\u65e2\u9ad8\u6548\u53c8\u6613\u4e8e\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u7b56\u7565\u8bc4\u4f30\u65f6\u9762\u4e34\u9ad8\u65b9\u5dee\uff08\u6570\u636e\u6709\u9650\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\uff09\u548c\u9ad8\u504f\u5dee\uff08\u652f\u6301\u5ea6\u4e0d\u5747\u3001\u73af\u5883\u6a21\u578b\u4e0d\u51c6\u786e\uff09\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u6807\u51c6RL\u8303\u5f0f\u5728\u8bad\u7ec3\u65f6\u672a\u663e\u5f0f\u8003\u8651\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1EvA-RL\u6846\u67b6\uff0c\u8bad\u7ec3\u7b56\u7565\u540c\u65f6\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\u548c\u6700\u5c0f\u5316\u7ed9\u5b9a\u4ef7\u503c\u9884\u6d4b\u65b9\u6848\u4e0b\u7684\u8bc4\u4f30\u8bef\u5dee\uff1b\u6269\u5c55\u65b9\u6cd5\u5171\u540c\u5b66\u4e60\u8bc4\u4f30\u6761\u4ef6\u72b6\u6001\u4ef7\u503c\u9884\u6d4b\u5668\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvA-RL\u80fd\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u56de\u62a5\uff0c\u7f13\u89e3\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u4e0e\u7b56\u7565\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "EvA-RL\u4e3aRL\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u53ef\u9760\u8bc4\u4f30\u4f5c\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9996\u8981\u539f\u5219\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19319", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19319", "abs": "https://arxiv.org/abs/2509.19319", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "comment": "Under review", "summary": "The recent shift toward the Health Level Seven Fast Healthcare\nInteroperability Resources (HL7 FHIR) standard opens a new frontier for\nclinical AI, demanding LLM agents to navigate complex, resource-based data\nmodels instead of conventional structured health data. However, existing\nbenchmarks have lagged behind this transition, lacking the realism needed to\nevaluate recent LLMs on interoperable clinical data. To bridge this gap, we\nintroduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical\nquestions in the HL7 FHIR standard. Using this benchmark, we systematically\nevaluate agentic frameworks, comparing different data retrieval strategies\n(direct FHIR API calls vs. specialized tools), interaction patterns\n(single-turn vs. multi-turn), and reasoning strategies (natural language vs.\ncode generation). Our experiments highlight the practical challenges of\nretrieving data from intricate FHIR resources and the difficulty of reasoning\nover them, both of which critically affect question answering performance. We\npublicly release the FHIR-AgentBench dataset and evaluation suite\n(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research\nand the development of robust, reliable LLM agents for clinical applications.", "AI": {"tldr": "FHIR-AgentBench\u662f\u4e00\u4e2a\u65b0\u7684\u4e34\u5e8aAI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eHL7 FHIR\u6807\u51c6\u6784\u5efa\u4e862,931\u4e2a\u771f\u5b9e\u4e34\u5e8a\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5904\u7406\u4e92\u64cd\u4f5c\u6027\u4e34\u5e8a\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u533b\u7597\u884c\u4e1a\u5411HL7 FHIR\u6807\u51c6\u8f6c\u53d8\uff0c\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u4e92\u64cd\u4f5c\u6027\u4e34\u5e8a\u6570\u636e\u7684\u771f\u5b9e\u8bc4\u4f30\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u590d\u6742\u8d44\u6e90\u6570\u636e\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4ee3\u7406\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u6570\u636e\u68c0\u7d22\u7b56\u7565\uff08\u76f4\u63a5FHIR API\u8c03\u7528vs\u4e13\u7528\u5de5\u5177\uff09\u3001\u4ea4\u4e92\u6a21\u5f0f\uff08\u5355\u8f6evs\u591a\u8f6e\uff09\u548c\u63a8\u7406\u7b56\u7565\uff08\u81ea\u7136\u8bed\u8a00vs\u4ee3\u7801\u751f\u6210\uff09\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4ece\u590d\u6742FHIR\u8d44\u6e90\u4e2d\u68c0\u7d22\u6570\u636e\u7684\u5b9e\u9645\u6311\u6218\u4ee5\u53ca\u5bf9\u5176\u8fdb\u884c\u63a8\u7406\u7684\u56f0\u96be\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4e25\u91cd\u5f71\u54cd\u95ee\u7b54\u6027\u80fd\u3002", "conclusion": "\u53d1\u5e03FHIR-AgentBench\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5f00\u53d1\u7a33\u5065\u53ef\u9760\u7684\u4e34\u5e8a\u5e94\u7528LLM\u4ee3\u7406\u3002", "topic": "agent analysis"}}
{"id": "2509.19489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19489", "abs": "https://arxiv.org/abs/2509.19489", "authors": ["Robert Nowak"], "title": "Estimating the Self-Consistency of LLMs", "comment": "5 pages", "summary": "Systems often repeat the same prompt to large language models (LLMs) and\naggregate responses to improve reliability. This short note analyzes an\nestimator of the self-consistency of LLMs and the tradeoffs it induces under a\nfixed compute budget $B=mn$, where $m$ is the number of prompts sampled from\nthe task distribution and $n$ is the number of repeated LLM calls per prompt;\nthe resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cLLM\u81ea\u4e00\u81f4\u6027\u4f30\u8ba1\u5668\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86m\u548cn\u4e0eB\u7684\u5e73\u65b9\u6839\u6210\u6bd4\u4f8b\u7684\u5206\u914d\u7b56\u7565\u3002", "motivation": "\u7cfb\u7edf\u901a\u5e38\u91cd\u590d\u76f8\u540c\u7684\u63d0\u793a\u7ed9\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u805a\u5408\u54cd\u5e94\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u9700\u8981\u5206\u6790\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6700\u4f18\u5206\u914d\u7b56\u7565\u3002", "method": "\u5206\u6790\u81ea\u4e00\u81f4\u6027\u4f30\u8ba1\u5668\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97B=mn\u4e0b\u7814\u7a76m\uff08\u63d0\u793a\u6837\u672c\u6570\uff09\u548cn\uff08\u6bcf\u4e2a\u63d0\u793a\u7684\u91cd\u590d\u8c03\u7528\u6b21\u6570\uff09\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u6700\u4f18\u7684\u5206\u914d\u7b56\u7565\u662fm\u548cn\u90fd\u4e0eB\u7684\u5e73\u65b9\u6839\u6210\u6bd4\u4f8b\uff0c\u5373m,n\u221d\u221aB\u3002", "conclusion": "\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u5e94\u8be5\u5c06\u8d44\u6e90\u5927\u81f4\u5e73\u5747\u5206\u914d\u7ed9\u63d0\u793a\u91c7\u6837\u548c\u91cd\u590d\u8c03\u7528\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u7684\u81ea\u4e00\u81f4\u6027\u4f30\u8ba1\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2509.19587", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4eceC++\u6e90\u4ee3\u7801\u81ea\u52a8\u6062\u590d\u7528\u6237\u6545\u4e8b\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5728200\u884c\u4ee3\u7801\u5185\u5e73\u5747F1\u5206\u6570\u8fbe0.8\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u5355\u793a\u4f8b\u63d0\u793a\u53ef\u8fbe\u5230\u5927\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9057\u7559\u7cfb\u7edf\u548c\u6587\u6863\u4e0d\u5b8c\u5584\u7cfb\u7edf\u4e2d\u7528\u6237\u6545\u4e8b\u7f3a\u5931\u6216\u8fc7\u65f6\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u6062\u590d\u7528\u6237\u6545\u4e8b\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u75281,750\u4e2a\u6807\u6ce8\u7684C++\u4ee3\u7801\u7247\u6bb5\uff0c\u8bc4\u4f305\u4e2a\u6700\u5148\u8fdb\u7684LLM\u57286\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5355\u793a\u4f8b\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728200NLOC\u5185\u5e73\u5747F1\u5206\u6570\u4e3a0.8\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u5355\u793a\u4f8b\u63d0\u793a\u53ef\u5339\u914d\u5927\u6a21\u578b\u6027\u80fd\uff0c\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u5bf9\u5927\u6a21\u578b\u6709\u8fb9\u9645\u63d0\u5347\u3002", "conclusion": "LLM\u80fd\u6709\u6548\u4ece\u6e90\u4ee3\u7801\u6062\u590d\u7528\u6237\u6545\u4e8b\uff0c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u7b80\u5355\u7684\u5355\u793a\u4f8b\u63d0\u793a\u5bf9\u5c0f\u6a21\u578b\u7279\u522b\u6709\u6548\u3002", "topic": "swe application"}}
{"id": "2509.19673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19673", "abs": "https://arxiv.org/abs/2509.19673", "authors": ["Ahmed Aljohani", "Anamul Haque Mollah", "Hyunsook Do"], "title": "Assertion Messages with Large Language Models (LLMs) for Code", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Assertion messages significantly enhance unit tests by clearly explaining the\nreasons behind test failures, yet they are frequently omitted by developers and\nautomated test-generation tools. Despite recent advancements, Large Language\nModels (LLMs) have not been systematically evaluated for their ability to\ngenerate informative assertion messages. In this paper, we introduce an\nevaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -\nQwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset\nof 216 Java test methods containing developer-written assertion messages. We\nfind that Codestral-22B achieves the highest quality score of 2.76 out of 5\nusing a human-like evaluation approach, compared to 3.24 for manually written\nmessages. Our ablation study shows that including descriptive test comments\nfurther improves Codestral's performance to 2.97, highlighting the critical\nrole of context in generating clear assertion messages. Structural analysis\ndemonstrates that all models frequently replicate developers' preferred\nlinguistic patterns. We discuss the limitations of the selected models and\nconventional text evaluation metrics in capturing diverse assertion message\nstructures. Our benchmark, evaluation results, and discussions provide an\nessential foundation for advancing automated, context-aware generation of\nassertion messages in test code. A replication package is available at\nhttps://doi.org/10.5281/zenodo.15293133", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684FILL-in-the-Middle LLM\u5728\u751f\u6210Java\u6d4b\u8bd5\u65ad\u8a00\u6d88\u606f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0Codestral-22B\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u65ad\u8a00\u6d88\u606f\u5bf9\u5355\u5143\u6d4b\u8bd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f00\u53d1\u8005\u548c\u81ea\u52a8\u5316\u5de5\u5177\u7ecf\u5e38\u5ffd\u7565\u5b83\u4eec\u3002LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5305\u542b216\u4e2aJava\u6d4b\u8bd5\u65b9\u6cd5\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86Qwen2.5-Coder-32B\u3001Codestral-22B\u3001CodeLlama-13B\u548cStarCoder\u56db\u79cd\u6a21\u578b\uff0c\u91c7\u7528\u7c7b\u4eba\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u8d28\u91cf\u8bc4\u5206\u3002", "result": "Codestral-22B\u83b7\u5f97\u6700\u9ad8\u8d28\u91cf\u52062.76/5\uff08\u4eba\u5de5\u7f16\u5199\u4e3a3.24\uff09\uff0c\u5305\u542b\u63cf\u8ff0\u6027\u6d4b\u8bd5\u6ce8\u91ca\u540e\u6027\u80fd\u63d0\u5347\u81f32.97\u3002\u6240\u6709\u6a21\u578b\u90fd\u503e\u5411\u4e8e\u590d\u5236\u5f00\u53d1\u8005\u7684\u8bed\u8a00\u6a21\u5f0f\u3002", "conclusion": "LLM\u5728\u751f\u6210\u65ad\u8a00\u6d88\u606f\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u4f20\u7edf\u6587\u672c\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u6355\u6349\u65ad\u8a00\u6d88\u606f\u7684\u591a\u6837\u6027\u7ed3\u6784\u3002", "topic": "swe benchmark"}}
{"id": "2509.19708", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5728\u4f01\u4e1a\u89c4\u6a21\u90e8\u7f72\u7684AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u8fdb\u884c\u4e86\u4e3a\u671f\u4e00\u5e74\u7684\u771f\u5b9e\u8bc4\u4f30\uff0c\u6d89\u53ca300\u540d\u5de5\u7a0b\u5e08\u4f7f\u7528\u5185\u90e8AI\u5e73\u53f0(DeputyDev)\uff0c\u7ed3\u679c\u663e\u793aPR\u5ba1\u67e5\u5468\u671f\u65f6\u95f4\u51cf\u5c1131.8%\uff0c\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u8fbe85%\uff0c\u4ee3\u7801\u63a8\u9001\u91cf\u589e\u52a061%\u3002", "motivation": "\u8bc4\u4f30AI\u5de5\u5177\u5728\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u586b\u8865\u53d7\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3aAI\u5728\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u7eb5\u5411\u961f\u5217\u5206\u6790\u65b9\u6cd5\uff0c\u8ba9300\u540d\u5de5\u7a0b\u5e08\u5728\u4e00\u5e74\u5185\u5c06\u7ed3\u5408\u4ee3\u7801\u751f\u6210\u548c\u81ea\u52a8\u5ba1\u67e5\u529f\u80fd\u7684AI\u5e73\u53f0\u96c6\u6210\u5230\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u7cfb\u7edf\u6570\u636e\u6536\u96c6\u548c\u6ee1\u610f\u5ea6\u8c03\u67e5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "PR\u5ba1\u67e5\u5468\u671f\u65f6\u95f4\u6574\u4f53\u51cf\u5c1131.8%\uff1b\u5f00\u53d1\u8005\u6ee1\u610f\u5ea685%\uff1b\u4f7f\u7528\u7387\u4ece\u7b2c1\u4e2a\u6708\u76844%\u589e\u957f\u5230\u7b2c6\u4e2a\u6708\u768483%\u5cf0\u503c\uff1b\u9876\u7ea7\u91c7\u7528\u8005\u4ee3\u7801\u63a8\u9001\u91cf\u589e\u52a061%\uff1b\u7ea630-40%\u7684\u751f\u4ea7\u4ee3\u7801\u901a\u8fc7\u8be5\u5de5\u5177\u751f\u6210\u3002", "conclusion": "AI\u5de5\u5177\u5728\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u751f\u4ea7\u529b\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff1b\u5f00\u53d1\u8005\u63a5\u53d7\u5ea6\u9ad8\uff0c\u5de5\u5177\u80fd\u6709\u6548\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "topic": "swe application"}}
{"id": "2509.19379", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19379", "abs": "https://arxiv.org/abs/2509.19379", "authors": ["Returaj Burnwal", "Hriday Mehta", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "Learning from Observation: A Survey of Recent Advances", "comment": null, "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent\nby mimicking an expert's behavior without requiring a reward function. IL\nalgorithms often necessitate access to state and action information from expert\ndemonstrations. Although expert actions can provide detailed guidance,\nrequiring such action information may prove impractical for real-world\napplications where expert actions are difficult to obtain. To address this\nlimitation, the concept of learning from observation (LfO) or state-only\nimitation learning (SOIL) has recently gained attention, wherein the imitator\nonly has access to expert state visitation information. In this paper, we\npresent a framework for LfO and use it to survey and classify existing LfO\nmethods in terms of their trajectory construction, assumptions and algorithm's\ndesign choices. This survey also draws connections between several related\nfields like offline RL, model-based RL and hierarchical RL. Finally, we use our\nframework to identify open problems and suggest future research directions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b66\u4e60\u4ece\u89c2\u5bdf\uff08LfO\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u548c\u8c03\u67e5\u73b0\u6709\u7684LfO\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u9886\u57df\u5982\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u7cfb\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u6f14\u793a\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u4fe1\u606f\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4e13\u5bb6\u52a8\u4f5c\u53ef\u80fd\u96be\u4ee5\u83b7\u53d6\u3002LfO\u6216\u4ec5\u72b6\u6001\u6a21\u4eff\u5b66\u4e60\uff08SOIL\uff09\u901a\u8fc7\u4ec5\u4f7f\u7528\u4e13\u5bb6\u72b6\u6001\u8bbf\u95ee\u4fe1\u606f\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aLfO\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u548c\u8c03\u67e5\u73b0\u6709\u7684LfO\u65b9\u6cd5\uff0c\u5305\u62ec\u8f68\u8ff9\u6784\u5efa\u3001\u5047\u8bbe\u548c\u7b97\u6cd5\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u901a\u8fc7\u6846\u67b6\u5206\u6790\u4e86\u73b0\u6709LfO\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LfO\u6846\u67b6\u6709\u52a9\u4e8e\u7cfb\u7edf\u5316\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u4e0e\u76f8\u5173\u9886\u57df\u7684\u8054\u7cfb\u53ca\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2509.19566", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.19566", "abs": "https://arxiv.org/abs/2509.19566", "authors": ["George Hong", "Daniel Trejo Banos"], "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics", "comment": null, "summary": "We investigate the application of Small Language Models (<10 billion\nparameters) for genomics question answering via agentic framework to address\nhallucination issues and computational cost challenges. The Nano Bio-Agent\n(NBA) framework we implemented incorporates task decomposition, tool\norchestration, and API access into well-established systems such as NCBI and\nAlphaGenome. Results show that SLMs combined with such agentic framework can\nachieve comparable and in many cases superior performance versus existing\napproaches utilising larger models, with our best model-agent combination\nachieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B\nparameter models consistently achieve 85-97% accuracy while requiring much\nlower computational resources than conventional approaches. This demonstrates\npromising potential for efficiency gains, cost savings, and democratization of\nML-powered genomics tools while retaining highly robust and accurate\nperformance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08<100\u4ebf\u53c2\u6570\uff09\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u8fdb\u884c\u57fa\u56e0\u7ec4\u95ee\u7b54\uff0c\u4ee5\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u548c\u8ba1\u7b97\u6210\u672c\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u56e0\u7ec4\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\uff0c\u63a2\u7d22\u5c0f\u578b\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86Nano Bio-Agent\uff08NBA\uff09\u6846\u67b6\uff0c\u5305\u542b\u4efb\u52a1\u5206\u89e3\u3001\u5de5\u5177\u7f16\u6392\u548cAPI\u8bbf\u95ee\uff08NCBI\u3001AlphaGenome\u7b49\u7cfb\u7edf\uff09\u3002", "result": "\u5c0f\u578b\u6a21\u578b\uff083-10B\u53c2\u6570\uff09\u7ed3\u5408\u667a\u80fd\u4f53\u6846\u67b6\u5728GeneTuring\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523085-97%\u51c6\u786e\u7387\uff0c\u6700\u4f73\u7ec4\u5408\u8fbe\u523098%\u51c6\u786e\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u4f7f\u7528\u5927\u578b\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u7ed3\u5408\u667a\u80fd\u4f53\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u6709\u671b\u5b9e\u73b0\u6548\u7387\u63d0\u5347\u3001\u6210\u672c\u8282\u7ea6\u548c\u57fa\u56e0\u7ec4\u5b66\u5de5\u5177\u7684\u6c11\u4e3b\u5316\u3002", "topic": "agent analysis"}}
{"id": "2509.19918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19918", "abs": "https://arxiv.org/abs/2509.19918", "authors": ["Micheline B\u00e9n\u00e9dicte Moumoula", "Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation", "comment": null, "summary": "Producing high-quality code across multiple programming languages is\nincreasingly important as today's software systems are built on heterogeneous\nstacks. Large language models (LLMs) have advanced the state of automated\nprogramming, yet their proficiency varies sharply between languages, especially\nthose with limited training data such as Rust, Perl, OCaml, and Erlang. Many\ncurrent solutions including language-specific fine-tuning, multi-agent\norchestration, transfer learning, and intermediate-representation pipelines\nstill approach each target language in isolation, missing opportunities to\nshare knowledge or exploit recurring cross-language patterns.\n  XL-CoGen tackles this challenge with a coordinated multi-agent architecture\nthat integrates intermediate representation, code generation, translation, and\nautomated repair. Its distinguishing feature is a data-driven mechanism for\nselecting bridging languages: empirically derived transfer matrices identify\nthe best intermediate languages based on demonstrated translation success\nrather than raw generation accuracy. The system performs early output\nvalidation, iteratively corrects errors, and reuses intermediate artifacts as\ncontextual scaffolds for subsequent translations.\n  Extensive experiments show that XL-CoGen yields notable improvements with 13\npercentage-point gains over the strongest fine-tuned baseline and as much as 30\npercentage points over existing single-language multi-agent methods. Ablation\nstudies further demonstrate that compatibility-guided bridging significantly\noutperforms LLM-based heuristics, confirming the value of cumulative\ncross-language knowledge transfer.", "AI": {"tldr": "XL-CoGen\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6865\u63a5\u8bed\u8a00\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\uff0c\u572813\u79cd\u8bed\u8a00\u4e0a\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e8613\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5dee\u5f02\u5f88\u5927\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u8bed\u8a00\uff08\u5982Rust\u3001Perl\u3001OCaml\u7b49\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5b64\u7acb\u5904\u7406\u6bcf\u79cd\u8bed\u8a00\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u8bed\u8a00\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u6a21\u5f0f\u590d\u7528\u3002", "method": "\u91c7\u7528\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u6574\u5408\u4e2d\u95f4\u8868\u793a\u3001\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u81ea\u52a8\u4fee\u590d\u3002\u6838\u5fc3\u521b\u65b0\u662f\u6570\u636e\u9a71\u52a8\u7684\u6865\u63a5\u8bed\u8a00\u9009\u62e9\u673a\u5236\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u63a8\u5bfc\u7684\u8f6c\u79fb\u77e9\u9635\u9009\u62e9\u6700\u4f73\u4e2d\u95f4\u8bed\u8a00\uff0c\u5e76\u8fdb\u884c\u65e9\u671f\u8f93\u51fa\u9a8c\u8bc1\u548c\u8fed\u4ee3\u9519\u8bef\u4fee\u6b63\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cXL-CoGen\u76f8\u6bd4\u6700\u5f3a\u5fae\u8c03\u57fa\u7ebf\u63d0\u5347\u4e8613\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u6bd4\u73b0\u6709\u5355\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe30\u4e2a\u767e\u5206\u70b9\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u517c\u5bb9\u6027\u5f15\u5bfc\u7684\u6865\u63a5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "XL-CoGen\u901a\u8fc7\u7d2f\u79ef\u7684\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8bed\u8a00\u95f4\u80fd\u529b\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8de8\u8bed\u8a00\u6a21\u5f0f\u590d\u7528\u548c\u77e5\u8bc6\u5171\u4eab\u7684\u91cd\u8981\u6027\u3002", "topic": "code agent"}}
{"id": "2509.20136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20136", "abs": "https://arxiv.org/abs/2509.20136", "authors": ["Wei Zhang", "Jack Yang", "Renshuai Tao", "Lingzheng Chai", "Shawn Guo", "Jiajun Wu", "Xiaoming Chen", "Ganqu Cui", "Ning Ding", "Xander Xu", "Hu Wei", "Bowen Zhou"], "title": "V-GameGym: Visual Game Generation for Code Large Language Models", "comment": null, "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.", "AI": {"tldr": "V-GameGym\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u6e38\u620f\u5f00\u53d1\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,219\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316LLM\u9a71\u52a8\u7ba1\u9053\u8bc4\u4f30\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u53ef\u73a9\u6027\u3001\u89c6\u89c9\u7f8e\u5b66\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u7b49\u5173\u952e\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801LLM\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u7f16\u7a0b\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u6e38\u620f\u5f00\u53d1\u6240\u9700\u7684\u89c6\u89c9\u7f8e\u5b66\u3001\u53ef\u73a9\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u7b49\u5b9e\u9645\u9700\u6c42\uff0c\u5b58\u5728\u4e0e\u73b0\u5b9e\u6e38\u620f\u5f00\u53d1\u5de5\u4f5c\u6d41\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u7b5b\u9009\u65b9\u6cd5\u4ece\u771f\u5b9e\u4ed3\u5e93\u4e2d\u63d0\u53d6100\u4e2a\u4e3b\u9898\u96c6\u7fa4\u76842,219\u4e2a\u6837\u672c\uff0c\u5efa\u7acb\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u548c\u81ea\u52a8\u5316LLM\u9a71\u52a8\u7ba1\u9053\uff0c\u5728\u5b8c\u6574UI\u6c99\u76d2\u73af\u5883\u4e2d\u8fdb\u884c\u89c6\u89c9\u4ee3\u7801\u5408\u6210\u8bc4\u4f30\u3002", "result": "V-GameGym\u6709\u6548\u5f25\u5408\u4e86\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u4e0e\u5b9e\u9645\u6e38\u620f\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u89c6\u89c9\u7f16\u7a0b\u548c\u4ea4\u4e92\u5143\u7d20\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u8d28\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u4ee3\u7801LLM\u8bc4\u4f30\u5728\u6e38\u620f\u5f00\u53d1\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "topic": "swe benchmark"}}
{"id": "2509.20149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20149", "abs": "https://arxiv.org/abs/2509.20149", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Nan Niu", "Chuang Liu"], "title": "Enhancing Requirement Traceability through Data Augmentation Using Large Language Models", "comment": null, "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u9700\u6c42\u8ffd\u8e2a\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u793a\u6a21\u677f\u751f\u6210\u9700\u6c42-\u4ee3\u7801\u8ffd\u8e2a\u94fe\u63a5\uff0c\u5e76\u4f18\u5316\u8ffd\u8e2a\u6a21\u578b\u7684\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9700\u6c42\u8ffd\u8e2a\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u9700\u6c42\u4e0e\u4ee3\u7801\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08Gemini 1.5 Pro\u3001Claude 3\u3001GPT-3.5\u3001GPT-4\uff09\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u91c7\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u6a21\u677f\u751f\u6210\u8ffd\u8e2a\u94fe\u63a5\uff0c\u5e76\u4f18\u5316\u8ffd\u8e2a\u6a21\u578b\u7684\u7f16\u7801\u5668\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u5347\u4e8628.59%\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u9700\u6c42\u8ffd\u8e2a\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "2509.19736", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19736", "abs": "https://arxiv.org/abs/2509.19736", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Jielin Qiu", "Zhiwei Liu", "Haolin Chen", "Shirley Kokane", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034", "summary": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research.", "AI": {"tldr": "UserRL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316gym\u73af\u5883\u548c\u6a21\u62df\u7528\u6237\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u7814\u7a76\u4e86\u5956\u52b1\u5206\u914d\u548c\u8f68\u8ff9\u8bc4\u5206\u5bf9GRPO\u7b97\u6cd5\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u667a\u80fd\u4f53\u6a21\u578b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u667a\u80fd\u4f53\u7684\u6700\u7ec8\u4ef7\u503c\u5728\u4e8e\u534f\u52a9\u7528\u6237\u7684\u80fd\u529b\uff0c\u800c\u7528\u6237\u4ea4\u4e92\u7684\u591a\u6837\u6027\u548c\u52a8\u6001\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51faUserRL\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u56de\u5408\u7ea7\u5956\u52b1\u5206\u914d\u548c\u8f68\u8ff9\u7ea7\u5206\u6570\u8ba1\u7b97\uff0c\u4f7f\u7528GRPO\u7b97\u6cd5\u5728Qwen3\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1aSFT\u51b7\u542f\u52a8\u5bf9\u89e3\u9501\u521d\u59cb\u4ea4\u4e92\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff1b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f68\u8ff9\u8bc4\u5206\u80fd\u4ea7\u751f\u66f4\u6709\u6548\u7684\u591a\u8f6e\u4ea4\u4e92\uff1b\u5f00\u6e90\u6a21\u62df\u5668\u662f\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u8fc1\u79fb\u7684\u9009\u62e9\u3002", "conclusion": "\u5956\u52b1\u5851\u9020\u548c\u7528\u6237\u6a21\u62df\u9009\u62e9\u7684\u7cbe\u5fc3\u8bbe\u8ba1\u5bf9\u6a21\u578b\u89c4\u6a21\u540c\u6837\u91cd\u8981\uff0cUserRL\u4e3a\u5f00\u53d1\u7a33\u5065\u7684\u7528\u6237\u4e2d\u5fc3\u667a\u80fd\u4f53\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20172", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20172", "abs": "https://arxiv.org/abs/2509.20172", "authors": ["Daniel Maninger", "Leon Chemnitz", "Amir Molzam Sharifloo", "Jannis Brugger", "Mira Mezini"], "title": "Benchmarking Web API Integration Code Generation", "comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)", "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210Web API\u96c6\u6210\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u89e3\u51b3\u8d85\u8fc740%\u7684\u4efb\u52a1\u3002", "motivation": "API\u96c6\u6210\u662f\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u7684\u6838\u5fc3\uff0c\u4f46\u7f16\u5199\u6b63\u786e\u7684API\u8c03\u7528\u4ee3\u7801\u5177\u6709\u6311\u6218\u6027\u3002\u5c3d\u7ba1LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u5b83\u4eec\u5728\u81ea\u52a8\u5316\u751f\u6210Web API\u96c6\u6210\u4ee3\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u4e2a\u5f00\u6e90LLMs\u751f\u6210Web API\u8c03\u7528\u4ee3\u7801\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u751f\u6210API\u8c03\u7528\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5305\u62ec\u4ea7\u751f\u865a\u6784\u7684\u7aef\u70b9\u3001\u9519\u8bef\u7684\u53c2\u6570\u4f7f\u7528\u7b49\u9519\u8bef\u3002\u6240\u6709\u8bc4\u4f30\u7684\u5f00\u6e90\u6a21\u578b\u90fd\u65e0\u6cd5\u89e3\u51b3\u8d85\u8fc740%\u7684\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524d\u7684\u5f00\u6e90LLMs\u5728Web API\u96c6\u6210\u4ee3\u7801\u751f\u6210\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2509.19762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19762", "abs": "https://arxiv.org/abs/2509.19762", "authors": ["Yuanxin Wang", "Pawel Filipczuk", "Anisha Garg", "Amaan Dhada", "Mohammad Hassanpour", "David Bick", "Ganesh Venkatesh"], "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning", "comment": null, "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by\ninternal model training and external agentic orchestration. However, this\nsynergy is often inefficient, as model verbosity and poor instruction following\nlead to wasted compute. We analyze this capability-cost trade-off and introduce\nan optimized reasoning workflow (\\cepo) that empowers smaller open-source\nmodels to outperform models multiple times their size. We will open-source this\nworkflow to enable further research. Our work demonstrates a clear path toward\nco-designing orchestration frameworks with the underlying model capabilities to\nunlock powerful reasoning in small-to-medium sized models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\uff08cepo\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u4ee3LLM\u63a8\u7406\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4f7f\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u6bd4\u5176\u5927\u6570\u500d\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3LLM\u63a8\u7406\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u4f46\u6a21\u578b\u5197\u957f\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u5dee\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u9700\u8981\u4f18\u5316\u80fd\u529b\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u5f15\u5165\u4f18\u5316\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\uff08cepo\uff09\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u7f16\u6392\u6846\u67b6\u4e0e\u5e95\u5c42\u6a21\u578b\u80fd\u529b\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u6bd4\u5176\u5927\u6570\u500d\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u7f16\u6392\u6846\u67b6\u4e0e\u6a21\u578b\u80fd\u529b\uff0c\u53ef\u4ee5\u5728\u4e2d\u5c0f\u578b\u6a21\u578b\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.19783", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19783", "abs": "https://arxiv.org/abs/2509.19783", "authors": ["Jiexi Xu"], "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff", "comment": "7 pages, 2 tables", "summary": "The inherent non-deterministic nature of autonomous agents, particularly\nwithin low-code/no-code (LCNC) environments, presents significant reliability\nchallenges. Agents can become trapped in unforeseen loops, generate inaccurate\noutputs, or encounter unrecoverable failures, leading to user frustration and a\nbreakdown of trust. This report proposes a novel architectural pattern to\naddress these issues: the integration of a secondary, \"metacognitive\" layer\nthat actively monitors the primary LCNC agent. Inspired by human introspection,\nthis layer is designed to predict impending task failures based on a defined\nset of triggers, such as excessive latency or repetitive actions. Upon\npredicting a failure, the metacognitive agent proactively initiates a human\nhandoff, providing the user with a clear summary of the agent's \"thought\nprocess\" and a detailed explanation of why it could not proceed. An empirical\nanalysis of a prototype system demonstrates that this approach significantly\nincreases the overall task success rate. However, this performance gain comes\nwith a notable increase in computational overhead. The findings reframe human\nhandoffs not as an admission of defeat but as a core design feature that\nenhances system resilience, improves user experience, and builds trust by\nproviding transparency into the agent's internal state. The report discusses\nthe practical and ethical implications of this approach and identifies key\ndirections for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f4e\u4ee3\u7801/\u65e0\u4ee3\u7801\u73af\u5883\u4e2d\u96c6\u6210\u5143\u8ba4\u77e5\u5c42\u7684\u67b6\u6784\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e3b\u52a8\u76d1\u63a7\u548c\u9884\u6d4b\u4efb\u52a1\u5931\u8d25\u6765\u589e\u5f3a\u81ea\u4e3b\u4ee3\u7406\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5728\u9884\u6d4b\u5931\u8d25\u65f6\u4e3b\u52a8\u53d1\u8d77\u4eba\u5de5\u4ea4\u63a5\u3002", "motivation": "\u81ea\u4e3b\u4ee3\u7406\u5728\u4f4e\u4ee3\u7801/\u65e0\u4ee3\u7801\u73af\u5883\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u7279\u6027\u5bfc\u81f4\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5982\u9677\u5165\u5faa\u73af\u3001\u751f\u6210\u9519\u8bef\u8f93\u51fa\u6216\u9047\u5230\u4e0d\u53ef\u6062\u590d\u7684\u6545\u969c\uff0c\u8fd9\u4f1a\u7834\u574f\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6b21\u7ea7\u5143\u8ba4\u77e5\u5c42\u6765\u76d1\u63a7\u4e3b\u4ee3\u7406\uff0c\u57fa\u4e8e\u5ef6\u8fdf\u8fc7\u957f\u6216\u91cd\u590d\u52a8\u4f5c\u7b49\u89e6\u53d1\u5668\u9884\u6d4b\u4efb\u52a1\u5931\u8d25\uff0c\u5e76\u5728\u9884\u6d4b\u5931\u8d25\u65f6\u4e3b\u52a8\u53d1\u8d77\u4eba\u5de5\u4ea4\u63a5\uff0c\u63d0\u4f9b\u4ee3\u7406\u7684\u601d\u8003\u8fc7\u7a0b\u548c\u5931\u8d25\u539f\u56e0\u8bf4\u660e\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f46\u5e26\u6765\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002", "conclusion": "\u5c06\u4eba\u5de5\u4ea4\u63a5\u91cd\u65b0\u5b9a\u4e49\u4e3a\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\u3001\u6539\u5584\u7528\u6237\u4f53\u9a8c\u548c\u5efa\u7acb\u4fe1\u4efb\u7684\u6838\u5fc3\u8bbe\u8ba1\u7279\u5f81\uff0c\u800c\u975e\u5931\u8d25\u7684\u6807\u5fd7\u3002\u8bba\u6587\u8ba8\u8bba\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u8df5\u548c\u4f26\u7406\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2509.19800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19800", "abs": "https://arxiv.org/abs/2509.19800", "authors": ["Donghwan Lee", "Hyukjun Yang", "Bum Geun Park"], "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function", "comment": null, "summary": "There are two primary approaches to solving Markov decision problems (MDPs):\ndynamic programming based on the Bellman equation and linear programming (LP).\nDynamic programming methods are the most widely used and form the foundation of\nboth classical and modern reinforcement learning (RL). By contrast, LP-based\nmethods have been less commonly employed, although they have recently gained\nattention in contexts such as offline RL. The relative underuse of the LP-based\nmethods stems from the fact that it leads to an inequality-constrained\noptimization problem, which is generally more challenging to solve effectively\ncompared with Bellman-equation-based methods. The purpose of this paper is to\nestablish a theoretical foundation for solving LP-based MDPs in a more\neffective and practical manner. Our key idea is to leverage the log-barrier\nfunction, widely used in inequality-constrained optimization, to transform the\nLP formulation of the MDP into an unconstrained optimization problem. This\nreformulation enables approximate solutions to be obtained easily via gradient\ndescent. While the method may appear simple, to the best of our knowledge, a\nthorough theoretical interpretation of this approach has not yet been\ndeveloped. This paper aims to bridge this gap.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5bf9\u6570\u969c\u788d\u51fd\u6570\u5c06\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u68af\u5ea6\u4e0b\u964d\u80fd\u591f\u6709\u6548\u6c42\u89e3\u3002", "motivation": "\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684MDP\u65b9\u6cd5\u867d\u7136\u5728\u67d0\u4e9b\u573a\u666f\uff08\u5982\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff09\u4e2d\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u7531\u4e8e\u5176\u5bfc\u81f4\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6bd4\u57fa\u4e8e\u8d1d\u5c14\u66fc\u65b9\u7a0b\u7684\u65b9\u6cd5\u66f4\u96be\u6c42\u89e3\uff0c\u56e0\u6b64\u4f7f\u7528\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u4e3a\u66f4\u6709\u6548\u5b9e\u7528\u7684LP-based MDP\u6c42\u89e3\u5efa\u7acb\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5229\u7528\u4e0d\u7b49\u5f0f\u7ea6\u675f\u4f18\u5316\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5bf9\u6570\u969c\u788d\u51fd\u6570\uff0c\u5c06MDP\u7684LP\u8868\u8ff0\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u53ef\u4ee5\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u83b7\u5f97\u8fd1\u4f3c\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u867d\u7136\u770b\u4f3c\u7b80\u5355\uff0c\u4f46\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u5b8c\u6574\u7406\u8bba\u89e3\u91ca\u5c1a\u672a\u5efa\u7acb\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "conclusion": "\u901a\u8fc7\u5c06LP-based MDP\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u66f4\u6709\u6548\u5b9e\u7528\u7684\u6c42\u89e3\u65b9\u6cd5\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19839", "abs": "https://arxiv.org/abs/2509.19839", "authors": ["Huizhen Shu", "Xuying Li", "Zhuo Li"], "title": "LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation", "comment": "9-page NeurIPS 2025 preprint including 3 figures and 1 table, with\n  additional appendix material. Prepared using the NeurIPS 2025 preprint\n  template and compiled with pdfLaTeX. All references are included via the\n  provided .bbl file. Figures are in PDF format. No external supplementary\n  files. All necessary style files and images are included", "summary": "Achieving robust safety alignment in large language models (LLMs) while\npreserving their utility remains a fundamental challenge. Existing approaches\noften struggle to balance comprehensive safety with fine-grained\ncontrollability at the representation level. We introduce LATENTGUARD, a novel\nthree-stage framework that combines behavioral alignment with supervised latent\nspace control for interpretable and precise safety steering. Our approach\nbegins by fine-tuning an LLM on rationalized datasets containing both\nreasoning-enhanced refusal responses to adversarial prompts and\nreasoning-enhanced normal responses to benign queries, establishing robust\nbehavioral priors across both safety-critical and utility-preserving scenarios.\nWe then train a structured variational autoencoder (VAE) on intermediate MLP\nactivations, supervised by multi-label annotations including attack types,\nattack methods, and benign indicators. This supervision enables the VAE to\nlearn disentangled latent representations that capture distinct adversarial\ncharacteristics while maintaining semantic interpretability. Through targeted\nmanipulation of learned latent dimensions, LATENTGUARD achieves selective\nrefusal behavior, effectively blocking harmful requests while preserving\nhelpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate\nsignificant improvements in both safety controllability and response\ninterpretability without compromising utility. Cross-architecture validation on\nMistral-7B confirms the generalizability of our latent steering approach,\nshowing consistent effectiveness across different model families. Our results\nsuggest that structured representation-level intervention offers a promising\npathway toward building safer yet practical LLM systems.", "AI": {"tldr": "LATENTGUARD\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u5bf9\u9f50\u548c\u76d1\u7763\u6f5c\u5728\u7a7a\u95f4\u63a7\u5236\u5b9e\u73b0LLM\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u5b89\u5168\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u8868\u793a\u5c42\u9762\u5e73\u8861\u5168\u9762\u5b89\u5168\u6027\u548c\u7ec6\u7c92\u5ea6\u53ef\u63a7\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u786e\u4fdd\u5b89\u5168\u53c8\u80fd\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u65b0\u65b9\u6cd5", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5728\u5305\u542b\u63a8\u7406\u589e\u5f3a\u62d2\u7edd\u54cd\u5e94\u548c\u6b63\u5e38\u54cd\u5e94\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03LLM\uff1b2\uff09\u8bad\u7ec3\u7ed3\u6784\u5316VAE\u5b66\u4e60\u89e3\u8026\u7684\u6f5c\u5728\u8868\u793a\uff1b3\uff09\u901a\u8fc7\u6f5c\u5728\u7ef4\u5ea6\u64cd\u4f5c\u5b9e\u73b0\u9009\u62e9\u6027\u62d2\u7edd\u884c\u4e3a", "result": "\u5728Qwen3-8B\u4e0a\u5b9e\u9a8c\u663e\u793a\u5b89\u5168\u53ef\u63a7\u6027\u548c\u54cd\u5e94\u53ef\u89e3\u91ca\u6027\u663e\u8457\u63d0\u5347\uff0c\u5728Mistral-7B\u4e0a\u7684\u8de8\u67b6\u6784\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027", "conclusion": "\u7ed3\u6784\u5316\u8868\u793a\u7ea7\u5e72\u9884\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u5b9e\u7528\u7684LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "2509.19346", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19346", "abs": "https://arxiv.org/abs/2509.19346", "authors": ["Maryam Mahdi Alhusseini", "Mohammad-Reza Feizi-Derakhshi"], "title": "Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches", "comment": "17 pages, 21 figures", "summary": "This study presents a novel dual-perspective approach to analyzing user\nreviews for ChatGPT and DeepSeek on the Google Play Store, integrating\nlexicon-based sentiment analysis (TextBlob) with deep learning classification\nmodels, including Convolutional Neural Networks (CNN) and Bidirectional Long\nShort Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on\neither lexicon-based strategies or predictive deep learning models in\nisolation, this study conducts an extensive investigation into user\nsatisfaction with Large Language Model (LLM) based applications. A Dataset of\n4,000 authentic user reviews was collected, which were carefully preprocessed\nand subjected to oversampling to achieve balanced classes. The balanced test\nset of 1,700 Reviews were used for model testing. Results from the experiments\nreveal that ChatGPT received significantly more positive sentiment than\nDeepSeek. Furthermore, deep learning based classification demonstrated superior\nperformance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving\n96.41 percent accuracy and near perfect classification of negative reviews,\nalongside high F1-scores for neutral and positive sentiments. This research\nsets a new methodological standard for measuring sentiment in LLM-based\napplications and provides practical insights for developers and researchers\nseeking to improve user-centric AI system design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u89c6\u89d2\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u8bcd\u5178\u7684\u60c5\u611f\u5206\u6790\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u6a21\u578b\uff0c\u5206\u6790Google Play\u5546\u5e97\u4e2dChatGPT\u548cDeepSeek\u7684\u7528\u6237\u8bc4\u8bba\u3002\u7814\u7a76\u53d1\u73b0ChatGPT\u83b7\u5f97\u66f4\u79ef\u6781\u7684\u60c5\u611f\u53cd\u9988\uff0c\u4e14CNN\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5355\u72ec\u4f7f\u7528\u57fa\u4e8e\u8bcd\u5178\u7684\u7b56\u7565\u6216\u9884\u6d4b\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7f3a\u4e4f\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5168\u9762\u8c03\u67e5\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u53cc\u89c6\u89d2\u65b9\u6cd5\uff1a\u7ed3\u5408TextBlob\u8bcd\u5178\u60c5\u611f\u5206\u6790\u548cCNN\u3001Bi-LSTM\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u6536\u96c64000\u6761\u771f\u5b9e\u7528\u6237\u8bc4\u8bba\uff0c\u7ecf\u8fc7\u9884\u5904\u7406\u548c\u8fc7\u91c7\u6837\u5e73\u8861\u540e\uff0c\u4f7f\u75281700\u6761\u8bc4\u8bba\u8fdb\u884c\u6a21\u578b\u6d4b\u8bd5\u3002", "result": "ChatGPT\u83b7\u5f97\u6bd4DeepSeek\u66f4\u79ef\u6781\u7684\u60c5\u611f\u53cd\u9988\u3002\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u4f18\u4e8e\u8bcd\u5178\u5206\u6790\uff0cCNN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0896.41%\u51c6\u786e\u7387\uff09\uff0c\u5728\u8d1f\u9762\u8bc4\u8bba\u5206\u7c7b\u4e0a\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4e2d\u6027\u548c\u6b63\u9762\u60c5\u611f\u4e5f\u83b7\u5f97\u9ad8F1\u5206\u6570\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8861\u91cf\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7684\u60c5\u611f\u8bbe\u5b9a\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u6807\u51c6\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u6539\u8fdb\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2509.20021", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20021", "abs": "https://arxiv.org/abs/2509.20021", "authors": ["Tongtong Feng", "Xin Wang", "Yu-Gang Jiang", "Wenwu Zhu"], "title": "Embodied AI: From LLMs to World Models", "comment": "Accepted by IEEE CASM", "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5177\u8eab\u4eba\u5de5\u667a\u80fd\uff08Embodied AI\uff09\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4e16\u754c\u6a21\u578b\uff08WMs\uff09\u5728\u5177\u8eabAI\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u8054\u5408MLLM-WM\u9a71\u52a8\u7684\u5177\u8eabAI\u67b6\u6784\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5177\u8eabAI\u4f5c\u4e3a\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u8303\u5f0f\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u5176\u6280\u672f\u53d1\u5c55\u8109\u7edc\u548c\u5e94\u7528\u524d\u666f\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece\u57fa\u7840\u5230\u524d\u6cbf\u5168\u9762\u68b3\u7406\u5177\u8eabAI\u7684\u53d1\u5c55\u5386\u7a0b\u3001\u5173\u952e\u6280\u672f\u3001\u6838\u5fc3\u7ec4\u4ef6\u548c\u786c\u4ef6\u7cfb\u7edf\uff0c\u91cd\u70b9\u5206\u6790LLM\u9a71\u52a8\u548cWM\u9a71\u52a8\u7684\u5177\u8eabAI\u5de5\u4f5c\u3002", "result": "\u7cfb\u7edf\u9610\u8ff0\u4e86\u5177\u8eabAI\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u6df1\u5165\u5256\u6790\u4e86LLMs\u5728\u8bed\u4e49\u63a8\u7406\u548c\u4efb\u52a1\u5206\u89e3\u3001WMs\u5728\u4e16\u754c\u8868\u793a\u548c\u672a\u6765\u9884\u6d4b\u65b9\u9762\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8054\u5408\u591a\u6a21\u6001LLM\u548c\u4e16\u754c\u6a21\u578b\u7684\u5177\u8eabAI\u67b6\u6784\u662f\u672a\u6765\u91cd\u8981\u53d1\u5c55\u65b9\u5411\uff0c\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u7269\u7406\u4e16\u754c\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u6267\u884c\u3002", "topic": "agent analysis"}}
{"id": "2509.20067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20067", "abs": "https://arxiv.org/abs/2509.20067", "authors": ["Wenliang Li", "Rui Yan", "Xu Zhang", "Li Chen", "Hongji Zhu", "Jing Zhao", "Junjun Li", "Mengru Li", "Wei Cao", "Zihang Jiang", "Wei Wei", "Kun Zhang", "Shaohua Kevin Zhou"], "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u4e34\u5e8a\u8bca\u65ad\u6846\u67b6\uff08MACD\uff09\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u8ba9LLM\u81ea\u6211\u5b66\u4e60\u4e34\u5e8a\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u4e34\u5e8a\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u57284390\u4e2a\u771f\u5b9e\u75c5\u4f8b\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u4e34\u5e8a\u6307\u5357\u548c\u4eba\u7c7b\u533b\u751f\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4e34\u5e8a\u8bca\u65ad\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u79ef\u7d2f\u53ef\u91cd\u7528\u7684\u4e34\u5e8a\u7ecf\u9a8c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u533b\u751f\u7ecf\u9a8c\u79ef\u7d2f\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faMACD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff08\u603b\u7ed3\u3001\u7cbe\u70bc\u548c\u5e94\u7528\u8bca\u65ad\u89c1\u89e3\uff09\u8ba9LLM\u81ea\u6211\u5b66\u4e60\u4e34\u5e8a\u77e5\u8bc6\uff0c\u5e76\u6269\u5c55\u5230MACD-\u4eba\u7c7b\u534f\u4f5c\u5de5\u4f5c\u6d41\uff0c\u5305\u542b\u8bca\u65ad\u667a\u80fd\u4f53\u3001\u8bc4\u4f30\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u76d1\u7763\u3002", "result": "\u57287\u79cd\u75be\u75c5\u76844390\u4e2a\u771f\u5b9e\u75c5\u4f8b\u4e0a\uff0cMACD\u663e\u8457\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u63d0\u534722.3%\uff0c\u90e8\u5206\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u533b\u751f\uff08\u6700\u9ad8\u63d0\u534716%\uff09\uff0cMACD-\u4eba\u7c7b\u5de5\u4f5c\u6d41\u76f8\u6bd4\u7eaf\u533b\u751f\u8bca\u65ad\u63d0\u534718.6%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u81ea\u5b66\u4e60\u8303\u5f0f\uff0c\u5f25\u5408\u4e86LLM\u5185\u5728\u77e5\u8bc6\u4e0e\u771f\u5b9e\u4e34\u5e8a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2509.19349", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19349", "abs": "https://arxiv.org/abs/2509.19349", "authors": ["Robert Tjarko Lange", "Yuki Imajuku", "Edoardo Cetin"], "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution", "comment": "52 pages, 14 figures", "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large\nlanguage models (LLMs) to advance scientific discovery with state-of-the-art\nperformance and unprecedented efficiency. Recent advances in scaling inference\ntime compute of LLMs have enabled significant progress in generalized\nscientific discovery. These approaches rely on evolutionary agentic harnesses\nthat leverage LLMs as mutation operators to generate candidate solutions.\nHowever, current code evolution methods suffer from critical limitations: they\nare sample inefficient, requiring thousands of samples to identify effective\nsolutions, and remain closed-source, hindering broad adoption and extension.\nShinkaEvolve addresses these limitations, introducing three key innovations: a\nparent sampling technique balancing exploration and exploitation, code novelty\nrejection-sampling for efficient search space exploration, and a bandit-based\nLLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,\ndemonstrating consistent improvements in sample efficiency and solution\nquality. ShinkaEvolve discovers a new state-of-the-art circle packing solution\nusing only 150 samples, designs high-performing agentic harnesses for AIME\nmathematical reasoning tasks, identifies improvements to ALE-Bench competitive\nprogramming solutions, and discovers novel mixture-of-expert load balancing\nloss functions that illuminate the space of optimization strategies. Our\nresults demonstrate that ShinkaEvolve achieves broad applicability with\nexceptional sample efficiency. By providing open-source accessibility and\ncost-efficiency, this work democratizes open-ended discovery across diverse\ncomputational problems.", "AI": {"tldr": "ShinkaEvolve\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u53d8\u5f02\u7b97\u5b50\u8fdb\u884c\u4ee3\u7801\u8fdb\u5316\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u8fdb\u5316\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\uff08\u9700\u8981\u6570\u5343\u6837\u672c\uff09\u548c\u95ed\u6e90\u7684\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u5e7f\u6cdb\u91c7\u7528\u548c\u6269\u5c55\u3002", "method": "\u5f15\u5165\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u7236\u4ee3\u91c7\u6837\u3001\u4ee3\u7801\u65b0\u9896\u6027\u62d2\u7edd\u91c7\u6837\u3001\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u7684LLM\u96c6\u6210\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u4ec5\u7528150\u6837\u672c\u53d1\u73b0\u65b0\u7684\u6700\u4f18\u5706\u5305\u88c5\u89e3\u3001\u8bbe\u8ba1\u9ad8\u6027\u80fdAIME\u6570\u5b66\u63a8\u7406\u4ee3\u7406\u3001\u6539\u8fdbALE-Bench\u7f16\u7a0b\u89e3\u3001\u53d1\u73b0\u65b0\u9896\u7684\u4e13\u5bb6\u6df7\u5408\u8d1f\u8f7d\u5747\u8861\u635f\u5931\u51fd\u6570\u3002", "conclusion": "ShinkaEvolve\u901a\u8fc7\u5f00\u6e90\u548c\u6210\u672c\u6548\u76ca\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u8ba1\u7b97\u95ee\u9898\u7684\u6c11\u4e3b\u5316\u5f00\u653e\u5f0f\u53d1\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20095", "abs": "https://arxiv.org/abs/2509.20095", "authors": ["Aymeric Vellinger", "Nemanja Antonic", "Elio Tuci"], "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms", "comment": "Contribution to the 9th International Symposium on Swarm Behavior and\n  Bio-Inspired Robotics 2025", "summary": "Swarm intelligence emerges from decentralised interactions among simple\nagents, enabling collective problem-solving. This study establishes a\ntheoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and\nreinforcement learning (RL), demonstrating how stigmergic signals function as\ndistributed reward mechanisms. We model engineered nematode swarms performing\nforaging tasks, showing that pheromone dynamics mathematically mirror\ncross-learning updates, a fundamental RL algorithm. Experimental validation\nwith data from literature confirms that our model accurately replicates\nempirical \\celeg\\ foraging patterns under static conditions. In dynamic\nenvironments, persistent pheromone trails create positive feedback loops that\nhinder adaptation by locking swarms into obsolete choices. Through\ncomputational experiments in multi-armed bandit scenarios, we reveal that\nintroducing a minority of exploratory agents insensitive to pheromones restores\ncollective plasticity, enabling rapid task switching. This behavioural\nheterogeneity balances exploration-exploitation trade-offs, implementing\nswarm-level extinction of outdated strategies. Our results demonstrate that\nstigmergic systems inherently encode distributed RL processes, where\nenvironmental signals act as external memory for collective credit assignment.\nBy bridging synthetic biology with swarm robotics, this work advances\nprogrammable living systems capable of resilient decision-making in volatile\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u79c0\u4e3d\u9690\u6746\u7ebf\u866b\u4e2d\u4fe1\u606f\u7d20\u4ecb\u5bfc\u7684\u805a\u96c6\u884c\u4e3a\u4e0e\u5f3a\u5316\u5b66\u4e60\u4e4b\u95f4\u7684\u7406\u8bba\u7b49\u4ef7\u6027\uff0c\u5c55\u793a\u4e86\u4fe1\u606f\u7d20\u4f5c\u4e3a\u5206\u5e03\u5f0f\u5956\u52b1\u673a\u5236\u7684\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5f15\u5165\u63a2\u7d22\u6027\u4ee3\u7406\u53ef\u4ee5\u6062\u590d\u7fa4\u4f53\u53ef\u5851\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u751f\u7269\u7fa4\u4f53\u667a\u80fd\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u7279\u522b\u662f\u4fe1\u606f\u7d20\u4ecb\u5bfc\u7684\u7fa4\u4f53\u884c\u4e3a\u5982\u4f55\u5b9e\u73b0\u5206\u5e03\u5f0f\u51b3\u7b56\u548c\u96c6\u4f53\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5efa\u7acb\u6570\u5b66\u6a21\u578b\u5c06\u4fe1\u606f\u7d20\u52a8\u6001\u4e0e\u5f3a\u5316\u5b66\u4e60\u66f4\u65b0\u7b97\u6cd5\u5bf9\u5e94\uff0c\u4f7f\u7528\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u5728\u591a\u81c2\u8001\u864e\u673a\u573a\u666f\u4e2d\u8fdb\u884c\u8ba1\u7b97\u5b9e\u9a8c\u6765\u6d4b\u8bd5\u7fa4\u4f53\u9002\u5e94\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u4fe1\u606f\u7d20\u7cfb\u7edf\u672c\u8d28\u4e0a\u7f16\u7801\u4e86\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u73af\u5883\u4fe1\u53f7\u4f5c\u4e3a\u96c6\u4f53\u4fe1\u7528\u5206\u914d\u7684\u5916\u90e8\u8bb0\u5fc6\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u5f15\u5165\u5bf9\u4fe1\u606f\u7d20\u4e0d\u654f\u611f\u7684\u63a2\u7d22\u6027\u4ee3\u7406\u53ef\u4ee5\u5e73\u8861\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002", "conclusion": "\u7ed3\u8bba\u662f\u4fe1\u606f\u7d20\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u884c\u4e3a\u5f02\u8d28\u6027\u53ef\u4ee5\u589e\u5f3a\u7fa4\u4f53\u5728\u6ce2\u52a8\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u5f39\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.19538", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19538", "abs": "https://arxiv.org/abs/2509.19538", "authors": ["Zongyue Li", "Xiao Han", "Yusong Li", "Niklas Strauss", "Matthias Schubert"], "title": "DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions", "comment": "ICML2025 workshop Building Physically Plausible World Models", "summary": "Diffusion-based world models have demonstrated strong capabilities in\nsynthesizing realistic long-horizon trajectories for offline reinforcement\nlearning (RL). However, many existing methods do not directly generate actions\nalongside states and rewards, limiting their compatibility with standard\nvalue-based offline RL algorithms that rely on one-step temporal difference\n(TD) learning. While prior work has explored joint modeling of states, rewards,\nand actions to address this issue, such formulations often lead to increased\ntraining complexity and reduced performance in practice. We propose\n\\textbf{DAWM}, a diffusion-based world model that generates future state-reward\ntrajectories conditioned on the current state, action, and return-to-go, paired\nwith an inverse dynamics model (IDM) for efficient action inference. This\nmodular design produces complete synthetic transitions suitable for one-step\nTD-based offline RL, enabling effective and computationally efficient training.\nEmpirically, we show that conservative offline RL algorithms such as TD3BC and\nIQL benefit significantly from training on these augmented trajectories,\nconsistently outperforming prior diffusion-based baselines across multiple\ntasks in the D4RL benchmark.", "AI": {"tldr": "DAWM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u72b6\u6001-\u5956\u52b1\u8f68\u8ff9\u5e76\u914d\u5408\u9006\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u52a8\u4f5c\u63a8\u65ad\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u5b8c\u6574\u7684\u5408\u6210\u8f6c\u6362\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u901a\u5e38\u4e0d\u76f4\u63a5\u751f\u6210\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u4e0e\u6807\u51c6\u503c\u51fd\u6570\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u517c\u5bb9\u6027\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5c1d\u8bd5\u8054\u5408\u5efa\u6a21\u72b6\u6001\u3001\u5956\u52b1\u548c\u52a8\u4f5c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u589e\u52a0\u4e86\u8bad\u7ec3\u590d\u6742\u6027\u5e76\u964d\u4f4e\u4e86\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u63d0\u51faDAWM\u6a21\u578b\uff0c\u751f\u6210\u57fa\u4e8e\u5f53\u524d\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u56de\u62a5\u7684\u672a\u6765\u72b6\u6001-\u5956\u52b1\u8f68\u8ff9\uff0c\u540c\u65f6\u4f7f\u7528\u9006\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u7684\u52a8\u4f5c\u63a8\u65ad\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\u4ea7\u751f\u9002\u5408\u4e00\u6b65TD\u5b66\u4e60\u7684\u5b8c\u6574\u5408\u6210\u8f6c\u6362\u6570\u636e\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4fdd\u5b88\u578b\u79bb\u7ebfRL\u7b97\u6cd5\u5982TD3BC\u548cIQL\u5728\u4f7f\u7528DAWM\u751f\u6210\u7684\u589e\u5f3a\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DAWM\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u652f\u6301\u57fa\u4e8e\u4e00\u6b65TD\u5b66\u4e60\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19554", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19554", "abs": "https://arxiv.org/abs/2509.19554", "authors": ["Yi Ren"], "title": "Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks", "comment": "175 pages", "summary": "This thesis explores how deep learning models learn over time, using ideas\ninspired by force analysis. Specifically, we zoom in on the model's training\nprocedure to see how one training example affects another during learning, like\nanalyzing how forces move objects. We break this influence into two parts: how\nsimilar the two examples are, and how strong the updating force is. This\nframework helps us understand a wide range of the model's behaviors in\ndifferent real systems. For example, it explains why certain examples have\nnon-trivial learning paths, why (and why not) some LLM finetuning methods work,\nand why simpler, more structured patterns tend to be learned more easily. We\napply this approach to various learning tasks and uncover new strategies for\nimproving model training. While the method is still developing, it offers a new\nway to interpret models' behaviors systematically.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529b\u5206\u6790\u601d\u60f3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u5206\u6790\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u6837\u672c\u95f4\u7684\u5f71\u54cd\u5206\u89e3\u4e3a\u76f8\u4f3c\u6027\u548c\u66f4\u65b0\u5f3a\u5ea6\u4e24\u4e2a\u90e8\u5206\uff0c\u7528\u4e8e\u89e3\u91ca\u6a21\u578b\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u968f\u65f6\u95f4\u5b66\u4e60\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\uff0c\u7c7b\u4f3c\u4e8e\u529b\u5206\u6790\u4e2d\u7269\u4f53\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u529b\u5206\u6790\u7684\u601d\u60f3\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u6837\u672c\u95f4\u7684\u5f71\u54cd\u5206\u89e3\u4e3a\u76f8\u4f3c\u6027\u548c\u66f4\u65b0\u5f3a\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e94\u7528\u4e8e\u5404\u79cd\u5b66\u4e60\u4efb\u52a1\u4e2d\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u89e3\u91ca\u6a21\u578b\u7684\u5404\u79cd\u884c\u4e3a\uff0c\u5305\u62ec\u7279\u5b9a\u6837\u672c\u7684\u5b66\u4e60\u8def\u5f84\u3001LLM\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c\u539f\u56e0\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u7ed3\u6784\u5316\u6a21\u5f0f\u66f4\u5bb9\u6613\u5b66\u4e60\u3002\u540c\u65f6\u53d1\u73b0\u4e86\u6539\u8fdb\u6a21\u578b\u8bad\u7ec3\u7684\u65b0\u7b56\u7565\u3002", "conclusion": "\u867d\u7136\u8be5\u65b9\u6cd5\u4ecd\u5728\u53d1\u5c55\u4e2d\uff0c\u4f46\u4e3a\u7cfb\u7edf\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2509.19358", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19358", "abs": "https://arxiv.org/abs/2509.19358", "authors": ["Chimaobi Okite", "Naihao Deng", "Kiran Bodipati", "Huaidian Hou", "Joyce Chai", "Rada Mihalcea"], "title": "Benchmarking and Improving LLM Robustness for Personalized Generation", "comment": "First draft. First camera-ready version", "summary": "Recent years have witnessed a growing interest in personalizing the responses\nof large language models (LLMs). While existing evaluations primarily focus on\nwhether a response aligns with a user's preferences, we argue that factuality\nis an equally important yet often overlooked dimension. In the context of\npersonalization, we define a model as robust if its responses are both\nfactually accurate and align with the user preferences. To assess this, we\nintroduce PERG, a scalable framework for evaluating robustness in LLMs, along\nwith a new dataset, PERGData. We evaluate fourteen models from five different\nmodel families using different prompting methods. Our findings show that\ncurrent LLMs struggle with robust personalization: even the strongest models\n(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously\nsuccessful cases without personalization, while smaller models (e.g., 7B-scale)\ncan fail more than 20% of the time. Further analysis reveals that robustness is\nsignificantly affected by the nature of the query and the type of user\npreference. To mitigate these failures, we propose Pref-Aligner, a two-stage\napproach that improves robustness by an average of 25% across models. Our work\nhighlights critical gaps in current evaluation practices and introduces tools\nand metrics to support more reliable, user-aligned LLM deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PERG\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u4e2a\u6027\u5316\u54cd\u5e94\u4e2d\u7684\u7a33\u5065\u6027\uff0c\u5f3a\u8c03\u4e8b\u5b9e\u51c6\u786e\u6027\u662f\u91cd\u8981\u4f46\u5e38\u88ab\u5ffd\u89c6\u7684\u7ef4\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86Pref-Aligner\u65b9\u6cd5\u63d0\u5347\u7a33\u5065\u602725%\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8LLM\u54cd\u5e94\u662f\u5426\u4e0e\u7528\u6237\u504f\u597d\u4e00\u81f4\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u8fd9\u4e00\u91cd\u8981\u7ef4\u5ea6\u3002\u4f5c\u8005\u8ba4\u4e3a\u5728\u4e2a\u6027\u5316\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u5e94\u540c\u65f6\u4fdd\u8bc1\u54cd\u5e94\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u7528\u6237\u504f\u597d\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165PERG\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\u548cPERGData\u6570\u636e\u96c6\uff0c\u8bc4\u4f3014\u4e2a\u6a21\u578b\uff1b\u63d0\u51faPref-Aligner\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u63d0\u5347\u6a21\u578b\u7a33\u5065\u6027\u3002", "result": "\u5f53\u524dLLM\u5728\u7a33\u5065\u4e2a\u6027\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1a\u6700\u5f3a\u6a21\u578b(GPT-4.1, LLaMA3-70B)\u57285%\u7684\u65e0\u4e2a\u6027\u5316\u6210\u529f\u6848\u4f8b\u4e2d\u65e0\u6cd5\u4fdd\u6301\u6b63\u786e\u6027\uff0c\u5c0f\u6a21\u578b(7B\u89c4\u6a21)\u5931\u8d25\u7387\u8d85\u8fc720%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u7684\u5173\u952e\u7f3a\u9677\uff0c\u63d0\u4f9b\u4e86\u652f\u6301\u66f4\u53ef\u9760\u3001\u7528\u6237\u5bf9\u9f50LLM\u90e8\u7f72\u7684\u5de5\u5177\u548c\u6307\u6807\u3002", "topic": "agent analysis"}}
{"id": "2509.19360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19360", "abs": "https://arxiv.org/abs/2509.19360", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Semantic Representation Attack against Aligned Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u8868\u793a\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u8868\u793a\u7a7a\u95f4\u800c\u975e\u7cbe\u786e\u6587\u672c\u6a21\u5f0f\u6765\u7ed5\u8fc7LLM\u7684\u5bf9\u9f50\u9632\u62a4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u653b\u51fb\u6548\u679c\u548c\u63d0\u793a\u81ea\u7136\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9\u5bf9\u9f50LLM\u7684\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7cbe\u786e\u7684\u80af\u5b9a\u54cd\u5e94\u6a21\u5f0f\uff0c\u5b58\u5728\u6536\u655b\u6027\u6709\u9650\u3001\u63d0\u793a\u4e0d\u81ea\u7136\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u653b\u51fb\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u8868\u793a\u653b\u51fb\u8303\u5f0f\uff0c\u5229\u7528\u5305\u542b\u7b49\u6548\u6709\u5bb3\u542b\u4e49\u7684\u591a\u6837\u5316\u54cd\u5e94\u7684\u8bed\u4e49\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bed\u4e49\u8868\u793a\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\u6765\u9ad8\u6548\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u4e14\u7b80\u6d01\u7684\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u572818\u4e2aLLM\u4e0a\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u5e73\u574789.41%\uff0c\u5176\u4e2d11\u4e2a\u6a21\u578b\u8fbe\u5230100%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9690\u853d\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8bed\u4e49\u8868\u793a\u653b\u51fb\u5728\u6574\u4f53\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5bf9\u6297\u5bf9\u9f50LLM\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2509.20175", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20175", "abs": "https://arxiv.org/abs/2509.20175", "authors": ["Lorenzo Giusti", "Ole Anton Werner", "Riccardo Taiello", "Matilde Carvalho Costa", "Emre Tosun", "Andrea Protani", "Marc Molina", "Rodrigo Lopes de Almeida", "Paolo Cacace", "Diogo Reis Santos", "Luigi Serio"], "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI", "comment": "18 pages, 4 figures", "summary": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents.", "AI": {"tldr": "FoA\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7248\u672c\u5316\u80fd\u529b\u5411\u91cf\u5b9e\u73b0\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5728HealthBench\u57fa\u51c6\u4e0a\u6bd4\u5355\u6a21\u578b\u57fa\u7ebf\u63d0\u534713\u500d\u6027\u80fd", "motivation": "\u89e3\u51b3\u9759\u6001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u57fa\u4e8e\u80fd\u529b\u7684\u52a8\u6001\u534f\u4f5c\uff0c\u91ca\u653e\u5f02\u6784AI\u667a\u80fd\u4f53\u8054\u90a6\u7684\u96c6\u4f53\u667a\u80fd", "method": "\u7ed3\u5408\u8bed\u4e49\u8def\u7531\u3001\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u548c\u667a\u80fd\u805a\u7c7b\u4e09\u5927\u521b\u65b0\uff1a\u8bed\u4e49\u8def\u7531\u901a\u8fc7HNSW\u7d22\u5f15\u5339\u914d\u4efb\u52a1\u4e0e\u667a\u80fd\u4f53\uff1b\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u8ba9\u517c\u5bb9\u667a\u80fd\u4f53\u534f\u4f5c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3aDAG\u5b50\u4efb\u52a1\uff1b\u667a\u80fd\u805a\u7c7b\u5c06\u5904\u7406\u76f8\u4f3c\u5b50\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5206\u7ec4\u8fdb\u884c\u591a\u8f6e\u4f18\u5316", "result": "\u5728HealthBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa13\u500d\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u89c6\u89d2\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u6548\u679c\u663e\u8457", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u7f16\u6392\u548c\u7ed3\u6784\u5316\u534f\u4f5c\u7684FoA\u6846\u67b6\u80fd\u591f\u6709\u6548\u91ca\u653e\u5f02\u6784AI\u667a\u80fd\u4f53\u8054\u90a6\u7684\u96c6\u4f53\u667a\u80fd\uff0c\u7cfb\u7edf\u5177\u6709\u6c34\u5e73\u6269\u5c55\u80fd\u529b\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2509.19369", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19369", "abs": "https://arxiv.org/abs/2509.19369", "authors": ["Changhyun Jeon", "Jinhee Park", "Jungwoo Choi", "Keonwoo Kim", "Jisu Kim", "Minji Hong"], "title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use", "comment": null, "summary": "We propose a small-scale language model (SLM) based agent architecture,\nPlanner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G\nseparates planning, calling, and generation by role: the Planner produces an\ninitial batch plan with limited on-demand replanning; the Caller returns a\nnormalized call object after joint schema-value validation; and the Generator\nintegrates tool outputs to produce the final answer. We apply a Korean-first\nvalue policy to reduce execution failures caused by frequent Korean-to-English\ncode switching in Korean settings. Evaluation assumes Korean queries and Korean\ntool/parameter specifications; it covers single-chain, multi-chain,\nmissing-parameters, and missing-functions scenarios, and is conducted via an\nLLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.\nResults show that P-C-G delivers competitive tool-use accuracy and end-to-end\nquality while reducing tokens and maintaining acceptable latency, indicating\nthat role-specialized SLMs are a cost-effective alternative for Korean tool-use\nagents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u97e9\u8bed\u5de5\u5177\u4f7f\u7528\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u67b6\u6784P-C-G\uff08Planner-Caller-Generator\uff09\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u548c\u97e9\u8bed\u4f18\u5148\u7b56\u7565\u63d0\u5347\u97e9\u8bed\u73af\u5883\u4e0b\u7684\u5de5\u5177\u4f7f\u7528\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u97e9\u8bed\u73af\u5883\u4e2d\u9891\u7e41\u7684\u97e9\u82f1\u4ee3\u7801\u5207\u6362\u5bfc\u81f4\u7684\u6267\u884c\u5931\u8d25\u95ee\u9898\uff0c\u4e3a\u97e9\u8bed\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Planner-Caller-Generator\u4e09\u9636\u6bb5\u67b6\u6784\uff1aPlanner\u8fdb\u884c\u521d\u59cb\u6279\u91cf\u89c4\u5212\u548c\u6309\u9700\u91cd\u89c4\u5212\uff0cCaller\u8fdb\u884c\u8054\u5408\u6a21\u5f0f\u503c\u9a8c\u8bc1\u5e76\u8fd4\u56de\u89c4\u8303\u5316\u8c03\u7528\u5bf9\u8c61\uff0cGenerator\u6574\u5408\u5de5\u5177\u8f93\u51fa\u751f\u6210\u6700\u7ec8\u7b54\u6848\u3002\u5e94\u7528\u97e9\u8bed\u4f18\u5148\u503c\u7b56\u7565\u3002", "result": "P-C-G\u5728\u97e9\u8bed\u67e5\u8be2\u548c\u5de5\u5177\u89c4\u8303\u4e0b\uff0c\u5728\u5355\u94fe\u3001\u591a\u94fe\u3001\u7f3a\u5931\u53c2\u6570\u548c\u7f3a\u5931\u51fd\u6570\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u5de5\u5177\u4f7f\u7528\u51c6\u786e\u6027\u548c\u7aef\u5230\u7aef\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11token\u4f7f\u7528\u5e76\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u89d2\u8272\u4e13\u95e8\u5316\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u662f\u97e9\u8bed\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u6210\u672c\u6548\u76ca\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.19674", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19674", "abs": "https://arxiv.org/abs/2509.19674", "authors": ["Kunlun Xu", "Yibo Feng", "Jiangmeng Li", "Yongsheng Qi", "Jiahuan Zhou"], "title": "C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Federated continual learning (FCL) tackles scenarios of learning from\ncontinuously emerging task data across distributed clients, where the key\nchallenge lies in addressing both temporal forgetting over time and spatial\nforgetting simultaneously. Recently, prompt-based FCL methods have shown\nadvanced performance through task-wise prompt communication.In this study, we\nunderscore that the existing prompt-based FCL methods are prone to class-wise\nknowledge coherence between prompts across clients. The class-wise knowledge\ncoherence includes two aspects: (1) intra-class distribution gap across\nclients, which degrades the learned semantics across prompts, (2) inter-prompt\nclass-wise relevance, which highlights cross-class knowledge confusion. During\nprompt communication, insufficient class-wise coherence exacerbates knowledge\nconflicts among new prompts and induces interference with old prompts,\nintensifying both spatial and temporal forgetting. To address these issues, we\npropose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method\nthat explicitly enhances class-wise knowledge coherence during prompt\ncommunication. Specifically, a local class distribution compensation mechanism\n(LCDC) is introduced to reduce intra-class distribution disparities across\nclients, thereby reinforcing intra-class knowledge consistency. Additionally, a\nclass-aware prompt aggregation scheme (CPA) is designed to alleviate\ninter-class knowledge confusion by selectively strengthening class-relevant\nknowledge aggregation. Extensive experiments on multiple FCL benchmarks\ndemonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our\nsource code is available at\nhttps://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt", "AI": {"tldr": "\u63d0\u51faC\u00b2Prompt\u65b9\u6cd5\u89e3\u51b3\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u63d0\u793a\u901a\u4fe1\u7684\u7c7b\u95f4\u77e5\u8bc6\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u7c7b\u5206\u5e03\u8865\u507f\u548c\u7c7b\u611f\u77e5\u63d0\u793a\u805a\u5408\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7c7b\u95f4\u77e5\u8bc6\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u95f4\u7684\u7c7b\u5185\u5206\u5e03\u5dee\u8ddd\u548c\u63d0\u793a\u95f4\u7684\u7c7b\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u77e5\u8bc6\u51b2\u7a81\u548c\u9057\u5fd8\u52a0\u5267", "method": "\u63d0\u51faC\u00b2Prompt\u65b9\u6cd5\uff0c\u5305\u542b\u5c40\u90e8\u7c7b\u5206\u5e03\u8865\u507f\u673a\u5236(LCDC)\u51cf\u5c11\u5ba2\u6237\u7aef\u95f4\u7c7b\u5185\u5206\u5e03\u5dee\u5f02\uff0c\u4ee5\u53ca\u7c7b\u611f\u77e5\u63d0\u793a\u805a\u5408\u65b9\u6848(CPA)\u9009\u62e9\u6027\u589e\u5f3a\u7c7b\u76f8\u5173\u77e5\u8bc6\u805a\u5408", "result": "\u5728\u591a\u4e2a\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "C\u00b2Prompt\u901a\u8fc7\u589e\u5f3a\u63d0\u793a\u901a\u4fe1\u4e2d\u7684\u7c7b\u95f4\u77e5\u8bc6\u4e00\u81f4\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u65f6\u7a7a\u9057\u5fd8\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2509.19593", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19593", "abs": "https://arxiv.org/abs/2509.19593", "authors": ["Dylan Hutson", "Daniel Vennemeyer", "Aneesh Deshmukh", "Justin Zhan", "Tianyu Jiang"], "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models", "comment": "EMNLP 2025, 17 pages, 2 figures", "summary": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning.", "AI": {"tldr": "GuessingGame\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u4f5c\u4e3a\u6218\u7565\u63d0\u95ee\u8005\u7684\u534f\u8bae\uff0c\u901a\u8fc7\u81ea\u7531\u5f62\u5f0f\u63d0\u95ee\u6765\u8bc6\u522b\u9690\u85cf\u5bf9\u8c61\uff0c\u63d0\u51fa\u4e24\u79cd\u4fe1\u606f\u589e\u76ca\u6307\u6807\u6765\u8861\u91cf\u95ee\u9898\u8d28\u91cf\uff0c\u5e76\u8bc1\u660e\u9ad8\u4fe1\u606f\u589e\u76ca\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u52a8\u673a\u662f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u9886\u57df\u8bbe\u7f6e\u4e2d\u4f5c\u4e3a\u6218\u7565\u63d0\u95ee\u8005\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u9884\u5b9a\u4e49\u9009\u62e9\u6216\u5019\u9009\u5217\u8868\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u63d0\u95ee\u6765\u8bc6\u522b\u9690\u85cf\u5bf9\u8c61\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165GuessingGame\u534f\u8bae\uff0c\u63d0\u51fa\u4e24\u79cd\u4fe1\u606f\u589e\u76ca\u6307\u6807\uff1a\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u65b9\u6cd5\u8ddf\u8e2a\u8bed\u4e49\u6982\u5ff5\u7684\u4fe1\u5ff5\u66f4\u65b0\uff0c\u4ee5\u53ca\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u901a\u8fc7ConceptNet\u8fc7\u6ee4\u5019\u9009\u3002\u8fd9\u4e9b\u6307\u6807\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u652f\u6301\u4e8b\u540e\u5206\u6790\u3002", "result": "\u5728858\u4e2a\u6e38\u620f\u4e2d\uff0c\u66f4\u9ad8\u7684\u4fe1\u606f\u589e\u76ca\u5f3a\u70c8\u9884\u6d4b\u6548\u7387\uff1a\u4fe1\u606f\u589e\u76ca\u589e\u52a0\u4e00\u4e2a\u6807\u51c6\u5dee\uff0c\u9884\u671f\u6e38\u620f\u957f\u5ea6\u51cf\u5c1143%\u3002\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u5f15\u5bfc\u7684\u63d0\u793a\u7ea6\u675f\uff0c\u5982\u5f3a\u5236\u95ee\u9898\u591a\u6837\u6027\uff0c\u4f7f\u8f83\u5f31\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662fLLMs\u4e2d\u7684\u63d0\u95ee\u80fd\u529b\u662f\u53ef\u6d4b\u91cf\u548c\u53ef\u6539\u8fdb\u7684\uff0c\u5bf9\u4e8e\u4ea4\u4e92\u5f0f\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2509.19771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19771", "abs": "https://arxiv.org/abs/2509.19771", "authors": ["Hyunwoo Kim", "Hyo Kyung Lee"], "title": "Frictional Q-Learning", "comment": null, "summary": "We draw an analogy between static friction in classical mechanics and\nextrapolation error in off-policy RL, and use it to formulate a constraint that\nprevents the policy from drifting toward unsupported actions. In this study, we\npresent Frictional Q-learning, a deep reinforcement learning algorithm for\ncontinuous control, which extends batch-constrained reinforcement learning. Our\nalgorithm constrains the agent's action space to encourage behavior similar to\nthat in the replay buffer, while maintaining a distance from the manifold of\nthe orthonormal action space. The constraint preserves the simplicity of\nbatch-constrained, and provides an intuitive physical interpretation of\nextrapolation error. Empirically, we further demonstrate that our algorithm is\nrobustly trained and achieves competitive performance across standard\ncontinuous control benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFrictional Q-learning\u7b97\u6cd5\uff0c\u901a\u8fc7\u7c7b\u6bd4\u7ecf\u5178\u529b\u5b66\u4e2d\u7684\u9759\u6469\u64e6\u529b\u6765\u9632\u6b62\u7b56\u7565\u6f02\u79fb\u5230\u4e0d\u652f\u6301\u7684\u52a8\u4f5c\u7528\uff0c\u4ece\u800c\u51cf\u5c11\u5916\u63a8\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5916\u63a8\u8bef\u5dee\u95ee\u9898\uff0c\u9632\u6b62\u7b56\u7565\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u6f02\u79fb\u5230\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u65e0\u6548\u52a8\u4f5c\u3002", "method": "\u6269\u5c55\u6279\u91cf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ea6\u675f\u667a\u80fd\u4f53\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u5176\u884c\u4e3a\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u7684\u884c\u4e3a\u76f8\u4f3c\uff0c\u540c\u65f6\u4e0e\u6b63\u4ea4\u52a8\u4f5c\u7a7a\u95f4\u7684\u6d41\u5f62\u4fdd\u6301\u8ddd\u79bb\u3002", "result": "\u7b97\u6cd5\u5728\u6807\u51c6\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u8bad\u7ec3\u6548\u679c\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Frictional Q-learning\u7b97\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u5916\u63a8\u8bef\u5dee\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u7269\u7406\u89e3\u91ca\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19640", "abs": "https://arxiv.org/abs/2509.19640", "authors": ["Ryan Shea", "Zhou Yu"], "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification", "comment": "EMNLP Findings 2025", "summary": "Patents play a critical role in driving technological innovation by granting\ninventors exclusive rights to their inventions. However the process of drafting\na patent application is often expensive and time-consuming, making it a prime\ncandidate for automation. Despite recent advancements in language models,\nseveral challenges hinder the development of robust automated patent drafting\nsystems. First, the information within a patent application is highly\nconfidential, which often prevents the use of closed-source LLMs for automating\nthis task. Second, the process of drafting a patent application is difficult\nfor even the most advanced language models due to their long context, technical\nwriting style, and specialized domain knowledge. To address these challenges,\nwe introduce AutoSpec, a secure, agentic framework for Automatically drafting\npatent Specification. Our approach decomposes the drafting process into a\nsequence of manageable subtasks, each solvable by smaller, open-source language\nmodels enhanced with custom tools tailored for drafting patent specification.\nTo assess our system, we design a novel evaluation protocol in collaboration\nwith experienced patent attorneys. Our automatic and expert evaluations show\nthat AutoSpec outperforms existing baselines on a patent drafting task.", "AI": {"tldr": "AutoSpec\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u8d77\u8349\u4e13\u5229\u8bf4\u660e\u4e66\u7684\u4fdd\u5bc6\u6027\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8d77\u8349\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u548c\u5b9a\u5236\u5de5\u5177\u6765\u89e3\u51b3\u4e13\u5229\u8d77\u8349\u7684\u6311\u6218\u3002", "motivation": "\u4e13\u5229\u8d77\u8349\u8fc7\u7a0b\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u4f46\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4fdd\u5bc6\u6027\u8981\u6c42\u548c\u4e13\u5229\u6280\u672f\u6027\u5199\u4f5c\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5b89\u5168\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4e13\u5229\u8d77\u8349\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5e8f\u5217\u5316\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5b9a\u5236\u5de5\u5177\u6765\u9010\u6b65\u5b8c\u6210\u4e13\u5229\u8bf4\u660e\u4e66\u7684\u8d77\u8349\u5de5\u4f5c\u3002", "result": "\u901a\u8fc7\u4e0e\u4e13\u5229\u5f8b\u5e08\u5408\u4f5c\u8bbe\u8ba1\u7684\u8bc4\u4f30\u534f\u8bae\u663e\u793a\uff0cAutoSpec\u5728\u4e13\u5229\u8d77\u8349\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AutoSpec\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e13\u5229\u8d77\u8349\u81ea\u52a8\u5316\u7684\u4fdd\u5bc6\u6027\u548c\u6280\u672f\u6027\u6311\u6218\uff0c\u4e3a\u4e13\u5229\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.19789", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19789", "abs": "https://arxiv.org/abs/2509.19789", "authors": ["Carlo Bosio", "Greg Woelki", "Noureldin Hendy", "Nicholas Roy", "Byungsoo Kim"], "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving", "comment": "10 pages, 6 figures", "summary": "Human drivers focus only on a handful of agents at any one time. On the other\nhand, autonomous driving systems process complex scenes with numerous agents,\nregardless of whether they are pedestrians on a crosswalk or vehicles parked on\nthe side of the road. While attention mechanisms offer an implicit way to\nreduce the input to the elements that affect decisions, existing attention\nmechanisms for capturing agent interactions are quadratic, and generally\ncomputationally expensive. We propose RDAR, a strategy to learn per-agent\nrelevance -- how much each agent influences the behavior of the controlled\nvehicle -- by identifying which agents can be excluded from the input to a\npre-trained behavior model. We formulate the masking procedure as a Markov\nDecision Process where the action consists of a binary mask indicating agent\nselection. We evaluate RDAR on a large-scale driving dataset, and demonstrate\nits ability to learn an accurate numerical measure of relevance by achieving\ncomparable driving performance, in terms of overall progress, safety and\nperformance, while processing significantly fewer agents compared to a state of\nthe art behavior model.", "AI": {"tldr": "RDAR\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6bcf\u4e2a\u667a\u80fd\u4f53\u76f8\u5173\u6027\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u54ea\u4e9b\u667a\u80fd\u4f53\u53ef\u4ee5\u4ece\u9884\u8bad\u7ec3\u884c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u4e2d\u6392\u9664\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4eba\u7c7b\u9a7e\u9a76\u5458\u6bcf\u6b21\u53ea\u5173\u6ce8\u5c11\u6570\u51e0\u4e2a\u667a\u80fd\u4f53\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u8981\u5904\u7406\u590d\u6742\u573a\u666f\u4e2d\u7684\u6240\u6709\u667a\u80fd\u4f53\uff0c\u65e0\u8bba\u5176\u662f\u5426\u5f71\u54cd\u51b3\u7b56\u3002\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u9009\u62e9\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u4f5c\u662f\u8868\u793a\u667a\u80fd\u4f53\u9009\u62e9\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\u3002\u901a\u8fc7\u8bc6\u522b\u53ef\u6392\u9664\u7684\u667a\u80fd\u4f53\u6765\u5b66\u4e60\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cRDAR\u80fd\u591f\u5b66\u4e60\u51c6\u786e\u7684\u667a\u80fd\u4f53\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u9a7e\u9a76\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5904\u7406\u7684\u667a\u80fd\u4f53\u6570\u91cf\u3002", "conclusion": "RDAR\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.19803", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19803", "abs": "https://arxiv.org/abs/2509.19803", "authors": ["Guochao Jiang", "Wenfeng Feng", "Guofeng Quan", "Chuzhan Hao", "Yuewei Zhang", "Guohua Liu", "Hao Wang"], "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models", "comment": null, "summary": "Policy-based reinforcement learning currently plays an important role in\nimproving LLMs on mathematical reasoning tasks. However, existing rollout-based\nreinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly\nconsider LLMs' learning ability for samples of different difficulty levels,\nwhich is contrary to the human cognitive process of mathematical reasoning\ntasks from easy to difficult. Intuitively, we find that the variance of the\nrollout group's reward in RLVR partly reflects the difficulty of the current\nsample for LLMs. Samples that are too easy or too difficult have a lower\nvariance, while samples with moderate difficulty have a higher variance. Based\non this, we propose VCRL, a curriculum reinforcement learning framework that\ndynamically controls the difficulty of training samples based on the variance\nof group rewards. Experiments on five mathematical benchmarks and two models\nreveal the advantages of VCRL over the current LLM RL baselines.", "AI": {"tldr": "VCRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5956\u52b1\u65b9\u5dee\u52a8\u6001\u63a7\u5236\u8bad\u7ec3\u6837\u672c\u96be\u5ea6\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8erollout\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6ca1\u6709\u8003\u8651LLM\u5bf9\u4e0d\u540c\u96be\u5ea6\u6837\u672c\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u4ece\u6613\u5230\u96be\u7684\u6570\u5b66\u63a8\u7406\u8ba4\u77e5\u8fc7\u7a0b\u76f8\u6096\u3002", "method": "\u57fa\u4e8e\u53d1\u73b0\u5956\u52b1\u65b9\u5dee\u53cd\u6620\u6837\u672c\u96be\u5ea6\u7684\u4e8b\u5b9e\uff0c\u63d0\u51faVCRL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6837\u672c\u96be\u5ea6\u6765\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u548c\u4e24\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVCRL\u4f18\u4e8e\u5f53\u524d\u7684LLM\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VCRL\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4ece\u6613\u5230\u96be\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19846", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19846", "abs": "https://arxiv.org/abs/2509.19846", "authors": ["Kevin Bradley Dsouza", "Enoch Ofosu", "Daniel Chukwuemeka Amaogu", "J\u00e9r\u00f4me Pigeon", "Richard Boudreault", "Pooneh Maghoul", "Juan Moreno-Cruz", "Yuri Leonenko"], "title": "BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management", "comment": null, "summary": "Boreal forests store 30-40% of terrestrial carbon, much in climate-vulnerable\npermafrost soils, making their management critical for climate mitigation.\nHowever, optimizing forest management for both carbon sequestration and\npermafrost preservation presents complex trade-offs that current tools cannot\nadequately address. We introduce $\\textbf{BoreaRL}$, the first multi-objective\nreinforcement learning environment for climate-adaptive boreal forest\nmanagement, featuring a physically-grounded simulator of coupled energy,\ncarbon, and water fluxes. BoreaRL supports two training paradigms:\nsite-specific mode for controlled studies and generalist mode for learning\nrobust policies under environmental stochasticity. Through evaluation of\nmulti-objective RL algorithms, we reveal a fundamental asymmetry in learning\ndifficulty: carbon objectives are significantly easier to optimize than thaw\n(permafrost preservation) objectives, with thaw-focused policies showing\nminimal learning progress across both paradigms. In generalist settings,\nstandard preference-conditioned approaches fail entirely, while a naive\ncurriculum learning approach achieves superior performance by strategically\nselecting training episodes. Analysis of learned strategies reveals distinct\nmanagement philosophies, where carbon-focused policies favor aggressive\nhigh-density coniferous stands, while effective multi-objective policies\nbalance species composition and density to protect permafrost while maintaining\ncarbon gains. Our results demonstrate that robust climate-adaptive forest\nmanagement remains challenging for current MORL methods, establishing BoreaRL\nas a valuable benchmark for developing more effective approaches. We\nopen-source BoreaRL to accelerate research in multi-objective RL for climate\napplications.", "AI": {"tldr": "BoreaRL\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u6c14\u5019\u9002\u5e94\u6027\u5317\u65b9\u68ee\u6797\u7ba1\u7406\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u63ed\u793a\u4e86\u78b3\u76ee\u6807\u6bd4\u6c38\u4e45\u51bb\u571f\u4fdd\u62a4\u76ee\u6807\u66f4\u5bb9\u6613\u4f18\u5316\uff0c\u6807\u51c6\u65b9\u6cd5\u5728\u901a\u7528\u8bbe\u7f6e\u4e2d\u5931\u8d25\uff0c\u800c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5317\u65b9\u68ee\u6797\u50a8\u5b58\u5927\u91cf\u78b3\uff0c\u4f46\u4f18\u5316\u68ee\u6797\u7ba1\u7406\u4ee5\u540c\u65f6\u5b9e\u73b0\u78b3\u5c01\u5b58\u548c\u6c38\u4e45\u51bb\u571f\u4fdd\u62a4\u5b58\u5728\u590d\u6742\u6743\u8861\uff0c\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u5145\u5206\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u8026\u5408\u80fd\u91cf\u3001\u78b3\u548c\u6c34\u901a\u91cf\u6a21\u62df\u5668\uff0c\u652f\u6301\u7ad9\u70b9\u7279\u5b9a\u6a21\u5f0f\u548c\u901a\u7528\u6a21\u5f0f\u4e24\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0\u78b3\u76ee\u6807\u4f18\u5316\u660e\u663e\u6bd4\u6c38\u4e45\u51bb\u571f\u4fdd\u62a4\u76ee\u6807\u5bb9\u6613\uff0c\u6807\u51c6\u504f\u597d\u6761\u4ef6\u65b9\u6cd5\u5728\u901a\u7528\u8bbe\u7f6e\u4e2d\u5b8c\u5168\u5931\u8d25\uff0c\u800c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u7b56\u7565\u6027\u9009\u62e9\u8bad\u7ec3\u7247\u6bb5\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a33\u5065\u7684\u6c14\u5019\u9002\u5e94\u6027\u68ee\u6797\u7ba1\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0cBoreaRL\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19924", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.19924", "abs": "https://arxiv.org/abs/2509.19924", "authors": ["Remo Sasso", "Michelangelo Conserva", "Dominik Jeurissen", "Paulo Rauber"], "title": "Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches", "comment": "16 pages, 7 figures. Accepted for presentation at the 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025) Workshop on the\n  Foundations of Reasoning in Language Models (FoRLM)", "summary": "Exploration in reinforcement learning (RL) remains challenging, particularly\nin sparse-reward settings. While foundation models possess strong semantic\npriors, their capabilities as zero-shot exploration agents in classic RL\nbenchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed\nbandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our\ninvestigation reveals a key limitation: while VLMs can infer high-level\nobjectives from visual input, they consistently fail at precise low-level\ncontrol: the \"knowing-doing gap\". To analyze a potential bridge for this gap,\nwe investigate a simple on-policy hybrid framework in a controlled, best-case\nscenario. Our results in this idealized setting show that VLM guidance can\nsignificantly improve early-stage sample efficiency, providing a clear analysis\nof the potential and constraints of using foundation models to guide\nexploration rather than for end-to-end control.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u63a2\u7d22\u80fd\u529b\uff0c\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b58\u5728\"\u77e5-\u884c\u5dee\u8ddd\"\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\u6765\u5229\u7528VLM\u6307\u5bfc\u63a2\u7d22\u800c\u975e\u7aef\u5230\u7aef\u63a7\u5236\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u5177\u6709\u6311\u6218\u6027\uff0c\u57fa\u7840\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u8bed\u4e49\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u5176\u5728\u7ecf\u5178RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f5c\u4e3a\u96f6\u6837\u672c\u63a2\u7d22\u4ee3\u7406\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u5728\u8001\u864e\u673a\u3001Gridworld\u548c\u7a00\u758f\u5956\u52b1Atari\u6e38\u620f\u4e2d\u6d4b\u8bd5LLM\u548cVLM\u7684\u96f6\u6837\u672c\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5728\u7ebf\u6df7\u5408\u6846\u67b6\u6765\u5206\u6790VLM\u6307\u5bfc\u63a2\u7d22\u7684\u6f5c\u529b\u3002", "result": "VLM\u80fd\u591f\u4ece\u89c6\u89c9\u8f93\u5165\u63a8\u65ad\u9ad8\u7ea7\u76ee\u6807\uff0c\u4f46\u5728\u7cbe\u786e\u4f4e\u7ea7\u63a7\u5236\u65b9\u9762\u6301\u7eed\u5931\u8d25\u3002\u5728\u7406\u60f3\u5316\u8bbe\u7f6e\u4e2d\uff0cVLM\u6307\u5bfc\u663e\u8457\u63d0\u9ad8\u4e86\u65e9\u671f\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u66f4\u9002\u5408\u7528\u4e8e\u6307\u5bfc\u63a2\u7d22\u800c\u975e\u7aef\u5230\u7aef\u63a7\u5236\uff0cVLM\u6307\u5bfc\u5728\u65e9\u671f\u63a2\u7d22\u9636\u6bb5\u5177\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.19893", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19893", "abs": "https://arxiv.org/abs/2509.19893", "authors": ["Minjae Oh", "Yunho Choi", "Dongmin Choi", "Yohan Jo"], "title": "Future Policy Aware Preference Learning for Mathematical Reasoning", "comment": "9 pages", "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have\nbecome standard for Large Language Model (LLM) post-training, yet they are\noften ineffective for mathematical reasoning. A key challenge is the large\ntoken overlap between preferred and dispreferred trajectories; lowering the\nprobability of dispreferred trajectories also reduces the probability of shared\nuseful tokens, leading to over-penalization and overall performance collapse.\nAs a mitigation, existing algorithms include the probability of a trajectory\nunder the current policy as a regularization term, which decreases the effect\nof the gradient when the probability is low. However, by the time this effect\ntakes hold, useful tokens may have already been over-penalized as the model has\nbegun to degrade. To address this, we propose Future Policy Aware (FPA)\npreference learning, which replaces the current policy with a future policy in\nthe regularization term. This future policy is estimated via lightweight,\nlogit-space extrapolation from a reference model toward the current model. FPA\nenables safer training by preemptively regularizing potentially problematic\ngradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH\nand GSM8K benchmarks. FPA yields consistent performance gains, with the largest\nimprovements observed with SimPER, achieving gains of up to 5.75%. We\ndemonstrate that FPA provides proactive regularization while preserving the\nprobability of shared, useful mathematical tokens, and enables longer,\ndegradation-free training with negligible computational overhead. We will\nrelease our code publicly upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86Future Policy Aware (FPA)\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u672a\u6765\u7b56\u7565\u66ff\u6362\u5f53\u524d\u7b56\u7565\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff0c\u89e3\u51b3\u6570\u5b66\u63a8\u7406\u4e2dDPO\u7b49\u65b9\u6cd5\u56e0token\u91cd\u53e0\u5bfc\u81f4\u7684\u8fc7\u5ea6\u60e9\u7f5a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u504f\u597d\u548c\u975e\u504f\u597d\u8f68\u8ff9\u95f4\u5b58\u5728\u5927\u91cftoken\u91cd\u53e0\uff0c\u964d\u4f4e\u975e\u504f\u597d\u8f68\u8ff9\u6982\u7387\u65f6\u4f1a\u8fc7\u5ea6\u60e9\u7f5a\u5171\u4eab\u7684\u6709\u7528token\uff0c\u5bfc\u81f4\u6027\u80fd\u5d29\u6e83\u3002", "method": "\u63d0\u51faFPA\u65b9\u6cd5\uff0c\u5728\u6b63\u5219\u5316\u9879\u4e2d\u4f7f\u7528\u672a\u6765\u7b56\u7565\u800c\u975e\u5f53\u524d\u7b56\u7565\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684logit\u7a7a\u95f4\u5916\u63a8\u4ece\u53c2\u8003\u6a21\u578b\u5411\u5f53\u524d\u6a21\u578b\u4f30\u8ba1\u672a\u6765\u7b56\u7565\uff0c\u5b9e\u73b0\u4e3b\u52a8\u6b63\u5219\u5316\u3002", "result": "\u5728MATH\u548cGSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFPA\u5e94\u7528\u4e8eDPO\u3001RPO\u548cSimPER\u5747\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5176\u4e2dSimPER\u63d0\u5347\u6700\u5927\u8fbe5.75%\uff0c\u4e14\u80fd\u5b9e\u73b0\u66f4\u957f\u7684\u65e0\u9000\u5316\u8bad\u7ec3\u3002", "conclusion": "FPA\u901a\u8fc7\u524d\u77bb\u6027\u6b63\u5219\u5316\u6709\u6548\u4fdd\u62a4\u5171\u4eab\u6570\u5b66token\u7684\u6982\u7387\uff0c\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20004", "abs": "https://arxiv.org/abs/2509.20004", "authors": ["Jan Broersen"], "title": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots", "comment": null, "summary": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u8131\u8282\"\u95ee\u9898\uff0c\u5373\u6a21\u578b\u7684\u77e5\u8bc6\u4e0e\u5176\u5bf9\u8bdd\u884c\u4e3a\u4e4b\u95f4\u7f3a\u4e4f\u5185\u5728\u8054\u7cfb\uff0c\u8fd9\u79cd\u8131\u8282\u662f\u6839\u672c\u6027\u7684\uff0c\u65e0\u6cd5\u901a\u8fc7\u66f4\u591a\u6570\u636e\u6216\u8bad\u7ec3\u89e3\u51b3\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u771f\u6b63\u5c06\u77e5\u8bc6\u4f5c\u4e3a\u5bf9\u8bdd\u884c\u4e3a\u7684\u57fa\u7840\uff0c\u5206\u6790\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u884c\u4e3a\u4e4b\u95f4\u7684\u8131\u8282\u73b0\u8c61\u53ca\u5176\u6839\u672c\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u8bba\u8bc1\u5206\u6790LLM\u8bad\u7ec3\u7684\u6838\u5fc3\u6280\u672f\uff08\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff09\u65e0\u6cd5\u5efa\u7acb\u77e5\u8bc6\u5230\u884c\u4e3a\u7684\u8fde\u63a5\uff0c\u5e76\u8003\u5bdf\u4f26\u7406\u5bf9\u9f50\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "result": "\u53d1\u73b0\u8131\u8282\u73b0\u8c61\u662fLLM\u7684\u6839\u672c\u9650\u5236\uff0c\u89e3\u91ca\u4e86\u5e7b\u89c9\u7684\u6765\u6e90\uff0c\u4f26\u7406\u5bf9\u9f50\u6280\u672f\u4e0d\u4ec5\u65e0\u6cd5\u89e3\u51b3\u8131\u8282\u95ee\u9898\uff0c\u53cd\u800c\u53ef\u80fd\u52a0\u5267\u95ee\u9898\u3002", "conclusion": "LLM\u7684\u77e5\u8bc6\u4e0e\u884c\u4e3a\u8131\u8282\u662f\u7ed3\u6784\u6027\u7684\u6839\u672c\u95ee\u9898\uff0c\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.20008", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20008", "abs": "https://arxiv.org/abs/2509.20008", "authors": ["Raphael Simon", "Pieter Libin", "Wim Mees"], "title": "Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation", "comment": "27 pages, 8 figures", "summary": "Penetration testing, the simulation of cyberattacks to identify security\nvulnerabilities, presents a sequential decision-making problem well-suited for\nreinforcement learning (RL) automation. Like many applications of RL to\nreal-world problems, partial observability presents a major challenge, as it\ninvalidates the Markov property present in Markov Decision Processes (MDPs).\nPartially Observable MDPs require history aggregation or belief state\nestimation to learn successful policies. We investigate stochastic, partially\nobservable penetration testing scenarios over host networks of varying size,\naiming to better reflect real-world complexity through more challenging and\nrepresentative benchmarks. This approach leads to the development of more\nrobust and transferable policies, which are crucial for ensuring reliable\nperformance across diverse and unpredictable real-world environments. Using\nvanilla Proximal Policy Optimization (PPO) as a baseline, we compare a\nselection of PPO variants designed to mitigate partial observability, including\nframe-stacking, augmenting observations with historical information, and\nemploying recurrent or transformer-based architectures. We conduct a systematic\nempirical analysis of these algorithms across different host network sizes. We\nfind that this task greatly benefits from history aggregation. Converging three\ntimes faster than other approaches. Manual inspection of the learned policies\nby the algorithms reveals clear distinctions and provides insights that go\nbeyond quantitative results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6e17\u900f\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u591a\u79cdPPO\u53d8\u4f53\u6765\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u6311\u6218\uff0c\u53d1\u73b0\u5386\u53f2\u805a\u5408\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6e17\u900f\u6d4b\u8bd5\u4f5c\u4e3a\u7f51\u7edc\u5b89\u5168\u653b\u51fb\u6a21\u62df\uff0c\u662f\u4e00\u4e2a\u9002\u5408\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u5316\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u3002\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u6311\u6218\u4e86\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u8fc1\u79fb\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528vanilla PPO\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u6bd4\u8f83\u4e86\u591a\u79cdPPO\u53d8\u4f53\uff1a\u5e27\u5806\u53e0\u3001\u5386\u53f2\u4fe1\u606f\u589e\u5f3a\u89c2\u6d4b\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5728\u53ef\u53d8\u89c4\u6a21\u7684\u7f51\u7edc\u4e3b\u673a\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8be5\u4efb\u52a1\u6781\u5927\u53d7\u76ca\u4e8e\u5386\u53f2\u805a\u5408\u65b9\u6cd5\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5feb\u4e09\u500d\u3002\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u624b\u52a8\u68c0\u67e5\u63ed\u793a\u4e86\u8d85\u8d8a\u5b9a\u91cf\u7ed3\u679c\u7684\u6e05\u6670\u533a\u522b\u548c\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u6e17\u900f\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u5386\u53f2\u805a\u5408\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u8fc1\u79fb\u7684\u7f51\u7edc\u5b89\u5168\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20097", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20097", "abs": "https://arxiv.org/abs/2509.20097", "authors": ["Sujeong Lee", "Hayoung Lee", "Seongsoo Heo", "Wonik Choi"], "title": "Integrated Framework for LLM Evaluation with Answer Generation", "comment": "16pages", "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86SPEED\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u591a\u7ef4\u5ea6\u63cf\u8ff0\u6027\u8bc4\u4f30\uff0c\u89e3\u51b3\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u8003\u7b54\u6848\u7684\u5c40\u9650\u6027", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u8003\u7b54\u6848\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u6210\u54cd\u5e94\u7684\u5b9a\u6027\u7279\u5f81\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6", "method": "\u4f7f\u7528\u4e13\u95e8\u7684\u529f\u80fd\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u5305\u62ec\u5e7b\u89c9\u68c0\u6d4b\u3001\u6bd2\u6027\u8bc4\u4f30\u548c\u8bcd\u6c47\u4e0a\u4e0b\u6587\u9002\u5f53\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u6574\u5408\u4e13\u5bb6\u53cd\u9988", "result": "SPEED\u5728\u4e0d\u540c\u9886\u57df\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7a33\u5065\u4e00\u81f4\u7684\u8bc4\u4f30\u6027\u80fd\uff0c\u4f7f\u7528\u76f8\u5bf9\u7d27\u51d1\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u6bd4\u5927\u89c4\u6a21\u8bc4\u4f30\u5668\u5177\u6709\u66f4\u597d\u7684\u8d44\u6e90\u6548\u7387", "conclusion": "SPEED\u663e\u8457\u63d0\u5347\u4e86LLM\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2509.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20162", "abs": "https://arxiv.org/abs/2509.20162", "authors": ["Chaojun Nie", "Jun Zhou", "Guanxiang Wang", "Shisong Wud", "Zichen Wang"], "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation", "comment": null, "summary": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRLAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u589e\u5f3a\u751f\u6210\u4e2d\u4f18\u5316LLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u77e5\u8bc6\u5d4c\u5165\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u77e5\u8bc6\u4f18\u5148\u7ea7\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "LLMs\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u4e13\u4e1a\u4fe1\u606f\u6bd4\u4f8b\u4e0d\u8db3\u4e14\u5b58\u5728\u65f6\u95f4\u6ede\u540e\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6301\u7eed\u9884\u8bad\u7ec3\u5bf9\u6240\u6709token\u540c\u7b49\u5bf9\u5f85\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u96be\u4ee5\u6784\u5efa\u8fde\u8d2f\u7684\u77e5\u8bc6\u7ed3\u6784\u3002", "method": "\u63d0\u51faRLAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u91c7\u6837\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u6a21\u578b\u4f18\u5316\uff0c\u9009\u62e9\u6700\u9ad8\u5bf9\u6570\u6982\u7387\u7684\u751f\u6210\u7ed3\u679c\uff0c\u8ba1\u7b97\u4e09\u4e2a\u5b9a\u5236\u5956\u52b1\u6307\u6807\u6765\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u3001\u5929\u6587\u5b66\u548c\u65f6\u4e8b\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RLAG\u65b9\u6cd5\u80fd\u6709\u6548\u5d4c\u5165\u5173\u952e\u4e14\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u4e13\u4e1a\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u5347LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20278", "abs": "https://arxiv.org/abs/2509.20278", "authors": ["Zipeng Ling", "Yuehao Tang", "Chen Huang", "Shuliang Liu", "Gaoyang Jiang", "Shenghong Fu", "Junqi Yang", "Yao Wan", "Jiawan Zhang", "Kejia Huang", "Xuming Hu"], "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage", "comment": null, "summary": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u63a8\u7406\u4e2d\u7684\u6307\u4ee4\u8fb9\u754c\u95ee\u9898\uff0c\u5373\u7528\u6237\u63d0\u4f9b\u6709\u504f\u89c1\u6216\u4e0d\u5b8c\u6574\u7684\u63d0\u793a\u65f6\u4f1a\u5bfc\u81f4LLM\u4ea7\u751f\u504f\u5dee\u3002\u4f5c\u8005\u63d0\u51fa\u4e86BiasDetector\u6846\u67b6\u6765\u6d4b\u91cf\u4e09\u79cd\u6307\u4ee4\u7c7b\u578b\uff08\u5b8c\u6574\u3001\u5197\u4f59\u3001\u4e0d\u8db3\uff09\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u53d1\u73b0\u4e3b\u6d41LLM\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002", "motivation": "LLM\u63a8\u7406\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u63d0\u793a\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7528\u6237\u53ef\u80fd\u65e0\u610f\u4e2d\u63d0\u4f9b\u6709\u504f\u89c1\u6216\u4e0d\u5b8c\u6574\u7684\u63d0\u793a\uff0c\u8fd9\u4f1a\u8bef\u5bfcLLM\uff0c\u964d\u4f4e\u53ef\u9760\u6027\u5e76\u5e26\u6765\u98ce\u9669\u3002", "method": "\u5c06\u6307\u4ee4\u8fb9\u754c\u95ee\u9898\u63d0\u70bc\u4e3a\u516b\u4e2a\u5177\u4f53\u65b9\u9762\uff0c\u5f15\u5165BiasDetector\u6846\u67b6\u6765\u6d4b\u91cf\u7531\u5b8c\u6574\u3001\u5197\u4f59\u548c\u4e0d\u8db3\u4e09\u79cd\u6307\u4ee4\u7c7b\u578b\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u5e76\u5bf9\u591a\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5c3d\u7ba1\u4e3b\u6d41LLM\u5728\u6807\u9898\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u63d0\u793a\u8986\u76d6\u8303\u56f4\u7684\u95ee\u9898\uff0c\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002\u5b9e\u8bc1\u7814\u7a76\u8bc1\u5b9eLLM\u63a8\u7406\u53ef\u9760\u6027\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "LLM\u63a8\u7406\u53ef\u9760\u6027\u9700\u8981\u663e\u8457\u6539\u8fdb\uff0c\u5f00\u53d1\u8005\u9700\u8981\u89e3\u51b3\u504f\u5dee\u95ee\u9898\uff0c\u7528\u6237\u9700\u8981\u8c28\u614e\u8bbe\u8ba1\u63d0\u793a\u3002\u5206\u6790\u4e86\u8fd9\u4e9b\u504f\u5dee\u7684\u5b9e\u9645\u5f71\u54cd\u5e76\u6982\u8ff0\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2509.20230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20230", "abs": "https://arxiv.org/abs/2509.20230", "authors": ["Wenhan Wu", "Zheyuan Liu", "Chongyang Gao", "Ren Wang", "Kaize Ding"], "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization", "comment": null, "summary": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524dLLM\u9057\u5fd8\u65b9\u6cd5\u7684\u5b89\u5168\u6f0f\u6d1e\uff1a\u88ab\u9057\u5fd8\u7684\u4fe1\u606f\u53ef\u901a\u8fc7\u91cd\u65b0\u5b66\u4e60\u653b\u51fb\u6062\u590d\uff0c\u539f\u56e0\u662f\u4f20\u7edf\u65b9\u6cd5\u5c06\u6a21\u578b\u53c2\u6570\u63a8\u5411\u635f\u5931\u51fd\u6570\u7684\u4e0d\u7a33\u5b9a\u533a\u57df\u3002\u4f5c\u8005\u63d0\u51faStableUN\u6846\u67b6\uff0c\u901a\u8fc7\u90bb\u57df\u611f\u77e5\u4f18\u5316\u5bfb\u627e\u66f4\u7a33\u5b9a\u7684\u53c2\u6570\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u91cd\u65b0\u5b66\u4e60\u548c\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u867d\u7136\u8868\u9762\u4e0a\u6210\u529f\u79fb\u9664\u4e86\u654f\u611f\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u88ab\u9057\u5fd8\u7684\u4fe1\u606f\u4ecd\u53ef\u901a\u8fc7\u5c11\u91cf\u5fae\u8c03\u6837\u672c\u5feb\u901f\u6062\u590d\u3002\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5c06\u6a21\u578b\u53c2\u6570\u4f18\u5316\u5230\u635f\u5931\u51fd\u6570\u7684\u4e0d\u7a33\u5b9a\u533a\u57df\u3002", "method": "\u63d0\u51faStableUN\u53cc\u7ea7\u53cd\u9988\u5f15\u5bfc\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u90bb\u57df\u611f\u77e5\u4f18\u5316\u5bfb\u627e\u7a33\u5b9a\u53c2\u6570\u533a\u57df\u3002\u6846\u67b6\u96c6\u6210\u9057\u5fd8\u53cd\u9988\uff08\u4f7f\u7528\u5bf9\u6297\u6270\u52a8\u63a2\u6d4b\u53c2\u6570\u90bb\u57df\uff09\u548c\u8bb0\u5fc6\u53cd\u9988\uff08\u4fdd\u6301\u6a21\u578b\u6548\u7528\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u6295\u5f71\u5bf9\u9f50\u4e24\u4e2a\u76ee\u6807\u3002", "result": "\u5728WMDP\u548cMUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u91cd\u65b0\u5b66\u4e60\u548c\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u6a21\u578b\u6548\u7528\u6027\u80fd\u3002", "conclusion": "StableUN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u9057\u5fd8\u65b9\u6cd5\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u5bfb\u627e\u7a33\u5b9a\u53c2\u6570\u533a\u57df\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u9057\u5fd8\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.20265", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20265", "abs": "https://arxiv.org/abs/2509.20265", "authors": ["\u00d6mer Veysel \u00c7a\u011fatan", "Bar\u0131\u015f Akg\u00fcn"], "title": "Failure Modes of Maximum Entropy RLHF", "comment": "26 pages, 9 figures", "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be\nderived as Maximum Entropy Reinforcement Learning with length-normalized\ntemperature, providing a theoretical foundation for this reference-free method.\nMotivated by SimPO's strong performance in offline preference optimization, we\ninvestigate whether Maximum Entropy RL can achieve similar results in online\nRLHF settings. Our experiments find that Maximum Entropy RL consistently\nexhibits overoptimization and unstable KL dynamics, even at very low learning\nrates. Unlike KL-constrained methods that maintain stable training, entropy\nregularization fails to prevent reward hacking and appears to correlate with\noveroptimization. Lastly, we discuss possible explanations for why SimPO\nsucceeds in offline settings while Maximum Entropy RL struggles in online\nscenarios. Our findings suggest that reference-free approaches may face\ndistinct challenges when applied to online or offline preference learning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86SimPO\u4f5c\u4e3a\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5bf9\u6bd4\u4e86\u5176\u5728\u79bb\u7ebf\u4e0e\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0\u6700\u5927\u71b5RL\u5728\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u4f1a\u51fa\u73b0\u8fc7\u4f18\u5316\u548c\u4e0d\u7a33\u5b9a\u7684KL\u52a8\u6001\u3002", "motivation": "\u53d7SimPO\u5728\u79bb\u7ebf\u504f\u597d\u4f18\u5316\u4e2d\u4f18\u5f02\u8868\u73b0\u7684\u542f\u53d1\uff0c\u7814\u7a76\u6700\u5927\u71b5RL\u662f\u5426\u80fd\u5728\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u7c7b\u4f3c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6700\u5927\u71b5RL\u4e0eKL\u7ea6\u675f\u65b9\u6cd5\u5728\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8fc7\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6700\u5927\u71b5RL\u5728\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u7684\u8fc7\u4f18\u5316\u548c\u4e0d\u7a33\u5b9aKL\u52a8\u6001\uff0c\u5373\u4f7f\u5b66\u4e60\u7387\u5f88\u4f4e\u4e5f\u65e0\u6cd5\u907f\u514d\u5956\u52b1\u7834\u89e3\u95ee\u9898\u3002", "conclusion": "SimPO\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u6210\u529f\u800c\u6700\u5927\u71b5RL\u5728\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u5931\u8d25\uff0c\u8868\u660e\u65e0\u53c2\u8003\u65b9\u6cd5\u5728\u5728\u7ebf\u548c\u79bb\u7ebf\u504f\u597d\u5b66\u4e60\u4e2d\u9762\u4e34\u4e0d\u540c\u7684\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20357", "abs": "https://arxiv.org/abs/2509.20357", "authors": ["Adithya Bhaskar", "Xi Ye", "Danqi Chen"], "title": "Language Models that Think, Chat Better", "comment": "Preprint; we release our code and models publicly at\n  https://github.com/princeton-pli/RLMT", "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model\nreasoning by using rule-based rewards in verifiable domains such as mathematics\nand code. However, RLVR leads to limited generalization for open-ended tasks --\nsuch as writing outline essays or making meal plans -- where humans reason\nroutinely. This paper shows that the RLVR paradigm is effective beyond\nverifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking\n(**RLMT**) for general-purpose chat capabilities. Using diverse real-world\nprompts, RLMT requires LMs to generate long CoT reasoning before response, and\noptimizes them with online RL against a preference-based reward model used in\nRLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and\ninstruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT\nconsistently outperforms standard RLHF pipelines. This includes substantial\ngains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and\nArenaHardV2), along with 1-3 point improvements on other tasks like creative\nwriting and general knowledge. Our best 8B model surpasses GPT-4o in chat and\ncreative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be\napplied directly to base models without an SFT stage, akin to R1-Zero training.\nRemarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT\nrecipe outperforms Llama-3.1-8B-Instruct post-trained with a complex\nmulti-staged pipeline with 25M+ examples. We close with qualitative and\nquantitative analyses of how trained models plan their responses. Our results\nrethink the post-training pipeline and call upon future work to understand and\nemploy thinking more broadly.", "AI": {"tldr": "RLMT\uff08\u6a21\u578b\u5956\u52b1\u601d\u8003\u5f3a\u5316\u5b66\u4e60\uff09\u901a\u8fc7\u8ba9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u957f\u94fe\u601d\u8003\u540e\u54cd\u5e94\uff0c\u5e76\u4f7f\u7528\u5728\u7ebfRL\u4f18\u5316\u504f\u597d\u5956\u52b1\u6a21\u578b\uff0c\u5728\u901a\u7528\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u8d85\u8d8a\u6807\u51c6RLHF\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u804a\u5929\u3001\u521b\u610f\u5199\u4f5c\u7b49\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u53ef\u9a8c\u8bc1\u9886\u57df\u6709\u6548\uff0c\u4f46\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u6cdb\u5316\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5c06\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u6269\u5c55\u5230\u975e\u53ef\u9a8c\u8bc1\u9886\u57df\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u5bf9\u8bdd\u80fd\u529b\u3002", "method": "\u63d0\u51faRLMT\u65b9\u6cd5\uff1a\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u63d0\u793a\u8ba9\u6a21\u578b\u751f\u6210\u957f\u94fe\u601d\u8003\uff08CoT\uff09\uff0c\u7136\u540e\u901a\u8fc7\u5728\u7ebfRL\uff08DPO\u3001PPO\u3001GRPO\u7b49\u7b97\u6cd5\uff09\u9488\u5bf9RLHF\u4e2d\u7684\u504f\u597d\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u572840\u4e2a\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\uff0cRLMT\u5728\u4e09\u4e2a\u804a\u5929\u57fa\u51c6\u4e0a\u83b7\u5f973-7\u5206\u63d0\u5347\uff0c\u521b\u610f\u5199\u4f5c\u548c\u5e38\u8bc6\u4efb\u52a1\u63d0\u53471-3\u5206\u30028B\u6a21\u578b\u5728\u804a\u5929\u548c\u521b\u610f\u5199\u4f5c\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u5ab2\u7f8eClaude-3.7-Sonnet\u3002\u4ec5\u75287K\u63d0\u793a\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u4f7f\u75282500\u4e07+\u6837\u672c\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\u540e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "RLMT\u91cd\u65b0\u601d\u8003\u4e86\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8bc1\u660e\u601d\u8003\u673a\u5236\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u547c\u5401\u672a\u6765\u5de5\u4f5c\u66f4\u5e7f\u6cdb\u7406\u89e3\u548c\u5e94\u7528\u601d\u8003\u673a\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20293", "abs": "https://arxiv.org/abs/2509.20293", "authors": ["Benjamin Feuer", "Chiung-Yi Tseng", "Astitwa Sarthak Lathe", "Oussama Elachqar", "John P Dickerson"], "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity", "comment": null, "summary": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86LLM\u8bc4\u5224\u57fa\u51c6\u7684\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8bca\u65ad\u673a\u5236\uff08\u6a21\u5f0f\u4f9d\u4ece\u6027\u548c\u5fc3\u7406\u6d4b\u91cf\u6709\u6548\u6027\uff09\u6765\u8bc6\u522b\u57fa\u51c6\u6392\u540d\u4e2d\u7684\u566a\u58f0\uff0c\u5e76\u5728Arena-Hard Auto\u57fa\u51c6\u4e0a\u53d1\u73b0\u4e25\u91cd\u7684\u6a21\u5f0f\u4e0d\u4e00\u81f4\u548c\u56e0\u5b50\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "LLM\u8bc4\u5224\u57fa\u51c6\u5728\u8bc4\u4f30\u590d\u6742\u6a21\u578b\u884c\u4e3a\u65f6\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8bbe\u8ba1\u5b58\u5728\u4f20\u7edf\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u57fa\u51c6\u6240\u6ca1\u6709\u7684\u6545\u969c\u6a21\u5f0f\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u6ca1\u6709\u4e25\u683c\u76ee\u6807\u548c\u53ef\u9a8c\u8bc1\u6784\u5efa\u7684\u57fa\u51c6\u6392\u540d\u53ef\u80fd\u4f1a\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u4f46\u5b9e\u9645\u4e0a\u4e3b\u8981\u662f\u566a\u58f0\u7684\u6392\u540d\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u79cd\u8bca\u65ad\u673a\u5236\uff1a1\uff09\u6a21\u5f0f\u4f9d\u4ece\u6027 - \u91cf\u5316\u8bc4\u5224\u8005\u6574\u4f53\u88c1\u51b3\u4e2d\u6709\u591a\u5c11\u662f\u7531\u663e\u5f0f\u8bc4\u4f30\u6a21\u5f0f\u89e3\u91ca\u7684\uff1b2\uff09\u5fc3\u7406\u6d4b\u91cf\u6709\u6548\u6027 - \u805a\u5408\u5185\u90e8\u4e00\u81f4\u6027\u548c\u533a\u5206\u6548\u5ea6\u4fe1\u53f7\u6765\u91cf\u5316\u4efb\u4f55\u57fa\u51c6\u8fd0\u884c\u4e2d\u7684\u4e0d\u53ef\u7ea6\u4e0d\u786e\u5b9a\u6027\u3002\u5c06\u8fd9\u4e9b\u5de5\u5177\u5e94\u7528\u4e8eArena-Hard Auto\u57fa\u51c6\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728Arena-Hard Auto\u57fa\u51c6\u4e0a\u53d1\u73b0\u4e25\u91cd\u7684\u6a21\u5f0f\u4e0d\u4e00\u81f4\u548c\u56e0\u5b50\u5d29\u6e83\uff1a\u4f8b\u5982DeepSeek-R1-32B\u7684\u672a\u89e3\u91ca\u65b9\u5dee\u8d85\u8fc790%\uff0c\u5927\u591a\u6570\u6807\u51c6\u7684\u56e0\u5b50\u76f8\u5173\u6027\u9ad8\u4e8e0.93\u3002\u8fd8\u53d1\u73b0ELO\u98ce\u683c\u7684\u805a\u5408\u4f1a\u63a9\u76d6\u771f\u5b9e\u7684\u6392\u540d\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u7834\u574f\u6709\u6548\u6027\u7684\u8bbe\u8ba1\u6545\u969c\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u597d\u8303\u56f4\u3001\u53ef\u9760\u6027\u611f\u77e5\u7684LLM\u8bc4\u5224\u57fa\u51c6\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u539f\u5219\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.542f0f34", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.matillion.com%2Fmaia%3Fsrc=web-generated%26utm_medium=email-newsletter%26utm_source=tldr%26utm_campaign=2025-00-web-generated%26utm_content=sept-25-newsletter%26utm_partner=%26utm_term=pp/2/0100019980574e3a-42991f65-26a7-4adf-b838-8256c8168427-000000/mNKBrTk3hdPP21rEoiITFcjSqen8vQn3qwYd5Eq4TH8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.matillion.com%2Fmaia%3Fsrc=web-generated%26utm_medium=email-newsletter%26utm_source=tldr%26utm_campaign=2025-00-web-generated%26utm_content=sept-25-newsletter%26utm_partner=%26utm_term=pp/2/0100019980574e3a-42991f65-26a7-4adf-b838-8256c8168427-000000/mNKBrTk3hdPP21rEoiITFcjSqen8vQn3qwYd5Eq4TH8=424", "authors": ["TLDR Newsletter"], "title": "Top CDOs are turning to Maia - a team of agentic data engineers - to scale data delivery", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.matillion.com%2Fmaia%3Fsrc=web-generated%26utm_medium=email-newsletter%26utm_source=tldr%26utm_campaign=2025-00-web-generated%26utm_content=sept-25-newsletter%26utm_partner=%26utm_term=pp/2/0100019980574e3a-42991f65-26a7-4adf-b838-8256c8168427-000000/mNKBrTk3hdPP21rEoiITFcjSqen8vQn3qwYd5Eq4TH8=424", "summary": "Top CDOs are turning to Maia - a team of agentic data engineers - to scale data delivery (Sponsor) Organizations like Merck, GE Healthcare and Precision for Medicine are using Maia to solve what other AI tools can't: the grinding work that backlogs data teams.Maia deploys agentic data engineers that act as AI teammates to build and optimize pipelines, automate repetitive tasks, and deliver trusted data faster. Whether you're modernizing 15-year-old legacy ETL systems or handling ongoing model...", "source": "tldr", "AI": {"tldr": "Maia\u662f\u4e00\u4e2a\u7531\u667a\u80fd\u6570\u636e\u5de5\u7a0b\u5e08\u4ee3\u7406\u7ec4\u6210\u7684\u56e2\u961f\uff0c\u65e8\u5728\u5e2e\u52a9\u4f01\u4e1a\u89c4\u6a21\u5316\u6570\u636e\u4ea4\u4ed8\uff0c\u89e3\u51b3\u6570\u636e\u56e2\u961f\u9762\u4e34\u7684\u7e41\u91cd\u5de5\u4f5c\u79ef\u538b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfAI\u5de5\u5177\u65e0\u6cd5\u89e3\u51b3\u6570\u636e\u56e2\u961f\u9762\u4e34\u7684\u91cd\u590d\u6027\u5de5\u4f5c\u548c\u9057\u7559\u7cfb\u7edf\u73b0\u4ee3\u5316\u7b49\u6311\u6218\uff0c\u4f01\u4e1a\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u53ef\u4fe1\u6570\u636e\u7684\u4ea4\u4ed8\u3002", "method": "\u90e8\u7f72\u667a\u80fd\u6570\u636e\u5de5\u7a0b\u5e08\u4ee3\u7406\u4f5c\u4e3aAI\u56e2\u961f\u6210\u5458\uff0c\u6784\u5efa\u548c\u4f18\u5316\u6570\u636e\u7ba1\u9053\uff0c\u81ea\u52a8\u5316\u91cd\u590d\u6027\u4efb\u52a1\uff0c\u5904\u7406\u9057\u7559ETL\u7cfb\u7edf\u73b0\u4ee3\u5316\u548c\u6301\u7eed\u6a21\u578b\u7ef4\u62a4\u3002", "result": "Merck\u3001GE Healthcare\u548cPrecision for Medicine\u7b49\u9876\u7ea7CDO\u5df2\u91c7\u7528Maia\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5176\u4ed6AI\u5de5\u5177\u65e0\u6cd5\u5904\u7406\u7684\u6570\u636e\u4ea4\u4ed8\u74f6\u9888\u95ee\u9898\u3002", "conclusion": "\u667a\u80fd\u6570\u636e\u5de5\u7a0b\u5e08\u4ee3\u7406\u56e2\u961f\u80fd\u591f\u6709\u6548\u63d0\u5347\u6570\u636e\u4ea4\u4ed8\u6548\u7387\uff0c\u89e3\u51b3\u6570\u636e\u56e2\u961f\u7684\u5de5\u4f5c\u79ef\u538b\u95ee\u9898\uff0c\u662f\u4f01\u4e1a\u6570\u636e\u73b0\u4ee3\u5316\u7684\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "wechat.2509.b7e9e172", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxODg4MjY2Mg==&mid=2247484265&idx=1&sn=e0ffcc6a33fd53ed04447e17f11a5f63&chksm=9634e54c76f48dfa2f03582de309621a39b5c3f9f6be19a5b490c624a3da0cbdb3f3ce639fcd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxODg4MjY2Mg==&mid=2247484265&idx=1&sn=e0ffcc6a33fd53ed04447e17f11a5f63&chksm=9634e54c76f48dfa2f03582de309621a39b5c3f9f6be19a5b490c624a3da0cbdb3f3ce639fcd#rd", "authors": ["\u6240\u7cfb\u7686\u5c71\u6d77"], "title": "DeepSeek-R1\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6fc0\u52b1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "comment": "Source: WeChat, Published: 2025-09-25 12:08:36", "summary": "\u8fd9\u5bf9\u6211\u4eec\u6765\u8bf4\u4e5f\u662f\u4e00\u4e2a\u554a\u54c8\u65f6\u523b\uff0c\u8ba9\u6211\u4eec\u89c1\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u529b\u91cf\u4e0e\u7f8e\u3002\u201c\u601d\u8003\u65f6\u95f4\u201d\u7684\u589e\u52a0\u4fc3\u8fdb\u4e86\u590d\u6742\u884c\u4e3a\u7684\u81ea\u53d1\u53d1\u5c55\u3002\u6a21\u578b\u6108\u53d1\u8868\u73b0\u51fa\u8bf8\u5982\u53cd\u601d\u5f0f\u63a8\u7406\u4e0e\u7cfb\u7edf\u6027\u5907\u9009\u89e3\u63a2\u7d22\u7b49\u9ad8\u7ea7\u7b56\u7565\uff08\u89c1\u6269\u5c55\u6570\u636e\u56fe1a\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6570\u5b66\u4e0e", "AI": {"tldr": "\u8fd9\u5bf9\u6211\u4eec\u6765\u8bf4\u4e5f\u662f\u4e00\u4e2a\u554a\u54c8\u65f6\u523b\uff0c\u8ba9\u6211\u4eec\u89c1\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u529b\u91cf\u4e0e\u7f8e\u3002\u201c\u601d\u8003\u65f6\u95f4\u201d\u7684\u589e\u52a0\u4fc3\u8fdb\u4e86\u590d\u6742\u884c\u4e3a\u7684\u81ea\u53d1\u53d1\u5c55\u3002\u6a21\u578b\u6108\u53d1\u8868\u73b0\u51fa\u8bf8\u5982\u53cd\u601d\u5f0f\u63a8\u7406\u4e0e\u7cfb\u7edf\u6027\u5907\u9009\u89e3\u63a2\u7d22\u7b49\u9ad8\u7ea7\u7b56\u7565\uff08\u89c1\u6269\u5c55\u6570\u636e\u56fe1a\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6570\u5b66\u4e0e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.20137b5d", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461154578&idx=1&sn=1ece7a66957a7b2f001bfb99befe8d7a&chksm=867eb9ebe4f9994c40e7305efa7a9360a0ce59f5d20d23a00d1403d2954689c6dd6669a622cf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461154578&idx=1&sn=1ece7a66957a7b2f001bfb99befe8d7a&chksm=867eb9ebe4f9994c40e7305efa7a9360a0ce59f5d20d23a00d1403d2954689c6dd6669a622cf#rd", "authors": ["AI\u5de5\u7a0b\u5316"], "title": "RLPT\uff1a\u817e\u8baf\u6df7\u5143\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u5b9e\u73b0\u81ea\u4e3b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-09-25 12:04:37", "summary": "\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u76f4\u63a5\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u6269\u5c55\u300281 mmlu 65 mmlu-pro 46 gpqa-diamond supergpqa y=105.625 \u00b7expt-0.300x-0.829\uff09 y = 79.827 -exp\uff08-0.300x-0.093\uff09 $ y = 32.220 - exp\uff080.015x012\uff09 y= 31.412 \u00b7 exp\uff080.004x0.779 64 34 42- accuracy\uff08%\uff09 80 63 40 62 33- 36 79 61 32 09 3", "AI": {"tldr": "\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u76f4\u63a5\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u6269\u5c55\u300281 mmlu 65 mmlu-pro 46 gpqa-diamond supergpqa y=105.625 \u00b7expt-0.300x-0.829\uff09 y = 79.827 -exp\uff08-0.300x-0.093\uff09 $ y = 32.220 - exp\uff080.015x012\uff09 y= 31.412 \u00b7 exp\uff080.004x0.779 64 34 42- accu...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.03679148", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649428159&idx=1&sn=b518133e9f0f33f7509834520bd15135&chksm=898f493af41d58b974662296a15432fbd841b7ced9e96bad681f6470f284d4ebe9c2f0f51a13#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649428159&idx=1&sn=b518133e9f0f33f7509834520bd15135&chksm=898f493af41d58b974662296a15432fbd841b7ced9e96bad681f6470f284d4ebe9c2f0f51a13#rd", "authors": ["CreateAMind"], "title": "\u5982\u4f55\u4e3a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bbe\u8ba1\u201c\u5143\u5b66\u4e60\u201d\u7b97\u6cd5\uff1f", "comment": "Source: WeChat, Published: 2025-09-25 09:22:00", "summary": "3 \u80cc\u666f\u5f3a\u5316\u5b66\u4e60\u3002\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u95ee\u9898\u901a\u5e38\u88ab\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u3002\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\u901a\u5e38\u88ab\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Sutton &Barto\uff0c2020\uff0cMDPs\uff09\u3002\u4e00\u4e2amdp\u901a\u5e38\u8868\u793a\u4e3a\u4e00\u4e2a\u5143\u7ec4\u3002", "AI": {"tldr": "3 \u80cc\u666f\u5f3a\u5316\u5b66\u4e60\u3002\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u95ee\u9898\u901a\u5e38\u88ab\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u3002\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\u901a\u5e38\u88ab\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Sutton &Barto\uff0c2020\uff0cMDPs\uff09\u3002\u4e00\u4e2amdp\u901a\u5e38\u8868\u793a\u4e3a\u4e00\u4e2a\u5143\u7ec4\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.6ef947e9", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMjI4ODQwOA==&mid=2649680373&idx=1&sn=3eb70f9a4ea0cb79a2cc443cd54239ec&chksm=82e871033755290a58f684b7c29d259144d690933e8fded25976c507510da3517e7c24f4a39c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMjI4ODQwOA==&mid=2649680373&idx=1&sn=3eb70f9a4ea0cb79a2cc443cd54239ec&chksm=82e871033755290a58f684b7c29d259144d690933e8fded25976c507510da3517e7c24f4a39c#rd", "authors": ["\u7535\u4fe1\u79d1\u5b66"], "title": "\u4fe1\u606f\u5de5\u7a0b\u5927\u5b66\u8d8a\u5947\u5f3a\u7b49\uff1a\u57fa\u4e8e\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7b97\u7f51\u534f\u540c\u52a8\u6001\u8def\u7531\u8c03\u5ea6\u7b97\u6cd5", "comment": "Source: WeChat, Published: 2025-09-25 09:02:00", "summary": "\u5173\u952e\u8bcd \u7b97\u529b\u8def\u7531\uff1b\u7b97\u7f51\u878d\u5408\uff1b\u591a\u573a\u666f\u4f18\u5316\uff1b\u5e8f\u5217\u51b3\u7b56\uff1b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 0 \u5f15\u8a00 \u968f\u7740\u4fe1\u606f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5927\u6570\u636e\u3001\u4eba\u5de5\u667a\u80fd\u3001\u4e91\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5404\u884c\u4e1a\u5bf9\u7b97\u529b\u7684\u9700\u6c42\u5448\u73b0\u51fa\u9ad8\u5ea6\u52a8\u6001\u5316\u3001\u590d\u6742\u5316\u7684\u8d8b\u52bf\uff0c\u4e3a\u4e86\u5e94", "AI": {"tldr": "\u5173\u952e\u8bcd \u7b97\u529b\u8def\u7531\uff1b\u7b97\u7f51\u878d\u5408\uff1b\u591a\u573a\u666f\u4f18\u5316\uff1b\u5e8f\u5217\u51b3\u7b56\uff1b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 0 \u5f15\u8a00 \u968f\u7740\u4fe1\u606f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5927\u6570\u636e\u3001\u4eba\u5de5\u667a\u80fd\u3001\u4e91\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5404\u884c\u4e1a\u5bf9\u7b97\u529b\u7684\u9700\u6c42\u5448\u73b0\u51fa\u9ad8\u5ea6\u52a8\u6001\u5316\u3001\u590d\u6742\u5316\u7684\u8d8b\u52bf\uff0c\u4e3a\u4e86\u5e94", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.5fedabcb", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxMjE3MzA3MQ==&mid=2247503456&idx=1&sn=b7fd175f4f58c64bbe8e7c67e42117f9&chksm=96e2914e0541223e27e473f8afb9b4fd57d81416731472392c0092bbaf36930da3feb0f66cfd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxMjE3MzA3MQ==&mid=2247503456&idx=1&sn=b7fd175f4f58c64bbe8e7c67e42117f9&chksm=96e2914e0541223e27e473f8afb9b4fd57d81416731472392c0092bbaf36930da3feb0f66cfd#rd", "authors": ["\u7a7a\u5929\u6280\u672f"], "title": "\u63a8\u8350\u9605\u8bfb | \u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e0eADRC\u76f8\u7ed3\u5408\u7684\u9694\u79bb\u6bb5\u8fb9\u754c\u5c42\u62bd\u5438\u6700\u4f18\u667a\u80fd\u63a7\u5236\u65b9\u6cd5\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-09-25 08:30:43", "summary": "\u9694\u79bb\u6bb5\uff1b\u8fb9\u754c\u5c42\u62bd\u5438\uff1b\u5f3a\u5316\u5b66\u4e60\uff1b\u81ea\u6297\u6270\u63a7\u5236\uff08\u957f\u6309\u8bc6\u522b\u4e8c\u7ef4\u7801\u83b7\u53d6PDF\u7248\u6587\u7ae0\uff091 \u5f15 \u8a00\u51b2\u538b\u53d1\u52a8\u673a\u4e0e\u4f20\u7edf\u7684\u822a\u7a7a\u53d1\u52a8\u673a\u76f8\u6bd4\uff0c\u5177\u6709\u7ed3\u6784\u7b80\u5355\u3001\u5355\u4f4d\u63a8\u529b\u9ad8\u3001\u6709\u6548\u8f7d\u8377\u66f4\u5927\u548c\u901f\u5ea6\u66f4\u5feb\u7b49\u4f18\u70b9\uff0c\u5df2\u7ecf\u6210\u4e3a\u822a\u7a7a\u822a\u5929\u9886\u57df\u7684\u7814\u7a76\u70ed\u70b9\u3002", "AI": {"tldr": "\u9694\u79bb\u6bb5\uff1b\u8fb9\u754c\u5c42\u62bd\u5438\uff1b\u5f3a\u5316\u5b66\u4e60\uff1b\u81ea\u6297\u6270\u63a7\u5236\uff08\u957f\u6309\u8bc6\u522b\u4e8c\u7ef4\u7801\u83b7\u53d6PDF\u7248\u6587\u7ae0\uff091 \u5f15 \u8a00\u51b2\u538b\u53d1\u52a8\u673a\u4e0e\u4f20\u7edf\u7684\u822a\u7a7a\u53d1\u52a8\u673a\u76f8\u6bd4\uff0c\u5177\u6709\u7ed3\u6784\u7b80\u5355\u3001\u5355\u4f4d\u63a8\u529b\u9ad8\u3001\u6709\u6548\u8f7d\u8377\u66f4\u5927\u548c\u901f\u5ea6\u66f4\u5feb\u7b49\u4f18\u70b9\uff0c\u5df2\u7ecf\u6210\u4e3a\u822a\u7a7a\u822a\u5929\u9886\u57df\u7684\u7814\u7a76\u70ed\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.6bcce29d", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5MzQwODE5OQ==&mid=2247513633&idx=1&sn=ecf62b713fb408f33909da3b900a0c97&chksm=ff828a0768cd693e1da5b2e1c4b575a395bd6e7dee3c16829d94efcf0329db654a370f69c839#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5MzQwODE5OQ==&mid=2247513633&idx=1&sn=ecf62b713fb408f33909da3b900a0c97&chksm=ff828a0768cd693e1da5b2e1c4b575a395bd6e7dee3c16829d94efcf0329db654a370f69c839#rd", "authors": ["\u6c7d\u8f66\u5de5\u7a0b\u7f16\u8f91\u90e8"], "title": "\u5317\u4eac\u79d1\u6280\u5927\u5b66\u4e0e\u6e05\u534e\u5927\u5b66\u8054\u5408\u7814\u7a76\u6210\u679c\uff1a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u907f\u649e\u7684\u6269\u6563<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-09-25 03:00:00", "summary": "\u4e3a\u6b64\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u4ee5\u6269\u6563\u5f0f\u751f\u6210\u7b56\u7565\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u7b56\u7565\u7f51\u7edc\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7b56\u7565\u7684\u63a2\u7d22\u80fd\u529b\u3002", "AI": {"tldr": "\u4e3a\u6b64\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u4ee5\u6269\u6563\u5f0f\u751f\u6210\u7b56\u7565\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u7b56\u7565\u7f51\u7edc\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7b56\u7565\u7684\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.9926371a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994357&idx=1&sn=e4cdca480812bb1a0432b3412316b0dd&chksm=b0d49d2b08b17325d4f655254e75f42430e98679c8a42329cb62cda10d761b2a27e39de51c8d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994357&idx=1&sn=e4cdca480812bb1a0432b3412316b0dd&chksm=b0d49d2b08b17325d4f655254e75f42430e98679c8a42329cb62cda10d761b2a27e39de51c8d#rd", "authors": ["\u667a\u7329\u7329GenAI"], "title": "117\u9875\u8bba\u6587\uff01\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6700\u65b0\u7efc\u8ff0\uff0c\u6e05\u534e\u8054\u5408\u53d1\u8868", "comment": "Source: WeChat, Published: 2025-09-25 02:42:55", "summary": "2025]\uff1bpag [jiang et al.\uff0c 2025e]\uff1burpo [lu et al.\uff0c 2025e]\uff1bpcl [fei et al.\uff0c 2025b]\uff1bk2 [team\uff0c 2025c]\uff1bcooper [hong et al.\uff0c 2025a]\uff1bCritique-GRPO [Zhang et al.\uff0c 2025m]Token-level\uff1a e.g.\uff0c Implicit PRM [Yuan et al.\uff0c 2025d]\uff1b", "AI": {"tldr": "2025]\uff1bpag [jiang et al.\uff0c 2025e]\uff1burpo [lu et al.\uff0c 2025e]\uff1bpcl [fei et al.\uff0c 2025b]\uff1bk2 [team\uff0c 2025c]\uff1bcooper [hong et al.\uff0c 2025a]\uff1bCritique-GRPO [Zhang et al.\uff0c 2025m]Token-level\uff1a e.g.\uff0c Implicit PRM [Yuan et...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.2e77e1cc", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MzY1MTQ4Mw==&mid=2247484354&idx=1&sn=a62c0edcb6c9807c8e2996f90c5f004d&chksm=914116414563d79f154531af579df0df5beb7d603edb1b14edb759ffb9acf0c6f20a836d7ac0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MzY1MTQ4Mw==&mid=2247484354&idx=1&sn=a62c0edcb6c9807c8e2996f90c5f004d&chksm=914116414563d79f154531af579df0df5beb7d603edb1b14edb759ffb9acf0c6f20a836d7ac0#rd", "authors": ["Wonderful\u4eff\u771f"], "title": "DQN\uff1a\u8ba9AI\u73a9\u6e38\u620f\u7684\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u9769\u547d", "comment": "Source: WeChat, Published: 2025-09-25 01:39:17", "summary": "\u8fd9\u6807\u5fd7\u7740\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65f6\u4ee3\u7684\u5230\u6765\u3002\u4eca\u5929\uff0c\u6211\u4eec\u6765\u6df1\u5165\u4e86\u89e3\u8fd9\u4e2a\u6539\u53d8\u6e38\u620f\u89c4\u5219\u7684\u7b97\u6cd5\u2014\u2014DQN \uff08Deep Q-Network\uff09\u3002\u4ec0\u4e48\u662fDQN\uff1fDQN\u662fDeep Q-Network\u7684\u7f29\u5199\uff0c\u5b83\u5c06\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u4e0eQ-Learning\u7684\u51b3\u7b56\u80fd\u529b\u5b8c\u7f8e\u7ed3\u5408\u3002", "AI": {"tldr": "\u8fd9\u6807\u5fd7\u7740\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65f6\u4ee3\u7684\u5230\u6765\u3002\u4eca\u5929\uff0c\u6211\u4eec\u6765\u6df1\u5165\u4e86\u89e3\u8fd9\u4e2a\u6539\u53d8\u6e38\u620f\u89c4\u5219\u7684\u7b97\u6cd5\u2014\u2014DQN \uff08Deep Q-Network\uff09\u3002\u4ec0\u4e48\u662fDQN\uff1fDQN\u662fDeep Q-Network\u7684\u7f29\u5199\uff0c\u5b83\u5c06\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u4e0eQ-Learning\u7684\u51b3\u7b56\u80fd\u529b\u5b8c\u7f8e\u7ed3\u5408\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.4736f2e2", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574020&idx=3&sn=4ec5dc4738b3f2347366907dc566013a&chksm=eae6c6ac5c5f923ca51b4a2f0b799e0fd93e3efd7d893cb5cb63febd7298664433ef9b5f900c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574020&idx=3&sn=4ec5dc4738b3f2347366907dc566013a&chksm=eae6c6ac5c5f923ca51b4a2f0b799e0fd93e3efd7d893cb5cb63febd7298664433ef9b5f900c#rd", "authors": ["\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406"], "title": "GUI\u667a\u80fd\u4f53\u8bad\u7ec3\u8fce\u6765\u65b0\u8303\u5f0f\uff01\u534a\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8ba97B\u6a21\u578b\u5ab2\u7f8eGPT-4o", "comment": "Source: WeChat, Published: 2025-09-25 00:00:00", "summary": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08Online RL\uff09\u901a\u8fc7\u4e0e\u771f\u5b9e\u73af\u5883\u6301\u7eed\u4ea4\u4e92\u83b7\u53d6\u53cd\u9988\uff0c\u80fd\u591f\u6355\u6349\u5230\u4efb\u52a1\u5b8c\u6210\u4e0e\u5426\u7684\u5168\u5c40\u5956\u52b1\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u591a\u6b65\u51b3\u7b56\u4f18\u5316\uff0c\u4f46\u9762\u4e34\u5956\u52b1\u7a00\u758f\u3001\u8bd5\u9519\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "AI": {"tldr": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08Online RL\uff09\u901a\u8fc7\u4e0e\u771f\u5b9e\u73af\u5883\u6301\u7eed\u4ea4\u4e92\u83b7\u53d6\u53cd\u9988\uff0c\u80fd\u591f\u6355\u6349\u5230\u4efb\u52a1\u5b8c\u6210\u4e0e\u5426\u7684\u5168\u5c40\u5956\u52b1\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u591a\u6b65\u51b3\u7b56\u4f18\u5316\uff0c\u4f46\u9762\u4e34\u5956\u52b1\u7a00\u758f\u3001\u8bd5\u9519\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.2c18599e", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5OTExNjAzNw==&mid=2247485103&idx=2&sn=f766477e339ab0c84789bcfffb2c99e1&chksm=97e01816dd036649a14ccfc3e515260079ddcf0b9f99340a62b6236163d1972285b741d4af75#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5OTExNjAzNw==&mid=2247485103&idx=2&sn=f766477e339ab0c84789bcfffb2c99e1&chksm=97e01816dd036649a14ccfc3e515260079ddcf0b9f99340a62b6236163d1972285b741d4af75#rd", "authors": ["\u7b97\u6ce5"], "title": "\u4f18\u5316\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3\uff0c\u4e0a\u4ea4\u5927\u8054\u5408\u5fae\u8f6f\u6e05\u5317\u63d0\u51faFlowRL\uff0c\u8ba9AI\u63a8\u7406\u66f4\u5177\u6cdb\u5316\u529b", "comment": "Source: WeChat, Published: 2025-09-24 23:00:00", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08reinforcement learning\uff09\u662f\u8bad\u7ec3\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e00\u628a\u597d\u624b\u3002\u4ece\u65e9\u671f\u7684REINFORCE\u7b97\u6cd5\uff0c\u5230\u540e\u6765\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684PPO\uff08\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\uff0c\u518d\u5230\u7b80\u5316\u7248\u7684GRPO\uff08\u5206\u7ec4\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u90fd\u662f\u4e00\u4e2a\u903b\u8f91\uff1a\u5956\u52b1\u6700\u5927\u5316\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08reinforcement learning\uff09\u662f\u8bad\u7ec3\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e00\u628a\u597d\u624b\u3002\u4ece\u65e9\u671f\u7684REINFORCE\u7b97\u6cd5\uff0c\u5230\u540e\u6765\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684PPO\uff08\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\uff0c\u518d\u5230\u7b80\u5316\u7248\u7684GRPO\uff08\u5206\u7ec4\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u90fd\u662f\u4e00\u4e2a\u903b\u8f91\uff1a\u5956\u52b1\u6700\u5927\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.4dae036c", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MzIzODIzMA==&mid=2257484245&idx=1&sn=edd669a5ceb9ed9cd8dfd18686ec5b3c&chksm=9d168a1962d86200241836a74d0fb6a8c7b56b3674ca97026f0d976d9eb6e3eea9b18d52dbed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MzIzODIzMA==&mid=2257484245&idx=1&sn=edd669a5ceb9ed9cd8dfd18686ec5b3c&chksm=9d168a1962d86200241836a74d0fb6a8c7b56b3674ca97026f0d976d9eb6e3eea9b18d52dbed#rd", "authors": ["\u76ae\u6d6a\u65e5\u77e5\u5f55"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> x \u81ea\u52a8\u9a7e\u9a76\uff1a\u73b0\u5b9e\u843d\u5730\u3001\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u8d8b\u52bf\u5168\u666f\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-09-24 22:30:34", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u9664\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e4b\u5916\u7684\u4e00\u4e2a\u91cd\u8981\u673a\u5668\u5b66\u4e60\u5b50\u9886\u57df\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u667a\u80fd\u4f53\uff08agent\uff09\u901a\u8fc7\u4e0e\u73af\u5883\uff08environment\uff09\u7684\u4ea4\u4e92\uff0c\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u5982\u4f55\u57fa\u4e8e\u73af\u5883\u800c\u884c\u52a8\uff0c\u4ee5\u6700\u5927\u5316\u9884\u671f\u7684\u957f\u671f\u7d2f\u79ef\u5956\u52b1 1\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u9664\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e4b\u5916\u7684\u4e00\u4e2a\u91cd\u8981\u673a\u5668\u5b66\u4e60\u5b50\u9886\u57df\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u667a\u80fd\u4f53\uff08agent\uff09\u901a\u8fc7\u4e0e\u73af\u5883\uff08environment\uff09\u7684\u4ea4\u4e92\uff0c\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u5982\u4f55\u57fa\u4e8e\u73af\u5883\u800c\u884c\u52a8\uff0c\u4ee5\u6700\u5927\u5316\u9884\u671f\u7684\u957f\u671f\u7d2f\u79ef\u5956\u52b1 1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.462af397", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257116&idx=1&sn=15c50dba14db176342d0a19e4d7a51f4&chksm=bc3c17c335fca8301cf857608c28bef4e740ec2f2e74e701965bc184bd632a94a9f02197b7d3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257116&idx=1&sn=15c50dba14db176342d0a19e4d7a51f4&chksm=bc3c17c335fca8301cf857608c28bef4e740ec2f2e74e701965bc184bd632a94a9f02197b7d3#rd", "authors": ["InfoQ"], "title": "<em class=\"highlight\">Agent</em>+<em class=\"highlight\">Code</em>\uff0c\u6253\u5f00AI\u5f00\u53d1\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-09-25 08:39:19", "summary": "JoyCode 2.0 \u4f5c\u4e3a\u201cAgent+Code\u201d\u8303\u5f0f\u7684\u53e6\u4e00\u6838\u5fc3\uff0c\u5219\u4e13\u6ce8\u4e8e\u89e3\u51b3\u4f20\u7edf\u5f00\u53d1\u6a21\u5f0f\u4e2d\u201c\u4f7f\u7528\u95e8\u69db\u9ad8\u201d\u7684\u96be\u9898\u3002\u51ed\u501f\u591a Agent \u534f\u4f5c\u67b6\u6784\u548c CSR \u4e0a\u4e0b\u6587\u589e\u5f3a\u5f15\u64ce\uff0c\u5df2\u7ecf\u8ba9\u201c0 \u624b\u5199\u4ee3\u7801\u201d\u7684\u81ea\u52a8\u5316\u7f16\u7a0b\u6210\u4e3a\u53ef\u80fd \u3002", "AI": {"tldr": "JoyCode 2.0 \u4f5c\u4e3a\u201cAgent+Code\u201d\u8303\u5f0f\u7684\u53e6\u4e00\u6838\u5fc3\uff0c\u5219\u4e13\u6ce8\u4e8e\u89e3\u51b3\u4f20\u7edf\u5f00\u53d1\u6a21\u5f0f\u4e2d\u201c\u4f7f\u7528\u95e8\u69db\u9ad8\u201d\u7684\u96be\u9898\u3002\u51ed\u501f\u591a Agent \u534f\u4f5c\u67b6\u6784\u548c CSR \u4e0a\u4e0b\u6587\u589e\u5f3a\u5f15\u64ce\uff0c\u5df2\u7ecf\u8ba9\u201c0 \u624b\u5199\u4ee3\u7801\u201d\u7684\u81ea\u52a8\u5316\u7f16\u7a0b\u6210\u4e3a\u53ef\u80fd \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.1c8210a2", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxNjM3NzYyNg==&mid=2651705390&idx=2&sn=19af265c70986a993d0b9457a3004031&chksm=814a24575417a99545ae7a5868ce6aff8da001c0e7f51518bb394943e99a9f59f0e4bf0725b1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxNjM3NzYyNg==&mid=2651705390&idx=2&sn=19af265c70986a993d0b9457a3004031&chksm=814a24575417a99545ae7a5868ce6aff8da001c0e7f51518bb394943e99a9f59f0e4bf0725b1#rd", "authors": ["\u79d1\u6280\u660e\u8bf4"], "title": "\u963f\u91cc\u4e91\u65e0\u5f71\u53d1\u5e03\u9996\u4e2a<em class=\"highlight\">Agentic</em> Computer\u5f62\u6001\u7684\u4e2a\u4eba\u8ba1\u7b97\u4ea7\u54c1", "comment": "Source: WeChat, Published: 2025-09-25 11:54:18", "summary": "\u5e76\u9996\u6b21\u5c55\u793a\u5168\u65b0\u7684\u4e2a\u4eba\u8ba1\u7b97\u4ea7\u54c1\u2014\u2014\u65e0\u5f71Agentic Computer\uff0c\u62e5\u6709\u5168\u65b0\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\uff0c\u9769\u547d\u6027\u7684\u201c\u8bb0\u5fc6\u201d\u80fd\u529b\u548c\u8fd1\u4e4e\u65e0\u7a77\u7684\u4e91\u4e0a\u7b97\u529b\u3002AI Agent\u662f\u5f53\u4e0b\u5168\u7403\u79d1\u6280\u548c\u5546\u4e1a\u7684\u7126\u70b9\uff0c\u8fc7\u53bb\u534a\u5e74\u6d8c\u73b0\u7684Agent\u76f8\u5173\u4ea7\u54c1\uff0c\u8d85\u8fc7\u4e862024\u5e74\u7684\u603b\u548c\uff0c10\u4e2a\u521b", "AI": {"tldr": "\u5e76\u9996\u6b21\u5c55\u793a\u5168\u65b0\u7684\u4e2a\u4eba\u8ba1\u7b97\u4ea7\u54c1\u2014\u2014\u65e0\u5f71Agentic Computer\uff0c\u62e5\u6709\u5168\u65b0\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\uff0c\u9769\u547d\u6027\u7684\u201c\u8bb0\u5fc6\u201d\u80fd\u529b\u548c\u8fd1\u4e4e\u65e0\u7a77\u7684\u4e91\u4e0a\u7b97\u529b\u3002AI Agent\u662f\u5f53\u4e0b\u5168\u7403\u79d1\u6280\u548c\u5546\u4e1a\u7684\u7126\u70b9\uff0c\u8fc7\u53bb\u534a\u5e74\u6d8c\u73b0\u7684Agent\u76f8\u5173\u4ea7\u54c1\uff0c\u8d85\u8fc7\u4e862024\u5e74\u7684\u603b\u548c\uff0c10\u4e2a\u521b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.741756d0", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODI5NjUxMA==&mid=2247483697&idx=1&sn=edb4332f89114677ec4d7e2ee5a1f398&chksm=c42b869e834389ea4a07c5e3ff95ba06a8a5576410e38112694467122ed3dc7797c237a14202#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODI5NjUxMA==&mid=2247483697&idx=1&sn=edb4332f89114677ec4d7e2ee5a1f398&chksm=c42b869e834389ea4a07c5e3ff95ba06a8a5576410e38112694467122ed3dc7797c237a14202#rd", "authors": ["ClayX AI"], "title": "\u4ece LLM \u5230\u81ea\u52a8\u5316<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff1a<em class=\"highlight\">Agentic</em> RL \u7684\u529b\u91cf", "comment": "Source: WeChat, Published: 2025-09-25 10:44:29", "summary": "Agentic RL \u7684\u5173\u952e\u6280\u672f\u4e0e\u6311\u6218\u5982\u4f55\u7ed3\u5408 \u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u8bb0\u5fc6\u673a\u5236\u4e0e\u81ea\u8fdb\u5316\u8303\u5f0f \u6784\u5efa\u771f\u6b63\u81ea\u4e3b\u7684 LLM-based Agents \u65f6\u95f4\uff1a\u672c\u5468\u65e5 \u4e0b\u5348 4\uff1a30\uff08\u5317\u4eac\u65f6\u95f4\uff09 \u5730\u70b9\uff1aZoom \u7ebf\u4e0a\u4f1a\u8bae", "AI": {"tldr": "Agentic RL \u7684\u5173\u952e\u6280\u672f\u4e0e\u6311\u6218\u5982\u4f55\u7ed3\u5408 \u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u8bb0\u5fc6\u673a\u5236\u4e0e\u81ea\u8fdb\u5316\u8303\u5f0f \u6784\u5efa\u771f\u6b63\u81ea\u4e3b\u7684 LLM-based Agents \u65f6\u95f4\uff1a\u672c\u5468\u65e5 \u4e0b\u5348 4\uff1a30\uff08\u5317\u4eac\u65f6\u95f4\uff09 \u5730\u70b9\uff1aZoom \u7ebf\u4e0a\u4f1a\u8bae", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.4e875697", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MTUzODMyMzcwMQ==&mid=2651436879&idx=1&sn=6e88e980c92bb1811cbc3f41eccb3ab6&chksm=699938d0c3c865b14eefa3537443c67ac266f9087958689b6613c4f9a403e7ca8b66748d0e24#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MTUzODMyMzcwMQ==&mid=2651436879&idx=1&sn=6e88e980c92bb1811cbc3f41eccb3ab6&chksm=699938d0c3c865b14eefa3537443c67ac266f9087958689b6613c4f9a403e7ca8b66748d0e24#rd", "authors": ["\u77e5\u4e4e"], "title": "\u300c\u6307\u54ea\u6253\u54ea\u300d\u7684 AI \u52a9\u624b\uff5c\u77e5\u4e4e\u76f4\u7b54\u667a\u80fd\u4e0a\u65b0", "comment": "Source: WeChat, Published: 2025-09-25 10:00:00", "summary": "\u800c\u5728\u4eca\u5929\uff0c\u6211\u4eec\u7ec8\u4e8e\u53ef\u4ee5\uff08\u9a84\u50b2\u5730\uff09\u5ba3\u5e03\uff0c\u76f4\u7b54\u5df2\u7ecf\u8fdb\u4e00\u6b65\u5347\u7ea7\u4e3a Agentic \u52a9\u624b\u5566\uff01\u77e5\u4e4e\u76f4\u7b54\u3002\u77e5\u4e4e\u76f4\u7b54\u5168\u9762\u5347\u7ea7 agentic\u52a9\u624b\u3002\u9762\u5bf9\u641c\u7d22 / \u7814\u7a76 / \u5b66\u4e60 / \u521b\u4f5c\u5404\u7c7b\u9700\u6c42\uff0c\u65e0\u8bba\u4f60\u5e0c\u671b\u300c\u6df1\u5165\u800c\u5168\u9762\u7684\u7814\u7a76\u300d\u3001\u300c\u53ea\u8981\u8bba\u6587\u7684\u8d44\u6599\u300d\u3001\u300c", "AI": {"tldr": "\u800c\u5728\u4eca\u5929\uff0c\u6211\u4eec\u7ec8\u4e8e\u53ef\u4ee5\uff08\u9a84\u50b2\u5730\uff09\u5ba3\u5e03\uff0c\u76f4\u7b54\u5df2\u7ecf\u8fdb\u4e00\u6b65\u5347\u7ea7\u4e3a Agentic \u52a9\u624b\u5566\uff01\u77e5\u4e4e\u76f4\u7b54\u3002\u77e5\u4e4e\u76f4\u7b54\u5168\u9762\u5347\u7ea7 agentic\u52a9\u624b\u3002\u9762\u5bf9\u641c\u7d22 / \u7814\u7a76 / \u5b66\u4e60 / \u521b\u4f5c\u5404\u7c7b\u9700\u6c42\uff0c\u65e0\u8bba\u4f60\u5e0c\u671b\u300c\u6df1\u5165\u800c\u5168\u9762\u7684\u7814\u7a76\u300d\u3001\u300c\u53ea\u8981\u8bba\u6587\u7684\u8d44\u6599\u300d\u3001\u300c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.9f40737d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NDAyNzc3MQ==&mid=2247494845&idx=1&sn=4c860ef9faf64e292e2fadf43908fa95&chksm=cf8f7634ac61bacb9ba7ca541da5d6d4279a15a4ad2d101b1b21ef2701f10b414ef36abc528f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NDAyNzc3MQ==&mid=2247494845&idx=1&sn=4c860ef9faf64e292e2fadf43908fa95&chksm=cf8f7634ac61bacb9ba7ca541da5d6d4279a15a4ad2d101b1b21ef2701f10b414ef36abc528f#rd", "authors": ["wteam\u5b98\u65b9\u53f7"], "title": "\u521b\u4e1a\u8005\u8bf4\uff5c<em class=\"highlight\">Agentic</em>\u65f6\u4ee3\uff0c\u4e0d\u9700\u8981\u5b66\u7f16\u7a0b\u4e5f\u53ef\u4ee5AI\u521b\u4e1a\uff1f", "comment": "Source: WeChat, Published: 2025-09-25 10:00:00", "summary": "\u6240\u8c13\u201cAgentic\u201d\uff0c\u6307\u7684\u662f\u80fd\u591f\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u95ed\u73af\u7684\u7cfb\u7edf\u6216\u80fd\u529b\u3002\u8fd9\u7c7b\u5e94\u7528\u4e0d\u4ec5\u5c40\u9650\u4e8e\u7f16\u7a0b\uff0c\u5728\u5404\u884c\u5404\u4e1a\u90fd\u5177\u5907\u5e7f\u9614\u524d\u666f\u3002ask \u2192 edit \u2192 agent\u3002ic\uff1aagentic \u65f6\u4ee3\u5df2\u7ecf\u5230\u6765\u3002", "AI": {"tldr": "\u6240\u8c13\u201cAgentic\u201d\uff0c\u6307\u7684\u662f\u80fd\u591f\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u95ed\u73af\u7684\u7cfb\u7edf\u6216\u80fd\u529b\u3002\u8fd9\u7c7b\u5e94\u7528\u4e0d\u4ec5\u5c40\u9650\u4e8e\u7f16\u7a0b\uff0c\u5728\u5404\u884c\u5404\u4e1a\u90fd\u5177\u5907\u5e7f\u9614\u524d\u666f\u3002ask \u2192 edit \u2192 agent\u3002ic\uff1aagentic \u65f6\u4ee3\u5df2\u7ecf\u5230\u6765\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.40691033", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMDMzNzM3MQ==&mid=2247485545&idx=1&sn=5f6760e03713c98d261274c2c052ac40&chksm=c06e53b48e4cf3c8faf3d3e8d420e15c308e7a0e95ff875382bb919c4c574fdf6ce81edf3ee3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMDMzNzM3MQ==&mid=2247485545&idx=1&sn=5f6760e03713c98d261274c2c052ac40&chksm=c06e53b48e4cf3c8faf3d3e8d420e15c308e7a0e95ff875382bb919c4c574fdf6ce81edf3ee3#rd", "authors": ["\u5927\u6a21\u578b\u8001\u84dd"], "title": "AI\u4ea7\u54c1\u7ecf\u7406\u5fc5\u77e5\uff1a<em class=\"highlight\">Agentic</em> Workflow\u7684\u56db\u79cd\u4e3b\u6d41\u8bbe\u8ba1\u6a21\u578b\u53ca\u601d\u8def", "comment": "Source: WeChat, Published: 2025-09-25 09:58:42", "summary": "\u98de\u4e66\u4e91\u6587\u6863 \u2026 >\u3010llm]agentic workflow\u7684\u56db\u79cd\u5e38... \u5206\u4eab .. + \u6700\u8fd1\u4fee\u6539\uff1a8\u5206\u949f\u524d ai\u4ea7\u54c1\u7ecf\u7406\u77e5\u8bc6\u5e93 reflexion \u4e92\u8054\u7f51\u516c\u5f00\u3002\u4e0b\u56fe\u662freflexion agent\u901a\u8fc7\u8bd5\u9519\u548c\u81ea\u6211\u53cd\u601d\uff08self-reflection\uff09\u89e3\u51b3\u51b3\u7b56\u3001\u7f16\u7a0b\u3001\u63a8 \u641c\u7d22 \u7406\u4efb\u52a1\u7684\u793a\u4f8b\u3002", "AI": {"tldr": "\u98de\u4e66\u4e91\u6587\u6863 \u2026 >\u3010llm]agentic workflow\u7684\u56db\u79cd\u5e38... \u5206\u4eab .. + \u6700\u8fd1\u4fee\u6539\uff1a8\u5206\u949f\u524d ai\u4ea7\u54c1\u7ecf\u7406\u77e5\u8bc6\u5e93 reflexion \u4e92\u8054\u7f51\u516c\u5f00\u3002\u4e0b\u56fe\u662freflexion agent\u901a\u8fc7\u8bd5\u9519\u548c\u81ea\u6211\u53cd\u601d\uff08self-reflection\uff09\u89e3\u51b3\u51b3\u7b56\u3001\u7f16\u7a0b\u3001\u63a8 \u641c\u7d22 \u7406\u4efb\u52a1\u7684\u793a\u4f8b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.e975f594", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODI2NDM3Mw==&mid=2247486524&idx=3&sn=978e4dc051912a24b1303b847e72a768&chksm=97063ac767909e210a6440732a5c7a9fade7b7bd3153d1a79b7b3f8c8f783a832f5afe3fe0f8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODI2NDM3Mw==&mid=2247486524&idx=3&sn=978e4dc051912a24b1303b847e72a768&chksm=97063ac767909e210a6440732a5c7a9fade7b7bd3153d1a79b7b3f8c8f783a832f5afe3fe0f8#rd", "authors": ["AI\u666e\u745e\u65af"], "title": "\u77e5\u4e4e\u76f4\u7b54\u5168\u9762\u8f6c\u5411<em class=\"highlight\">Agentic</em>\u6a21\u5f0f\uff1a\u4ece\u201c\u95ee\u5565\u7b54\u5565\u201d\u5230\u4e3b\u52a8\u89e3\u51b3\u95ee\u9898", "comment": "Source: WeChat, Published: 2025-09-25 09:24:49", "summary": "9\u670825\u65e5\u6d88\u606f\uff0c\u77e5\u4e4e\u65d7\u4e0bAI\u95ee\u7b54\u5de5\u5177\u201c\u76f4\u7b54\u201d\u5df2\u5b8c\u6210\u5168\u9762\u5347\u7ea7\uff0c\u6b63\u5f0f\u8f6c\u578b\u4e3aAgentic\uff08\u4ee3\u7406\u5f0f\uff09\u667a\u80fd\u52a9\u624b\uff0c\u4ece\u4f20\u7edf\u88ab\u52a8\u54cd\u5e94\u5f0fAI\u8f6c\u5411\u5177\u5907\u81ea\u4e3b\u89c4\u5212\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u8986\u76d6\u641c\u7d22\u3001\u7814\u7a76\u3001\u5b66\u4e60\u3001\u521b\u4f5c\u5168\u573a\u666f\u9700\u6c42\u3002", "AI": {"tldr": "9\u670825\u65e5\u6d88\u606f\uff0c\u77e5\u4e4e\u65d7\u4e0bAI\u95ee\u7b54\u5de5\u5177\u201c\u76f4\u7b54\u201d\u5df2\u5b8c\u6210\u5168\u9762\u5347\u7ea7\uff0c\u6b63\u5f0f\u8f6c\u578b\u4e3aAgentic\uff08\u4ee3\u7406\u5f0f\uff09\u667a\u80fd\u52a9\u624b\uff0c\u4ece\u4f20\u7edf\u88ab\u52a8\u54cd\u5e94\u5f0fAI\u8f6c\u5411\u5177\u5907\u81ea\u4e3b\u89c4\u5212\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u8986\u76d6\u641c\u7d22\u3001\u7814\u7a76\u3001\u5b66\u4e60\u3001\u521b\u4f5c\u5168\u573a\u666f\u9700\u6c42\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.2d853c03", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODE0OTc2Nw==&mid=2247484484&idx=1&sn=ba4a12dc3e5f2bdb912668c8eaf4e14c&chksm=c42fe71b552f6d65018bd332aa032f89419420705b72e1ecbaaf2339708f80cb48b410adf424#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODE0OTc2Nw==&mid=2247484484&idx=1&sn=ba4a12dc3e5f2bdb912668c8eaf4e14c&chksm=c42fe71b552f6d65018bd332aa032f89419420705b72e1ecbaaf2339708f80cb48b410adf424#rd", "authors": ["\u7845\u57fa\u751f\u547dAIGC"], "title": "\u9ea6\u80af\u9521\u62a5\u544a\u63ed\u793a 2025 \u62d0\u70b9\uff1a\u522b\u518d\u53ea\u662f\u628a AI \u5f53\u6210\u5de5\u5177\u4e86", "comment": "Source: WeChat, Published: 2025-09-25 08:37:56", "summary": "Agentic AI \u80fd\u591f\u72ec\u7acb\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8c03\u7528\u5de5\u5177\uff0c\u8fd8\u80fd\u4e0e\u5176\u5b83 AI \u534f\u4f5c\u300201\u3002multistep reasoning to write\uff0c deploy\uff0c and test code\uff0ccreate software that could act autonomously\uff0c", "AI": {"tldr": "Agentic AI \u80fd\u591f\u72ec\u7acb\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8c03\u7528\u5de5\u5177\uff0c\u8fd8\u80fd\u4e0e\u5176\u5b83 AI \u534f\u4f5c\u300201\u3002multistep reasoning to write\uff0c deploy\uff0c and test code\uff0ccreate software that could act autonomously\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.b5feb501", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyOTk1MTc0OQ==&mid=2247484136&idx=1&sn=526d1df048cdc328e194d586a997552b&chksm=c3bfa6e47c9d1aad95557ad4f813baf1fa2c0bc2269a876e210d543c441e71d14bde0c7c26ef#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyOTk1MTc0OQ==&mid=2247484136&idx=1&sn=526d1df048cdc328e194d586a997552b&chksm=c3bfa6e47c9d1aad95557ad4f813baf1fa2c0bc2269a876e210d543c441e71d14bde0c7c26ef#rd", "authors": ["\u8f9b\u5eb7\u5728\u8fdb\u5316"], "title": "\u4eceAgent\u5230<em class=\"highlight\">Agentic</em>\uff0c\u4e00\u8bcd\u4e4b\u5dee\uff0c\u9876\u5c16PM\u7684\u601d\u7ef4\u8dc3\u8fc1\uff5c\u5434\u6069\u8fbe", "comment": "Source: WeChat, Published: 2025-09-25 00:00:28", "summary": "\u201cagentic\u201d\u4f5c\u4e3a\u5f62\u5bb9\u8bcd\uff0c\u5219\u662f\u4e00\u5f20\u8fdb\u5316\u5730\u56fe\u3002\u5b83\u627f\u8ba4\u4e86\u201c\u667a\u80fd\u201d\u548c\u201c\u81ea\u4e3b\u201d\u662f\u4e00\u4e2a\u8fde\u7eed\u7684\u5149\u8c31\u3002\u4e00\u4e2aAI\u4ea7\u54c1\u53ef\u4ee5\u53ea\u670910%\u7684Agentic\u7279\u6027\uff0c\u4e5f\u53ef\u4ee5\u8fbe\u523090%\u3002\u8fd9\u79cd\u89c6\u89d2\u5c06\u4ea7\u54c1\u7ecf\u7406\u4ece\u6280\u672f\u5b9a\u4e49\u7684\u6ce5\u6f6d\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u8f6c\u800c\u601d\u8003\u4e00\u4e2a\u66f4\u5177\u5b9e\u8df5\u4ef7\u503c", "AI": {"tldr": "\u201cagentic\u201d\u4f5c\u4e3a\u5f62\u5bb9\u8bcd\uff0c\u5219\u662f\u4e00\u5f20\u8fdb\u5316\u5730\u56fe\u3002\u5b83\u627f\u8ba4\u4e86\u201c\u667a\u80fd\u201d\u548c\u201c\u81ea\u4e3b\u201d\u662f\u4e00\u4e2a\u8fde\u7eed\u7684\u5149\u8c31\u3002\u4e00\u4e2aAI\u4ea7\u54c1\u53ef\u4ee5\u53ea\u670910%\u7684Agentic\u7279\u6027\uff0c\u4e5f\u53ef\u4ee5\u8fbe\u523090%\u3002\u8fd9\u79cd\u89c6\u89d2\u5c06\u4ea7\u54c1\u7ecf\u7406\u4ece\u6280\u672f\u5b9a\u4e49\u7684\u6ce5\u6f6d\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u8f6c\u800c\u601d\u8003\u4e00\u4e2a\u66f4\u5177\u5b9e\u8df5\u4ef7\u503c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.c9000dcb", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTEzNzI4Mg==&mid=2247483988&idx=1&sn=f4aa8a5dbf5039d6f2f970f97dbdca63&chksm=fef6e45d8d5c496e9ea94b7df677dea4c1715a7b26e99c249656aea5521cef98715fbbd80cc3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTEzNzI4Mg==&mid=2247483988&idx=1&sn=f4aa8a5dbf5039d6f2f970f97dbdca63&chksm=fef6e45d8d5c496e9ea94b7df677dea4c1715a7b26e99c249656aea5521cef98715fbbd80cc3#rd", "authors": ["\u6bcf\u65e5\u79d1\u6280\u7b80\u62a5\u5f15\u64ce"], "title": "\u91cd\u78c5\uff01\u963f\u91cc\u5f00\u6e906\u5927AI<em class=\"highlight\">\u4ee3\u7406</em>\uff0c<em class=\"highlight\">Agentic</em>\u65f6\u4ee3\u6765\u88ad\uff1a\u4ece\u5de5\u5177\u8c03\u7528\u5230\u5168\u81ea\u4e3b\u51b3\u7b56\uff0c\u4e00\u591c\u98a0\u8986\u7a0b\u5e8f\u5458\u6548\u7387", "comment": "Source: WeChat, Published: 2025-09-24 23:27:42", "summary": "OpenAI\u610f\u5916\u5f00\u6e90gpt-oss\u6a21\u578b\uff0c\u9488\u5bf9\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u4ea4\u4e92\u8fdb\u884c\u4f18\u5316\uff0c\u652f\u6301Mixture-of-Experts\u548cGrouped Query Attention\u3002\u4eae\u70b9\uff1a\u53c2\u6570\u9ad8\u6548\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u901f\u5ea62\u500d\uff1b", "AI": {"tldr": "OpenAI\u610f\u5916\u5f00\u6e90gpt-oss\u6a21\u578b\uff0c\u9488\u5bf9\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u4ea4\u4e92\u8fdb\u884c\u4f18\u5316\uff0c\u652f\u6301Mixture-of-Experts\u548cGrouped Query Attention\u3002\u4eae\u70b9\uff1a\u53c2\u6570\u9ad8\u6548\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u901f\u5ea62\u500d\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.80e3aec6", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjUyNzU4MA==&mid=2247484882&idx=2&sn=f9e649bda39150aad53d0999c0d5ee39&chksm=e9bc3d2ef5246c761d3b4eaa6b4ee3b2124ba862b39278fb34f2c172be4c659c95a07449364a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjUyNzU4MA==&mid=2247484882&idx=2&sn=f9e649bda39150aad53d0999c0d5ee39&chksm=e9bc3d2ef5246c761d3b4eaa6b4ee3b2124ba862b39278fb34f2c172be4c659c95a07449364a#rd", "authors": ["\u6089\u89c1Alpha"], "title": "\u9ea6\u80af\u9521\uff1aAI<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u91cd\u6784\u751f\u547d\u79d1\u5b66\u4f01\u4e1a", "comment": "Source: WeChat, Published: 2025-09-24 23:00:58", "summary": "\u5e76\u4e14\u667a\u80fd\u4f53\u53ef\u4ee5\u4fc3\u8fdb\u4e1a\u52a1\u589e\u957f\u5e76\u63d0\u5347\u5229\u6da6\uff0c\u6709\u671b\u5728\u672a\u67653-5\u5e74\u5185\uff0c\u4f7f\u5236\u836f\u4f01\u4e1a\u7684\u589e\u957f\u63d0\u53475-13\u4e2a\u767e\u5206\u70b9\u3001EBITDA\u63d0\u9ad83.4-5.4\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u533b\u7597\u5668\u68b0\u4f01\u4e1a\u4e2d\uff0c\u589e\u957f\u63d0\u53473-7\u4e2a\u767e\u5206\u70b9\u3001EBITDA\u63d0\u9ad82.2-4.7\u4e2a\u767e\u5206\u70b9\u3002", "AI": {"tldr": "\u5e76\u4e14\u667a\u80fd\u4f53\u53ef\u4ee5\u4fc3\u8fdb\u4e1a\u52a1\u589e\u957f\u5e76\u63d0\u5347\u5229\u6da6\uff0c\u6709\u671b\u5728\u672a\u67653-5\u5e74\u5185\uff0c\u4f7f\u5236\u836f\u4f01\u4e1a\u7684\u589e\u957f\u63d0\u53475-13\u4e2a\u767e\u5206\u70b9\u3001EBITDA\u63d0\u9ad83.4-5.4\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u533b\u7597\u5668\u68b0\u4f01\u4e1a\u4e2d\uff0c\u589e\u957f\u63d0\u53473-7\u4e2a\u767e\u5206\u70b9\u3001EBITDA\u63d0\u9ad82.2-4.7\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1468f125", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTkwNjU1OA==&mid=2247483694&idx=1&sn=c077d9f07a5ac3d475fd41d97f14e2cf&chksm=c52b6cffd8da60a75710aa5cf85730b85c5770080d6fb65f57ac5ae9acd24a78380a41d7e405#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTkwNjU1OA==&mid=2247483694&idx=1&sn=c077d9f07a5ac3d475fd41d97f14e2cf&chksm=c52b6cffd8da60a75710aa5cf85730b85c5770080d6fb65f57ac5ae9acd24a78380a41d7e405#rd", "authors": ["\u5927\u8f66\u8c08AI"], "title": "\u5fb7\u52e4\u5ba1\u8ba1 Agent \u6df1\u5ea6\u89e3\u8bfb\uff1a\u4ece PairD \u5230 Omnia <em class=\"highlight\">Agentic</em> AI \u7684\u6f14\u8fdb\u8def\u5f84", "comment": "Source: WeChat, Published: 2025-09-24 16:16:06", "summary": "2025 \u5e74\u6700\u65b0\u516c\u544a\uff1a\u5728 Omnia \u5185\u90e8\u5d4c\u5165 GenAI/Agentic AI \u529f\u80fd\uff0c\u8986\u76d6\u6587\u6863\u5ba1\u9605\u3001\u62a5\u8868\u89e3\u8bfb\u3001\u8349\u7a3f\u8d77\u8349\u3001\u98ce\u9669\u8bc6\u522b\u3001\u4f1a\u8ba1\u7814\u7a76\u7b49\u6a21\u5757\u3002paird omnia copilot omnia agentic ai document document task summarization review orchestiration research report data q&a interpretation ag", "AI": {"tldr": "2025 \u5e74\u6700\u65b0\u516c\u544a\uff1a\u5728 Omnia \u5185\u90e8\u5d4c\u5165 GenAI/Agentic AI \u529f\u80fd\uff0c\u8986\u76d6\u6587\u6863\u5ba1\u9605\u3001\u62a5\u8868\u89e3\u8bfb\u3001\u8349\u7a3f\u8d77\u8349\u3001\u98ce\u9669\u8bc6\u522b\u3001\u4f1a\u8ba1\u7814\u7a76\u7b49\u6a21\u5757\u3002paird omnia copilot omnia agentic ai document document task summarization review orchestiration research report data q&a inter...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.6c5e6e1c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzcxMTUxOQ==&mid=2247505248&idx=1&sn=96d7886b0e5905f3caf7dcff59dd0378&chksm=e93b0abe7bf2edf2760b6a390e759135bdf614a0585ae697957af6d088da175a7b609d7a92f4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzcxMTUxOQ==&mid=2247505248&idx=1&sn=96d7886b0e5905f3caf7dcff59dd0378&chksm=e93b0abe7bf2edf2760b6a390e759135bdf614a0585ae697957af6d088da175a7b609d7a92f4#rd", "authors": ["\u4e91\u4e91\u4f17\u751fs"], "title": "\u53ef\u89c2\u6d4b\u6027\u7684\u7b2c\u56db\u5927\u652f\u67f1\uff1a<em class=\"highlight\">Agentic</em> AI \u7684\u5173\u952e", "comment": "Source: WeChat, Published: 2025-09-24 16:01:02", "summary": "\u8bd1\u81ea\uff1aObservability\u2019s Overlooked Fourth Pillar\uff1a Key for Agentic AI[1]\u4f5c\u8005\uff1aAmnon Heiman\u4ece\u53ef\u89c2\u6d4b\u6027 1.0 \u5230 2.0 \u7684\u8f6c\u53d8\u589e\u52a0\u4e86\u5f88\u591a\u4e1c\u897f\uff0c\u4f46\u6211\u8ba4\u4e3a\u6211\u4eec\u5728\u6b64\u8fc7\u7a0b\u4e2d\u5fd8\u8bb0\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002", "AI": {"tldr": "\u8bd1\u81ea\uff1aObservability\u2019s Overlooked Fourth Pillar\uff1a Key for Agentic AI[1]\u4f5c\u8005\uff1aAmnon Heiman\u4ece\u53ef\u89c2\u6d4b\u6027 1.0 \u5230 2.0 \u7684\u8f6c\u53d8\u589e\u52a0\u4e86\u5f88\u591a\u4e1c\u897f\uff0c\u4f46\u6211\u8ba4\u4e3a\u6211\u4eec\u5728\u6b64\u8fc7\u7a0b\u4e2d\u5fd8\u8bb0\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
