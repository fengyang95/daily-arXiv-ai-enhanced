<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究系统评估了18种语义相似度测量方法，发现常用指标存在严重问题，某些方法将语义相反内容误判为高度相似，而LLM方法在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 评估不同语义相似度测量方法的有效性，特别是在软件工程应用中，因为现有方法可能只是识别表面模式而非真正理解语义关系。

Method: 创建系统测试框架，对文本和代码应用受控变化，测试18种不同方法（包括基于词的方法、嵌入技术、LLM系统和结构感知算法）处理不同类型语义关系的能力。

Result: 嵌入方法错误地将语义相反内容识别为相似的概率高达99.9%，转换器方法有时将相反含义评为比同义词更相似。从欧几里得距离切换到余弦相似度使结果改善24-66%。LLM方法在区分语义差异方面表现更好。

Conclusion: 当前常用的语义相似度测量方法存在显著缺陷，需要更可靠的评估方法，LLM方法在语义理解方面表现相对更好。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [2] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 该研究识别并表征了导致LLM幻觉的关键符号属性，发现即使模型规模增大，符号元素（如修饰符和命名实体）仍然是导致幻觉的主要原因。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM幻觉问题已被广泛研究，但导致其内在易受幻觉影响的属性尚未被识别和研究。本研究旨在识别和表征这些关键属性，以定位模型内部机制的漏洞。

Method: 使用HaluEval和TruthfulQA两个数据集，将原有的问答格式转换为多种其他格式，以确定导致幻觉的属性。研究分析了不同规模的Gemma模型（2B、9B、27B）在不同任务和数据集上的表现。

Result: Gemma-2-2B的平均幻觉率为79.0%，随着模型规模增大，Gemma-2-9B降至73.6%，Gemma-2-27B降至63.9%。但修饰符（84.76%-94.98%）和命名实体（83.87%-93.96%）在所有模型和数据集上仍然导致高幻觉率。

Conclusion: 符号元素持续混淆模型，表明LLM在处理此类输入时存在根本性弱点，且这种弱点不受模型规模影响。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [3] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 提出了一个多智能体框架，通过角色提示和RAG技术提升金融问答准确性，在3,532个金融教育问题上比零-shot思维链基线提高6.6-8.3%


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法难以捕捉金融领域所需的专业推理能力，金融问题需要多步定量推理、领域术语理解和现实场景理解

Method: 使用基于角色的多智能体框架，包含基础生成器、证据检索器和专家评审器，结合RAG从6本金融教科书检索上下文证据

Result: 批判性精炼使答案准确率比零-shot思维链基线提高6.6-8.3%，Gemini-2.0-Flash表现最佳，GPT-4o-mini达到与金融调优模型相当的性能

Conclusion: 该方法为增强金融问答提供了一种经济有效的解决方案，并为多智能体金融LLM系统的进一步研究提供了见解

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [4] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门针对MCP标准的新基准测试，包含33个服务器、188个工具和600个查询，用于评估语言代理在工具交互中的真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估在MCP标准下的代理真实性能，导致对其操作价值的误解和能力差异的无法可靠区分。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床，开发了600个系统设计的查询，分布在6个不同复杂度的交互类别中，并引入了MCP-Eval结果导向评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了标准化和可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的代理，加速真正能力和互操作性AI系统的发展。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [5] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文对开源大语言模型在多标签意图分类任务上的性能进行了全面分析，使用MultiWOZ 2.1数据集比较了LLama2-7B、Mistral-7B和Yi-6B模型在少样本设置下的表现，并与BERT监督学习基线进行对比。


<details>
  <summary>Details</summary>
Motivation: 研究开源大语言模型在消费级硬件上处理复杂多意图对话分类任务的有效性，为任务导向聊天机器人的自然语言理解提供实用框架。

Method: 使用MultiWOZ 2.1数据集，在少样本设置下（提示中包含20个示例）测试三个开源LLM模型，并与BERT监督分类器进行性能对比，评估指标包括准确率、精确率、召回率、F1分数等。

Result: Mistral-7B-v0.1在14个意图类别中的11个上F分数表现最佳，加权平均F分数为0.50，但在整体性能上仍逊于BERT监督分类器。

Conclusion: 开源LLM在多标签意图分类任务中展现潜力，但监督学习方法仍具有性能优势，研究为小型开源模型在复杂对话理解中的应用提供了参考框架。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [6] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文比较了传统心理问卷与生态效度问卷在测量大语言模型人格特质时的差异，发现传统问卷存在测量不稳定、产生误导性印象等问题，建议避免使用传统心理问卷评估LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用人类设计的心理问卷（如BFI、PVQ）来测量大语言模型的人格特质和价值观，但这些问卷缺乏生态效度，无法反映LLMs在真实用户查询场景中的表现。需要明确两种问卷的差异及其带来的启示。

Method: 对传统心理问卷和生态效度问卷进行全面的比较分析，评估它们在测量LLMs人格特质时的差异和效果。

Result: 研究发现：1）传统问卷与生态效度问卷产生显著不同的人格特征剖面；2）传统问卷项目数量不足导致测量不稳定；3）传统问卷造成LLMs具有稳定特质的误导印象；4）对角色提示的LLMs产生夸张的人格剖面。

Conclusion: 传统心理问卷不适合用于评估大语言模型，建议避免使用这类问卷进行LLMs的人格特质测量。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [7] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出了一个系统框架，用于生成高质量、与人口分布对齐的AI角色集，通过社交媒体数据生成叙事角色，质量评估，重要性采样实现心理测量分布对齐，以及任务特定适配模块。


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注代理框架和模拟环境设计，忽视了角色生成的复杂性和非代表性角色集引入的偏见问题，需要构建能真实反映现实世界人口多样性和分布的角色集。

Method: 利用LLM从长期社交媒体数据生成叙事角色，进行严格质量评估过滤低质量档案，应用重要性采样实现与参考心理测量分布（如大五人格特质）的全局对齐，并引入任务特定模块适配目标子群体。

Result: 实验表明该方法显著减少了人口层面的偏见，能够为广泛的研究和政策应用提供准确、灵活的社交模拟。

Conclusion: 该方法为LLM驱动的社交模拟提供了高质量、人口对齐的角色集生成框架，解决了现有研究中的代表性不足问题。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [8] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 提出云边协同的多智能体提示框架，包含GuideLLM、SolverLLM和JudgeLLM三个组件，并创建RefactorCoderQA基准测试，在多个编程领域达到76.84%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性，需要优化大语言模型的推理和问题解决能力，特别是在真实编程场景中的表现。

Method: 采用云边协同架构，包含边缘部署的轻量级GuideLLM提供方法指导、云端强大SolverLLM生成代码解决方案、以及自动评估器JudgeLLM。创建RefactorCoderQA多领域编程基准。

Result: 微调模型RefactorCoder-MoE达到76.84%的整体准确率，显著超越开源和商业基线模型，人类评估验证了解释性、准确性和实用性。

Conclusion: 提出的架构在编程任务中表现优异，系统级指标分析提供了性能特征和权衡的深入见解。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [9] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动合成复杂问题和多轮强化学习训练，显著提升了开源大语言模型在深度搜索任务中的表现，在BrowseComp基准上取得了新的开源竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型在深度搜索任务中表现不佳，主要受限于长时程推理能力和缺乏足够难度的监督数据。

Method: 1) 从开放知识图谱自动合成复杂难找的问题；2) 应用端到端多轮强化学习来增强LLM的深度搜索长时程推理能力

Result: DeepDive-32B在BrowseComp上超越WebSailor、DeepSeek-R1-Browse和Search-o1，多轮RL训练显著提升了深度搜索能力，并支持测试时工具调用扩展和并行采样

Conclusion: DeepDive通过自动数据合成和多轮RL训练有效解决了深度搜索中的挑战，为开源模型在该领域提供了有竞争力的解决方案

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi提出了新的多维度评估指标，在软件工程任务中综合考虑准确性和资源消耗（token和时间），重新评估AI系统的整体效能。


<details>
  <summary>Details</summary>
Motivation: 现有AI软件工程排行榜（如SWE-bench）仅关注解决方案准确性，忽视了资源受限环境下的成本效益问题，这是AI系统实际部署的关键因素。

Method: 在SWE-bench基准的子集上，使用新的多维度指标（准确性vs资源消耗）重新评估流行的AI问题解决系统，分析效能表现。

Result: 发现系统效能不仅取决于框架本身，还与基础模型的整合程度相关；识别出"token雪球效应"和"昂贵失败"模式；观察到token预算和时间预算下的效能权衡。

Conclusion: 资源效率是AI系统实际部署的关键因素，需要综合考虑准确性和成本效益的平衡，这对项目预算管理和强化学习规模化至关重要。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [11] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec是一种基于LLM的规范生成方法，通过程序切片和逻辑删除两个新阶段，专门处理包含复杂循环结构的程序，显著提高了生成规范的准确性、相关性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在处理包含复杂循环结构的程序时往往生成不相关的规范，且验证工具的严格证明义务和设计约束会导致规范不完整和模糊。

Method: 提出SLD-Spec方法，包含两个新阶段：(1)切片阶段将函数分解为包含独立循环结构的代码片段；(2)逻辑删除阶段使用LLM推理过滤错误候选规范。

Result: 在简单数据集上比最先进的AutoSpec多验证5个程序，运行时间减少23.73%。在复杂循环程序数据集上，95.1%的断言和90.91%的程序通过验证。

Conclusion: SLD-Spec通过程序切片和逻辑删除有效解决了复杂循环程序的规范生成问题，逻辑删除对提升规范正确性和相关性至关重要，程序切片显著贡献于规范完整性。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [12] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL是一个集成SonarQube和大型语言模型的Web应用，通过三个模块自动化代码问题检测、修复和评估，在563个文件中处理7599个问题，证明能显著减少人工工作并降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码问题数量和种类大幅增长，需要高效的自动化工具来检测、解决和评估代码问题。

Method: 开发WALL Web应用，集成SonarQube和LLMs（GPT-3.5 Turbo和GPT-4o），包含问题提取工具、代码问题修订器和代码比较工具三个模块。

Result: 在563个文件7599个问题上进行实验，证明能有效减少人工工作并保持高质量修订，混合使用成本效益型和先进LLMs能显著降低成本和提升修订率。

Conclusion: WALL展示了自动化代码质量管理的有效性，未来工作将集成开源LLMs并消除人工干预，实现完全自动化。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [13] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 使用LLM自动化软件设计文档评审，分析11个评审视角，开发新技术让LLM理解含表格的复杂设计文档，实验证实LLM能识别设计文档中的不一致性


<details>
  <summary>Details</summary>
Motivation: 自动化软件设计文档评审过程，提高评审效率，减少人工评审工作量

Method: 分析设计文档评审方法并组织11个评审视角，开发新技术使LLM能理解含表格的复杂设计文档，使用GPT进行实验评估设计项和描述的一致性

Result: 确认LLM可以在评审过程中识别软件设计文档中的不一致性

Conclusion: LLM能够有效应用于软件设计文档的自动化评审，特别是在识别不一致性方面

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [14] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS是一种基于机器学习的测试选择方法，通过词袋表示代码变更文件，无需覆盖率映射，在工业部署中仅选择15%的测试用例，执行时间减少5.9倍，流水线加速5.6倍，故障检测率超过95%。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展和测试套件增长，在高频代码提交环境下，高效管理测试过程变得日益困难，需要更智能的测试选择方法。

Method: 提出T-TS机器学习方法，使用变更文件的词袋表示，结合跨文件和额外预测特征，避免使用覆盖率映射。

Result: 在生产环境中，T-TS仅选择15%的测试用例，执行时间减少5.9倍，流水线加速5.6倍，检测超过95%的测试失败。

Conclusion: T-TS在工业环境中表现出色，显著提升测试效率，实现公开可用以支持进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [15] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 基于82,845个真实开发者-LLM对话的分析显示，LLM响应比开发者提示长14倍，68%为多轮对话，主要涉及网页设计和神经网络训练任务。不同编程语言的代码存在特定问题，但通过多轮交互可以改善文档质量和导入处理。


<details>
  <summary>Details</summary>
Motivation: 了解开发者在实际工作中如何与LLM交互，以及这些对话动态如何影响任务结果、代码质量和软件工程工作流程。

Method: 利用CodeChat数据集（包含82,845个真实开发者-LLM对话和368,506个代码片段），分析对话模式、主题分布和代码质量问题。

Result: 发现LLM响应显著长于开发者提示，多轮对话占68%。不同语言存在特定代码问题：Python和JavaScript的未定义变量问题严重，Java缺乏注释，C++缺少头文件，C#存在未解析命名空间。多轮对话可以改善某些问题。

Conclusion: 开发者与LLM的对话是动态的，错误提示和明确修复请求能有效解决代码问题，多轮交互有助于提高代码质量。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: LLM智能体在UNO游戏中帮助其他玩家获胜的能力测试，发现虽然模型能超越随机基准，但很少能显著帮助他人


<details>
  <summary>Details</summary>
Motivation: 测试LLM作为主动参与者帮助人类完成目标的能力，特别是在协作游戏场景中的表现

Method: 构建工具让decoder-only LLM在RLCard游戏环境中作为智能体参与UNO游戏，使用两种不同的提示策略，评估从1B到70B不同规模模型的性能

Result: 所有模型在玩UNO时都能成功超越随机基准表现，但很少有模型能够显著帮助另一个玩家获胜

Conclusion: LLM在竞争性游戏中表现良好，但在协作帮助他人方面能力有限，模型规模对性能有影响但并非决定性因素

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [17] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA是一个保护隐私的多智能体系统，通过将工作空间划分为私有和公共区域，使用匿名化机制处理敏感数据，并通过知识增强和逻辑增强模块减少语义损失。


<details>
  <summary>Details</summary>
Motivation: 解决LLM多智能体系统在处理隐私数据时无法安全使用远程高性能LLM的问题，需要设计隐私保护机制。

Method: 提出GAMA系统，划分私有和公共空间，使用匿名化机制，并引入DRKE（基于领域规则的知识增强）和DLE（基于反证的逻辑增强）模块来缓解匿名化带来的语义损失。

Result: 在两个公开问答数据集上表现优于最先进模型，在新设计的隐私保护数据集上也显示出卓越的隐私保护效果。

Conclusion: GAMA在任务处理和隐私保护方面都表现出色，为隐私敏感的多智能体应用提供了有效解决方案。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [18] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents是一个基于多极任务处理图和IF-THEN规则的多智能体协作框架，用于处理具有不确定性的复杂任务，在知识型和逻辑型问答任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理高度复杂且具有不确定性的任务时，仍然面临有效任务规划的挑战，经常产生误导性或错误的输出，阻碍任务执行。

Method: 提出XAgents框架，使用多极任务处理图实现动态任务规划和处理任务不确定性，在子任务处理中集成领域特定的IF-THEN规则约束智能体行为，并通过全局规则增强智能体间协作。

Result: 在三个不同数据集上的评估表明，XAgents在知识型和逻辑型问答任务中持续超越最先进的单智能体和多智能体方法。

Conclusion: XAgents通过多极任务处理图和规则约束机制有效解决了复杂任务规划中的不确定性挑战，提升了多智能体系统的性能。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [19] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"沙盒经济"框架分析AI智能体经济，从起源（涌现vs意图）和隔离程度（可渗透vs不可渗透）两个维度进行表征，讨论了拍卖机制、使命经济和基础设施等设计选择来引导AI市场发展。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI智能体的快速采用，正在形成超越人类直接监管的新经济层，需要框架来分析这一新兴系统并确保其与人类长期繁荣保持一致。

Method: 提出沙盒经济分析框架，从起源和隔离程度两个维度对AI智能体经济进行表征，探讨拍卖机制、使命经济设计和社会技术基础设施等可能的解决方案。

Result: 当前趋势指向自发涌现的大规模可渗透AI智能体经济，既带来前所未有的协调机会，也面临系统性经济风险和加剧不平等的挑战。

Conclusion: 需要主动设计可引导的智能体市场，通过拍卖机制、使命经济和社会技术基础设施等设计选择，确保技术转变与人类长期集体繁荣保持一致。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [20] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI是一个用于临床自然语言推理的模块化代理推理框架，通过将知识访问与推理分离，使用特定推理家族的求解器来提高推理准确性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前假设认为扩大数据和参数规模会提高内部表示的结构化和泛化能力，但作者质疑这一假设在临床自然语言推理中的有效性，特别是在推理过程不明确时LLMs倾向于使用启发式方法。

Method: 提出CARENLI框架，将临床NLI基准分解为四个推理家族（因果归因、组合基础、认知验证、风险状态抽象），使用规划器、验证器和精炼器来实施可审计程序，为每个前提-陈述对路由到特定家族求解器。

Result: 在四个LLMs上，CARENLI将保真度提高了多达42个百分点，在因果归因中达到98.0%，在风险状态抽象中达到81.2%。验证器以接近顶级的可靠性标记违规，精炼器纠正了大量认知错误。

Conclusion: LLMs通常保留相关事实但在推理不明确时默认使用启发式方法，CARENLI使这种分离变得明确，同时为更安全、可审计的推理提供了框架。路由和家族分类是主要瓶颈。

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [21] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转变为因果推理任务，显著提高了步骤级准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法准确率极低（低于17%），无法进行有效的反事实推理来确定纠正单个动作是否能避免任务失败。

Method: 提出Abduct-Act-Predict (A2P) Scaffolding框架，通过三个结构化步骤：溯因推理（推断行动背后的根本原因）、行动（定义最小纠正干预）和预测（模拟后续轨迹验证干预有效性）。

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（比基线16.67%提高2.85倍），在Hand-Crafted数据集上达到29.31%准确率（比基线12.07%提高2.43倍）。

Conclusion: 通过因果推理框架重新定义问题，A2P Scaffolding为自动化故障归因提供了更稳健、可验证且准确度显著提高的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [22] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 该论文提出了一个信息论框架，通过分析状态-动作互信息模式来诊断RL代理在部署时的异常故障，能够区分传感器故障和驱动器故障。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的RL代理面临传感器故障、驱动器磨损和环境变化等问题，但缺乏内在机制来检测和诊断这些故障。

Method: 使用信息论框架分析状态-动作互信息模式，通过控制扰动实验验证信息指标对系统故障的差异诊断能力。

Result: 成功学习表现出特征性信息签名：状态-动作互信息从0.84增加到2.83比特（增长238%）；信息指标能够区分传感器故障（广泛崩溃所有信息通道）和驱动器故障（选择性破坏动作-结果可预测性）。

Conclusion: 信息模式既可作为学习的签名，也可作为系统健康的诊断工具，为能够自主故障检测和基于信息论原理进行策略调整的自适应RL系统奠定基础。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了完全自改进的交易代理，在加密货币交易中表现出色。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格波动受多种因素驱动且标注训练数据稀缺昂贵，需要一种无需人工监督的自改进交易系统。

Method: 从指令调优LLM开始，代理在闭环架构中迭代扮演三个角色（执行者、评判者、元评判者），利用多模态市场输入和内部偏好反馈，持续优化交易策略和评估标准。

Result: 在不同市场机制下的实验表明，Meta-RL-Crypto在真实市场的技术指标上表现良好，优于其他基于LLM的基线方法。

Conclusion: 该架构成功实现了无需人工监督的自改进交易代理，在加密货币交易预测方面具有显著优势。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [24] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成和token使用，忽略了增量解码方法和延迟时间，而延迟对用户体验和智能体工作流至关重要。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统根据每个查询决定应用哪种策略和分配多少计算资源，同时显式考虑token成本和延迟时间。

Result: 在推理基准测试中，该方法始终优于静态策略，实现了良好的准确率-成本权衡，并且适合实际部署。

Conclusion: 动态计算分配框架能够有效提升LLM性能，在保持实用性的同时优化准确率和成本之间的平衡。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [25] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 该论文探索使用LLMs作为科学机器学习代理，通过生成代码来求解ODE问题，而非直接预测解函数。作者引入了两个新数据集来评估LLMs在科学计算任务中的能力，发现通过适当的提示和微调，LLMs可以可靠地解决简单ODE问题。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法直接使用神经网络预测目标值存在准确性和鲁棒性挑战，本文探索替代方案：利用LLMs编写代码来调用成熟的数值算法，将学习负担从学习解函数转移到做出领域感知的数值选择。

Method: 引入两个新数据集：诊断性误导问题和1000个多样化ODE任务的大规模基准测试。评估开源和闭源LLM模型在无引导vs领域知识引导提示、现成vs微调变体下的表现，测量代码可执行性和数值有效性。

Result: 研究发现，在提供充分上下文和引导提示的情况下，较新的指令跟随模型在两个评估标准上都达到高准确率。开源系统无需微调就能表现良好，而较老或较小的模型仍能从微调中受益。

Conclusion: 精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理，为科学机器学习提供了新的可行途径。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [26] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G边缘网络的隐私保护、实时资源管理，通过跨层协同优化MAC层和应用层资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理，传统集中式方法面临隐私泄露和可扩展性问题。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络学习分散式策略，结合椭圆曲线Diffie Hellman密钥交换实现安全聚合，保护原始数据隐私。

Result: 仿真结果表明Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保强大的隐私保护和动态环境下的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了有效的隐私保护资源管理解决方案，能够满足URLLC、eMBB和mMTC等6G特定服务需求。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [27] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 研究表明，简单的提示注入攻击对LLM同行评审高度有效，可达100%接受率，且LLM评审普遍存在接受偏向（>95%），这对LLM在学术评审中的应用讨论有重大影响。


<details>
  <summary>Details</summary>
Motivation: 调查作者使用隐藏提示注入操纵评审分数的可行性和技术成功率，回应关于LLM在科学同行评审中使用的争议。

Method: 系统评估使用多种LLM对2024年ICLR论文生成的1000条评审，分析提示注入的有效性和评审偏向。

Result: 1) 非常简单提示注入高度有效，接受率可达100%；2) LLM评审普遍偏向接受（许多模型>95%）。

Conclusion: 提示注入攻击的可行性和LLM评审的系统性偏向对LLM在同行评审中的使用讨论具有重大影响。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [28] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习的离线方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择

Result: CDQAC在多种数据源上表现优异，始终优于原始数据生成启发式算法和SOTA方法，仅需10-20个训练实例

Conclusion: CDQAC是高效样本利用的离线RL方法，意外发现在随机启发式数据上训练效果优于高质量数据

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [29] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: 提出了IGPO（Inpainting Guided Policy Optimization）框架，利用掩码扩散大语言模型的inpainting能力指导强化学习探索，在数学推理任务上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在LLM对齐中的探索挑战——稀疏奖励信号和样本浪费问题，利用dLLMs独特的inpainting能力来引导探索

Method: IGPO框架在在线采样时策略性地插入部分真实推理轨迹，结合监督微调、基于熵的过滤等技术，提出在合成重写的简洁轨迹上进行SFT

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLMs实现了新的SOTA结果

Conclusion: inpainting能力可以有效指导RL探索，IGPO框架成功解决了梯度消失问题并提高了样本效率

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>
