<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出使用知识图谱结构化表示临床文档，用于自动化ICD编码任务，在保持90%信息的同时减少23%文本量，将Macro-F1分数提升3.20%并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 临床文档到标准化词汇的映射对临床研究和患者护理很重要，但人工编码耗时且难以扩展。现有方法主要利用外部知识增强输出表示，而对输入文档的外部资源表示研究不足。

Method: 构建文档级知识图谱来结构化表示输入文档，提供患者状况的全面结构化视图。将该图谱集成到最先进的ICD编码架构PLM-ICD中。

Result: 知识图谱用23%的原始文本保留了90%的信息，在ICD-9编码任务上将Macro-F1分数提升最多3.20%，同时提高了训练效率。图谱中的实体和关系类型带来了性能提升。

Conclusion: 使用知识图谱结构化表示临床文档能有效提升自动化ICD编码性能，不仅提高了准确率和效率，还增强了模型的可解释性潜力。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出CLAP方法，通过跨层注意力探测技术检测LLM幻觉，在多个模型和任务上优于基线方法，支持细粒度检测和检测后缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其生成不准确文本（幻觉）的可靠性问题日益突出，需要有效的检测方法。

Method: Cross-Layer Attention Probing (CLAP)，一种新颖的激活探测技术，将整个残差流中的LLM激活作为联合序列处理。

Result: 在5个LLM和3个任务上的实证评估显示，CLAP在贪婪解码和高温采样响应中都优于基线方法，支持细粒度幻觉检测。

Conclusion: CLAP能够有效检测幻觉，支持检测后缓解策略，提高LLM可靠性，且在分布外场景下仍保持高可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC是一个基于规则驱动的LLM指纹框架，通过多轮对话的上下文关联编码来实现所有权验证，解决了现有方法在隐蔽性、鲁棒性和泛化性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛部署引发了知识产权保护担忧，现有指纹方法存在可检测性、易受攻击和泛化能力差等问题，需要一种更可靠的解决方案。

Method: 采用规则驱动的指纹框架，编码多轮对话中的上下文关联（如反事实关系），而非依赖词级或单轮触发器，支持黑盒访问下的指纹验证。

Result: 在多个LLM架构上的广泛实验表明，CTCC相比现有方法具有更强的隐蔽性和鲁棒性，能有效减少误报和指纹泄露。

Conclusion: CTCC为实际LLM部署场景中的所有权验证提供了可靠实用的解决方案，支持在部分触发器暴露情况下的持续构建。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [4] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出使用大型语言模型生成基于代理的交通模型中个体出行日记的方案，通过开源数据生成虚拟人物并合成日记，验证显示LLM在出行目的确定方面表现优异，整体真实性与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量专有家庭出行调查数据，成本高昂且数据获取困难，需要开发一种基于开源数据的替代方案来生成个体出行日记。

Method: 使用美国社区调查和智能位置数据库的开源数据随机生成虚拟人物，通过直接提示方式合成出行日记，并采用包含四个指标（出行次数、时间间隔、出行目的、交通方式）的综合真实度评分体系进行验证。

Result: LLM生成的日记与传统方法（负二项式出行生成+多项式Logit模式/目的选择）相比具有相当的整体真实度（LLM均值：0.485 vs 0.455），在出行目的确定方面表现更优，且一致性更好。

Conclusion: LLM在零样本条件下具有可行性，为未来合成出行日记评估系统建立了可量化的真实度度量标准，展示了在交通建模中的应用潜力。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [5] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench是一个基于权威精神病学教科书和案例集的专业基准测试，包含11个问答任务和5300多个专家标注项目，用于评估LLM在精神病学实践中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要依赖小型临床访谈语料库、社交媒体帖子或合成对话，临床有效性有限，无法捕捉精神病学推理的复杂性，需要更专业的评估基准。

Method: 基于专家验证的精神病学教科书和案例集构建基准测试，包含诊断推理、治疗计划、纵向随访等11个任务类型，使用传统指标和LLM-as-judge相似性评分框架评估多种前沿LLM。

Result: 结果显示在临床一致性和安全性方面存在显著差距，特别是在多轮随访和管理任务中，表明需要专门的模型调优和更强大的评估范式。

Conclusion: PsychiatryBench为高风险心理健康应用中LLM性能的基准测试和改进提供了一个模块化、可扩展的平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [6] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究比较了SFT和ORPO两种训练方法对小型LLM进行接纳承诺疗法(ACT)训练的效果，发现ORPO方法在治疗保真度和共情方面显著优于SFT和基础模型，而思维链推理仅对SFT模型有显著帮助。


<details>
  <summary>Details</summary>
Motivation: 探索不同后训练方法和显式推理对小型开源大语言模型提供接纳承诺疗法能力的影响，旨在提高LLM在心理治疗应用中的效果。

Method: 使用Mistral-Large生成的50组合成ACT转录本，对Llama-3.2-3b-Instruct进行两种训练：监督微调(SFT)和odds ratio策略优化(ORPO)，每种方法都包含有/无思维链推理步骤。通过模拟治疗会话评估模型在ACT保真度量表和治疗师共情量表上的表现。

Result: ORPO训练模型在ACT保真度(χ²=185.15, p<.001)和治疗共情(χ²=140.37, p<.001)方面显著优于SFT和基础模型。思维链推理仅对SFT模型有显著益处，平均提高ACT-FM得分2.68分(p<.001)，但对ORPO模型无显著优势。

Conclusion: 偏好对齐策略优化能有效培养小型LLM的ACT能力，显式推理的效用高度依赖于底层训练范式，ORPO通过学习治疗过程而非模仿内容展现出优越性。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [7] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究系统评估了18种语义相似度测量方法，发现常用指标存在严重问题，某些嵌入方法将语义对立内容误判为相似度高达99.9%，而LLM方法在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码搜索、API推荐等软件工程应用中广泛用于语义相似度评估，需要验证这些方法是否真正理解语义关系还是仅识别表面模式。

Method: 研究测试了18种不同方法（包括基于词的方法、嵌入技术、LLM系统和结构感知算法），建立了系统测试框架，通过受控文本和代码变化来评估各方法处理不同类型语义关系的能力。

Result: 结果显示嵌入方法经常错误识别语义对立内容（相似度高达0.99），而LLM方法能更好区分语义差异（相似度0.00-0.29）。从欧几里得距离切换到余弦相似度使嵌入方法性能提升24-66%。

Conclusion: 当前常用的语义相似度度量方法存在显著缺陷，LLM方法在语义理解方面表现更优，但需要改进距离计算方式以提高准确性。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [8] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 该研究识别并表征了导致大语言模型产生幻觉的关键符号属性，发现符号元素（如修饰语和命名实体）是模型产生幻觉的根本原因，且随着模型规模增大幻觉率仅略有下降。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的幻觉问题已被广泛研究，但导致模型内在易产生幻觉的属性尚未被识别和研究。本研究旨在识别和表征这些关键属性，以定位模型内部机制的漏洞。

Method: 使用HaluEval和TruthfulQA两个数据集，将现有的问答格式转换为多种其他格式，以确定符号属性是导致幻觉的原因。测试了Gemma-2系列不同规模的模型（2B、9B、27B）。

Result: Gemma-2-2B的平均幻觉率为79.0%，随着模型规模增大，Gemma-2-9B降至73.6%，Gemma-2-27B降至63.9%。修饰语（84.76%-94.98%）和命名实体（83.87%-93.96%）在所有模型和数据集上都表现出极高的幻觉率。

Conclusion: 符号元素持续混淆大语言模型，表明这些模型在处理此类输入时存在根本性弱点，且这种弱点不受模型规模影响。符号属性是导致幻觉的内在脆弱性来源。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [9] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 提出了一个轻量级的两阶段实体链接系统BIBERT-Pipe，用于处理生物医学文本中的多语言嵌套命名实体链接任务，在BioNNE 2025多语言赛道中排名第三


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学实体链接基准主要针对英语和平坦实体，缺乏对多语言和嵌套实体的研究，需要填补这一空白

Method: 采用两阶段检索-排序管道：检索阶段使用原始预训练模型，排序阶段进行领域特定微调；使用可学习的[Ms]/[Me]标签包装提及；通过三个互补数据源自动扩展训练语料

Result: 在BioNNE 2025多语言排行榜上排名第三，证明了这些最小但原则性修改的有效性和竞争力

Conclusion: 提出的轻量级方法在保持原有实体链接模型完整性的同时，通过针对性的组件修改成功解决了多语言嵌套实体链接的挑战

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [10] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 提出了一个多智能体框架，通过角色提示和检索增强生成技术，显著提升金融问答的准确性，相比零样本思维链基线提高6.6-8.3%


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融教育问答中难以捕捉专业推理需求，金融领域需要多步定量推理、专业术语理解和现实场景理解

Method: 使用多智能体框架，包括基础生成器、证据检索器和专家评审器，结合检索增强生成技术从6本金融教科书中获取上下文证据

Result: 基于批判的细化方法比零样本思维链基线提高6.6-8.3%的准确率，Gemini-2.0-Flash表现最佳，GPT-4o-mini达到与金融调优模型相当的性能

Conclusion: 提供了一种成本效益高的方法来增强金融问答，为多智能体金融LLM系统的进一步研究提供了见解

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [11] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs是一个基于Hugging Face构建的框架，旨在解决手语处理研究中存在的代码复杂、可复现性低和比较不公平等问题，支持更多样化的数据模态和任务。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究相比口语语言研究面临更多挑战，包括复杂的临时代码、低可复现性和不公平比较。现有工具如Hugging Face在手语实验集成方面不够灵活。

Method: 在Hugging Face基础上构建MultimodalHugs框架，增加抽象层以支持更多数据模态和任务，特别是手语姿态估计数据和文本字符像素数据等多样化模态。

Result: 通过定量实验证明MultimodalHugs能够有效适应多种数据模态，为手语处理研究提供更好的实验支持和可复现性保障。

Conclusion: MultimodalHugs框架扩展了Hugging Face生态系统，不仅适用于手语处理，还可广泛应用于其他不符合Hugging Face标准模板的多模态用例，提高了研究的可复现性和公平性。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [12] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门针对MCP协议的综合基准测试，包含33个服务器、188个工具和600个查询，用于评估语言代理在工具交互中的真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估在MCP新范式下的真实代理性能，导致对其实际操作价值的认知失真，无法可靠区分不同代理的能力差异。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床；开发了包含6个不同交互复杂度类别的600个系统设计查询；引入了MCP-Eval这一新颖的结果导向评估方法，优先考虑真实世界任务成功率。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench旨在为研究社区提供一个标准化、可靠的框架，以构建、验证和推进能够充分利用MCP变革性优势的代理，从而加速真正能力和互操作性AI系统的发展。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [13] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一个无监督幻觉检测框架，利用LLM内部表示来识别生成内容的事实正确性，无需标注数据，计算成本低且适用于实时检测


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实正确性无关的代理信号，导致检测偏向表面特征，限制了跨数据集和场景的泛化能力

Method: 通过提示LLM仔细验证给定陈述的真实性，获取其情境化嵌入作为特征，并将每个响应的不确定性作为真实性的软伪标签

Result: 实验结果表明IRIS consistently outperforms existing unsupervised methods

Conclusion: IRIS是一个完全无监督、计算成本低的方法，即使在少量训练数据下也能良好工作，适用于实时幻觉检测

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [14] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文对开源大语言模型在多标签意图分类任务上的性能进行了全面分析，比较了LLama2-7B、Mistral-7B和Yi-6B在MultiWOZ 2.1数据集上的few-shot表现，并与BERT监督学习基线对比。


<details>
  <summary>Details</summary>
Motivation: 研究开源大语言模型在消费级硬件上处理复杂多意图对话分类任务的有效性，为任务导向聊天机器人的自然语言理解提供实用框架。

Method: 使用MultiWOZ 2.1数据集，在few-shot设置下（提示中包含20个示例）测试三个开源LLM模型，并与基于BERT的监督分类器进行性能对比，评估指标包括准确率、精确率、召回率和多种F1分数。

Result: Mistral-7B-v0.1在14个意图类别中的11个上F分数表现最佳，加权平均F分数为0.50，具有较低的Hamming Loss和较高的Jaccard相似度。但BERT监督分类器的性能仍优于最佳few-shot生成式LLM。

Conclusion: 开源LLM在多标签意图分类任务中展现出潜力，Mistral-7B表现最佳，但监督学习方法仍具有性能优势。研究为小规模开源LLM在复杂多意图对话检测中的应用提供了实用框架。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [15] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文比较了传统心理测量问卷与生态效度问卷在测量大语言模型个性特征时的差异，发现传统问卷存在测量不稳定、产生误导性结果等问题，建议避免使用传统心理问卷评估LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用人类设计的心理问卷（如BFI、PVQ）来测量大语言模型的个性特征和价值观，但这些问卷缺乏生态效度，无法反映LLMs在真实用户查询场景中的文本生成特性。需要明确两种问卷的差异及其提供的洞察。

Method: 对两种类型的问卷进行全面的比较分析：传统心理测量问卷和生态效度问卷，评估它们在测量LLMs个性特征时的表现差异。

Result: 分析发现传统问卷存在四个主要问题：1）与生态效度问卷产生显著不同的LLMs特征剖面；2）项目数量不足导致测量不稳定；3）产生LLMs具有稳定构念的误导性印象；4）对角色提示的LLMs产生夸大的特征剖面。

Conclusion: 研究警告不要使用传统心理问卷来评估大语言模型，建议开发更适合LLMs特性的评估方法。代码将在发表后发布。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [16] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出一个系统框架，用于生成高质量、与人口分布对齐的LLM驱动社交模拟角色集，通过社交媒体数据生成、质量评估和重要性采样来减少群体偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注智能体框架和模拟环境设计，往往忽视角色生成的复杂性以及非代表性角色集引入的潜在偏见，需要构建能真实反映现实人群多样性和分布的角色集。

Method: 利用LLM从长期社交媒体数据生成叙事角色，进行严格质量评估筛选低质量档案，应用重要性采样实现与参考心理测量分布（如大五人格特质）的全局对齐，并引入任务特定模块针对特定子人群进行适配。

Result: 广泛实验表明，该方法显著减少了群体层面的偏见，能够为广泛的研究和政策应用实现准确、灵活的社交模拟。

Conclusion: 提出的系统框架能够生成高质量、人口对齐的角色集，有效解决LLM社交模拟中的代表性偏差问题，为计算社会科学提供更可靠的模拟基础。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [17] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出Self Improving Faithfulness Aware Contrastive Tuning框架，通过自指导机制自动生成对比学习数据，提升LLM在知识冲突任务中的忠实性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型任务中因偏好依赖内部参数知识而非提供上下文而导致的不忠实响应问题

Method: 使用自指导机制自动生成高质量结构化对比学习数据（锚样本、语义等价正样本、模拟不忠实场景的负样本），然后应用对比学习训练模型

Result: 在ECARE KRE和COSE KRE基准测试中，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提升上下文召回率6.2%，显著减少对内部记忆的依赖

Conclusion: SI FACT在增强LLM上下文忠实性方面具有强有效性和高数据效率，为构建更主动和可信的语言模型提供了实用途径

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [18] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 提出了一种云边协作的多智能体提示框架，包含GuideLLM、SolverLLM和JudgeLLM三个组件，并创建了RefactorCoderQA基准测试，在多个技术领域取得了76.84%的最优准确率。


<details>
  <summary>Details</summary>
Motivation: 为了优化大语言模型的推理和问题解决能力，克服现有基准测试的局限性，需要一种更有效的架构来提升多领域编程任务的性能。

Method: 采用云边协作架构，包含三个专门组件：边缘部署的轻量级GuideLLM提供方法指导，云端SolverLLM生成代码解决方案，以及自动评估器JudgeLLM。使用RefactorCoderQA基准进行系统评估。

Result: 微调模型RefactorCoder-MoE实现了76.84%的整体准确率，显著优于领先的开源和商业基线模型。人类评估验证了生成解决方案的可解释性、准确性和实用性。

Conclusion: 提出的云边协作多智能体框架有效提升了LLMs的编程问题解决能力，RefactorCoderQA基准为多领域编码任务提供了全面的评估标准，系统性能指标分析揭示了架构的性能特征和权衡。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [19] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive是一个通过多轮强化学习增强大语言模型深度搜索能力的系统，使用知识图谱自动合成复杂问题，在BrowseComp基准上达到开源模型最佳性能


<details>
  <summary>Details</summary>
Motivation: 解决现有开源大语言模型在深度搜索任务中表现不佳的问题，主要是长时程推理能力不足和缺乏足够难度的监督数据

Method: 1. 从开放知识图谱自动合成复杂、困难且难以找到的问题；2. 应用端到端多轮强化学习来增强LLMs的深度搜索长时程推理能力

Result: DeepDive-32B在BrowseComp基准上取得新的开源竞争性结果，优于WebSailor、DeepSeek-R1-Browse和Search-o1，多轮RL训练显著提升深度搜索能力

Conclusion: DeepDive通过自动合成训练数据和多轮RL训练有效提升了LLMs的深度搜索能力，支持工具调用和并行采样的测试时扩展，所有资源已开源

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神疾病诊断中表现出色，特别是在小样本数据场景下。


<details>
  <summary>Details</summary>
Motivation: 脑网络标记数据有限，现有自监督学习方法的数据增强策略可能破坏脑图的关键结构语义，需要一种能够保护结构语义的表示学习方法。

Method: 提出两阶段框架：1）预训练阶段在小标记子集上训练边缘掩码器捕获关键结构语义；2）自监督学习阶段利用结构先验指导结构感知的数据增强过程。

Result: 在两个真实世界精神疾病数据集上，SAM-BG超越了最先进方法，特别是在小标记数据设置下，并发现了具有临床相关性的连接模式。

Conclusion: SAM-BG通过结构语义保护有效提升了脑图表示学习的性能和可解释性，为精神疾病诊断提供了更准确和可解释的解决方案。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [21] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT是一种解耦跨注意力迁移框架，允许在推理时仅使用单一传感器，通过跨模态知识迁移提升性能，减少硬件冗余。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要配对传感器数据，限制了在资源受限环境中的部署。需要一种方法能够在推理时仅使用单一传感器，同时利用其他模态的知识。

Method: 提出D-CAT框架，结合自注意力模块进行特征提取和新型跨注意力对齐损失，强制对齐不同传感器的特征空间，而不需要耦合两个模态的分类管道。

Result: 在三个多模态人类活动数据集上评估，在分布内场景中，从高性能模态迁移可获得10% F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能。

Conclusion: D-CAT通过实现单传感器推理和跨模态知识迁移，减少了感知系统的硬件冗余，同时保持准确性，适用于成本敏感或自适应部署场景。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [22] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自改进的加密货币交易代理，无需人工监督，在多种市场环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格变动受到链上活动、新闻流和社交情绪等多种快速变化因素驱动，且标记训练数据稀缺昂贵。

Method: 从基础指令调优LLM开始，代理在闭环架构中迭代交替扮演三个角色（执行者、评判者和元评判者），利用多模态市场输入和内部偏好反馈，持续改进交易策略和评估标准。

Result: 在多样化市场机制下的实验表明，Meta-RL-Crypto在真实市场技术指标上表现良好，并优于其他基于LLM的基线方法。

Conclusion: 该研究提出了一个创新的自改进交易代理架构，成功解决了加密货币预测中的复杂性和数据稀缺问题，为自动化交易系统提供了新的解决方案。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [23] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失来实现动态预算分配，在长上下文推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法根据任务需求灵活调整不同层和注意力头的缓存预算。

Method: 通过分析层注意力输出损失，推导出跨头比较的新度量标准，实现层级压缩和动态头预算分配；通过对比跨层信息，实现动态层预算分配。

Result: 在多个基准测试（LongBench、Needle-In-A-Haystack、Ruler、InfiniteBench）上表现出优越性能，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存驱逐和动态预算分配策略，无需训练或多策略组合，在各种任务类型中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [24] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO框架通过分离风险校准和偏好优化，为医疗补助人群的健康管理提供保守、可审计的决策支持，在控制不良事件风险的同时保持高安全覆盖率。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务，必须确保安全、公平和可审计性。传统方法难以在控制风险的同时优化服务偏好。

Method: 提出混合自适应符合离线强化学习(HACO)框架：1)训练轻量级风险模型预测不良事件；2)使用符合阈值屏蔽不安全行动；3)在安全子集上学习偏好策略。使用270万次决策数据进行验证。

Result: HACO实现了强风险区分能力(AUC~0.81)，校准阈值在α=0.10时为τ~0.038，同时保持高安全覆盖率。亚组分析显示不同人口统计群体间存在系统性价值差异。

Conclusion: 符合风险门控与离线强化学习相结合，能够为人群健康管理团队提供保守且可审计的决策支持，强调了公平性审计的重要性。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [25] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出基于单头交叉注意力机制的统一路由框架，动态选择最优LLM，在RouterBench基准上实现6.6%的AIQ提升和2.9%的最大性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的计算成本和性能差异给实际部署带来挑战，需要一种成本效益高的动态路由方案。

Method: 使用单头交叉注意力机制联合建模查询和模型嵌入，预测响应质量和生成成本，并采用指数奖励函数平衡性能与成本。

Result: 在多样化LLM池和领域的RouterBench基准测试中，相比现有路由器获得显著性能提升，架构轻量且跨域泛化能力强。

Conclusion: 该方法为成本感知的LLM路由建立了新标准，实现了性能与效率的优化平衡。

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [26] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成和token使用，忽略了增量解码方法和延迟时间的重要性，特别是在需要高效多查询的智能体工作流中。

Method: 建立动态计算分配和方法选择框架，在每查询基础上决定应用哪种策略和分配多少计算资源，同时显式考虑token成本和wall-clock延迟。

Result: 在推理基准测试中，该方法持续优于静态策略，实现了良好的准确率-成本权衡，且适合实际部署。

Conclusion: 动态计算分配框架能够有效平衡LLM性能和资源消耗，特别适用于对延迟敏感的应用场景。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [27] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探讨使用LLMs作为科学机器学习代理，通过生成数值算法代码来解决ODE问题，而非直接学习解函数。作者引入了两个新数据集来评估LLMs在科学计算任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法直接预测目标值存在精度和鲁棒性挑战，本文探索使用LLMs编写数值算法代码的替代方案，将负担从学习解函数转移到做出领域感知的数值选择。

Method: 引入诊断数据集和大规模ODE基准测试，评估开源和闭源LLM模型在无引导与领域知识引导提示、现成与微调变体下的表现，测量代码可执行性和数值有效性。

Result: 研究发现，在足够上下文和引导提示下，较新的指令跟随模型在两个评估标准上都达到高精度。开源系统无需微调表现强劲，而较老或较小模型仍能从微调中受益。

Conclusion: 初步结果表明，精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [28] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: 提出了FedBiF联邦学习框架，通过在本地训练期间直接学习量化模型参数，每次仅更新单个比特来减少通信开销，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在显著的通信开销问题，现有量化方法通常在本地训练后应用量化，导致量化误差影响模型精度。

Method: 服务器先量化模型参数并发送给客户端，每个客户端每次只更新多比特参数表示中的单个比特，冻结其余比特。

Result: 在5个数据集上的实验表明，FedBiF实现了优异的通信压缩，促进模型稀疏性，在仅使用1bpp上行和3bpp下行通信时达到与FedAvg相当的精度。

Conclusion: FedBiF框架有效解决了联邦学习的通信开销问题，通过逐比特更新策略在保持精度的同时显著减少通信成本。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [29] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络中的隐私保护、实时资源管理，通过跨层协调和加密聚合协议实现高效能效和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理，传统集中式方法面临隐私泄露和可扩展性问题。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络学习分散化策略，结合椭圆曲线Diffie Hellman密钥交换的安全聚合协议保护隐私，将资源管理问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保强大的隐私保护和在动态资源受限6G边缘网络中的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了一种有效的隐私保护资源管理解决方案，能够同时优化多个性能指标并满足6G特定服务要求，具有很好的实用价值。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [30] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出通过神经网络近似综合测量作为连续函数来重新优化解码器模型的方法，提高了表面码解码精度


<details>
  <summary>Details</summary>
Motivation: 传统解码器只能获取误差概率分布，因为从输入获得的正确预测不唯一，需要解决这个问题

Method: 使用神经网络将综合测量近似为连续函数进行数学插值，重新优化解码器模型

Result: 在码距5和7的多层感知机解码器，以及卷积、循环神经网络和Transformer解码器上，重新优化的解码器都比原始模型精度更高

Conclusion: 将表面码解码问题重新构建为可通过深度学习解决的回归问题是一个有效策略，该方法具有普遍有效性

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [31] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现：简单的提示词注入攻击在LLM科学评审中极其有效（可达100%接受率），且LLM评审普遍存在接受偏向（>95%），这对LLM在同行评审中的使用讨论具有重大影响。


<details>
  <summary>Details</summary>
Motivation: 针对作者使用隐藏提示词注入操纵评审分数的现象，研究这种攻击的可行性和技术成功率，以影响关于LLM在科学同行评审中使用的持续讨论。

Method: 使用多种LLM对2024年ICLR论文的1000篇评审进行系统评估，分析简单提示词注入的效果和LLM评审的偏向性。

Result: 1) 非常简单提示词注入高度有效，最高可达100%接受率；2) LLM评审普遍偏向接受（许多模型>95%接受率）。

Conclusion: 研究结果对LLM在同行评审中的使用讨论具有重大影响，揭示了当前LLM评审系统的脆弱性和系统性偏向问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [32] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择，实现离线学习

Result: CDQAC在各种数据源上表现优异，始终优于原始数据生成启发式方法，超越最先进的离线和在线RL基线，仅需10-20个训练实例即可学习高质量策略

Conclusion: CDQAC是高效的离线RL调度方法，特别令人惊讶的是，使用随机启发式生成的数据训练效果优于使用遗传算法和优先级调度规则生成的高质量数据

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [33] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种针对掩码扩散大语言模型的强化学习框架，通过部分真实推理轨迹的修复引导来提升探索效率和样本利用率，在数学推理任务上取得了SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在LLM对齐中的探索挑战，如稀疏奖励和样本浪费问题，利用dLLM的修复能力来指导探索

Method: 提出IGPO框架，在在线采样中策略性地插入部分真实推理轨迹，结合监督微调和强化学习，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得了显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: IGPO通过修复引导有效解决了RL探索效率问题，为dLLM的强化学习对齐提供了新的有效方法

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: AutoIND LLM平台可将IND申请的非临床书面总结起草时间减少约97%，从约100小时降至3-4小时，质量评分达70-78%，无关键监管错误，但仍需专家完善以提高质量。


<details>
  <summary>Details</summary>
Motivation: IND申请准备过程耗时且依赖专业知识，减缓了早期临床开发进程，需要寻找提高效率的方法。

Method: 使用AutoIND LLM平台生成IND非临床书面总结(eCTD模块2.6.2, 2.6.4, 2.6.6)，记录起草时间并与人工起草时间对比，由盲评监管写作评估员从7个预设类别评估质量。

Result: 起草时间减少97%(从~100小时降至3.7小时和2.6小时)，质量评分分别为69.6%和77.9%，无关键监管错误，但在重点突出、简洁性和清晰度方面存在不足。

Conclusion: AutoIND能显著加速IND起草，但专家监管写作者仍需完善输出以达到提交质量，系统性缺陷为模型改进提供了路线图。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [35] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 该研究开发了一个基于RAG的智能知识助手系统，专门用于山羊养殖健康管理，通过表格文本化和决策树文本化方法处理异构知识，在验证集和测试集上分别达到87.90%和84.22%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在畜牧业应用有限，主要受限于知识源的可用性、多样性和复杂性，需要专门的知识处理方法来提升模型对异构数据的理解能力。

Method: 采用检索增强生成(RAG)技术，提出表格文本化和决策树文本化两种结构化知识处理方法，建立覆盖疾病防治、营养管理、饲养管理等五个关键领域的山羊养殖知识库，并集成在线搜索模块实现实时信息检索。

Result: 异构知识融合方法取得最佳效果，验证集平均准确率87.90%，测试集84.22%。在文本、表格、决策树三类问答任务中准确率均超过85%，证明了模块化设计中结构化知识融合的有效性。

Conclusion: 该系统在山羊养殖实际应用中表现出稳健性和可靠性，错误分析显示遗漏是主要错误类型，为进一步提升检索覆盖率和上下文整合提供了改进方向。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [36] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 研究测试LLM在UNO游戏中作为助手帮助其他玩家获胜的能力，发现虽然所有模型都能超越随机基准，但只有少数能显著帮助其他玩家


<details>
  <summary>Details</summary>
Motivation: 测试大型语言模型是否能作为主动参与者真正帮助人类完成目标，特别是在协作游戏场景中的表现

Method: 构建工具让仅解码器LLM在RLCard游戏环境中作为代理参与UNO游戏，接收完整游戏状态信息，使用两种不同的提示策略，评估从1B到70B参数的不同规模模型

Result: 所有模型在玩UNO时都能成功超越随机基准，但只有少数模型能够显著帮助其他玩家获胜

Conclusion: 虽然LLM在单独玩游戏时表现良好，但在协作帮助他人方面的能力仍然有限，模型规模对性能有影响但并非决定性因素

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [37] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA是一个保护隐私的多智能体系统，通过将工作空间划分为私有和公共区域，使用匿名化机制处理敏感数据，并引入DRKE和DLE模块来减少语义损失。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中的广泛应用，当任务涉及隐私数据时，需要在不牺牲性能的前提下实现隐私保护。

Method: 提出GAMA系统，划分私有和公共空间，采用匿名化机制，并引入领域规则知识增强(DRKE)和反证逻辑增强(DLE)来缓解匿名化带来的语义损失。

Result: 在两个公开问答数据集上表现优于现有最先进模型，在新设计的隐私保护数据集上也显示出卓越的隐私保护效果。

Conclusion: GAMA在保持任务处理性能的同时，有效实现了隐私保护，为LLM-based多智能体系统的隐私安全提供了可行解决方案。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [38] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents是一个基于多极任务处理图和IF-THEN规则的多智能体协作框架，通过动态任务规划和规则约束来解决复杂任务中的不确定性，在知识型和逻辑型问答任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然提升了多智能体系统的能力，但在处理高度复杂和不确定的任务时，仍然存在任务规划效果不佳、产生误导性输出等问题，需要更好的解决方案。

Method: 提出XAgents框架，采用多极任务处理图实现动态任务规划和处理不确定性，在子任务处理中集成领域特定的IF-THEN规则约束智能体行为，并通过全局规则增强智能体间协作。

Result: 在三个不同数据集上的评估表明，XAgents在知识型和逻辑型问答任务中 consistently 超越最先进的单智能体和多智能体方法。

Conclusion: XAgents通过创新的多极任务处理图和规则集成机制，有效解决了复杂任务中的不确定性问题，为多智能体系统提供了更可靠的任务执行能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [39] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 提出了"沙盒经济"框架来分析新兴的AI代理经济，将其分为涌现vs有意设计、可渗透vs不可渗透两个维度，讨论了可引导AI代理市场的设计选择以确保技术转型符合人类长期繁荣。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的快速采用，正在形成一个超越人类直接监督的新经济层，需要分析这一新兴系统并设计可控的市场机制来应对机遇和挑战。

Method: 提出沙盒经济分析框架，从起源（涌现/有意）和分离度（可渗透/不可渗透）两个维度进行表征，探讨拍卖机制、AI"任务经济"设计和社会技术基础设施等设计选择。

Result: 当前趋势指向一个自发涌现的庞大且高度可渗透的AI代理经济，既带来前所未有的协调机会，也带来系统性经济风险和加剧不平等的挑战。

Conclusion: 需要主动设计可引导的代理市场，通过拍卖机制、集体目标协调和信任安全基础设施，确保技术转型与人类长期集体繁荣相一致。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [40] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI是一个用于临床自然语言推理的模块化代理推理框架，通过将知识访问与推理分离，显著提高了推理准确性和可审计性


<details>
  <summary>Details</summary>
Motivation: 挑战数据规模和参数扩展会自动产生结构化、可泛化内部表示的假设，特别是在临床NLI领域需要更安全、可审计的推理方法

Method: 开发CARENLI框架，将前提-陈述对路由到四个特定推理家族的求解器，通过规划器、验证器和精炼器强制执行可审计程序

Result: 在四个LLM上，CARENLI将保真度提高了多达42个百分点，在因果归因中达到98.0%，在风险状态抽象中达到81.2%，验证器以接近顶级的可靠性标记违规

Conclusion: LLMs通常保留相关事实但在推理未明确指定时默认使用启发式方法，CARENLI明确了这种分离，同时为更安全、可审计的推理提供了框架

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [41] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转变为因果推理任务，显著提高了步骤级准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法准确率极低（低于17%），无法进行有效的反事实推理来确定纠正单个动作是否能避免任务失败。

Method: 提出Abduct-Act-Predict (A2P) Scaffolding框架，通过三个结构化步骤：溯因推理（推断行动背后的隐藏原因）、行动（定义最小纠正干预）、预测（模拟后续轨迹验证干预效果）。

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（比基线16.67%提高2.85倍），在Hand-Crafted数据集上达到29.31%准确率（比基线12.07%提高2.43倍）。

Conclusion: 通过因果推理的视角重新构建问题，A2P Scaffolding为自动化故障归因提供了更稳健、可验证且准确度显著更高的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [42] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 该论文提出了一个信息论框架，通过分析状态-动作互信息模式来诊断RL代理在部署时的异常故障，能够区分传感器故障和驱动器故障。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的RL代理面临传感器故障、驱动器磨损和环境变化等问题，但缺乏内在机制来检测和诊断这些故障。

Method: 使用信息论框架分析状态-动作互信息模式，通过受控扰动实验验证信息指标对系统故障的差异诊断能力。

Result: 成功学习表现出特征性信息签名：状态-动作互信息从0.84增长到2.83比特（238%增长）；信息指标能够区分传感器故障（广泛的信息通道崩溃）和驱动器故障（选择性破坏动作-结果可预测性）。

Conclusion: 信息模式既是学习的签名，也是系统健康的诊断工具，为能够基于信息论原理进行自主故障检测和政策调整的自适应RL系统奠定了基础。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [43] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi提出新的多维度指标来评估AI系统在软件工程任务中的综合效能，不仅考虑准确性还考虑资源消耗（token和时间），发现在SWE-bench基准测试中AI系统的效能取决于脚手架与基础模型的整合质量，并识别出"token雪球效应"和"昂贵失败"等系统性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI软件工程排行榜（如SWE-bench）仅关注解决方案准确性，忽略了资源受限环境中的效能因素。任何AI系统不仅要正确，还必须具有成本效益。

Method: 引入SWE-Effi指标集，在多维度上重新评估AI系统效能，定义为结果准确性（如问题解决率）与资源消耗（如token和时间）之间的平衡。在SWE-bench子集上使用新指标对流行的AI问题解决系统进行重新排名。

Result: 发现AI系统效能不仅取决于脚手架本身，还取决于其与基础模型的整合质量；识别出"token雪球效应"和"昂贵失败"模式（在无法解决的任务上消耗过多资源）；观察到token预算和时间预算下的效能存在明显权衡。

Conclusion: 综合效能评估对AI系统的实际部署至关重要，资源效率是实现强性能的关键，这些发现对项目管理预算和可扩展的强化学习具有重要意义。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [44] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec是一种针对复杂循环程序的LLM辅助规范生成方法，通过程序切片和逻辑删除两个新阶段，显著提高了生成规范的正确性、相关性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在处理包含复杂循环结构的程序时往往产生不相关的规范，且验证工具的严格证明义务和设计约束会导致规范不完整和模糊。

Method: 提出SLD-Spec方法，包含两个新阶段：(1)切片阶段将函数分解为包含独立循环结构的代码片段；(2)逻辑删除阶段应用LLM推理过滤错误候选规范。

Result: 在简单数据集上比最先进的AutoSpec多验证5个程序，运行时间减少23.73%。在复杂循环程序数据集上，95.1%的断言和90.91%的程序通过验证。

Conclusion: SLD-Spec通过程序切片和逻辑删除有效解决了复杂循环程序的规范生成问题，逻辑删除对提升规范正确性和相关性至关重要，程序切片对规范完整性贡献显著。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [45] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL是一个集成SonarQube和大型语言模型的Web应用，通过三个模块自动化代码问题检测、修复和评估，在563个文件7599个问题上验证了有效性，能显著降低人工成本并提高修复率。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码问题数量和种类急剧增长，需要高效的问题检测、修复和评估工具来应对这一挑战。

Method: 开发WALL Web应用，集成SonarQube和LLMs（GPT-3.5 Turbo和GPT-4o），包含问题提取工具、代码问题修订器和代码比较工具三个模块，形成自动化流水线。

Result: 在563个文件7599个问题上实验证明，WALL能有效减少人工工作量并保持高质量修订，采用成本效益型和先进LLMs的混合方法能显著降低成本并提高修订率。

Conclusion: WALL展示了自动化代码质量管理的可行性，未来工作将集成开源LLMs并消除人工干预，实现完全自动化的代码质量管理。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [46] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究表明，通过精心设计的提示策略（特别是思维链提示）可以提高小型语言模型在代码生成任务中的能源效率，但效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件开发中的高能耗和碳足迹问题日益受到关注，小型语言模型作为更可持续的替代方案，需要研究如何通过提示工程提高其能源效率。

Method: 评估4个开源小型语言模型在150个Python编程问题上的表现，测试4种提示策略（角色提示、零样本、少样本、思维链），测量运行时间、内存使用和能耗，并与人工编写基准进行比较。

Result: 思维链提示为Qwen2.5-Coder和StableCode-3B带来一致的节能效果，而CodeLlama-7B和Phi-3-Mini-4K在任何提示策略下都无法超越基准。

Conclusion: 提示策略的效益具有模型依赖性，精心设计的提示可以引导小型语言模型实现更环保的软件开发。

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [47] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 使用LLM自动化软件设计文档评审过程的研究，通过分析11个评审视角并开发新技术，验证了LLM能够识别设计文档中的不一致性问题


<details>
  <summary>Details</summary>
Motivation: 探索利用大型语言模型(LLM)来自动化软件设计文档的评审过程，提高评审效率和准确性

Method: 分析设计文档评审方法并组织11个评审视角，针对可评审的视角开发新技术使LLM能够理解包含表格数据的复杂设计文档，使用GPT评估实际业务中不同设计文档间设计项和描述的一致性

Result: 实验证实LLM可以在评审过程中识别软件设计文档中的不一致性问题

Conclusion: LLM能够有效应用于软件设计文档的自动化评审，特别是在识别文档间一致性问题上表现出实用价值

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [48] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: LLM生成的Python代码在能源效率上表现不一，人类代码在服务器上更高效16%，树莓派上高3%，但在PC上LLM代码比人类代码高效25%。绿色软件专家编写的代码在所有平台上都比所有LLM代码高效17-30%。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的Python代码与人类编写代码在能源效率方面的对比，特别是在绿色软件开发背景下。

Method: 使用EvoEval基准测试中的9个编程问题的363个解决方案，测试6个主流LLM和4种提示技术，并与人类开发的解决方案进行比较，在三种硬件平台上测量能源消耗（总计约881小时）。

Result: 人类解决方案在服务器上节能16%，树莓派上节能3%；LLM在PC上比人类开发者节能25%。提示技术不能持续带来节能效果，最节能的提示因硬件平台而异。绿色软件专家的代码在所有硬件平台上都比所有LLM代码节能至少17-30%。

Conclusion: 尽管LLM展现出相对良好的代码生成能力，但没有LLM生成的代码比经验丰富的绿色软件开发人员编写的代码更节能，表明目前开发节能Python代码仍需要大量人类专业知识。

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [49] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: Stencil-Lifting是一个自动将低级语言编写的模板内核转换为等效DSL实现的系统，通过分层递归提升理论和算法实现高效转换，相比现有系统速度提升31.62倍和5.8倍。


<details>
  <summary>Details</summary>
Motivation: 解决现有验证提升系统在模板内核抽象方面的效率瓶颈，弥合传统优化技术与现代DSL范式之间的差距。

Method: 提出分层递归提升理论，使用不变子图表示模板内核，每个顶点关联基于谓词的摘要；开发分层递归提升算法，通过收敛递归过程保证终止性。

Result: 在两个不同测试套件的多样化模板基准和四个实际应用上评估，相比最先进的STNG和Dexter系统分别实现31.62倍和5.8倍的速度提升。

Conclusion: Stencil-Lifting显著提高了低级模板内核到DSL实现的转换效率，有效连接了传统优化技术和现代DSL范式。

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [50] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS是一种基于机器学习的工业测试选择方法，通过词袋表示提交文件变化，无需覆盖率映射，能选择15%的测试用例，减少5.9倍执行时间，检测95%以上的测试失败。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展和测试套件增长，高频代码提交使得测试过程管理变得困难，需要高效的测试选择方法来提升测试效率。

Method: 使用机器学习方法，将提交表示为变更文件的词袋，融入跨文件和额外预测特征，避免使用覆盖率映射。

Result: 在生产环境中，T-TS仅选择15%的测试，减少5.9倍执行时间，加速流水线5.6倍，检测超过95%的测试失败。

Conclusion: T-TS是一种高效的工业测试选择方法，显著提升测试效率，实现已公开以支持进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [51] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 基于82,845个真实开发者-LLM对话的分析显示，LLM响应比开发者提示长14倍，68%为多轮对话，代码生成存在语言特定的质量问题，但通过指出错误并明确要求修复的提示能有效改进代码质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件开发中广泛应用，但缺乏对开发者实际交互方式、对话动态如何影响任务结果和代码质量的深入理解。

Method: 利用CodeChat数据集（包含82,845个真实开发者-LLM对话，368,506个代码片段，覆盖20+编程语言），进行对话长度、轮次、主题分析和五种编程语言的代码质量评估。

Result: LLM响应长度中位数是提示的14倍；68%为多轮对话；Python和JavaScript代码常含未定义变量(83.4%/75.3%)；Java代码缺少注释(75.9%)；C++常省略头文件(41.1%)；C#存在未解析命名空间(49.2%)。指出错误并要求修复的提示最有效。

Conclusion: 开发者与LLM的对话具有复杂动态特性，代码质量问题具有语言特异性，但通过迭代对话和针对性提示可以显著改善代码质量，为优化LLM辅助开发工具提供了重要见解。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>
