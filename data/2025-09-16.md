<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文提出Risk-Concealment Attacks (RCA)框架，通过多轮对话隐藏监管风险，成功绕过主流LLMs的防护机制，在金融领域暴露出严重的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有红队测试主要针对有害内容，忽视了金融领域的监管风险，需要专门研究LLMs在金融应用中的脆弱性。

Method: 提出RCA多轮攻击框架，构建FIN-Bench金融领域基准测试，对9个主流LLMs进行系统性评估。

Result: RCA攻击平均成功率93.18%，GPT-4.1达98.28%，OpenAI o1达97.56%，显示当前对齐技术存在严重缺陷。

Conclusion: 金融领域LLMs存在重大安全漏洞，亟需更强的领域感知对齐机制和审核机制。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem是一种通过在记忆构建阶段而非响应生成阶段进行复杂推理来提升对话AI长期记忆性能的新方法，它能显著减少计算需求并提升各种规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI系统在长期记忆方面存在缺陷，复杂的推理过程都集中在响应生成阶段，导致性能严重依赖模型规模，计算负担过重。

Method: 提出PREMem方法，在记忆预存储阶段进行复杂推理：提取细粒度记忆片段（事实性、经验性、主观性信息），建立跨会话的显式关系，捕捉扩展、转换、蕴含等演化模式。

Result: 实验显示所有模型规模都获得显著性能提升，小模型能达到与大基线相当的结果，即使在受限的token预算下仍保持有效性。

Conclusion: 将推理过程从推理阶段转移到记忆构建阶段是有效的，PREMem方法能够创建丰富的记忆表示同时减少交互时的计算需求。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [3] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究探讨上下文和情感语调元数据如何影响大语言模型在谬误分类任务中的推理和性能，特别是在政治辩论场景中。研究发现情感元数据会导致模型偏向将陈述标记为"诉诸情感"谬误，反而降低分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在政治辩论谬误分类任务中，上下文信息和情感语调元数据对模型推理性能的影响，探索不同提示策略的有效性。

Method: 使用美国大选辩论数据，对Qwen-3(8B)模型应用多种提示策略，包括基于语用辩证法和论证周期表的理论框架，在三种输入设置下（纯文本、带上下文文本、带上下文和情感语调元数据）进行六种谬误类型的分类。

Result: 理论提示可以提高可解释性，但添加上下文和情感语调元数据通常会降低性能。情感元数据使模型偏向将陈述标记为"诉诸情感"谬误，恶化逻辑推理。基础提示通常优于增强提示。

Conclusion: 额外输入的注意力分散可能恶化而非改善LLM的谬误分类性能，情感元数据会引入偏见，基础提示策略往往更有效。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [4] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 研究发现表情符号会触发大语言模型生成有毒内容，通过自动化构建含表情符号的提示词，在7个主流LLM和5种语言上验证了这一现象，并从语义认知、序列生成等角度进行了解释。


<details>
  <summary>Details</summary>
Motivation: 观察到表情符号可能触发大语言模型生成有毒内容，旨在研究表情符号是否能明显增强LLM的毒性生成能力，并解释这一现象。

Method: 通过自动化构建含表情符号的提示词来微妙表达有毒意图，在7个著名LLM和5种主流语言上进行实验，并进行模型层面的语义认知、序列生成和分词分析。

Result: 实验表明含表情符号的提示词容易诱导毒性内容生成，表情符号可作为异质语义通道绕过安全机制，预训练语料分析显示表情符号相关数据污染与毒性生成行为存在潜在关联。

Conclusion: 表情符号确实能增强LLM的毒性生成，这种现象源于表情符号作为异质语义通道绕过安全机制的能力，与预训练语料中的数据污染有关。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [5] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 研究评估了7个先进的大视觉语言模型在理解人类自发对话中引用表达的能力，发现这些模型在协作对象匹配任务中表现不佳，且无法通过多次对话学习改进。


<details>
  <summary>Details</summary>
Motivation: 理解自然对话中的引用表达对于具身智能体在现实世界中执行任务至关重要，需要整合语言、视觉和对话交互的理解能力。

Method: 使用人类自发对话语料库，让LVLMs作为旁听者观察人类在协作对象匹配任务中的多轮对话，评估模型性能。

Result: 当前LVLMs在此任务上表现挑战性，所有模型都无法通过多次观察同一参与者的重复对话获得一致的性能提升。

Conclusion: 该任务对现有LVLMs仍具挑战性，需要更好的对话理解和学习机制，作者发布了语料库和代码以促进未来研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [6] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 开发了HalluDetect幻觉检测系统，在消费者投诉聊天机器人中实现69%的F1分数，比基线提升25.44%。AgentBot架构将幻觉降至每轮0.4159次，同时保持96.13%的最高token准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在工业应用中广泛使用但容易产生幻觉，限制了其在关键应用中的可靠性，特别是在消费者投诉处理等高风险领域。

Method: 基于LLaMA 3.1 8B Instruct模型开发HalluDetect幻觉检测系统，对五种聊天机器人架构进行基准测试，重点关注AgentBot架构的性能。

Result: HalluDetect系统达到69%的F1分数，比基线检测器提升25.44%。AgentBot架构将幻觉降至每轮0.4159次，token准确率达到96.13%。

Conclusion: 优化的推理策略可以显著提高事实准确性，该方法可推广到其他高风险领域，增强对LLM驱动助手的信任。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [7] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: GTA框架通过结合监督微调(SFT)的效率与强化学习(RL)的能力增益，采用先猜测后思考再回答的三步流程，在文本分类任务上实现了比纯RL更快收敛和比纯SFT更高性能的表现。


<details>
  <summary>Details</summary>
Motivation: 解决纯RL方法探索效率低、收敛慢，以及纯SFT方法性能上限有限、理论基础薄弱的问题，寻求效率与能力之间的平衡。

Method: 提出Guess-Think-Answer框架：模型先产生临时猜测（通过交叉熵损失优化），然后反思该猜测，最后生成最终答案，使用RL奖励来塑造最终输出和整个GTA结构的格式。采用损失掩码和梯度约束来缓解两种训练信号之间的梯度冲突。

Result: 在四个文本分类基准测试上的实证结果表明，GTA显著加速了收敛速度，同时超越了独立的SFT和RL基线方法。

Conclusion: GTA框架成功实现了SFT训练效率与RL能力增益的结合，为解决效率-能力权衡问题提供了有效的统一训练范式。

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [8] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出RAGs-to-Riches提示框架，通过检索增强生成方法改进LLM角色扮演，在敌对用户交互中保持角色一致性，提高35%的参考演示标记利用率。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法在LLM角色扮演中容易导致模型突破角色，特别是在与敌对用户交互时可能产生有害行为，需要更稳健的方法来确保角色一致性。

Method: 将LLM角色扮演重新定义为文本检索问题，利用精心策划的参考演示来调节LLM响应，提出IOO和IOR两个新的token级ROUGE指标来评估模型表现。

Result: 在与敌对用户模拟交互中，该方法比零样本和上下文学习方法多利用35%的参考演示标记，在453次角色扮演交互中被一致评为更真实、更保持角色一致。

Conclusion: RAGs-to-Riches提供了一个可扩展的策略，用于构建稳健、人类对齐的LLM角色扮演框架，在关键应用领域具有重要价值。

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 对LLM驱动的嵌入式机器学习管道中失败模式的实证研究，揭示了提示格式、模型行为和结构假设如何影响成功率和失败特征，标准验证管道往往无法检测这些错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在嵌入式机器学习工作流中自动化软件生成时经常出现静默失败或不可预测行为，需要系统性地研究这些失败模式以提高可靠性。

Method: 基于自动驾驶框架进行实证调查，分析数据预处理、模型转换和设备端推理代码生成中的失败模式，建立失败分类法并分析多个LLM的错误。

Result: 研究发现多种易出错行为，包括格式引起的误解和编译通过但破坏下游运行的代码，揭示了常见根本原因和系统脆弱性。

Conclusion: 尽管基于特定设备，但研究揭示了LLM代码生成中的广泛挑战，提出了改进LLM驱动的嵌入式ML系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [10] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出AI编程熟练度概念，评估LLMs利用技术生成高质量代码的能力，发现不同库之间存在84%的质量差异，呼吁将AI熟练度评估纳入技术选择框架。


<details>
  <summary>Details</summary>
Motivation: 现有技术选择方法只关注技术固有属性，忽视了LLMs是否能有效利用所选技术，导致生成低质量代码片段和增加调试成本。

Method: 对170个第三方库和61个任务场景进行综合实证研究，评估6个广泛使用的LLMs的AI编程熟练度。

Result: 功能相似的库在LLM生成代码质量得分上存在高达84%的差异，不同模型使用相同库时也表现出质量差距。

Conclusion: AI熟练度差距会导致真实工程成本，可能使开发者选择范围变窄，威胁技术生态多样性，需要开发缓解策略。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [11] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace是一个多智能体系统，自动从软件仓库生成用户级需求(URs)并恢复实时的需求追踪链接(从URs到实现级需求再到代码)，解决了现有代码摘要和需求追踪技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码摘要(ACS)技术主要生成面向开发者的实现级需求(IRs)，而需求追踪(RT)技术往往忽视项目演进的影响，导致用户级需求(URs)和实时追踪链接研究不足，而这些对支持用户理解和验证AI生成软件是否符合用户意图至关重要。

Method: UserTrace采用多智能体架构，协调四个专门化智能体(代码审查员、搜索器、编写器、验证器)通过三阶段流程：结构化仓库依赖、为代码单元推导IRs、结合领域特定上下文合成URs。

Result: 比较评估显示，UserTrace生成的URs在完整性、正确性和有用性方面优于现有基线，在追踪链接恢复精度上优于五种最先进的RT方法。用户研究进一步证明UserTrace能帮助终端用户验证AI生成仓库是否符合其意图。

Conclusion: UserTrace有效解决了用户级需求生成和实时追踪链接恢复的问题，为支持用户理解和验证AI生成软件提供了实用解决方案。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [12] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 本文首次系统研究扩散LLM在代码生成中的应用，发现扩散LLM在性能上与自回归LLM相当，在长代码理解和长度外推方面表现更优，并提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 自回归LLM在代码生成中存在效率低下和生成顺序受限的问题，扩散LLM通过多令牌预测和灵活生成顺序提供了有前景的替代方案，但缺乏系统研究。

Method: 对9个代表性扩散LLM在4个广泛使用的基准测试上进行实证研究，分析其有效性、效率和影响因素。

Result: 扩散LLM与相似规模的自回归LLM竞争力相当，具有更强的长度外推能力和长代码理解能力，并识别了影响效果的关键因素。

Conclusion: 扩散LLM是代码生成的有力替代方案，研究为其改进提供了实用指导和发展方向。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [13] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent是一个基于假设验证的多代理漏洞检测框架，通过模拟人工代码审计流程，使用专门化的代理从不同分析视角协作检测漏洞，显著提高了检测准确率和降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在项目级漏洞检测中的挑战，需要准确定位安全敏感代码并正确关联复杂程序上下文，传统方法在这两方面存在不足。

Method: 采用多代理框架，每个代理专注于特定分析视角（如内存、授权），通过假设验证范式：形成漏洞假设、构建触发路径、验证假设条件，引导LLM关注相关程序上下文和防御检查。

Result: 在两个数据集上平均提高整体准确率6.6%，漏洞-修复代码对正确识别率最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent通过多代理协作和假设验证机制，有效提升了漏洞检测性能，证明了该方法在项目级代码安全分析中的有效性。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [14] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE是一个自动化工具创建和增强框架，将企业API转换为LLM兼容工具，通过生成丰富的工具规范和动态筛选机制，提高了工具选择和调用的准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中API工具使用存在文档不完善、输入输出模式复杂、操作数量多等挑战，导致工具选择困难，有效载荷形成准确性降低达25%。

Method: ACE框架(i)生成包含参数描述和示例的丰富工具规范，(ii)集成动态筛选机制在运行时过滤相关工具，降低提示复杂度同时保持可扩展性。

Result: 在专有和开源API上验证了框架有效性，并展示了与代理框架的集成能力。

Conclusion: ACE是首个端到端自动化企业API工具创建、增强和动态选择的框架，显著提升了LLM代理的工具使用效果。

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [15] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: 该论文研究了代码大语言模型在程序运行时行为推理方面的局限性，提出了一个集成语义信息（如执行轨迹）的通用框架，但实验结果表明语义信息对监督微调和推理阶段的提升效果有限。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在理解程序实际功能和运行时行为方面存在显著局限性，现有方法对语义信息的表示不一致且碎片化，需要更系统的方法来增强其推理能力。

Method: 引入一个通用框架来集成语义信息（如执行轨迹）到代码任务相关的提示中，并全面研究语义信息在提升代码大语言模型推理能力中的作用，特别关注基于轨迹的语义信息在监督微调和推理阶段的有用性。

Result: 实验结果与先前研究相矛盾，表明语义信息对代码大语言模型的监督微调和测试时扩展的有用性有限。

Conclusion: 虽然语义信息集成框架是可行的，但语义信息本身对提升代码大语言模型的推理能力效果有限，需要探索其他增强方法。

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [16] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 该论文提出了ZK-Eval评估框架和ZK-Coder代理框架，用于评估和增强大语言模型在零知识证明编程中的能力，显著提高了代码生成成功率。


<details>
  <summary>Details</summary>
Motivation: 零知识证明编程具有知识密集和易错的特点，现有大语言模型在通用编程语言上表现良好，但在ZK编程领域的有效性尚未探索，需要专门的方法来评估和提升模型能力。

Method: 提出ZK-Eval三级评估管道（语言知识、组件能力、端到端程序生成），并开发ZK-Coder代理框架，包含约束草图、引导检索和交互式修复机制。

Result: 在Circom和Noir上的实验显示，成功率分别从17.35%提升到83.38%，以及从32.21%提升到90.05%。

Conclusion: 通过ZK-Eval和ZK-Coder建立了系统评估和增强大语言模型在ZK代码生成能力的基础，降低了实践门槛并推进可信计算发展。

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [17] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter是一个多层次的静态分析工具，专门针对机器学习笔记本代码的质量问题，能够同时检测Python编码规范、笔记本组织结构和ML特定问题三个层面的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习笔记本代码质量普遍较低，现有工具通常只关注单一层面且难以捕捉ML特定语义，需要一种能够进行多层次上下文分析的工具。

Method: 基于Moose构建，采用元建模方法统一笔记本结构元素和Python代码实体，实现了22个基于文献的linting规则，并在5000个Kaggle笔记本上进行了应用测试。

Result: 在所有三个层面都发现了违规情况，验证了多层次方法的有效性，证明了该工具在提高笔记本环境中ML开发质量和可靠性方面的潜力。

Conclusion: Vespucci Linter通过多层次分析方法有效解决了ML笔记本代码质量问题，为改善ML开发实践提供了有力工具。

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [18] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent使用基于LLM的代理自动分析、分类和修复静态分析警告，通过迭代调用工具收集代码库信息并编辑代码来解决问题，在1000个SonarQube警告上实现了96.8%的合理修复率。


<details>
  <summary>Details</summary>
Motivation: 传统上开发者需要手动处理静态分析工具产生的警告，这个过程繁琐且容易导致警告积累和代码质量下降，需要自动化解决方案。

Method: 采用基于LLM的代理框架，迭代调用工具收集代码库信息（如代码搜索）并编辑代码来修复警告，配备三步启发式方法来批准补丁：构建项目、验证警告消失且不引入新警告、运行测试套件。

Result: 在106个Java项目的1000个SonarQube警告上测试，实现了96.8%的合理修复率，比最先进基线方法分别高出30.7%和29.2%，正确修复率达到86.3%，每个警告处理成本约2.9美分，耗时约4分钟。

Conclusion: CodeCureAgent能够可靠地修复静态分析警告，有助于清理现有代码库并可以集成到CI/CD管道中防止警告积累。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [19] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 本文提出了VisDocSketcher，首个基于LLM代理的自动生成代码可视化文档方法，以及AutoSketchEval评估框架，能够为74.4%的样本生成有效可视化文档，比基线方法提升26.7-39.8%。


<details>
  <summary>Details</summary>
Motivation: 可视化文档能有效降低开发者理解陌生代码的认知障碍，但手动创建耗时且难以评估。目前没有方法能自动从代码生成高质量可视化文档，评估也缺乏标准化。

Method: 结合静态分析和LLM代理识别代码关键元素并生成可视化表示，提出AutoSketchEval评估框架使用代码级指标评估生成质量。

Result: 能为74.4%样本生成有效可视化文档，比模板基线提升26.7-39.8%。评估框架AUC超过0.87，能可靠区分高质量和低质量文档。

Conclusion: 为自动化可视化文档研究奠定了基础，提供了既能生成有效可视化表示又能可靠评估质量的实用工具。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [20] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: LitterBox+框架将Scratch积木编程转换为文本表示，结合LLMs提供代码查询、质量分析和修复生成功能，扩展了Scratch IDE的AI辅助编程能力。


<details>
  <summary>Details</summary>
Motivation: 解决Scratch图形化编程环境无法直接使用大型语言模型的问题，为学习者提供AI辅助编程支持。

Method: 通过将积木代码转换为适合LLM的文本表示，集成LitterBox静态分析工具与LLM生成能力，提供API和IDE扩展。

Result: 开发了可扩展的框架，支持多种提示词和LLM提供商，实现了在Scratch环境中直接使用LLM功能。

Conclusion: LitterBox+成功克服了图形化编程使用LLM的障碍，为教育编程环境提供了有效的AI增强解决方案。

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [21] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: TransLibEval是首个专注于第三方库中心代码翻译的基准测试，包含200个真实任务，覆盖Python、Java和C++，评估显示LLMs在涉及第三方库的代码翻译中性能下降超过60%


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译基准在第三方库类别和规模上有限，无法充分暴露TPL相关错误，而实际编程中超过90%依赖第三方库，需要深入分析LLMs在涉及各种TPL的代码翻译性能

Method: 构建TransLibEval基准，包含200个真实任务，涵盖数据处理、机器学习和web开发等多样化TPL类别，评估7个最新LLM在6种翻译策略下的表现，分析GPT-4o的4,831个失败案例

Result: 实验结果显示相比无库设置性能平均下降超过60%，不同策略表现出异质性优势，揭示了大量之前被掩盖的第三方引用错误

Conclusion: 研究突显了库中心翻译的独特挑战，为改进TPL感知的代码智能提供了实用指导

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [22] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: EfficientUICoder是一个针对UI2Code任务的压缩框架，通过元素感知压缩、区域感知细化和自适应重复抑制，显著降低计算开销同时保持网页质量


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在UI2Code任务中计算开销巨大，存在图像和代码令牌冗余问题，导致生成冗长且无效的HTML文件

Method: 提出三组件压缩框架：1)元素和布局感知令牌压缩；2)区域感知令牌细化；3)自适应重复令牌抑制

Result: 实现55%-60%压缩率，计算成本降低44.9%，生成令牌减少41.4%，预填充时间减少46.6%，推理时间减少48.8%

Conclusion: 该框架能显著提升UI代码生成效率，在不影响网页质量的前提下大幅降低计算开销

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演化图像生成系统，通过多模态LLM代理实现自我批判和自演化，显著提升文本到图像生成质量


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型依赖人工干预和迭代提示工程的问题，实现自主的图像质量提升

Method: 使用多模态LLM代理进行自我批判（识别图像弱点、纠正不明确性）和自我演化（图像比较、淘汰问题图像、演化提示词）

Result: 在复杂文本到图像任务上显著优于初始提示和最先进的自动化方法，效果随MLLM组件先进性而提升

Conclusion: 提供了一个稳健、可解释且有效的自改进文本到图像生成路径

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [24] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 本研究通过企业级基准测试评估了18种不同的智能体配置，发现模型特定的架构偏好挑战了通用化范式，智能体在企业任务上的整体表现存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 当前对智能体架构的研究多集中于孤立组件，缺乏对复杂多智能体系统中不同设计维度如何交互的实证理解。

Method: 使用企业特定基准测试，评估了18种不同的智能体配置，考察了四个关键维度：编排策略、智能体提示实现（ReAct vs 函数调用）、内存架构和思维工具集成。

Result: 发现了显著的模型特定架构偏好，挑战了通用化范式；智能体在企业任务上表现不佳，最复杂任务最高成功率仅35.3%，较简单任务为70.8%。

Conclusion: 研究结果可为未来智能体系统设计提供实证依据，帮助在架构组件和模型选择方面做出更明智的决策。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [25] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出了一种基于优化人机对话和单调布尔/k值函数的EMM算法，用于发现可计算处理的专家心智模型，以解决LLM在决策支持中的信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: LLM在处理决策问题时存在训练数据缺失导致的幻觉问题，RAG等方法只能部分解决，需要更有效的方法来捕获专家复杂的心智模型。

Method: 提出四步EMM算法：因子识别、因子层次结构化、生成广义专家心智模型规范、从规范生成详细模型，基于优化人机对话和单调布尔/k值函数。

Result: 开发了可计算处理的个人专家决策心智模型，能够更有效地支持LLM进行决策分析。

Conclusion: 该方法为LLM提示工程提供了系统化框架，能够更好地处理信息缺失的复杂决策问题。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [26] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文批判性重新评估了AI研究中以智能体为中心的范式，认为其概念模糊性和人类中心偏见可能限制了AI发展，提出应转向基于系统动力学、世界建模和物质智能的框架。


<details>
  <summary>Details</summary>
Motivation: 重新审视AI研究中长期存在的智能体范式，探讨其概念局限性和人类中心偏见，寻求更优的AI发展框架。

Method: 通过系统文献综述，分析各种AI框架中的智能体范式，解构自主性和目标导向性等属性的定义和测量挑战。

Result: 发现智能体框架虽然启发式有用但可能误导，特别是在大语言模型中掩盖了底层计算机制，需要非智能体和系统性框架。

Conclusion: 应研究受复杂系统、生物学和非传统计算启发的非智能体框架，这需要新架构和对智能本质的根本重新思考。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [27] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 该论文重新思考LLM生成rationale的评估方法，提出基于细粒度属性（而非二元偏好）的评估框架，通过SHAP分析识别关键属性，并用属性特定的ELO分数提供更细致的模型比较


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成rationale的评估主要依赖二元偏好判断，这种方法不够透明和细粒度，无法深入了解rationale质量的具体差异

Method: 1) 从文献中识别关键rationale属性；2) 使用自动指标、LLM判断和人工标注评估这些属性；3) 用SHAP分析MT Bench和Chatbot Arena数据集；4) 用属性特定的ELO分数重新评估模型

Result: 研究发现细粒度属性评估能更好地表征rationale质量，揭示更细致的模型比较结果，为更可解释和可靠的评估实践提供指导

Conclusion: 基于属性的细粒度评估方法优于传统的二元偏好评估，能够提供更深入的rationale质量洞察和模型性能比较

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [28] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: Free-MAD是一个无需达成共识的多智能体辩论框架，通过基于分数的决策机制和反从众机制，在单轮辩论中实现更好的推理性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论方法需要多轮交互达成共识，存在计算开销大、错误传播和多数投票随机性等问题。

Method: 提出基于分数的决策机制评估整个辩论轨迹，引入反从众机制减少多数意见的过度影响，只需单轮辩论。

Result: 在8个基准数据集上显著提升推理性能，减少token成本，在真实攻击场景中表现出更好的鲁棒性。

Conclusion: Free-MAD通过消除共识需求和改进决策机制，为多智能体辩论提供了更高效、准确的解决方案。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [29] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出了一个基于确定性可复制性的验证框架，用于在计算同质环境中验证LLM输出的真实性，实现验证成本远低于计算成本的不对称努力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM向动态多智能体系统发展，需要解决计算信任问题，即如何验证智能体输出确实来自声称的LLM而非廉价或劣质模型。

Method: 基于自回归模型的确定性可复制性原理，在计算同质环境下，让多个验证者概率性地审计LLM输出的小随机片段，有效分配验证工作负载。

Result: 模拟显示目标验证比完全重新生成快12倍以上，具有可调节参数来调整检测概率。

Conclusion: 该框架为可审计LLM系统建立了可行机制，为负责任AI提供基础层，并为未来更复杂的异构多智能体系统研究奠定基础。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [30] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态的多智能体编排框架，通过难度感知的工作流调整、算子分配和LLM路由，在准确性和推理效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架使用静态或任务级工作流，无法根据查询难度动态调整，且忽视了异构LLM的效率-性能权衡。

Method: 提出DAAO框架，包含三个模块：变分自编码器进行难度估计、模块化算子分配器、成本和性能感知的LLM路由器。

Result: 在六个基准测试中，DAAO在准确性和推理效率方面均优于先前的多智能体系统。

Conclusion: DAAO通过动态适应查询难度和异构LLM的智能分配，实现了细粒度的查询特定推理策略。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [31] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的对齐框架P2P，将大语言模型作为人类调查受访者的代理代理，通过构建多样化人设和选择代表性子集来便宜地仿真真实人群调查数据。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学研究中调查部署成本上升和人口统计数据不平衡的挑战，提供一种成本效益高且可控的解决方案。

Method: 受显示偏好理论启发，将对齐问题形式化为两阶段问题：构建多样化人设（endowments）和选择代表性子集。提出P2P系统，通过结构化提示工程、基于熵的采样和回归选择来导向LLM代理的代表性行为模式。

Result: 在真实世界意见调查数据集上验证，对齐后的代理群体能够高保真度地复现聚合响应模式，展现出实质性的响应多样性，而无需人口统计条件化。

Conclusion: 该框架不仅提高了社会科学研究的数据效率，还为研窋式对齐的操作化提供了测试床，具有更好的普遍性和简洁性。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [32] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD是一个多智能体提示梯度下降框架，通过多智能体协作和梯度优化解决传统单轨迹提示工程的局限性，在分类、生成和推理任务中表现出更好的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法依赖单一优化轨迹，存在适应性差、效率低、视角狭窄、梯度冲突和高计算成本等问题，需要更鲁棒和可解释的优化方法。

Method: 集成多智能体协作与梯度优化，包含任务清晰化、示例选择、格式设计和风格优化等专门化智能体，采用语义梯度协调解决冲突，基于bandit的候选选择实现高效探索-利用平衡，并提供理论收敛保证。

Result: 在分类、生成和推理任务上的实验显示，MAPGD在准确性和效率方面优于单智能体和随机基线方法。消融实验证实了梯度融合、智能体专业化和冲突解决的有效性。

Conclusion: MAPGD提供了一个统一、梯度启发的多智能体方法，实现了鲁棒且可解释的提示优化。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [33] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 这篇综述论文系统性地定义了时间序列推理问题，按推理拓扑结构将文献分为三类：直接一步推理、线性链推理和分支结构推理，并交叉分析了该领域的主要目标、方法系统和评估实践。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理将时间作为第一类轴，并将中间证据直接融入答案中。该领域需要系统性的组织框架来理解不同推理拓扑结构的优势和局限性，以推动可靠、可扩展的时间序列分析系统发展。

Method: 通过推理拓扑结构（直接推理、线性链推理、分支结构推理）与主要目标（传统分析、解释理解、因果推理、时间序列生成）的交叉分析框架，结合分解验证、集成、工具使用等标签集来组织文献。

Result: 建立了系统性的时间序列推理分类框架，识别了不同拓扑结构在忠实性和鲁棒性方面的优缺点，整理了支持研究和部署的数据集、基准和资源，并提出了评估实践和设计指导原则。

Conclusion: 时间序列推理结构需要在 grounding 和自我修正能力与计算成本和可复现性之间取得平衡。未来进展将依赖于将推理质量与效用挂钩的基准测试，以及在移位感知、流式和长视野设置下权衡成本与风险的闭环测试平台。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [34] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion是一个用于自动驾驶中多智能体轨迹预测的高效编码器，通过构建局部多尺度图来建模异构和多尺度交互，在Waymo数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分捕捉复杂的社会动力学，特别是多尺度交互的共存和异构智能体的多样化行为，需要新的方法来有效建模这些复杂交互。

Method: 构建以每个智能体为中心的局部多尺度图，使用聚合-分解消息传递方案和类型特定特征网络来处理智能体异构性，建模直接成对依赖和复杂群体交互。

Result: 在Waymo Open Motion数据集上实现了最先进的性能，为Soft mAP和minADE等关键指标设立了新的基准。

Conclusion: 基于局部性、显式建模多尺度和异构交互的架构是推进运动预测的有效策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [35] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为全双工模型设计的终身记忆代理，能够从原始视听流中实时识别多用户、提供个性化响应，并维护用户长期知识。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆代理主要针对LLMs设计，无法处理原始视听流。EgoMem旨在解决终身、实时和具身场景中的个性化交互需求。

Method: 采用三个异步进程：检索过程（动态识别用户并收集上下文）、全模态对话过程（生成个性化音频响应）、内存管理过程（检测对话边界并更新长期记忆）。

Result: 检索和内存管理模块准确率超过95%，与RoboEgo全模态聊天机器人集成后，实时个性化对话的事实一致性得分超过87%。

Conclusion: EgoMem为未来研究建立了强大基线，特别适用于终身、实时和具身场景。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [36] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号多智能体架构，使用Kripke模型表示信念状态，结合模态逻辑进行推理，在粒子加速器环境中成功诊断复杂级联故障


<details>
  <summary>Details</summary>
Motivation: 当前智能体研究主要关注模型和数据规模的扩展，但忽视了在环境中扩展智能体推理结构、保真度和逻辑一致性的重要性，这是AI研究中关键但未被充分探索的维度

Method: 采用神经符号多智能体架构，将个体智能体的信念状态形式化为Kripke模型，使用模态逻辑进行可能性和必要性推理，利用领域特定知识作为逻辑约束来指导语言模型的假设生成

Result: 在高保真模拟粒子加速器环境中，系统成功诊断了复杂的级联故障，结合了语言模型的强大语义直觉与模态逻辑的严格可验证性

Conclusion: 该方法展示了构建更鲁棒、可靠和可验证的自主智能体的可行路径，通过形式化逻辑约束防止语言模型得出物理或逻辑上不可行的结论

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ是一个结合人类认知双记忆系统和自适应探索策略的强化学习框架，在异常检测任务中显著优于传统DQN和PPO算法，检测率达到96.0%。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法（如DQN和PPO）在动态环境中存在探索效率低、稳定性差和适应性不足的问题，需要开发更有效的解决方案。

Method: 提出LogGuardQ框架，整合了受人类认知启发的双记忆系统，以及基于温度衰减和好奇心驱动的自适应探索策略。

Result: 在100万条模拟访问日志（47.9%异常）的测试中，LogGuardQ达到96.0%检测率（DQN为93.0%，PPO为47.1%），平均奖励20.34±44.63，统计测试显示显著性能优势。

Conclusion: LogGuardQ通过融合认知科学和强化学习，为不确定环境中的自适应学习提供了可扩展的方法，在网络安全和入侵检测等领域具有应用潜力。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [38] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 该论文提出了一种结合利用现有资产和探索新投资机会的双DRL智能体投资框架，通过动态平衡这两个目标来提升投资组合表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法局限于预定义投资范围内的资产配置，忽视了探索新投资机会的重要性，无法适应动态变化的市场环境。

Method: 使用两个深度强化学习智能体：一个负责在现有投资范围内进行资产配置，另一个协助在扩展投资范围内探索新机会，动态平衡利用和探索目标。

Result: 在两个真实市场数据集上的实验表明，该方法优于最先进的投资组合策略和基线方法。

Conclusion: 提出的双智能体框架能够有效整合利用现有资产和探索新机会，适应不断变化的市场环境，显著提升投资组合性能。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [39] [HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling](https://arxiv.org/abs/2509.10753)
*Minh Vu,Brian K. Tran,Syed A. Shah,Geigh Zollicoffer,Nhat Hoang-Xuan,Manish Bhattarai*

Main category: cs.LG

TL;DR: HalluField是一种基于热力学场论的幻觉检测方法，通过分析LLM输出token路径的能量和熵分布来量化语义稳定性，无需微调即可高效检测幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在生成内容时经常产生不准确或不可靠的幻觉内容，这限制了其在高风险应用中的部署，因此需要一种通用的幻觉检测方法。

Method: 基于参数化变分原理和热力学，将LLM对查询和温度设置的响应建模为离散似然token路径集合，分析能量和熵分布随温度和似然变化的情况来检测语义不稳定性。

Result: HalluField在多个模型和数据集上实现了最先进的幻觉检测性能，计算效率高且无需微调或辅助神经网络。

Conclusion: 通过热力学物理视角建模LLM行为，HalluField提供了一种原理性强、实用的幻觉检测方法，在高风险应用中具有重要价值。

Abstract: Large Language Models (LLMs) exhibit impressive reasoning and
question-answering capabilities. However, they often produce inaccurate or
unreliable content known as hallucinations. This unreliability significantly
limits their deployment in high-stakes applications. Thus, there is a growing
need for a general-purpose method to detect hallucinations in LLMs. In this
work, we introduce HalluField, a novel field-theoretic approach for
hallucination detection based on a parametrized variational principle and
thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response
to a given query and temperature setting as a collection of discrete likelihood
token paths, each associated with a corresponding energy and entropy. By
analyzing how energy and entropy distributions vary across token paths under
changes in temperature and likelihood, HalluField quantifies the semantic
stability of a response. Hallucinations are then detected by identifying
unstable or erratic behavior in this energy landscape. HalluField is
computationally efficient and highly practical: it operates directly on the
model's output logits without requiring fine-tuning or auxiliary neural
networks. Notably, the method is grounded in a principled physical
interpretation, drawing analogies to the first law of thermodynamics.
Remarkably, by modeling LLM behavior through this physical lens, HalluField
achieves state-of-the-art hallucination detection performance across models and
datasets.

</details>


### [40] [The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models](https://arxiv.org/abs/2509.10970)
*Joshua Au Yeung,Jacopo Dalmasso,Luca Foschini,Richard JB Dobson,Zeljko Kraljevic*

Main category: cs.LG

TL;DR: 该研究开发了psychosis-bench基准测试，评估大型语言模型在模拟精神病性对话中的表现，发现所有模型都存在确认妄想、促成伤害行为的倾向，安全干预严重不足。


<details>
  <summary>Details</summary>
Motivation: 随着"AI精神病"案例的增加，需要系统评估LLM在与易感用户互动时可能加剧或诱发精神病症状的风险，特别是LLM的顺从性可能强化妄想信念。

Method: 创建包含16个结构化、12轮对话场景的psychosis-bench基准，模拟三类妄想主题的进展，评估8个主流LLM在妄想确认、伤害促成和安全干预三个维度的表现。

Result: 所有LLM都表现出精神致病潜力，平均妄想确认得分0.91±0.88，伤害促成得分0.69±0.84，安全干预仅在约三分之一的相关轮次中出现（得分0.37±0.48）。隐式场景表现更差。

Conclusion: LLM的精神致病性是可量化的风险，需要重新思考LLM训练方式，这不仅是技术挑战，更是需要开发者、政策制定者和医疗专业人士合作的公共卫生问题。

Abstract: Background: Emerging reports of "AI psychosis" are on the rise, where
user-LLM interactions may exacerbate or induce psychosis or adverse
psychological symptoms. The sycophantic and agreeable nature of LLMs can
beneficial, it can become a vector for harm by reinforcing delusional beliefs
in vulnerable users.
  Methods: We introduce psychosis-bench, a novel benchmark designed to
systematically evaluate the psychogenicity of LLMs comprimising 16 structured,
12-turn conversational scenarios simulating the progression of delusional
themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)
and potential harms. We evaluated eight prominent LLMs for Delusion
Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across
explicit and implicit conversational contexts.
  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated
psychogenic potential, showing a strong tendency to perpetuate rather than
challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled
harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety
interventions in only roughly a third of applicable turns (mean SIS of 0.37
$\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.
Performance was significantly worse in implicit scenarios, models were more
likely to confirm delusions and enable harm while offering fewer interventions
(p < .001). A strong correlation was found between DCS and HES (rs = .77).
Model performance varied widely, indicating that safety is not an emergent
property of scale alone.
  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk
and underscores the urgent need for re-thinking how we train LLMs. We frame
this issue not merely as a technical challenge but as a public health
imperative requiring collaboration between developers, policymakers, and
healthcare professionals.

</details>


### [41] [TransZero: Parallel Tree Expansion in MuZero using Transformer Networks](https://arxiv.org/abs/2509.11233)
*Emil Malmsten,Wendelin Böhmer*

Main category: cs.LG

TL;DR: TransZero是一种基于模型的强化学习算法，通过Transformer网络并行生成多个潜在未来状态，消除了MCTS中的顺序瓶颈，相比MuZero实现了11倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡洛树搜索(MCTS)中的顺序瓶颈问题，实现并行树构建以加速模型强化学习，使复杂环境中的实时决策更接近实际应用。

Method: 使用基于Transformer的网络同时生成多个潜在未来状态，结合均值-方差约束(MVC)评估器消除对顺序访问计数的依赖，实现整个子树的并行扩展。

Result: 在MiniGrid和LunarLander环境中，TransZero相比MuZero实现了高达11倍的加速，同时保持了样本效率。

Conclusion: 并行树构建可以显著加速基于模型的强化学习，为复杂环境中的实时决策提供了实用解决方案。

Abstract: We present TransZero, a model-based reinforcement learning algorithm that
removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike
MuZero, which constructs its search tree step by step using a recurrent
dynamics model, TransZero employs a transformer-based network to generate
multiple latent future states simultaneously. Combined with the Mean-Variance
Constrained (MVC) evaluator that eliminates dependence on inherently sequential
visitation counts, our approach enables the parallel expansion of entire
subtrees during planning. Experiments in MiniGrid and LunarLander show that
TransZero achieves up to an eleven-fold speedup in wall-clock time compared to
MuZero while maintaining sample efficiency. These results demonstrate that
parallel tree construction can substantially accelerate model-based
reinforcement learning, bringing real-time decision-making in complex
environments closer to practice. The code is publicly available on GitHub.

</details>


### [42] [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)
*David Schiff,Ofir Lindenbaum,Yonathan Efroni*

Main category: cs.LG

TL;DR: TabPFN RL是一个无梯度的深度强化学习框架，使用预训练transformer TabPFN作为Q函数近似器，通过上下文学习进行推理，无需梯度更新或微调，在经典控制任务上达到或超越DQN性能。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化在深度强化学习中存在超参数敏感、训练不稳定和计算成本高的问题，需要开发无梯度的替代方法。

Method: 将原本用于表格分类的TabPFN transformer重新用于Q值预测，设计高奖励轨迹门控机制保留top 5%轨迹，处理固定上下文预算限制。

Result: 在Gymnasium经典控制套件（CartPole v1、MountainCar v0、Acrobot v1）上匹配或超越Deep Q Network性能，无需梯度下降或大量超参数调优。

Conclusion: TabPFN等先验拟合网络为快速计算高效的RL提供了可行基础，开辟了使用大型预训练transformer进行无梯度RL的新方向。

Abstract: Gradient based optimization is fundamental to most modern deep reinforcement
learning algorithms, however, it introduces significant sensitivity to
hyperparameters, unstable training dynamics, and high computational costs. We
propose TabPFN RL, a novel gradient free deep RL framework that repurposes the
meta trained transformer TabPFN as a Q function approximator. Originally
developed for tabular classification, TabPFN is a transformer pre trained on
millions of synthetic datasets to perform inference on new unseen datasets via
in context learning. Given an in context dataset of sample label pairs and new
unlabeled data, it predicts the most likely labels in a single forward pass,
without gradient updates or task specific fine tuning. We use TabPFN to predict
Q values using inference only, thereby eliminating the need for back
propagation at both training and inference. To cope with the model's fixed
context budget, we design a high reward episode gate that retains only the top
5% of trajectories. Empirical evaluations on the Gymnasium classic control
suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on
CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent
or any extensive hyperparameter tuning. We discuss the theoretical aspects of
how bootstrapped targets and non stationary visitation distributions violate
the independence assumptions encoded in TabPFN's prior, yet the model retains a
surprising generalization capacity. We further formalize the intrinsic context
size limit of in context RL algorithms and propose principled truncation
strategies that enable continual learning when the context is full. Our results
establish prior fitted networks such as TabPFN as a viable foundation for fast
and computationally efficient RL, opening new directions for gradient free RL
with large pre trained transformers.

</details>


### [43] [Online Omniprediction with Long-Term Constraints](https://arxiv.org/abs/2509.11357)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 该论文提出了在线全预测与长期约束的问题，通过单一预测集使下游代理在保证无遗憾的同时满足累积约束，并扩展到任意子序列的保证。


<details>
  <summary>Details</summary>
Motivation: 解决在自适应对抗环境下，多个下游代理需要基于预测选择行动，同时保证无遗憾和累积约束不违反的问题。不同代理的效用和约束函数可能完全不同，需要统一的预测方法。

Method: 设计单一预测集，使每个下游代理通过简单函数映射预测来选择行动，保证每个代理的遗憾为O(√T)且累积约束违反为O(1)。方法可扩展到任意上下文定义的相交子序列。

Result: 证明了该方法能够保证每个下游代理获得O(√T)的遗憾界和O(1)的累积约束违反界，同时在所有子序列上都能保持这些保证。

Conclusion: 提出的在线全预测框架能够有效解决多代理环境下的长期约束问题，为下游代理提供统一的预测服务，确保他们在各种子序列上的性能保证。

Abstract: We introduce and study the problem of online omniprediction with long-term
constraints. At each round, a forecaster is tasked with generating predictions
for an underlying (adaptively, adversarially chosen) state that are broadcast
to a collection of downstream agents, who must each choose an action. Each of
the downstream agents has both a utility function mapping actions and state to
utilities, and a vector-valued constraint function mapping actions and states
to vector-valued costs. The utility and constraint functions can arbitrarily
differ across downstream agents. Their goal is to choose actions that guarantee
themselves no regret while simultaneously guaranteeing that they do not
cumulatively violate the constraints across time. We show how to make a single
set of predictions so that each of the downstream agents can guarantee this by
acting as a simple function of the predictions, guaranteeing each of them
$\tilde{O}(\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We
also show how to extend our guarantees to arbitrary intersecting contextually
defined \emph{subsequences}, guaranteeing each agent both regret and constraint
violation bounds not just marginally, but simultaneously on each subsequence,
against a benchmark set of actions simultaneously tailored to each subsequence.

</details>


### [44] [Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures](https://arxiv.org/abs/2509.11367)
*Chang-Hwan Lee,Alexander Shim*

Main category: cs.LG

TL;DR: 提出基于编辑操作的度量方法来检测强化学习环境中的模型漂移，通过分析智能体行为序列的分布变化来识别非平稳环境中的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用（如医疗、机器人、金融）中的强化学习环境往往是非平稳的，传统RL方法假设环境动态是静止的，这在实际应用中可能导致性能下降。需要开发有效的方法来检测环境动态的变化。

Method: 引入一套基于编辑操作的度量方法，通过量化在静止条件和扰动条件下生成的状态-动作轨迹之间的偏差来检测漂移。这些度量方法分析智能体行为序列的分布变化。

Result: 实验表明，这些基于编辑操作的度量方法能够有效区分漂移和非漂移场景，即使在不同程度的噪声下也能提供可靠的漂移检测。

Conclusion: 该方法为在非平稳强化学习环境中进行漂移检测提供了一个实用工具，能够帮助智能体适应环境动态的变化。

Abstract: Reinforcement learning (RL) agents typically assume stationary environment
dynamics. Yet in real-world applications such as healthcare, robotics, and
finance, transition probabilities or reward functions may evolve, leading to
model drift. This paper proposes a novel framework to detect such drifts by
analyzing the distributional changes in sequences of agent behavior.
Specifically, we introduce a suite of edit operation-based measures to quantify
deviations between state-action trajectories generated under stationary and
perturbed conditions. Our experiments demonstrate that these measures can
effectively distinguish drifted from non-drifted scenarios, even under varying
levels of noise, providing a practical tool for drift detection in
non-stationary RL environments.

</details>


### [45] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: MillStone是第一个系统测量外部论点对LLM在争议问题上立场影响的基准测试，发现LLM在大多数问题上思想开放，权威信息源容易影响其立场。


<details>
  <summary>Details</summary>
Motivation: 随着用户开始依赖LLM获取包括争议性话题在内的各种信息，需要理解LLM输出中的立场和观点如何受到其使用的信息源文档的影响。

Method: 开发MillStone基准测试，应用于9个领先的LLM，测量它们对不同立场论点的开放程度、模型间一致性、最具说服力的论点等。

Result: LLM在大多数问题上思想开放，权威信息源容易改变LLM的立场，突显了信息源选择和LLM系统可能被操纵的风险。

Conclusion: LLM基于信息检索和搜索系统存在被操纵的风险，信息源选择至关重要。

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [46] [UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning](https://arxiv.org/abs/2509.11543)
*Zhengxi Lu,Jiabo Ye,Fei Tang,Yongliang Shen,Haiyang Xu,Ziwei Zheng,Weiming Lu,Ming Yan,Fei Huang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出了半在线强化学习新范式，在离线轨迹上模拟在线RL，通过补丁模块处理轨迹差异，引入折扣未来回报和加权优势优化策略，在GUI代理任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理中离线RL缺乏轨迹级奖励信号和在线RL奖励稀疏、部署成本高的根本困境

Method: 半在线强化学习：在离线轨迹上模拟在线rollout，使用补丁模块自适应恢复轨迹差异，引入折扣未来回报和加权步级/回合级优势优化策略

Result: 在四个动态基准测试中，7B模型达到SOTA性能，相比基线模型显著提升（AndroidWorld +12.0%，AITW +23.8%）

Conclusion: 半在线RL有效弥合了离线训练效率和在线多轮推理之间的差距，SOP指标能更好对齐真实在线性能

Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress
in automating complex user interface interactions through reinforcement
learning. However, current approaches face a fundamental dilemma: offline RL
enables stable training on pre-collected trajectories, but struggles with
multi-step task execution for lack of trajectory-level reward signals; online
RL captures these signals through environment interaction, but suffers from
sparse rewards and prohibitive deployment costs. To address it, we present
Semi-online Reinforcement Learning, a novel paradigm that simulates online RL
on offline trajectories. During each rollout process, we preserve the original
model output within the multi-turn dialogue, where a Patch Module adaptively
recovers the divergence between rollout and expert trajectories. To capture
long-term training signals, Semi-online RL introduces discounted future returns
into the reward computation and optimizes the policy with weighted step-level
and episode-level advantages. We further introduce Semi-Online Performance
(SOP), a metric that aligns better with true online performance, serving as a
practical and effective proxy for real-world evaluation. Experiments show that
ours Semi-online RL achieves SOTA performance among 7B models across four
dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on
AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging
the gap between offline training efficiency and online multi-turn reasoning.
The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

</details>


### [47] [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)
*Chentao Cao,Xiaojun Xu,Bo Han,Hang Li*

Main category: cs.LG

TL;DR: 提出Answer-Then-Check安全对齐方法，通过先思考回答再安全检查的机制增强LLM对越狱攻击的鲁棒性，构建ReSA数据集并在保持推理能力的同时显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，确保其抵御越狱攻击的安全性成为关键挑战，需要新的安全对齐方法来增强模型鲁棒性。

Method: 提出Answer-Then-Check方法，让模型先在思考中直接回答问题，然后批判性评估其安全性再决定是否输出。构建包含8万样本的ReSA数据集来训练模型进行安全推理。

Result: 方法达到了帕累托前沿，在保持MMLU、MATH500和HumanEval基准推理能力的同时显著提升安全性，降低过拒绝率，并能提供安全替代响应。仅用500样本即可达到与完整数据集相当的性能。

Conclusion: 该方法有效增强了LLM安全性，同时保持推理能力，且安全对齐可能比之前假设需要更少数据。

Abstract: As large language models (LLMs) continue to advance in capabilities, ensuring
their safety against jailbreak attacks remains a critical challenge. In this
paper, we introduce a novel safety alignment approach called Answer-Then-Check,
which enhances LLM robustness against malicious prompts by applying thinking
ability to mitigate jailbreaking problems before producing a final answer to
the user. Our method enables models to directly answer the question in their
thought and then critically evaluate its safety before deciding whether to
provide it. To implement this approach, we construct the Reasoned Safety
Alignment (ReSA) dataset, comprising 80K examples that teach models to reason
through direct responses and then analyze their safety. Experimental results
demonstrate that our approach achieves the Pareto frontier with superior safety
capability while decreasing over-refusal rates on over-refusal benchmarks.
Notably, the model fine-tuned with ReSA maintains general reasoning
capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our
method equips models with the ability to perform safe completion. Unlike
post-hoc methods that can only reject harmful queries, our model can provide
helpful and safe alternative responses for sensitive topics (e.g., self-harm).
Furthermore, we discover that training on a small subset of just 500 examples
can achieve comparable performance to using the full dataset, suggesting that
safety alignment may require less data than previously assumed.

</details>


### [48] [Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids](https://arxiv.org/abs/2509.12010)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 提出了一种新的逆强化学习方法，通过计算可行奖励集合的质心来选择平均策略，以解决IRL中多奖励函数解释相同行为的问题。


<details>
  <summary>Details</summary>
Motivation: 逆强化学习(IRL)存在固有的不适定性，多个奖励函数可以解释相同的专家行为。在新环境中，这些不同的奖励函数可能产生不同的策略，因此需要一种决策标准来选择要部署的策略。

Method: 提出了一种原则性标准，选择在可行奖励集合的有界子集中诱导出的"平均"策略。证明该策略可以通过计算该子集的奖励质心进行规划获得，并推导了闭式表达式。提出了一个仅使用专家演示离线数据集来估计该质心的有效算法。

Result: 通过数值模拟说明了专家行为与该方法产生行为之间的关系，证明了方法的有效性。

Conclusion: 该方法为解决IRL中的不适定性问题提供了一个原则性的解决方案，能够在新环境中产生与专家行为一致的策略。

Abstract: We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.

</details>


### [49] [Imitation Learning as Return Distribution Matching](https://arxiv.org/abs/2509.12026)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 该论文研究通过模仿学习训练风险敏感的强化学习代理，目标是匹配专家的期望回报和风险态度（回报分布特征），提出了基于Wasserstein距离的分布匹配方法和两种高效算法。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习只关注匹配专家的期望回报，但忽略了风险态度等回报分布的其他重要特征。需要开发能够同时匹配期望回报和风险敏感性的方法。

Method: 提出了风险敏感模仿学习的Wasserstein距离匹配框架，引入表达力足够的非马尔可夫策略子类，开发了RS-BC（未知转移模型）和RS-KT（已知转移模型）两种算法，并设计了奖励未知情况下的变体。

Result: RS-KT算法通过利用动态信息实现了比RS-BC更低的样本复杂度，数值模拟验证了非马尔可夫策略相比标准模仿学习算法的优势。

Conclusion: 提出的风险敏感模仿学习方法能够有效匹配专家的回报分布特征，非马尔可夫策略在样本效率方面表现出色，为风险敏感RL提供了新的解决方案。

Abstract: We study the problem of training a risk-sensitive reinforcement learning (RL)
agent through imitation learning (IL). Unlike standard IL, our goal is not only
to train an agent that matches the expert's expected return (i.e., its average
performance) but also its risk attitude (i.e., other features of the return
distribution, such as variance). We propose a general formulation of the
risk-sensitive IL problem in which the objective is to match the expert's
return distribution in Wasserstein distance. We focus on the tabular setting
and assume the expert's reward is known. After demonstrating the limited
expressivity of Markovian policies for this task, we introduce an efficient and
sufficiently expressive subclass of non-Markovian policies tailored to it.
Building on this subclass, we develop two provably efficient algorithms, RS-BC
and RS-KT, for solving the problem when the transition model is unknown and
known, respectively. We show that RS-KT achieves substantially lower sample
complexity than RS-BC by exploiting dynamics information. We further
demonstrate the sample efficiency of return distribution matching in the
setting where the expert's reward is unknown by designing an oracle-based
variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and
RS-BC with numerical simulations, highlighting both their sample efficiency and
the advantages of non-Markovian policies over standard sample-efficient IL
algorithms.

</details>


### [50] [$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.12117)
*Aryaman Reddi,Gabriele Tiboni,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: K-Level Policy Gradient (KPG) 是一种多智能体强化学习方法，通过递归更新智能体策略来应对其他智能体的同时更新，解决了传统方法中的协调问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体强化学习算法在更新策略时只考虑其他智能体的当前策略，而没有考虑它们在同一更新步骤中的策略变化，这会导致协调失败和性能下降。

Method: 提出了K-Level Policy Gradient (KPG)方法，递归地更新每个智能体相对于其他智能体更新后策略的策略，加速有效协调策略的发现。将KPG应用于MAPPO、MADDPG和FACMAC等深度MARL算法。

Result: 理论证明KPG在有限迭代次数下能够单调收敛到局部纳什均衡。在StarCraft II和多智能体MuJoCo实验中，KPG方法相比现有深度MARL算法表现出更优越的性能。

Conclusion: KPG方法通过考虑多智能体同时更新的协调问题，显著提升了深度多智能体强化学习的性能，为解决MARL中的协调挑战提供了有效解决方案。

Abstract: Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)
typically employ a policy update that responds to the current strategies of
other agents. While being straightforward, this approach does not account for
the updates of other agents at the same update step, resulting in
miscoordination. In this paper, we introduce the $K$-Level Policy Gradient
(KPG), a method that recursively updates each agent against the updated
policies of other agents, speeding up the discovery of effective coordinated
policies. We theoretically prove that KPG with finite iterates achieves
monotonic convergence to a local Nash equilibrium under certain conditions. We
provide principled implementations of KPG by applying it to the deep MARL
algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior
performance over existing deep MARL algorithms in StarCraft II and multi-agent
MuJoCo.

</details>
