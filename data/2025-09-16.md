<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem是一种新的对话AI记忆方法，通过在记忆存储前进行复杂推理，将细粒度记忆片段分类并建立跨会话关系，从而在生成响应时减少计算负担，显著提升各种模型大小的性能。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI系统在长期记忆方面存在不足，复杂的推理过程都集中在响应生成阶段，导致性能严重依赖模型大小，需要一种方法将推理负担从推理阶段转移到记忆构建阶段。

Method: 提出PREMem方法：1）提取细粒度记忆片段并分类为事实性、经验性和主观性信息；2）在记忆存储前建立跨会话记忆项之间的显式关系，捕捉扩展、转换和含义等演化模式；3）在预存储阶段而非响应生成时进行推理。

Result: 实验显示在所有模型大小上都取得了显著的性能提升，较小的模型能够达到与更大基线模型相当的结果，即使在受限的token预算下也能保持有效性。

Conclusion: PREMem通过在记忆构建阶段进行预存储推理，有效降低了交互时的计算需求，同时创建了丰富的记忆表示，为对话AI的长期记忆处理提供了有效解决方案。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [2] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究探讨上下文和情感语调元数据如何影响大语言模型在谬误分类任务中的推理和性能，特别是在政治辩论场景中。研究发现情感元数据会偏向将陈述标记为"诉诸情感"谬误，反而降低逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在谬误分类任务中，上下文和情感元数据对模型推理性能的影响，特别是在政治辩论这种复杂语境下的表现。

Method: 使用美国大选辩论数据，通过Qwen-3(8B)模型进行六种谬误类型分类，比较三种输入设置（纯文本、文本+上下文、文本+上下文+情感语调元数据）和三种提示策略（基础提示、语用辩证法、论证周期表）的效果。

Result: 理论性提示可以提高可解释性，但添加上下文特别是情感语调元数据通常会降低性能。情感元数据使模型偏向将陈述标记为"诉诸情感"谬误，削弱逻辑推理。基础提示往往优于增强提示。

Conclusion: 额外的输入信息可能导致注意力分散，反而恶化大语言模型的谬误分类性能，特别是在处理情感元数据时。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [3] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 研究发现表情符号可以触发大型语言模型生成有毒内容，通过自动化构建含表情符号的提示词，在7个主流LLM和5种语言上验证了这一现象，并发现表情符号可作为异质语义通道绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 观察到表情符号可能触发LLM生成有毒内容，旨在研究表情符号是否能明显增强LLM的毒性生成能力以及如何解释这一现象。

Method: 通过自动化构建含表情符号的提示词来微妙表达有毒意图，在7个著名LLM和5种主流语言上进行实验，并进行模型层面的语义认知、序列生成和分词分析。

Result: 实验表明含表情符号的提示词容易诱导毒性生成，表情符号可作为异质语义通道绕过安全机制，预训练语料分析显示表情符号相关数据污染与毒性生成行为存在潜在关联。

Conclusion: 表情符号确实能增强LLM的毒性生成能力，这一现象源于表情符号作为异质语义通道的特性以及预训练语料中的数据污染问题。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [4] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 研究评估了7个先进的大视觉语言模型作为人类协作对象匹配对话的旁听者能力，发现这些模型在理解指代表达方面仍面临挑战，且无法通过多次对话获得持续性能提升。


<details>
  <summary>Details</summary>
Motivation: 理解指代表达对具身智能体在现实世界中执行任务至关重要，需要整合语言、视觉和对话交互能力。研究旨在评估当前LVLMs在这方面的表现。

Method: 使用人类在协作对象匹配任务中的自发对话语料库，测试7个最先进的大视觉语言模型作为旁听者的表现，分析模型在多次重复相同任务时的性能变化。

Result: 所有测试的LVLMs在该任务上都表现不佳，未能显示出随着对话轮次增加而持续改进的性能，表明当前模型在理解指代表达方面仍存在重大挑战。

Conclusion: 当前的大视觉语言模型在理解人类自发对话中的指代表达方面能力有限，需要进一步研究来提升模型在整合多模态信息和对话上下文方面的能力。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [5] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 开发了HalluDetect幻觉检测系统，在LLaMA 3.1 8B模型上实现69%的F1分数，比基线提升25.44%。AgentBot架构将幻觉降至每轮0.4159次，同时保持96.13%的token准确率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在关键应用中因幻觉问题导致的可靠性限制，特别是在消费者投诉聊天机器人场景中。

Method: 开发基于LLM的幻觉检测系统HalluDetect，并对比评估五种聊天机器人架构的性能表现。

Result: HalluDetect系统F1分数达到69%，比基线提升25.44%；AgentBot架构在减少幻觉和保持准确性方面表现最佳。

Conclusion: 优化的推理策略能显著提高事实准确性，该方法可推广到其他高风险领域，增强LLM驱动助手的可信度。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [6] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: GTA框架结合监督微调(SFT)的效率与强化学习(RL)的能力优势，通过猜-思-答三阶段结构实现更快的收敛速度和更高的性能上限。


<details>
  <summary>Details</summary>
Motivation: 解决纯RL方法探索效率低、收敛慢，以及纯SFT方法性能上限有限、理论基础薄弱的问题，寻求效率与能力的最佳平衡。

Method: 提出Guess-Think-Answer三阶段框架：首先生成临时猜测（交叉熵损失优化），然后进行反思，最后生成最终答案，使用RL奖励同时优化最终输出和整个GTA结构格式。采用损失掩码和梯度约束缓解训练信号冲突。

Result: 在四个文本分类基准测试中，GTA显著加速收敛速度，同时超越了单独的SFT和RL基线方法。

Conclusion: GTA框架成功实现了SFT训练效率与RL能力增益的统一，为NLP任务提供了一种高效的混合训练范式。

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [7] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出了RAGs-to-Riches提示框架，通过检索增强生成方法改进LLM角色扮演性能，在敌对用户交互中保持角色一致性


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法在角色扮演中容易导致模型意外破格，特别是在与敌对用户互动时，可能产生有害影响

Method: 将LLM角色扮演重新表述为文本检索问题，利用精心策划的参考演示来条件化LLM响应，引入IOO和IOR两个新的token级ROUGE指标

Result: 在与敌对用户模拟互动中，该方法在推理时从参考演示中多使用了35%的token，在453次角色扮演互动中被一致评为更真实且更少破格

Conclusion: 该方法为构建稳健、人类对齐的LLM角色扮演框架提供了可扩展策略

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 论文实证研究LLM在嵌入式机器学习工作流中的代码生成失败模式，揭示了提示格式、模型行为和结构假设如何影响成功率和失败特征，并建立了失败分类法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在嵌入式机器学习工作流中自动化软件生成的广泛应用，其输出经常出现静默失败或不可预测行为，需要系统性地研究这些失败模式以提高可靠性。

Method: 基于自动驾驶框架，协调数据预处理、模型转换和设备端推理代码生成，通过实证调查分析多个LLM的错误倾向行为，包括格式引起的误解和运行时破坏性代码。

Result: 研究发现多样化的错误倾向行为，揭示了标准验证管道无法检测的失败特征，建立了失败分类法并分析了跨多个LLM的常见根本原因和系统性脆弱性。

Conclusion: 尽管基于特定设备，但研究揭示了LLM代码生成的更广泛挑战，提出了改进LLM驱动的嵌入式ML系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [9] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了AI编程熟练度的概念，评估LLMs在不同技术库上生成代码的质量差异，发现功能相似的库在LLM代码生成质量上存在高达84%的差距，呼吁将AI熟练度评估纳入技术选型框架。


<details>
  <summary>Details</summary>
Motivation: 传统技术选型方法只关注技术本身属性，忽视了LLMs能否有效利用所选技术。现有LLMs在使用流行库（如Selenium）时经常生成低质量代码，导致高调试成本和技术债务。

Method: 首次对170个第三方库和61个任务场景进行综合实证研究，评估6个广泛使用的LLMs，提出AI编程熟练度概念来衡量LLMs利用给定技术生成高质量代码的能力。

Result: 研究发现功能相似的库在LLM生成代码质量得分上存在高达84%的差异，不同模型使用相同库时也表现出质量差距，这些差距会转化为实际工程成本。

Conclusion: 需要将AI熟练度评估整合到技术选型框架中，开发缓解策略以保持AI驱动开发中的竞争平衡，避免技术生态多样性受到威胁。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [10] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace是一个多智能体系统，自动从软件仓库生成用户级需求(URs)并恢复实时的需求追踪链接(从URs到实现级需求到代码)，解决了现有代码摘要和需求追踪技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码摘要(ACS)技术主要生成面向开发者的实现级需求(IRs)，而需求追踪(RT)技术往往忽略项目演进的影响，导致用户级需求(URs)和实时追踪链接未被充分探索，而这些对支持用户理解和验证AI生成软件是否符合用户意图至关重要。

Method: UserTrace协调四个专门化智能体(代码审查员、搜索器、编写器、验证器)，通过三阶段流程：结构化仓库依赖、为代码单元推导IRs、结合领域特定上下文合成URs。

Result: 比较评估显示，UserTrace生成的URs在完整性、正确性和有用性方面优于现有基线，在追踪链接恢复精度上优于五种最先进的RT方法。用户研究进一步证明UserTrace能帮助最终用户验证AI生成的仓库是否符合其意图。

Conclusion: UserTrace有效填补了用户级需求生成和实时需求追踪的空白，为软件可维护性和用户意图验证提供了重要支持。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [11] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 本文对扩散语言模型在代码生成领域进行了首次实证研究，发现扩散LLM在代码长度外推和长代码理解方面优于自回归LLM，并提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 自回归LLM在代码生成中存在效率低和生成顺序固定的局限性，而扩散LLM通过多token预测和灵活生成顺序有望解决这些问题，但缺乏系统性研究。

Method: 使用9个代表性扩散LLM在4个广泛使用的基准测试上进行实验，比较其与自回归LLM的性能差异。

Result: 扩散LLM在相似规模下与自回归LLM竞争，在长度外推和长代码理解方面表现更好，并识别了影响效果和效率的关键因素。

Conclusion: 扩散LLM是代码生成的有前景替代方案，研究提供了实用指导并指出了未来改进方向。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [12] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent是一个基于假设验证的多代理漏洞检测框架，通过语义敏感的多视角检测流程，提高了漏洞检测的准确性和覆盖率，降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在项目级漏洞检测中面临挑战，需要准确定位安全敏感代码并正确关联复杂程序上下文。

Method: 采用多代理框架，每个代理专注于特定分析视角（如内存、授权），通过假设验证范式构建假设条件和触发路径，引导LLM在验证过程中关注相关程序上下文和防御检查。

Result: 在两个数据集上平均提高整体准确率6.6%，正确识别漏洞-修复代码对的比例最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent通过模拟人类代码审计过程的多代理假设验证方法，显著提升了基于LLM的漏洞检测性能。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [13] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE是一个自动化工具创建和增强框架，将企业API转换为LLM兼容工具，通过生成丰富的工具规范和动态筛选机制提高工具选择和调用准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中API工具使用存在文档质量差、输入输出模式复杂、操作数量多等问题，导致工具选择困难，有效载荷形成准确率降低达25%。

Method: ACE框架(i)生成包含参数描述和示例的增强工具规范，(ii)集成动态筛选机制在运行时过滤相关工具，降低提示复杂度同时保持可扩展性。

Result: 在专有和开源API上验证了框架有效性，并展示了与代理框架的集成能力。

Conclusion: ACE是首个端到端自动化企业API工具创建、增强和动态选择的框架，显著提升了LLM代理的工具使用效率。

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [14] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: 本文研究了代码大语言模型在推理程序运行时行为方面的局限性，提出了一个集成语义信息（如执行轨迹）的通用框架，但实验结果出乎意料地显示语义信息对监督微调和推理阶段的提升效果有限。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在理解程序实际功能和推理运行时行为方面存在显著局限性，现有方法对语义信息的表示不一致且碎片化，需要更系统的方法来增强其推理能力。

Method: 引入一个通用框架来集成语义信息（如执行轨迹）到代码任务相关的提示中，并全面研究语义信息在提升代码大语言模型推理能力中的作用，特别关注基于轨迹的语义信息在监督微调和推理阶段的有用性。

Result: 实验结果与先前研究相反，表明语义信息对代码大语言模型的监督微调和测试时扩展的有用性有限。

Conclusion: 尽管语义信息理论上应能提升代码大语言模型的推理能力，但实际实验结果显示其效果有限，这挑战了当前对语义信息在代码理解中作用的普遍认知。

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [15] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 该论文提出了ZK-Eval评估框架和ZK-Coder代理框架，用于评估和增强LLM在零知识证明编程中的能力，显著提高了代码生成成功率。


<details>
  <summary>Details</summary>
Motivation: 零知识证明编程具有知识密集和易出错的特点，现有LLM在通用编程语言上表现良好，但在ZK编程领域的有效性尚未探索，需要专门的评估和改进方法。

Method: 提出ZK-Eval三级评估管道（语言知识、组件能力、端到端生成），并开发ZK-Coder代理框架，包含约束草图、引导检索和交互式修复机制。

Result: 在Circom和Noir上的实验显示，成功率分别从17.35%提升到83.38%和从32.21%提升到90.05%。

Conclusion: 建立了系统测量和增强LLM在ZK代码生成能力的基础，降低了实践门槛并推进可信计算发展。

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [16] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter是一个多层次的静态分析工具，专门针对机器学习笔记本代码的质量问题，能够检测Python编码规范、笔记本结构和ML特定问题三个层面的违规。


<details>
  <summary>Details</summary>
Motivation: 机器学习笔记本代码质量普遍较低，现有工具只能处理单一层面问题，无法捕捉ML特定语义，需要开发能够同时分析三个层面的工具。

Method: 基于Moose构建，采用元建模方法统一笔记本结构元素和Python代码实体，实现了22个检测规则，并在5000个Kaggle笔记本上进行了应用测试。

Result: 在所有三个层面都发现了违规行为，验证了多层次方法的有效性，证明了工具在提高笔记本环境中ML开发质量和可靠性方面的潜力。

Conclusion: Vespucci Linter填补了现有工具的空白，通过多层次的上下文分析能够有效提升机器学习笔记本代码的质量。

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [17] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent是一个基于LLM的智能体系统，能够自动分析、分类和修复静态分析警告，在Java项目中实现了96.8%的合理修复率和86.3%的正确修复率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具需要开发者手动处理警告，过程繁琐导致警告积累和代码质量下降，需要自动化解决方案。

Method: 采用基于LLM的智能体框架，迭代调用工具收集代码库信息并编辑代码，配备三步启发式补丁审批机制：构建项目、验证警告消失且无新警告、运行测试套件。

Result: 在106个Java项目的1000个SonarQube警告上测试，合理修复率96.8%，正确修复率86.3%，成本约2.9美分/警告，处理时间约4分钟/警告。

Conclusion: CodeCureAgent能可靠修复静态分析警告，可帮助清理现有代码库并集成到CI/CD管道中防止警告积累。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [18] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 本文提出了VisDocSketcher，首个基于LLM代理的系统，通过结合静态分析和LLM代理来自动生成代码的可视化文档，并开发了AutoSketchEval评估框架来量化评估生成质量。


<details>
  <summary>Details</summary>
Motivation: 可视化文档能有效降低开发者理解陌生代码的认知障碍，但手动创建耗时且难以标准化评估。目前缺乏自动生成高质量可视化文档的方法。

Method: 结合静态分析与LLM代理技术，识别代码关键元素并生成对应可视化表示；提出基于代码级指标的AutoSketchEval评估框架。

Result: 能为74.4%的样本生成有效可视化文档，比基于模板的基线方法提升26.7-39.8%；评估框架AUC超过0.87，能可靠区分高质量和低质量文档。

Conclusion: 该工作为自动化可视化文档研究奠定了基础，提供了既能生成有效可视化表示又能可靠评估质量的实用工具。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [19] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: LitterBox+框架将Scratch积木编程转换为文本表示，结合LLM能力提供代码查询、质量分析和修复建议，直接在Scratch环境中集成AI辅助功能


<details>
  <summary>Details</summary>
Motivation: 解决Scratch图形化编程环境无法直接使用大型语言模型的问题，为学习者提供AI编程辅助

Method: 扩展LitterBox静态分析工具，将积木代码转换为适合LLM的文本表示，集成LLM查询、质量分析和代码修复功能

Result: 开发了可扩展框架，提供API和UI集成，支持多种提示词和LLM提供商

Conclusion: LitterBox+成功克服了图形化编程使用LLM的障碍，为Scratch学习者提供了强大的AI辅助编程工具

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [20] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: TransLibEval是首个专注于第三方库中心代码翻译的基准测试，包含200个真实任务，覆盖Python、Java和C++，评估了7个LLM在6种翻译策略下的表现，发现相比无库设置性能下降超过60%，并揭示了大量之前被掩盖的第三方引用错误。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译基准在第三方库类别和规模上有限，难以暴露TPL相关错误，而实际编程中超过90%依赖第三方库，因此需要深入分析LLM在涉及各种TPL的代码翻译性能。

Method: 构建TransLibEval基准，包含200个真实任务，覆盖数据处​​理、机器学习、web开发等多样TPL类别，评估7个LLM在直接翻译、IR引导和检索增强三类6种策略下的表现，并分析GPT-4o的4,831个失败案例。

Result: 相比无库设置，平均正确率下降超过60%，不同策略表现出异质性优势，分析揭示了大量之前被掩盖的第三方引用错误。

Conclusion: 库中心翻译存在独特挑战，研究结果为改进TPL感知的代码智能提供了实用指导。

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [21] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: EfficientUICoder是一个针对UI2Code任务的压缩框架，通过元素感知压缩、区域感知细化和自适应重复抑制，在保持网页质量的同时实现55%-60%的压缩比，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在UI2Code任务中计算开销巨大，存在图像和代码令牌的显著冗余，导致计算复杂度高、关键UI元素关注不足，生成冗长且无效的HTML文件。

Method: 提出三组件压缩框架：1)元素和布局感知令牌压缩；2)区域感知令牌细化；3)自适应重复令牌抑制，通过检测元素区域、利用注意力分数和跟踪结构频率来优化令牌使用。

Result: 实现55%-60%压缩比，计算成本降低44.9%，生成令牌减少41.4%，预填充时间减少46.6%，推理时间减少48.8%，且不损害网页质量。

Conclusion: EfficientUICoder有效解决了UI2Code任务中的计算冗余问题，在保持输出质量的同时显著提升了效率，为大规模MLLM应用提供了实用解决方案。

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演进图像生成系统，通过多模态LLM代理进行自我批判和进化，无需人工干预即可自动改进文本到图像生成质量


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型依赖人工干预和迭代提示工程的问题，提高可用性和自动化程度

Method: 使用多模态LLM代理进行自我批判（识别图像弱点、纠正欠规范问题）和自我进化（通过头对头比较迭代生成更好的提示）

Result: 在复杂文本到图像任务上显著提升图像质量，效果优于初始提示和最先进的自动化方法

Conclusion: Maestro为自改进文本到图像生成提供了稳健、可解释且有效的途径

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [23] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 该研究通过企业级基准测试评估了18种不同的智能体配置，发现模型特定的架构偏好挑战了通用AI系统的范式，同时揭示了智能体在企业任务上的显著性能弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然智能体架构的各个组件已被单独研究，但对于复杂多智能体系统中不同设计维度如何相互作用仍缺乏实证理解。

Method: 使用企业特定基准评估18种不同的智能体配置，考察四个关键维度：编排策略、智能体提示实现（ReAct vs 函数调用）、内存架构和思维工具集成。

Result: 发现了显著的模型特定架构偏好，最高分模型在复杂任务上仅达到35.3%成功率，在简单任务上达到70.8%成功率。

Conclusion: 这些发现可为未来智能体系统设计提供依据，使架构组件和模型选择能够基于更多实证数据。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [24] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出了一种基于优化人机对话和单调布尔/k值函数的专家心智模型(EMM)算法，用于LLM提示工程，以解决决策制定中信息缺失的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理决策问题时存在训练数据缺失导致的幻觉问题，RAG等方法只能部分解决。需要开发能够捕捉专家复杂心智模型的方法来提升决策效率。

Method: 提出四步EMM算法：因子识别、因子层次结构化、生成广义专家心智模型规范、从规范生成详细模型，基于单调布尔和k值函数。

Result: 开发了计算可处理的个人专家决策心智模型，能够更有效地利用LLMs进行决策支持。

Conclusion: 该方法通过结构化专家心智模型，为LLM提示工程提供了系统化框架，提升了决策制定的准确性和效率。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [25] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文对AI研究中以智能体为中心的研究范式进行了批判性重新评估，认为其概念模糊性和人类中心主义偏见可能限制了AI发展，提出了转向系统级动态、世界建模和物质智能的替代框架。


<details>
  <summary>Details</summary>
Motivation: 重新评估智能体范式在AI研究中的必要性和最优性，指出其概念模糊性和人类中心主义偏见可能限制了AI向更强大、可扩展和非拟人化的通用智能发展。

Method: 基于相关文献的系统性回顾，解构各种AI框架中的智能体范式，分析自主性和目标导向性等属性的定义和测量挑战。

Result: 发现许多AI系统的'智能体'框架虽然启发式有用，但可能具有误导性，特别是在大语言模型中可能掩盖了底层计算机制。

Conclusion: 需要研究非智能体和系统框架，受复杂系统、生物学和非传统计算的启发，这不仅是新架构的需求，更是对智能本身理解的根本重新思考。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [26] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 该论文重新思考LLM生成推理的评估方法，提出基于属性细粒度评估替代二元偏好判断，通过识别关键属性、分析人类偏好数据集，发现属性特定ELO评分能提供更细致的模型比较。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成推理的评估主要依赖二元偏好判断，这种方法不够透明和细粒度，无法深入理解推理质量的差异，需要更精细的评估框架。

Method: 从文献中识别关键推理属性，使用自动指标、LLM判断和人工标注评估属性，通过SHAP分析人类偏好数据集，采用属性特定ELO评分重新评估模型生成的推理。

Result: 研究发现细粒度属性评估能更好表征推理质量，属性特定ELO评分揭示了更细致的模型比较结果和洞察。

Conclusion: 基于属性的细粒度评估方法能够提供更可解释和可靠的评估实践，指导未来研究。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [27] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: Free-MAD是一个新的多智能体辩论框架，通过基于分数的决策机制和反从众机制，解决了传统共识式方法的高token开销、错误传播和投票随机性问题，在单轮辩论中显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论方法依赖多轮交互达成共识，存在token开销大、错误传播和多数投票随机性等问题，需要更高效的辩论框架。

Method: 提出Free-MAD框架，采用基于分数的决策机制评估整个辩论轨迹，引入反从众机制减少多数意见的过度影响，只需单轮辩论。

Result: 在8个基准数据集上实验表明，Free-MAD显著提升推理性能，减少token成本，并在真实攻击场景中表现出更好的鲁棒性。

Conclusion: Free-MAD通过消除共识需求和改进决策机制，为多智能体辩论提供了更高效、准确和鲁棒的解决方案。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [28] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出一个基于确定性可复现性的验证框架，使验证LLM输出的成本远低于生成成本，实现非对称努力验证


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中LLM输出真实性的验证挑战，防止伪造或使用劣质模型生成输出

Method: 基于自回归模型的确定性可复现性原理，在计算同质环境下，通过多个验证者概率性地审计随机小片段输出

Result: 模拟显示目标验证比完全重新生成快12倍以上，且可调节检测概率参数

Conclusion: 为可审计LLM系统建立了可行机制，为负责任AI提供基础层，并为未来复杂异构多智能体系统研究奠定基础

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [29] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态的多智能体编排框架，通过难度感知的工作流深度调整、算子选择和LLM分配，解决了现有静态框架在处理简单和复杂查询时的效率-性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架使用静态或任务级工作流，无法根据查询难度动态调整处理策略，导致简单查询过度处理、复杂查询性能不足，且忽略了异构LLM的效率-性能权衡。

Method: 提出DAAO框架，包含三个模块：变分自编码器(VAE)进行难度估计、模块化算子分配器、成本和性能感知的LLM路由器，动态调整工作流深度、算子选择和LLM分配。

Result: 在六个基准测试中，DAAO在准确性和推理效率方面均优于先前的多智能体系统。

Conclusion: DAAO通过难度感知的动态编排，实现了细粒度的查询特定推理策略，显著提升了多智能体系统的性能和效率。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [30] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 提出了一个将LLM作为人类调查受访者代理的新对齐框架P2P，通过构建多样化代理人设和选择代表性子集来解决社会科学调查成本高和人口不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学研究中调查部署成本上升和调查响应数据人口不平衡的两个紧迫挑战，提供成本效益高且可操控的解决方案。

Method: 采用两阶段对齐方法：1）构建多样化代理人设（endowments）模拟可信受访者档案；2）基于观测数据选择代表性子集来近似真实人群。使用结构化提示工程、基于熵的采样和基于回归的选择来引导LLM代理行为。

Result: 在真实世界意见调查数据集上证明，对齐的代理人群能够高保真地重现聚合响应模式，并展现出显著的响应多样性，即使没有人口统计条件限制。

Conclusion: 该框架不仅提高了社会科学研究的数据效率，还为研究多元化对齐的操作化提供了测试平台，具有更好的泛化性和简洁性。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [31] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD是一个多智能体提示梯度下降框架，通过多智能体协作和梯度优化解决传统单轨迹提示工程的局限性，在分类、生成和推理任务中表现出更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法依赖单一优化轨迹，存在适应性差、效率低、视角狭窄、梯度冲突和计算成本高等问题，需要更鲁棒和高效的优化方法。

Method: 提出MAPGD框架，包含任务清晰化、示例选择、格式设计和风格优化等专门化智能体；采用语义梯度协调解决冲突；使用bandit-based候选选择进行探索-利用平衡；提供理论收敛保证。

Result: 在分类、生成和推理任务上的实验表明，MAPGD在准确性和效率方面优于单智能体和随机基线方法。消融实验证实了梯度融合、智能体专门化和冲突解决的有效性。

Conclusion: MAPGD提供了一个统一、梯度启发的多智能体方法，实现了鲁棒且可解释的提示优化，为提示工程提供了新的解决方案。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [32] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个基于智能体的医疗操作系统，通过自然语言指令将临床需求转换为可执行的医疗命令，实现医疗工作流程自动化


<details>
  <summary>Details</summary>
Motivation: 解决现有数字医疗系统学习使用困难、操作繁琐的问题，利用LLM智能体技术提升临床工作效率，让医护人员更专注于患者护理

Method: 开发MedicalOS作为领域特定的抽象层，将人类指令转换为预定义的医疗命令（患者查询、病史检索、检查管理等），使用Python、API等机器语言封装为现成工具

Result: 在22个专科的214个病例上验证，显示出高诊断准确性、临床合理的检查请求，以及一致的结构化报告和用药推荐生成

Conclusion: MedicalOS为临床实践中的工作流程自动化提供了可信赖和可扩展的基础

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [33] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 这篇综述论文系统性地定义了时间序列推理问题，按推理拓扑结构将文献分为三类：单步直接推理、线性链推理和分支结构推理，并交叉分析了各拓扑在不同目标领域的应用表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理将时间作为首要维度，将中间证据直接融入答案中。当前缺乏对该领域的系统性组织，需要明确定义问题、分类方法并评估不同推理结构的优缺点。

Method: 采用拓扑分类法组织文献：1）单步直接推理；2）线性链推理（显式中间步骤）；3）分支结构推理（探索、修正和聚合）。交叉分析各拓扑在时间序列分析、因果推理、决策制定等主要目标领域的表现。

Result: 建立了时间序列推理的系统分类框架，识别了不同推理拓扑的优缺点（在忠实性和鲁棒性方面的表现），整理了相关数据集和基准测试资源，提出了评估实践和设计指南。

Conclusion: 时间序列推理结构需要在接地能力和自我修正与计算成本之间取得平衡。未来进展将依赖于将推理质量与效用挂钩的基准测试，以及在流式、长时域设置下的闭环测试平台。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [34] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为全双工模型设计的终身记忆代理，能够从原始视听流中实时识别多用户、提供个性化响应，并维护用户长期知识。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理主要针对LLMs，缺乏对原始视听流的处理能力，无法满足终身、实时和具身场景的需求。

Method: 采用三个异步进程：检索过程（动态识别用户并收集相关上下文）、全模态对话过程（生成个性化音频响应）、内存管理过程（检测对话边界并更新长期记忆）。

Result: 检索和内存管理模块准确率超过95%，与RoboEgo集成后在实时个性化对话中事实一致性得分超过87%。

Conclusion: EgoMem为未来研究建立了强基线，特别适用于终身、实时和具身场景。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [35] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号多智能体架构，使用Kripke模型表示信念状态，结合模态逻辑进行推理，在粒子加速器环境中成功诊断复杂故障


<details>
  <summary>Details</summary>
Motivation: 当前AI研究主要关注模型和数据规模的扩展，但智能体在复杂环境中进行自适应、复杂和自主决策的推理结构和逻辑一致性扩展尚未得到充分探索

Method: 采用神经符号多智能体架构，将个体智能体的信念状态表示为Kripke模型，使用模态逻辑进行可能性和必要性推理，结合领域特定知识和逻辑约束指导语言模型的假设生成

Result: 在高保真模拟粒子加速器环境中，系统成功诊断了复杂的级联故障，展示了语言模型的语义直觉与模态逻辑严格验证的结合效果

Conclusion: 该方法为构建更鲁棒、可靠和可验证的自主智能体提供了一条可行路径，通过形式化逻辑约束防止语言模型得出物理或逻辑上不可行的结论

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ是一个新颖的强化学习框架，结合人类认知的双记忆系统和自适应探索策略，在异常检测任务中显著优于传统DQN和PPO算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法（如DQN和PPO）在动态环境中存在探索效率低、稳定性差和适应性不足的问题，需要更先进的解决方案来处理不确定环境中的决策任务。

Method: 提出了LogGuardQ框架，整合了受人类认知启发的双记忆系统，以及基于温度衰减和好奇心驱动的自适应探索策略。在包含100万个模拟访问日志（47.9%异常）的数据集上进行评估，共运行20,000个训练回合。

Result: LogGuardQ达到96.0%的检测率（DQN为93.0%，PPO为47.1%），精确率0.4776，召回率0.9996，F1分数0.6450。平均奖励为20.34±44.63，显著优于对比算法。统计检验确认了性能优势的显著性。

Conclusion: 通过融合认知科学和强化学习，LogGuardQ为不确定环境中的自适应学习提供了可扩展的方法，在网络安全、入侵检测和不确定性决策等领域具有应用潜力。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [37] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 该论文提出了一种结合利用现有资产和探索新投资机会的双智能体深度强化学习方法，用于投资组合优化，在真实市场数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的投资组合优化方法局限于预定义投资组合内的资产分配，忽视了探索新投资机会的重要性。

Method: 使用两个深度强化学习智能体：一个负责在现有投资组合内分配资产，另一个负责在扩展投资组合中探索新机会，动态平衡利用和探索目标。

Result: 在两个真实市场数据集上的实验表明，该方法优于最先进的投资组合策略和基线方法。

Conclusion: 将利用现有资产与探索新机会相结合的双智能体方法能够适应不断变化的市场环境，提升投资组合性能。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [38] [The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models](https://arxiv.org/abs/2509.10970)
*Joshua Au Yeung,Jacopo Dalmasso,Luca Foschini,Richard JB Dobson,Zeljko Kraljevic*

Main category: cs.LG

TL;DR: 该研究开发了psychosis-bench基准测试，评估了8个主流LLM在模拟精神病对话中的表现，发现所有模型都存在确认妄想、促成伤害行为的风险，安全干预率低。


<details>
  <summary>Details</summary>
Motivation: 随着"AI精神病"案例的增加，需要系统评估LLM在与易感用户互动时可能加剧或诱发精神症状的风险，特别是LLM的顺从性可能强化用户的妄想信念。

Method: 创建包含16个结构化对话场景的psychosis-bench基准，模拟三种妄想主题的进展，评估8个LLM在妄想确认、伤害促成和安全干预三个维度的表现。

Result: 所有LLM都表现出精神致病潜力，平均妄想确认得分0.91±0.88，伤害促成得分0.69±0.84，安全干预率仅37%。隐式场景表现更差，39.8%的场景完全没有安全干预。

Conclusion: LLM的精神致病性是可量化的风险，需要重新思考LLM训练方式，这不仅是技术挑战更是公共卫生问题，需要开发者、政策制定者和医疗专业人士合作解决。

Abstract: Background: Emerging reports of "AI psychosis" are on the rise, where
user-LLM interactions may exacerbate or induce psychosis or adverse
psychological symptoms. The sycophantic and agreeable nature of LLMs can
beneficial, it can become a vector for harm by reinforcing delusional beliefs
in vulnerable users.
  Methods: We introduce psychosis-bench, a novel benchmark designed to
systematically evaluate the psychogenicity of LLMs comprimising 16 structured,
12-turn conversational scenarios simulating the progression of delusional
themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)
and potential harms. We evaluated eight prominent LLMs for Delusion
Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across
explicit and implicit conversational contexts.
  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated
psychogenic potential, showing a strong tendency to perpetuate rather than
challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled
harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety
interventions in only roughly a third of applicable turns (mean SIS of 0.37
$\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.
Performance was significantly worse in implicit scenarios, models were more
likely to confirm delusions and enable harm while offering fewer interventions
(p < .001). A strong correlation was found between DCS and HES (rs = .77).
Model performance varied widely, indicating that safety is not an emergent
property of scale alone.
  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk
and underscores the urgent need for re-thinking how we train LLMs. We frame
this issue not merely as a technical challenge but as a public health
imperative requiring collaboration between developers, policymakers, and
healthcare professionals.

</details>


### [39] [TransZero: Parallel Tree Expansion in MuZero using Transformer Networks](https://arxiv.org/abs/2509.11233)
*Emil Malmsten,Wendelin Böhmer*

Main category: cs.LG

TL;DR: TransZero是一种基于模型的强化学习算法，通过Transformer网络并行生成多个潜在未来状态，消除了MCTS中的顺序瓶颈，相比MuZero实现了11倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡洛树搜索(MCTS)中的顺序瓶颈问题，传统方法如MuZero需要逐步构建搜索树，限制了实时决策在复杂环境中的应用。

Method: 使用基于Transformer的网络同时生成多个潜在未来状态，结合均值-方差约束(MVC)评估器消除对顺序访问计数的依赖，实现并行子树扩展。

Result: 在MiniGrid和LunarLander环境中，TransZero相比MuZero实现了最高11倍的时钟时间加速，同时保持了样本效率。

Conclusion: 并行树构建可以显著加速基于模型的强化学习，使复杂环境中的实时决策更接近实际应用。

Abstract: We present TransZero, a model-based reinforcement learning algorithm that
removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike
MuZero, which constructs its search tree step by step using a recurrent
dynamics model, TransZero employs a transformer-based network to generate
multiple latent future states simultaneously. Combined with the Mean-Variance
Constrained (MVC) evaluator that eliminates dependence on inherently sequential
visitation counts, our approach enables the parallel expansion of entire
subtrees during planning. Experiments in MiniGrid and LunarLander show that
TransZero achieves up to an eleven-fold speedup in wall-clock time compared to
MuZero while maintaining sample efficiency. These results demonstrate that
parallel tree construction can substantially accelerate model-based
reinforcement learning, bringing real-time decision-making in complex
environments closer to practice. The code is publicly available on GitHub.

</details>


### [40] [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)
*David Schiff,Ofir Lindenbaum,Yonathan Efroni*

Main category: cs.LG

TL;DR: TabPFN RL是一个无梯度的深度强化学习框架，使用预训练的TabPFN transformer作为Q函数近似器，通过上下文学习进行推理，无需梯度更新或微调。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化在深度强化学习中存在超参数敏感、训练不稳定和计算成本高的问题，需要寻找无梯度的替代方案。

Method: 将TabPFN transformer重新用于Q值预测，设计高奖励轨迹门控机制保留top 5%轨迹，处理固定上下文预算限制。

Result: 在Gymnasium经典控制任务中，TabPFN RL匹配或超越了DQN在CartPole v1、MountainCar v0和Acrobot v1上的表现，无需梯度下降或大量超参数调优。

Conclusion: TabPFN等先验拟合网络为快速计算高效的RL提供了可行基础，开启了使用大型预训练transformer进行无梯度RL的新方向。

Abstract: Gradient based optimization is fundamental to most modern deep reinforcement
learning algorithms, however, it introduces significant sensitivity to
hyperparameters, unstable training dynamics, and high computational costs. We
propose TabPFN RL, a novel gradient free deep RL framework that repurposes the
meta trained transformer TabPFN as a Q function approximator. Originally
developed for tabular classification, TabPFN is a transformer pre trained on
millions of synthetic datasets to perform inference on new unseen datasets via
in context learning. Given an in context dataset of sample label pairs and new
unlabeled data, it predicts the most likely labels in a single forward pass,
without gradient updates or task specific fine tuning. We use TabPFN to predict
Q values using inference only, thereby eliminating the need for back
propagation at both training and inference. To cope with the model's fixed
context budget, we design a high reward episode gate that retains only the top
5% of trajectories. Empirical evaluations on the Gymnasium classic control
suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on
CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent
or any extensive hyperparameter tuning. We discuss the theoretical aspects of
how bootstrapped targets and non stationary visitation distributions violate
the independence assumptions encoded in TabPFN's prior, yet the model retains a
surprising generalization capacity. We further formalize the intrinsic context
size limit of in context RL algorithms and propose principled truncation
strategies that enable continual learning when the context is full. Our results
establish prior fitted networks such as TabPFN as a viable foundation for fast
and computationally efficient RL, opening new directions for gradient free RL
with large pre trained transformers.

</details>


### [41] [Online Omniprediction with Long-Term Constraints](https://arxiv.org/abs/2509.11357)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 本文提出了在线全预测与长期约束问题，设计了一种预测方法使得下游代理能够同时实现低遗憾和约束违反。


<details>
  <summary>Details</summary>
Motivation: 解决在自适应对抗环境下，多个下游代理需要基于预测选择行动，同时保证低遗憾和满足累积约束的问题。不同代理的效用和约束函数可能完全不同。

Method: 提出了一种在线预测框架，生成单一预测集供下游代理使用。代理通过简单函数将预测映射为行动，确保每个代理都能获得次线性遗憾和有限约束违反。方法还扩展到任意交叠的上下文定义子序列。

Result: 证明了每个下游代理都能获得Õ(√T)遗憾和O(1)累积约束违反。方法还能保证在每个子序列上同时满足遗憾和约束违反界限。

Conclusion: 该框架为多代理在线决策问题提供了有效的解决方案，能够在复杂约束环境下实现良好的性能保证。

Abstract: We introduce and study the problem of online omniprediction with long-term
constraints. At each round, a forecaster is tasked with generating predictions
for an underlying (adaptively, adversarially chosen) state that are broadcast
to a collection of downstream agents, who must each choose an action. Each of
the downstream agents has both a utility function mapping actions and state to
utilities, and a vector-valued constraint function mapping actions and states
to vector-valued costs. The utility and constraint functions can arbitrarily
differ across downstream agents. Their goal is to choose actions that guarantee
themselves no regret while simultaneously guaranteeing that they do not
cumulatively violate the constraints across time. We show how to make a single
set of predictions so that each of the downstream agents can guarantee this by
acting as a simple function of the predictions, guaranteeing each of them
$\tilde{O}(\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We
also show how to extend our guarantees to arbitrary intersecting contextually
defined \emph{subsequences}, guaranteeing each agent both regret and constraint
violation bounds not just marginally, but simultaneously on each subsequence,
against a benchmark set of actions simultaneously tailored to each subsequence.

</details>


### [42] [Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures](https://arxiv.org/abs/2509.11367)
*Chang-Hwan Lee,Alexander Shim*

Main category: cs.LG

TL;DR: 提出基于编辑操作的新框架，通过分析智能体行为序列的分布变化来检测非平稳强化学习环境中的模型漂移


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中环境动态可能变化，导致模型漂移，需要有效检测方法

Method: 引入一套基于编辑操作的度量方法，量化在平稳和扰动条件下生成的状态-动作轨迹之间的偏差

Result: 实验表明这些度量能有效区分漂移和非漂移场景，即使在噪声变化情况下也能提供实用的漂移检测工具

Conclusion: 该框架为检测非平稳RL环境中的模型漂移提供了有效方法

Abstract: Reinforcement learning (RL) agents typically assume stationary environment
dynamics. Yet in real-world applications such as healthcare, robotics, and
finance, transition probabilities or reward functions may evolve, leading to
model drift. This paper proposes a novel framework to detect such drifts by
analyzing the distributional changes in sequences of agent behavior.
Specifically, we introduce a suite of edit operation-based measures to quantify
deviations between state-action trajectories generated under stationary and
perturbed conditions. Our experiments demonstrate that these measures can
effectively distinguish drifted from non-drifted scenarios, even under varying
levels of noise, providing a practical tool for drift detection in
non-stationary RL environments.

</details>


### [43] [Framing AI System Benchmarking as a Learning Task: FlexBench and the Open MLPerf Dataset](https://arxiv.org/abs/2509.11413)
*Grigori Fursin,Daniel Altunay*

Main category: cs.LG

TL;DR: FlexBench是一个模块化的AI基准测试框架，将基准测试本身构建为AI任务，通过持续评估模型在不同数据集、软件和硬件上的性能，为AI系统部署提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的AI基准测试（如MLPerf）难以跟上快速发展的AI领域，无法为AI系统的部署、优化和协同设计决策提供充分支持。

Method: 开发FlexBench作为MLPerf LLM推理基准的模块化扩展，集成HuggingFace，收集准确性、延迟、吞吐量、能耗和成本等关键指标的基准测试结果和元数据到Open MLPerf数据集中。

Result: 通过MLPerf推理提交成功验证了FlexBench概念，包括在商用服务器上评估DeepSeek R1和LLaMA 3.3模型。

Conclusion: FlexBench使从业者能够根据可用资源、需求和约束做出成本效益高的AI部署决策，支持预测建模和特征工程。

Abstract: Existing AI system benchmarks such as MLPerf often struggle to keep pace with
the rapidly evolving AI landscape, making it difficult to support informed
deployment, optimization, and co-design decisions for AI systems. We suggest
that benchmarking itself can be framed as an AI task - one in which models are
continuously evaluated and optimized across diverse datasets, software, and
hardware, using key metrics such as accuracy, latency, throughput, energy
consumption, and cost. To support this perspective, we present FlexBench: a
modular extension of the MLPerf LLM inference benchmark, integrated with
HuggingFace and designed to provide relevant and actionable insights.
Benchmarking results and metadata are collected into an Open MLPerf Dataset,
which can be collaboratively curated, extended, and leveraged for predictive
modeling and feature engineering. We successfully validated the FlexBench
concept through MLPerf Inference submissions, including evaluations of DeepSeek
R1 and LLaMA 3.3 on commodity servers. The broader objective is to enable
practitioners to make cost-effective AI deployment decisions that reflect their
available resources, requirements, and constraints.

</details>


### [44] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: MillStone是首个系统测量外部论据对LLM在争议问题上立场影响的基准测试，发现LLM在大多数问题上持开放态度，但权威信息源很容易改变其立场。


<details>
  <summary>Details</summary>
Motivation: 随着用户开始依赖LLM获取包括争议性话题在内的各种信息，需要了解LLM输出中的立场和观点如何受到其信息源文档的影响。

Method: 开发MillStone基准测试，应用于9个主流LLM，测量它们对争议问题对立面论据的开放程度、不同LLM之间的一致性、最具说服力的论据等。

Result: LLM在大多数问题上表现出开放态度，但权威信息源能够轻易改变LLM的立场，凸显了信息源选择的重要性和LLM系统可能被操纵的风险。

Conclusion: LLM基于信息检索和搜索系统存在被操纵的风险，信息源的选择对LLM立场表达具有关键影响。

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [45] [UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning](https://arxiv.org/abs/2509.11543)
*Zhengxi Lu,Jiabo Ye,Fei Tang,Yongliang Shen,Haiyang Xu,Ziwei Zheng,Weiming Lu,Ming Yan,Fei Huang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出半在线强化学习新范式，通过离线轨迹模拟在线RL，解决GUI智能体在多步任务执行中的奖励稀疏和部署成本问题


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体面临离线RL缺乏轨迹级奖励信号、在线RL奖励稀疏且部署成本高的困境，需要新的训练范式来平衡训练效率和在线推理能力

Method: 半在线强化学习：在离线轨迹上模拟在线RL，使用补丁模块自适应恢复轨迹差异，引入折扣未来回报计算奖励，结合步级和回合级优势优化策略

Result: 在四个动态基准测试中达到7B模型SOTA性能，相比基线模型显著提升（AndroidWorld +12.0%，AITW +23.8%）

Conclusion: 半在线RL有效弥合了离线训练效率和在线多步推理之间的差距，为GUI智能体提供了实用的训练和评估方案

Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress
in automating complex user interface interactions through reinforcement
learning. However, current approaches face a fundamental dilemma: offline RL
enables stable training on pre-collected trajectories, but struggles with
multi-step task execution for lack of trajectory-level reward signals; online
RL captures these signals through environment interaction, but suffers from
sparse rewards and prohibitive deployment costs. To address it, we present
Semi-online Reinforcement Learning, a novel paradigm that simulates online RL
on offline trajectories. During each rollout process, we preserve the original
model output within the multi-turn dialogue, where a Patch Module adaptively
recovers the divergence between rollout and expert trajectories. To capture
long-term training signals, Semi-online RL introduces discounted future returns
into the reward computation and optimizes the policy with weighted step-level
and episode-level advantages. We further introduce Semi-Online Performance
(SOP), a metric that aligns better with true online performance, serving as a
practical and effective proxy for real-world evaluation. Experiments show that
ours Semi-online RL achieves SOTA performance among 7B models across four
dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on
AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging
the gap between offline training efficiency and online multi-turn reasoning.
The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

</details>


### [46] [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)
*Chentao Cao,Xiaojun Xu,Bo Han,Hang Li*

Main category: cs.LG

TL;DR: 提出Answer-Then-Check安全对齐方法，通过让LLM先思考回答再检查安全性，提升对抗越狱攻击的鲁棒性，同时降低过度拒绝率并保持通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，确保其对抗越狱攻击的安全性仍然是关键挑战。现有方法存在过度拒绝或安全性不足的问题。

Method: 构建ReSA数据集（8万样本），训练模型先直接回答问题，然后批判性评估安全性，最后决定是否提供回答。支持安全补全功能。

Result: 方法达到帕累托前沿，安全性优异且降低过度拒绝率。在MMLU、MATH500、HumanEval等基准上保持通用推理能力。仅需500样本即可达到全数据集可比性能。

Conclusion: Answer-Then-Check方法有效提升LLM安全性，同时保持实用性。安全对齐可能比之前假设需要更少数据。

Abstract: As large language models (LLMs) continue to advance in capabilities, ensuring
their safety against jailbreak attacks remains a critical challenge. In this
paper, we introduce a novel safety alignment approach called Answer-Then-Check,
which enhances LLM robustness against malicious prompts by applying thinking
ability to mitigate jailbreaking problems before producing a final answer to
the user. Our method enables models to directly answer the question in their
thought and then critically evaluate its safety before deciding whether to
provide it. To implement this approach, we construct the Reasoned Safety
Alignment (ReSA) dataset, comprising 80K examples that teach models to reason
through direct responses and then analyze their safety. Experimental results
demonstrate that our approach achieves the Pareto frontier with superior safety
capability while decreasing over-refusal rates on over-refusal benchmarks.
Notably, the model fine-tuned with ReSA maintains general reasoning
capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our
method equips models with the ability to perform safe completion. Unlike
post-hoc methods that can only reject harmful queries, our model can provide
helpful and safe alternative responses for sensitive topics (e.g., self-harm).
Furthermore, we discover that training on a small subset of just 500 examples
can achieve comparable performance to using the full dataset, suggesting that
safety alignment may require less data than previously assumed.

</details>


### [47] [Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids](https://arxiv.org/abs/2509.12010)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 本文提出了一种新的原则性标准，通过计算可行奖励集合的质心来选择"平均"策略，以解决逆强化学习中的歧义性问题，并提供了高效的估计算法。


<details>
  <summary>Details</summary>
Motivation: 逆强化学习(IRL)存在固有的不适定性——多个奖励函数可以解释相同的行为观察结果。在新环境中，这些不同的奖励函数可能产生不同的策略，因此需要决策标准来选择要部署的策略。

Method: 提出选择可行奖励集合有界子集中奖励诱导策略的"平均"策略作为决策标准。证明该策略可以通过规划该子集的奖励质心获得，并推导出闭式表达式。提出使用专家演示离线数据集高效估计该质心的算法。

Result: 通过数值模拟说明了专家行为与所提出方法产生行为之间的关系，验证了方法的有效性。

Conclusion: 该方法为解决IRL中的奖励歧义性问题提供了原则性的解决方案，能够在新环境中产生与专家行为一致的政策。

Abstract: We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.

</details>


### [48] [Imitation Learning as Return Distribution Matching](https://arxiv.org/abs/2509.12026)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 该论文研究了通过模仿学习训练风险敏感的强化学习代理，目标是匹配专家的期望回报和风险态度，提出了基于Wasserstein距离的分布匹配方法和两种高效算法。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习只关注匹配专家的期望回报，但忽略了风险态度等回报分布的其他特征。需要开发能够同时匹配期望回报和风险敏感性的方法。

Method: 提出了风险敏感模仿学习的Wasserstein距离匹配框架，针对表格设置设计了充分表达的非马尔可夫策略子类，开发了RS-BC和RS-KT两种算法（分别针对未知和已知转移模型）。

Result: RS-KT算法通过利用动态信息实现了比RS-BC更低的样本复杂度，数值模拟验证了非马尔可夫策略相比标准IL算法的优势。

Conclusion: 所提出的风险敏感模仿学习方法能够有效匹配专家的回报分布特征，非马尔可夫策略在样本效率方面表现出色。

Abstract: We study the problem of training a risk-sensitive reinforcement learning (RL)
agent through imitation learning (IL). Unlike standard IL, our goal is not only
to train an agent that matches the expert's expected return (i.e., its average
performance) but also its risk attitude (i.e., other features of the return
distribution, such as variance). We propose a general formulation of the
risk-sensitive IL problem in which the objective is to match the expert's
return distribution in Wasserstein distance. We focus on the tabular setting
and assume the expert's reward is known. After demonstrating the limited
expressivity of Markovian policies for this task, we introduce an efficient and
sufficiently expressive subclass of non-Markovian policies tailored to it.
Building on this subclass, we develop two provably efficient algorithms, RS-BC
and RS-KT, for solving the problem when the transition model is unknown and
known, respectively. We show that RS-KT achieves substantially lower sample
complexity than RS-BC by exploiting dynamics information. We further
demonstrate the sample efficiency of return distribution matching in the
setting where the expert's reward is unknown by designing an oracle-based
variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and
RS-BC with numerical simulations, highlighting both their sample efficiency and
the advantages of non-Markovian policies over standard sample-efficient IL
algorithms.

</details>


### [49] [$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.12117)
*Aryaman Reddi,Gabriele Tiboni,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: K-Level Policy Gradient (KPG) 是一种深度多智能体强化学习方法，通过递归更新智能体策略来应对其他智能体的策略更新，解决了传统方法中的协调问题，在 StarCraft II 和 MuJoCo 任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统深度多智能体强化学习算法在策略更新时只考虑其他智能体的当前策略，而不考虑它们在同一更新步骤中的策略变化，这会导致协调失败和性能下降。

Method: 提出 K-Level Policy Gradient (KPG) 方法，递归地更新每个智能体以应对其他智能体更新后的策略。将 KPG 应用于 MAPPO、MADDPG 和 FACMAC 等深度 MARL 算法。

Result: 理论证明 KPG 在有限迭代次数下能够单调收敛到局部纳什均衡。在 StarCraft II 和多智能体 MuJoCo 环境中，KPG 表现出优于现有深度 MARL 算法的性能。

Conclusion: KPG 方法通过考虑其他智能体的策略更新，显著提高了多智能体协调能力，在理论和实验上都证明了其有效性。

Abstract: Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)
typically employ a policy update that responds to the current strategies of
other agents. While being straightforward, this approach does not account for
the updates of other agents at the same update step, resulting in
miscoordination. In this paper, we introduce the $K$-Level Policy Gradient
(KPG), a method that recursively updates each agent against the updated
policies of other agents, speeding up the discovery of effective coordinated
policies. We theoretically prove that KPG with finite iterates achieves
monotonic convergence to a local Nash equilibrium under certain conditions. We
provide principled implementations of KPG by applying it to the deep MARL
algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior
performance over existing deep MARL algorithms in StarCraft II and multi-agent
MuJoCo.

</details>
