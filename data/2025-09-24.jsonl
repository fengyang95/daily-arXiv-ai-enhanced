{"id": "2509.18337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18337", "abs": "https://arxiv.org/abs/2509.18337", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation", "comment": "15 pages, 4 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Commit messages play a key role in documenting the intent behind code\nchanges. However, they are often low-quality, vague, or incomplete, limiting\ntheir usefulness. Commit Message Generation (CMG) aims to automatically\ngenerate descriptive commit messages from code diffs to reduce developers'\neffort and improve message quality. Although recent advances in LLMs have shown\npromise in automating CMG, their performance remains limited. This paper aims\nto enhance CMG performance by retrieving similar diff-message pairs to guide\nLLMs to generate commit messages that are more precise and informative. We\nproposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message\nGeneration, structured in three phases: (1) Retrieve: retrieving the similar\ndiff-message pairs; (2) Augment: combining them with the query diff into a\nstructured prompt; and (3) Generate: generating commit messages corresponding\nto the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific\nterminologies and writing styles from the retrieved diff-message pairs, thereby\nproducing high-quality commit messages. We evaluated our method on various\nLLMs, including closed-source GPT models and open-source DeepSeek models.\nExperimental results show that CoRaCMG significantly boosts LLM performance\nacross four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,\nDeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when\naugmented with a single retrieved example pair. After incorporating the single\nexample pair, GPT-4o achieves the highest improvement rate, with BLEU\nincreasing by 89%. Moreover, performance gains plateau after more than three\nexamples are used, indicating diminishing returns. Further analysis shows that\nthe improvements are attributed to the model's ability to capture the\nterminologies and writing styles of human-written commit messages from the\nretrieved example pairs."}
{"id": "2509.18361", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18361", "abs": "https://arxiv.org/abs/2509.18361", "authors": ["Daye Nam", "Malgorzata Salawa", "Satish Chandra"], "title": "Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts", "comment": null, "summary": "Evaluating developer satisfaction with conversational AI assistants at scale\nis critical but challenging. User studies provide rich insights, but are\nunscalable, while large-scale quantitative signals from logs or in-product\nratings are often too shallow or sparse to be reliable. To address this gap, we\npropose and evaluate a new approach: using sentiment analysis of developer\nprompts to identify implicit signals of user satisfaction. With an analysis of\nindustrial usage logs of 372 professional developers, we show that this\napproach can identify a signal in ~8% of all interactions, a rate more than 13\ntimes higher than explicit user feedback, with reasonable accuracy even with an\noff-the-shelf sentiment analysis approach. This new practical approach to\ncomplement existing feedback channels would open up new directions for building\na more comprehensive understanding of the developer experience at scale."}
{"id": "2509.18454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18454", "abs": "https://arxiv.org/abs/2509.18454", "authors": ["Andrzej Białecki", "Piotr Białecki", "Piotr Sowiński", "Mateusz Budziak", "Jan Gajewski"], "title": "SC2Tools: StarCraft II Toolset and Dataset API", "comment": null, "summary": "Computer games, as fully controlled simulated environments, have been\nutilized in significant scientific studies demonstrating the application of\nReinforcement Learning (RL). Gaming and esports are key areas influenced by the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) solutions\nat scale. Tooling simplifies scientific workloads and is essential for\ndeveloping the gaming and esports research area.\n  In this work, we present ``SC2Tools'', a toolset containing multiple\nsubmodules responsible for working with, and producing larger datasets. We\nprovide a modular structure of the implemented tooling, leaving room for future\nextensions where needed. Additionally, some of the tools are not StarCraft~2\nexclusive and can be used with other types of data for dataset creation.\n  The tools we present were leveraged in creating one of the largest\nStarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch\nLightning application programming interface (API) for easy access to the data.\n  We conclude that alleviating the burden of data collection, preprocessing,\nand custom code development is essential for less technically proficient\nresearchers to engage in the growing gaming and esports research area. Finally,\nour solution provides some foundational work toward normalizing experiment\nworkflow in StarCraft~2"}
{"id": "2509.18548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18548", "abs": "https://arxiv.org/abs/2509.18548", "authors": ["Steven R Brandt", "Max Morris", "Patrick Diehl", "Christopher Bowen", "Jacob Tucker", "Lauren Bristol", "Golden G. Richard III"], "title": "Locking Down Science Gateways", "comment": null, "summary": "The most recent Linux kernels have a new feature for securing applications:\nLandlock. Like Seccomp before it, Landlock makes it possible for a running\nprocess to give up access to resources. For applications running as Science\nGateways, network access is required while starting up MPI, but for the sake of\nsecurity, it should be taken away prior to the reading of user-supplied\nparameter files. We explore the usefulness of Landlock by modifying and locking\ndown three mature scientific codes: The Einstein Toolkit (a code that studies\nthe dynamics of relativistic astrophysics, e.g. neutron star collisions),\nOcto-Tiger (a code for studying the dynamics of non-relativistic astrophysics,\ne.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).\nFinally, we implement a fully-functioning FUKA science gateway that relies on\nLandlock (instead of user authentication) for security."}
{"id": "2509.18113", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation."}
{"id": "2509.18101", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies."}
{"id": "2509.18103", "categories": ["cs.LG", "math.NT"], "pdf": "https://arxiv.org/pdf/2509.18103", "abs": "https://arxiv.org/abs/2509.18103", "authors": ["Jennifer Dodgson", "Michael Joedhitya", "Adith Ramdas", "Surender Suresh Kumar", "Adarsh Singh Chauhan", "Akira Rafhael", "Wang Mingshu", "Nordine Lotfi"], "title": "Machine Learnability as a Measure of Order in Aperiodic Sequences", "comment": null, "summary": "Research on the distribution of prime numbers has revealed a dual character:\ndeterministic in definition yet exhibiting statistical behavior reminiscent of\nrandom processes. In this paper we show that it is possible to use an\nimage-focused machine learning model to measure the comparative regularity of\nprime number fields at specific regions of an Ulam spiral. Specifically, we\ndemonstrate that in pure accuracy terms, models trained on blocks extracted\nfrom regions of the spiral in the vicinity of 500m outperform models trained on\nblocks extracted from the region representing integers lower than 25m. This\nimplies existence of more easily learnable order in the former region than in\nthe latter. Moreover, a detailed breakdown of precision and recall scores seem\nto imply that the model is favouring a different approach to classification in\ndifferent regions of the spiral, focusing more on identifying prime patterns at\nlower numbers and more on eliminating composites at higher numbers. This aligns\nwith number theory conjectures suggesting that at higher orders of magnitude we\nshould see diminishing noise in prime number distributions, with averages\n(density, AP equidistribution) coming to dominate, while local randomness\nregularises after scaling by log x. Taken together, these findings point toward\nan interesting possibility: that machine learning can serve as a new\nexperimental instrument for number theory. Notably, the method shows potential\n1 for investigating the patterns in strong and weak primes for cryptographic\npurposes."}
{"id": "2509.18808", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18808", "abs": "https://arxiv.org/abs/2509.18808", "authors": ["Zexun Zhan", "Shuzheng Gao", "Ruida Hu", "Cuiyun Gao"], "title": "SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration. However, existing benchmarks mainly formalize the task as a static,\nsingle-turn problem, overlooking the stepwise requirement changes and iterative\nworkflows in real-world software development. This mismatch limits the\nunderstanding of how well LLMs can support real-world development workflows.\nConstructing such iterative benchmarks is challenging due to the lack of public\ninteraction traces and the difficulty of creating discriminative, turn-specific\ntest cases.\n  To bridge this gap, we present SR-Eval, a benchmark specifically designed to\nassess LLMs on iterative code generation under Stepwise requirements\nRefinement. SR-Eval spans both function-level and repository-level tasks in\nPython and Java, enabling fine-grained and progressive evaluation across\nevolving requirements. The construction of SR-Eval follows a carefully designed\npipeline that first leverages a multi-agent-based requirement generation method\nto simulate the development process and recover the multi-round interaction\nprocess from final requirements, then employs a semantic-aware discriminative\ntest case generation component to ensure discriminative and consistent\nevaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857\nquestions at both function and repository levels. Using SR-Eval, we evaluate 11\nrepresentative LLMs with three prompting strategies that simulate different\nusage patterns. Results show that iterative code generation under stepwise\nrequirement refinement remains highly challenging: the best-performing model\nachieves only 22.67% completion rate on function-level tasks and 20.00% on\nrepository-level tasks. We further observe that prompting strategies\nsubstantially influence performance, highlighting the need for the development\nof advanced methods."}
{"id": "2509.18122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation."}
{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata."}
{"id": "2509.18104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18104", "abs": "https://arxiv.org/abs/2509.18104", "authors": ["Wenqian Li", "Youjia Yang", "Ruoxi Jia", "Yan Pang"], "title": "Data Valuation and Selection in a Federated Model Marketplace", "comment": null, "summary": "In the era of Artificial Intelligence (AI), marketplaces have become\nessential platforms for facilitating the exchange of data products to foster\ndata sharing. Model transactions provide economic solutions in data\nmarketplaces that enhance data reusability and ensure the traceability of data\nownership. To establish trustworthy data marketplaces, Federated Learning (FL)\nhas emerged as a promising paradigm to enable collaborative learning across\nsiloed datasets while safeguarding data privacy. However, effective data\nvaluation and selection from heterogeneous sources in the FL setup remain key\nchallenges. This paper introduces a comprehensive framework centered on a\nWasserstein-based estimator tailored for FL. The estimator not only predicts\nmodel performance across unseen data combinations but also reveals the\ncompatibility between data heterogeneity and FL aggregation algorithms. To\nensure privacy, we propose a distributed method to approximate Wasserstein\ndistance without requiring access to raw data. Furthermore, we demonstrate that\nmodel performance can be reliably extrapolated under the neural scaling law,\nenabling effective data selection without full-scale training. Extensive\nexperiments across diverse scenarios, such as label skew, mislabeled, and\nunlabeled sources, show that our approach consistently identifies\nhigh-performing data combinations, paving the way for more reliable FL-based\nmodel marketplaces."}
{"id": "2509.19136", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.5; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.19136", "abs": "https://arxiv.org/abs/2509.19136", "authors": ["Sébastien Salva", "Redha Taguelmimt"], "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "comment": null, "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard."}
{"id": "2509.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities."}
{"id": "2509.18105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics."}
{"id": "2509.19185", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.19185", "abs": "https://arxiv.org/abs/2509.19185", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "title": "An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications", "comment": null, "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents."}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent."}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications."}
{"id": "2509.18106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18106", "abs": "https://arxiv.org/abs/2509.18106", "authors": ["Elisa Tomassini", "Enrique García-Macías", "Filippo Ubertini"], "title": "Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks", "comment": null, "summary": "The growing use of permanent monitoring systems has increased data\navailability, offering new opportunities for structural assessment but also\nposing scalability challenges, especially across large bridge networks.\nManaging multiple structures requires tracking and comparing long-term\nbehaviour efficiently. To address this, knowledge transfer between similar\nstructures becomes essential. This study proposes a model-based transfer\nlearning approach using neural network surrogate models, enabling a model\ntrained on one bridge to be adapted to another with similar characteristics.\nThese models capture shared damage mechanisms, supporting a scalable and\ngeneralizable monitoring framework. The method was validated using real data\nfrom two bridges. The transferred model was integrated into a Bayesian\ninference framework for continuous damage assessment based on modal features\nfrom monitoring data. Results showed high sensitivity to damage location,\nseverity, and extent. This approach enhances real-time monitoring and enables\ncross-structure knowledge transfer, promoting smart monitoring strategies and\nimproved resilience at the network level."}
{"id": "2509.18163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux."}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent."}
{"id": "2509.18107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18107", "abs": "https://arxiv.org/abs/2509.18107", "authors": ["Huanyao Zhang", "Jiaye Lin", "Wentao Zhang", "Haitao Yuan", "Guoliang Li"], "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting involves predicting future values based\non historical observations. However, existing approaches primarily rely on\npredefined single-scale patches or lack effective mechanisms for multi-scale\nfeature fusion. These limitations hinder them from fully capturing the complex\npatterns inherent in time series, leading to constrained performance and\ninsufficient generalizability. To address these challenges, we propose a novel\narchitecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers\n(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both\nGeneral Pre-trained Models (GPM) and Domain-specific Models (DSM) for\nmulti-scale feature extraction. To accommodate the heterogeneity of temporal\nfeatures, AdaMixT incorporates a gating network that dynamically allocates\nweights among different experts, enabling more accurate predictions through\nadaptive multi-scale fusion. Comprehensive experiments on eight widely used\nbenchmarks, including Weather, Traffic, Electricity, ILI, and four ETT\ndatasets, consistently demonstrate the effectiveness of AdaMixT in real-world\nscenarios."}
{"id": "2509.18167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications."}
{"id": "2509.18180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR."}
{"id": "2509.18108", "categories": ["cs.LG", "cs.AI", "I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18108", "abs": "https://arxiv.org/abs/2509.18108", "authors": ["Adam Viktorin", "Tomas Kadavy", "Jozef Kovac", "Michal Pluhacek", "Roman Senkerik"], "title": "Solve it with EASE", "comment": "EASE framework landing paper", "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an\nopen-source and fully modular framework for iterative algorithmic solution\ngeneration leveraging large language models (LLMs). EASE integrates generation,\ntesting, analysis, and evaluation into a reproducible feedback loop, giving\nusers full control over error handling, analysis, and quality assessment. Its\narchitecture supports the orchestration of multiple LLMs in complementary\nroles-such as generator, analyst, and evaluator. By abstracting the complexity\nof prompt design and model management, EASE provides a transparent and\nextensible platform for researchers and practitioners to co-design algorithms\nand other generative solutions across diverse domains."}
{"id": "2509.18175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important."}
{"id": "2509.18181", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications."}
{"id": "2509.18109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18109", "abs": "https://arxiv.org/abs/2509.18109", "authors": ["Jonatan Katz Nielsen"], "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks", "comment": null, "summary": "Accurate recognition of vessel types from Automatic Identification System\n(AIS) tracks is essential for safety oversight and combating illegal,\nunreported, and unregulated (IUU) activity. This paper presents a strait-scale,\nmachine-learning pipeline that classifies moving vessels using only AIS data.\nWe analyze eight days of historical AIS from the Danish Maritime Authority\ncovering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After\nforward/backward filling voyage records, removing kinematic and geospatial\noutliers, and segmenting per-MMSI tracks while excluding stationary periods\n($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,\nSOG statistics), temporal, geospatial (Haversine distances, spans), and\nship-shape attributes computed from AIS A/B/C/D reference points (length,\nwidth, aspect ratio, bridge-position ratio). To avoid leakage, we perform\ngrouped train/test splits by MMSI and use stratified 5-fold cross-validation.\nAcross five classes (cargo, tanker, passenger, high-speed craft, fishing;\nN=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest\nwith SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall\n92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches\none-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the\nbridge-position ratio and maximum SOG as the most discriminative signals;\nprincipal errors occur between cargo and tanker, reflecting similar transit\nbehavior. We demonstrate operational value by backfilling missing ship types on\nunseen data and discuss improvements such as DBSCAN based trip segmentation and\ngradient-boosted ensembles to handle frequent-stop ferries and further lift\nperformance. The results show that lightweight features over AIS trajectories\nenable real-time vessel type classification in straits."}
{"id": "2509.18293", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability."}
{"id": "2509.18186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogić", "Ivan Luković"], "title": "An Outcome-Based Educational Recommender System", "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders."}
{"id": "2509.18110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18110", "abs": "https://arxiv.org/abs/2509.18110", "authors": ["Mrigank Dhingra", "Romit Maulik", "Adil Rasheed", "Omer San"], "title": "Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs", "comment": null, "summary": "Neural operator learning has emerged as a powerful approach for solving\npartial differential equations (PDEs) in a data-driven manner. However,\napplying principal component analysis (PCA) to high-dimensional solution fields\nincurs significant computational overhead. To address this, we propose a\npatch-based PCA-Net framework that decomposes the solution fields into smaller\npatches, applies PCA within each patch, and trains a neural operator in the\nreduced PCA space. We investigate two different patch-based approaches that\nbalance computational efficiency and reconstruction accuracy: (1)\nlocal-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off\nbetween computational cost and accuracy is analyzed, highlighting the\nadvantages and limitations of each approach. Furthermore, within each approach,\nwe explore two refinements for the most computationally efficient method: (i)\nintroducing overlapping patches with a smoothing filter and (ii) employing a\ntwo-step process with a convolutional neural network (CNN) for refinement. Our\nresults demonstrate that patch-based PCA significantly reduces computational\ncomplexity while maintaining high accuracy, reducing end-to-end pipeline\nprocessing time by a factor of 3.7 to 4 times compared to global PCA, thefore\nmaking it a promising technique for efficient operator learning in PDE-based\nsystems."}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time."}
{"id": "2509.18198", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd."}
{"id": "2509.18111", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18111", "abs": "https://arxiv.org/abs/2509.18111", "authors": ["Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection", "comment": null, "summary": "The reliability of artificial intelligence (AI) systems in open-world\nsettings depends heavily on their ability to flag out-of-distribution (OOD)\ninputs unseen during training. Recent advances in large-scale vision-language\nmodels (VLMs) have enabled promising few-shot OOD detection frameworks using\nonly a handful of in-distribution (ID) samples. However, existing prompt\nlearning-based OOD methods rely solely on softmax probabilities, overlooking\nthe rich discriminative potential of the feature embeddings learned by VLMs\ntrained on millions of samples. To address this limitation, we propose a novel\ncontext optimization (CoOp)-based framework that integrates subspace\nrepresentation learning with prompt tuning. Our approach improves ID-OOD\nseparability by projecting the ID features into a subspace spanned by prompt\nvectors, while projecting ID-irrelevant features into an orthogonal null space.\nTo train such OOD detection framework, we design an easy-to-handle end-to-end\nlearning criterion that ensures strong OOD detection performance as well as\nhigh ID classification accuracy. Experiments on real-world datasets showcase\nthe effectiveness of our approach."}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare."}
{"id": "2509.18215", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas Čyras", "José Ruiz Alarcón"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation."}
{"id": "2509.18112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18112", "abs": "https://arxiv.org/abs/2509.18112", "authors": ["Sheng Wong", "Ravi Shankar", "Beth Albert", "Gabriel Davis Jones"], "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis", "comment": "Preparing for journal", "summary": "Foundation models (FMs) and large language models (LLMs) demonstrate\nremarkable capabilities across diverse domains through training on massive\ndatasets. These models have demonstrated exceptional performance in healthcare\napplications, yet their potential for electronic fetal monitoring\n(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating\nfetal well-being, remains largely underexplored. Antepartum CTG interpretation\npresents unique challenges due to the complex nature of fetal heart rate (FHR)\npatterns and uterine activity, requiring sophisticated analysis of long\ntime-series data. The assessment of CTG is heavily based on subjective clinical\ninterpretation, often leading to variability in diagnostic accuracy and\ndeviation from timely pregnancy care. This study presents the first\ncomprehensive comparison of state-of-the-art AI approaches for automated\nantepartum CTG analysis. We systematically compare time-series FMs and LLMs\nagainst established CTG-specific architectures. Our evaluation encompasses over\n500 CTG recordings of varying durations reflecting real-world clinical\nrecordings, providing robust performance benchmarks across different modelling\nparadigms. Our results demonstrate that fine-tuned LLMs achieve superior\nperformance compared to both foundation models and domain-specific approaches,\noffering a promising alternative pathway for clinical CTG interpretation. These\nfindings provide critical insights into the relative strengths of different AI\nmethodologies for fetal monitoring applications and establish a foundation for\nfuture clinical AI development in prenatal care."}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."}
{"id": "2509.18216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time."}
{"id": "2509.18114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18114", "abs": "https://arxiv.org/abs/2509.18114", "authors": ["Javed I. Khan an Henry Uwabor Moye"], "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU", "comment": "12 pages, Technical Report 2025-07-01, Internetworking and Media\n  Communications Research Laboratories, Department of Computer Science, Kent\n  State University", "summary": "Autoregressive inference in large transformer-based language models (LLMs)\npresents significant challenges for runtime efficiency, particularly during the\ndecode phase where load imbalance across GPU shards can cause throughput\ndegradation and latency spikes. A DPU-assisted framework leveraged by\nBlueField-3 Data Processing Units can enable real-time detection and mitigation\nof load imbalance in multi-node tensor-parallel inference. By offloading\nmonitoring tasks to the DPU and analyzing GPU telemetry and inter-node\ncommunication patterns, the resulting system can provide actionable feedback to\ninference controllers and schedulers. The goal of this study is three-fold i)\nidentify the reported skews/imbalances/pathological conditions that arise in\nmuti-GPU execution of a) LLM tensor computing (both during training and\ninference), b) identify their impact on computational performance, and c) make\na critical assessment if those can be tracked for potential mitigation from a\nDPU network."}
{"id": "2509.18360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments."}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition."}
{"id": "2509.18115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18115", "abs": "https://arxiv.org/abs/2509.18115", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Towards Scalable and Structured Spatiotemporal Forecasting", "comment": null, "summary": "In this paper, we propose a novel Spatial Balance Attention block for\nspatiotemporal forecasting. To strike a balance between obeying spatial\nproximity and capturing global correlation, we partition the spatial graph into\na set of subgraphs and instantiate Intra-subgraph Attention to learn local\nspatial correlation within each subgraph; to capture the global spatial\ncorrelation, we further aggregate the nodes to produce subgraph representations\nand achieve message passing among the subgraphs via Inter-subgraph Attention.\nBuilding on the proposed Spatial Balance Attention block, we develop a\nmultiscale spatiotemporal forecasting model by progressively increasing the\nsubgraph scales. The resulting model is both scalable and able to produce\nstructured spatial correlation, and meanwhile, it is easy to implement. We\nevaluate its efficacy and efficiency against the existing models on real-world\nspatiotemporal datasets from medium to large sizes. The experimental results\nshow that it can achieve performance improvements up to 7.7% over the baseline\nmethods at low running costs."}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency."}
{"id": "2509.18221", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent."}
{"id": "2509.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}"}
{"id": "2509.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages."}
{"id": "2509.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands."}
{"id": "2509.18117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18117", "abs": "https://arxiv.org/abs/2509.18117", "authors": ["Eric Petit", "Denis Chêne"], "title": "Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs", "comment": "soumis {\\`a} la conf{\\'e}rence IHM 2025", "summary": "The paper presents a machine learning approach to design digital interfaces\nthat can dynamically adapt to different users and usage strategies. The\nalgorithm uses Bayesian statistics to model users' browsing behavior, focusing\non their habits rather than group preferences. It is distinguished by its\nonline incremental learning, allowing reliable predictions even with little\ndata and in the case of a changing environment. This inference method generates\na task model, providing a graphical representation of navigation with the usage\nstatistics of the current user. The algorithm learns new tasks while preserving\nprior knowledge. The theoretical framework is described, and simulations show\nthe effectiveness of the approach in stationary and non-stationary\nenvironments. In conclusion, this research paves the way for adaptive systems\nthat improve the user experience by helping them to better navigate and act on\ntheir interface."}
{"id": "2509.18401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement."}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value."}
{"id": "2509.18118", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.18118", "abs": "https://arxiv.org/abs/2509.18118", "authors": ["Marcelo Ribeiro", "Diogo Costa", "Gonçalo Moreira", "Sandro Pinto", "Tiago Gomes"], "title": "Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices", "comment": null, "summary": "Modern IoT devices increasingly rely on machine learning solutions to process\ndata locally. However, the lack of graphics processing units (GPUs) or\ndedicated accelerators on most platforms makes on-device training largely\ninfeasible, often requiring cloud-based services to perform this task. This\nprocedure often raises privacy-related concerns, and creates dependency on\nreliable and always-on connectivity. Federated Learning (FL) is a new trend\nthat addresses these issues by enabling decentralized and collaborative\ntraining directly on devices, but it requires highly efficient optimization\nalgorithms. L-SGD, a lightweight variant of stochastic gradient descent, has\nenabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).\nThis work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture\nthat still lacks robust support for on-device training. L-SGD was evaluated on\nboth Arm and RISC-V platforms using 32-bit floating-point arithmetic,\nhighlighting the performance impact of the absence of Floating-Point Units\n(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit\nquantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in\nmemory usage and a 2.2x speedup in training time, with negligible accuracy\ndegradation."}
{"id": "2509.18439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale."}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control."}
{"id": "2509.18119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL."}
{"id": "2509.18458", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development."}
{"id": "2509.18234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands."}
{"id": "2509.18120", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.18120", "abs": "https://arxiv.org/abs/2509.18120", "authors": ["Thanh Linh Nguyen", "Quoc-Viet Pham"], "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning", "comment": "Accepted in IEEE GLOBECOM 2025", "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or\nbanks) to collaboratively train artificial intelligence (AI) models while\npreserving data privacy by keeping data local. While prior work has primarily\naddressed statistical heterogeneity across organizations, a critical challenge\narises from economic competition, where organizations may act as market rivals,\nmaking them hesitant to participate in joint training due to potential utility\nloss (i.e., reduced net benefit). Furthermore, the combined effects of\nstatistical heterogeneity and inter-organizational competition on\norganizational behavior and system-wide social welfare remain underexplored. In\nthis paper, we propose CoCoGen, a coopetitive-compatible data generation\nframework, leveraging generative AI (GenAI) and potential game theory to model,\nanalyze, and optimize collaborative learning under heterogeneous and\ncompetitive settings. Specifically, CoCoGen characterizes competition and\nstatistical heterogeneity through learning performance and utility-based\nformulations and models each training round as a weighted potential game. We\nthen derive GenAI-based data generation strategies that maximize social\nwelfare. Experimental results on the Fashion-MNIST dataset reveal how varying\nheterogeneity and competition levels affect organizational behavior and\ndemonstrate that CoCoGen consistently outperforms baseline methods."}
{"id": "2509.18467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources."}
{"id": "2509.18382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model."}
{"id": "2509.18124", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18124", "abs": "https://arxiv.org/abs/2509.18124", "authors": ["Edmund Agyemang", "Lawrence Agbota", "Vincent Agbenyeavu", "Peggy Akabuah", "Bismark Bimpong", "Christopher Attafuah"], "title": "Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters", "comment": "13 pages, 6 figures and 4 tables", "summary": "This study explores the application of supervised machine learning algorithms\nto predict coffee ratings based on a combination of influential textual and\nnumerical attributes extracted from user reviews. Through careful data\npreprocessing including text cleaning, feature extraction using TF-IDF, and\nselection with SelectKBest, the study identifies key factors contributing to\ncoffee quality assessments. Six models (Decision Tree, KNearest Neighbors,\nMulti-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained\nand evaluated using optimized hyperparameters. Model performance was assessed\nprimarily using F1-score, Gmean, and AUC metrics. Results demonstrate that\nensemble methods (Extra Trees, Random Forest, and XGBoost), as well as\nMulti-layer Perceptron, consistently outperform simpler classifiers (Decision\nTrees and K-Nearest Neighbors) in terms of evaluation metrics such as F1\nscores, G-mean and AUC. The findings highlight the essence of rigorous feature\nselection and hyperparameter tuning in building robust predictive systems for\nsensory product evaluation, offering a data driven approach to complement\ntraditional coffee cupping by expertise of trained professionals."}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches."}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test."}
{"id": "2509.18125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18125", "abs": "https://arxiv.org/abs/2509.18125", "authors": ["Harsha Koduri"], "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment", "comment": null, "summary": "Healthcare systems face increasing pressure to allocate limited nursing\nresources efficiently while accounting for skill heterogeneity, patient acuity,\nstaff fatigue, and continuity of care. Traditional optimization and heuristic\nscheduling methods struggle to capture these dynamic, multi-constraint\nenvironments. I propose NurseSchedRL, a reinforcement learning framework for\nnurse-patient assignment that integrates structured state encoding, constrained\naction masking, and attention-based representations of skills, fatigue, and\ngeographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with\nfeasibility masks to ensure assignments respect real-world constraints, while\ndynamically adapting to patient arrivals and varying nurse availability. In\nsimulation with realistic nurse and patient data, NurseSchedRL achieves\nimproved scheduling efficiency, better alignment of skills to patient needs,\nand reduced fatigue compared to baseline heuristic and unconstrained RL\napproaches. These results highlight the potential of reinforcement learning for\ndecision support in complex, high-stakes healthcare workforce management."}
{"id": "2509.18514", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems."}
{"id": "2509.18400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment."}
{"id": "2509.18126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18126", "abs": "https://arxiv.org/abs/2509.18126", "authors": ["Bishal K C", "Amr Hilal", "Pawan Thapa"], "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning", "comment": null, "summary": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security."}
{"id": "2509.18535", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method."}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC."}
{"id": "2509.18127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety."}
{"id": "2509.18536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official."}
{"id": "2509.18436", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy)."}
{"id": "2509.18128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18128", "abs": "https://arxiv.org/abs/2509.18128", "authors": ["Amirreza Tootchi", "Xiaoping Du"], "title": "Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis", "comment": null, "summary": "Machine learning surrogates are increasingly employed to replace expensive\ncomputational models for physics-based reliability analysis. However, their use\nintroduces epistemic uncertainty from model approximation errors, which couples\nwith aleatory uncertainty in model inputs, potentially compromising the\naccuracy of reliability predictions. This study proposes a Gauss-Hermite\nquadrature approach to decouple these nested uncertainties and enable more\naccurate reliability analysis. The method evaluates conditional failure\nprobabilities under aleatory uncertainty using First and Second Order\nReliability Methods and then integrates these probabilities across realizations\nof epistemic uncertainty. Three examples demonstrate that the proposed approach\nmaintains computational efficiency while yielding more trustworthy predictions\nthan traditional methods that ignore model uncertainty."}
{"id": "2509.18577", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision"}
{"id": "2509.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing."}
{"id": "2509.18130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18130", "abs": "https://arxiv.org/abs/2509.18130", "authors": ["Zijie Zhou", "Huichen Ma"], "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model", "comment": null, "summary": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively."}
{"id": "2509.18585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA."}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting."}
{"id": "2509.18131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18131", "abs": "https://arxiv.org/abs/2509.18131", "authors": ["Jean-Michel Tucny", "Abhisek Ganguly", "Santosh Ansumali", "Sauro Succi"], "title": "Two ways to knowledge?", "comment": null, "summary": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight."}
{"id": "2509.18588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance."}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities."}
{"id": "2509.18133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical."}
{"id": "2509.18632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem."}
{"id": "2509.18633", "categories": ["cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies."}
{"id": "2509.18134", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18134", "abs": "https://arxiv.org/abs/2509.18134", "authors": ["Furan Xie", "Bing Liu", "Li Chai"], "title": "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization", "comment": null, "summary": "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network."}
{"id": "2509.18655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE."}
{"id": "2509.18667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens."}
{"id": "2509.18135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18135", "abs": "https://arxiv.org/abs/2509.18135", "authors": ["Shaoxun Wang", "Xingjun Zhang", "Qianyang Li", "Jiawei Cao", "Zhendong Tan"], "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting", "comment": null, "summary": "Inter-series correlations are crucial for accurate multivariate time series\nforecasting, yet these relationships often exhibit complex dynamics across\ndifferent temporal scales. Existing methods are limited in modeling these\nmulti-scale dependencies and struggle to capture their intricate and evolving\nnature. To address this challenge, this paper proposes a novel Static-Dynamic\nGraph Fusion network (SDGF), whose core lies in capturing multi-scale\ninter-series correlations through a dual-path graph structure learning\napproach. Specifically, the model utilizes a static graph based on prior\nknowledge to anchor long-term, stable dependencies, while concurrently\nemploying Multi-level Wavelet Decomposition to extract multi-scale features for\nconstructing an adaptively learned dynamic graph to capture associations at\ndifferent scales. We design an attention-gated module to fuse these two\ncomplementary sources of information intelligently, and a multi-kernel dilated\nconvolutional network is then used to deepen the understanding of temporal\npatterns. Comprehensive experiments on multiple widely used real-world\nbenchmark datasets demonstrate the effectiveness of our proposed model."}
{"id": "2509.18658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment."}
{"id": "2509.18681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models."}
{"id": "2509.18136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset"}
{"id": "2509.18713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios."}
{"id": "2509.18690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research."}
{"id": "2509.18137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18137", "abs": "https://arxiv.org/abs/2509.18137", "authors": ["Shaoheng Wang", "Yao Lu", "Yuqi Li", "Yaxin Gao", "Jiaqi Nie", "Shanqing Yu", "Yingli Tian", "Qi Xuan"], "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods", "comment": null, "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation\n(LoRA) can save significant costs in storage and computing, but its strong\nadaptability to a single task is often accompanied by insufficient cross-task\ngeneralization capabilities. To improve this, existing work combines LoRA with\nmixture-of-experts (MoE) to enhance the model's adaptability through expert\nmodules and routing mechanisms. However, existing LoRA-MoE methods lack unified\nstandards in models, datasets, hyperparameters, and evaluation methods, making\nit difficult to conduct fair comparisons between different methods. To this\nend, we proposed a unified benchmark named LoRALib. Specifically, we\nstandardized datasets from $40$ downstream tasks into a unified format,\nfine-tuned them using the same hyperparameters and obtained $680$ LoRA modules\nacross $17$ model architectures. Based on this LoRA library, we conduct\nlarge-scale experiments on $3$ representative LoRA-MoE methods and different\nLoRA selection mechanisms using the open-sourced testing tool OpenCompass.\nExtensive experiments show that LoRAMoE performs best, and that prioritizing\nLoRAs relevant to the target task can further improve the performance of MoE.\nWe hope these findings will inspire future work. Our datasets and LoRA library\nare available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset\nand https://huggingface.co/YaoLuzjut/models."}
{"id": "2509.18722", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field."}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions."}
{"id": "2509.18138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18138", "abs": "https://arxiv.org/abs/2509.18138", "authors": ["Tiantian Zhang"], "title": "Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts", "comment": null, "summary": "We introduce a new algorithm, \\emph{Rank-Induced Plackett--Luce Mirror\nDescent (RIPLM)}, which leverages the structural equivalence between the\n\\emph{rank benchmark} and the \\emph{distributional benchmark} established in\n\\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert\nidentities, RIPLM updates directly in the \\emph{rank-induced Plackett--Luce\n(PL)} parameterization. This ensures that the algorithm's played distributions\nremain within the class of rank-induced distributions at every round,\npreserving the equivalence with the rank benchmark. To our knowledge, RIPLM is\nthe first algorithm that is both (i) \\emph{rank-faithful} and (ii)\n\\emph{variance-adaptive} in the sleeping experts setting."}
{"id": "2509.18742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs."}
{"id": "2509.18771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress."}
{"id": "2509.18139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18139", "abs": "https://arxiv.org/abs/2509.18139", "authors": ["Akshay Murthy", "Shawn Sebastian", "Manil Shangle", "Huaduo Wang", "Sopam Dasgupta", "Gopal Gupta"], "title": "Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification", "comment": "7 pages", "summary": "Recently, the demand for Machine Learning (ML) models that can balance\naccuracy, efficiency, and interpreability has grown significantly.\nTraditionally, there has been a tradeoff between accuracy and explainability in\npredictive models, with models such as Neural Networks achieving high accuracy\non complex datasets while sacrificing internal transparency. As such, new\nrule-based algorithms such as FOLD-SE have been developed that provide tangible\njustification for predictions in the form of interpretable rule sets. The\nprimary objective of this study was to compare FOLD-SE and FOLD-R++, both\nrule-based classifiers, in binary classification and evaluate how FOLD-SE\nperforms against XGBoost, a widely used ensemble classifier, when applied to\nmulti-category classification. We hypothesized that because FOLD-SE can\ngenerate a condensed rule set in a more explainable manner, it would lose\nupwards of an average of 3 percent in accuracy and F1 score when compared with\nXGBoost and FOLD-R++ in multiclass and binary classification, respectively. The\nresearch used data collections for classification, with accuracy, F1 scores,\nand processing time as the primary performance measures. Outcomes show that\nFOLD-SE is superior to FOLD-R++ in terms of binary classification by offering\nfewer rules but losing a minor percentage of accuracy and efficiency in\nprocessing time; in tasks that involve multi-category classifications, FOLD-SE\nis more precise and far more efficient compared to XGBoost, in addition to\ngenerating a comprehensible rule set. The results point out that FOLD-SE is a\nbetter choice for both binary tasks and classifications with multiple\ncategories. Therefore, these results demonstrate that rule-based approaches\nlike FOLD-SE can bridge the gap between explainability and performance,\nhighlighting their potential as viable alternatives to black-box models in\ndiverse classification tasks."}
{"id": "2509.18750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers."}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives."}
{"id": "2509.18140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18140", "abs": "https://arxiv.org/abs/2509.18140", "authors": ["Iram Wajahat", "Amritpal Singh", "Fazel Keshtkar", "Syed Ahmad Chan Bukhari"], "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders", "comment": "6 pages, 6 figures", "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent\na significant global health burden, disproportionately impacting genetically\npredisposed populations such as the Pima Indians (a Native American tribe from\nsouth central Arizona). This study introduces a novel machine learning (ML)\nframework that integrates predictive modeling with gene-agnostic pathway\nmapping to identify high-risk individuals and uncover potential therapeutic\ntargets. Using the Pima Indian dataset, logistic regression and t-tests were\napplied to identify key predictors of T2DM, yielding an overall model accuracy\nof 78.43%. To bridge predictive analytics with biological relevance, we\ndeveloped a pathway mapping strategy that links identified predictors to\ncritical signaling networks, including insulin signaling, AMPK, and PPAR\npathways. This approach provides mechanistic insights without requiring direct\nmolecular data. Building upon these connections, we propose therapeutic\nstrategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1\nmodulators, and phytochemical, further validated through pathway enrichment\nanalyses. Overall, this framework advances precision medicine by offering\ninterpretable and scalable solutions for early detection and targeted\nintervention in metabolic disorders. The key contributions of this work are:\n(1) development of an ML framework combining logistic regression and principal\ncomponent analysis (PCA) for T2DM risk prediction; (2) introduction of a\ngene-agnostic pathway mapping approach to generate mechanistic insights; and\n(3) identification of novel therapeutic strategies tailored for high-risk\npopulations."}
{"id": "2509.18762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs."}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process."}
{"id": "2509.18141", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18141", "abs": "https://arxiv.org/abs/2509.18141", "authors": ["Yao Zhao", "Haoyue Sun", "Yantian Ding", "Yanxun Xu"], "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots", "comment": null, "summary": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots\nprovides valuable insights for evidence synthesis in clinical research.\nHowever, existing approaches often rely on manual digitization, which is\nerror-prone and lacks scalability. To address these limitations, we develop\nKM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD\ndirectly from KM plots with high accuracy, robustness, and reproducibility.\nKM-GPT integrates advanced image preprocessing, multi-modal reasoning powered\nby GPT-5, and iterative reconstruction algorithms to generate high-quality IPD\nwithout manual input or intervention. Its hybrid reasoning architecture\nautomates the conversion of unstructured information into structured data flows\nand validates data extraction from complex KM plots. To improve accessibility,\nKM-GPT is equipped with a user-friendly web interface and an integrated AI\nassistant, enabling researchers to reconstruct IPD without requiring\nprogramming expertise. KM-GPT was rigorously evaluated on synthetic and\nreal-world datasets, consistently demonstrating superior accuracy. To\nillustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer\nimmunotherapy trials, reconstructing IPD to facilitate evidence synthesis and\nbiomarker-based subgroup analyses. By automating traditionally manual processes\nand providing a scalable, web-based solution, KM-GPT transforms clinical\nresearch by leveraging reconstructed IPD to enable more informed downstream\nanalyses, supporting evidence-based decision-making."}
{"id": "2509.18775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings."}
{"id": "2509.18846", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting."}
{"id": "2509.18144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18144", "abs": "https://arxiv.org/abs/2509.18144", "authors": ["Yubo Yang", "Yichen Zhu", "Bo Jiang"], "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation", "comment": "9 pages", "summary": "Spatio-temporal data abounds in domain like traffic and environmental\nmonitoring. However, it often suffers from missing values due to sensor\nmalfunctions, transmission failures, etc. Recent years have seen continued\nefforts to improve spatio-temporal data imputation performance. Recently\ndiffusion models have outperformed other approaches in various tasks, including\nspatio-temporal imputation, showing competitive performance. Extracting and\nutilizing spatio-temporal dependencies as conditional information is vital in\ndiffusion-based methods. However, previous methods introduce error accumulation\nin this process and ignore the variability of the dependencies in the noisy\ndata at different diffusion steps. In this paper, we propose AdaSTI (Adaptive\nDependency Model in Diffusion-based Spatio-Temporal Imputation), a novel\nspatio-temporal imputation approach based on conditional diffusion model.\nInside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model\nfor pre-imputation with the imputed result used to extract conditional\ninformation by our designed Spatio-Temporal Conditionalizer (STC)network. We\nalso propose a Noise-Aware Spatio-Temporal (NAST) network with a gated\nattention mechanism to capture the variant dependencies across diffusion steps.\nExtensive experiments on three real-world datasets show that AdaSTI outperforms\nexisting methods in all the settings, with up to 46.4% reduction in imputation\nerror."}
{"id": "2509.18776", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices."}
{"id": "2509.18849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach."}
{"id": "2509.18145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18145", "abs": "https://arxiv.org/abs/2509.18145", "authors": ["Syed Ahmad Chan Bukhari", "Amritpal Singh", "Shifath Hossain", "Iram Wajahat"], "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records", "comment": "7 pages, 3 Figure", "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping\nsigns of physiological deterioration that require timely escalation of care.\nTraditional early warning systems, such as SOFA or MEWS, are limited by their\nfocus on single outcomes and fail to capture the multi-dimensional nature of\nclinical decline. This study proposes a multi-label classification framework to\npredict Care Escalation Triggers (CETs), including respiratory failure,\nhemodynamic instability, renal compromise, and neurological deterioration,\nusing the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are\ndefined through rule-based criteria applied to data from hours 24 to 72 (for\nexample, oxygen saturation below 90, mean arterial pressure below 65 mmHg,\ncreatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale\nscore greater than 2). Features are extracted from the first 24 hours and\ninclude vital sign aggregates, laboratory values, and static demographics. We\ntrain and evaluate multiple classification models on a cohort of 85,242 ICU\nstays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation\nmetrics include per-label precision, recall, F1-score, and Hamming loss.\nXGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,\n0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,\noutperforming baseline models. Feature analysis shows that clinically relevant\nparameters such as respiratory rate, blood pressure, and creatinine are the\nmost influential predictors, consistent with the clinical definitions of the\nCETs. The proposed framework demonstrates practical potential for early,\ninterpretable clinical alerts without requiring complex time-series modeling or\nnatural language processing."}
{"id": "2509.18792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs."}
{"id": "2509.18864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B."}
{"id": "2509.18147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18147", "abs": "https://arxiv.org/abs/2509.18147", "authors": ["Xinyu Mu", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks", "comment": null, "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations."}
{"id": "2509.18813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX."}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment."}
{"id": "2509.18150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18150", "abs": "https://arxiv.org/abs/2509.18150", "authors": ["Kean Shi", "Liang Chen", "Haozhe Zhao", "Baobao Chang"], "title": "Sparse Training Scheme for Multimodal LLM", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency."}
{"id": "2509.18843", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b."}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research."}
{"id": "2509.18151", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18151", "abs": "https://arxiv.org/abs/2509.18151", "authors": ["Jindi Lv", "Yuhao Zhou", "Yuxin Tian", "Qing Ye", "Wentao Feng", "Jiancheng Lv"], "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork", "comment": null, "summary": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples."}
{"id": "2509.18862", "categories": ["cs.CL", "I.2.7; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods."}
{"id": "2509.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/."}
{"id": "2509.18152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18152", "abs": "https://arxiv.org/abs/2509.18152", "authors": ["Zhenyu Qi", "Qing Yu", "Jichen Wang", "Yun-Bo Zhao", "Zerui Li", "Wenjun Lv"], "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation", "comment": null, "summary": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data."}
{"id": "2509.18880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection."}
{"id": "2509.18942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency."}
{"id": "2509.18153", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.18153", "abs": "https://arxiv.org/abs/2509.18153", "authors": ["Hanqun Cao", "Marcelo D. T. Torres", "Jingjie Zhang", "Zijun Gao", "Fang Wu", "Chunbin Gu", "Jure Leskovec", "Yejin Choi", "Cesar de la Fuente-Nunez", "Guangyong Chen", "Pheng-Ann Heng"], "title": "A deep reinforcement learning platform for antibiotic discovery", "comment": "42 pages, 16 figures", "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths\nannually by 2050, underscoring the urgent need for new antibiotics. Here we\npresent ApexAmphion, a deep-learning framework for de novo design of\nantibiotics that couples a 6.4-billion-parameter protein language model with\nreinforcement learning. The model is first fine-tuned on curated peptide data\nto capture antimicrobial sequence regularities, then optimised with proximal\npolicy optimization against a composite reward that combines predictions from a\nlearned minimum inhibitory concentration (MIC) classifier with differentiable\nphysicochemical objectives. In vitro evaluation of 100 designed peptides showed\nlow MIC values (nanomolar range in some cases) for all candidates (100% hit\nrate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial\nactivity against at least two clinically relevant bacteria. The lead molecules\nkilled bacteria primarily by potently targeting the cytoplasmic membrane. By\nunifying generation, scoring and multi-objective optimization with deep\nreinforcement learning in a single pipeline, our approach rapidly produces\ndiverse, potent candidates, offering a scalable route to peptide antibiotics\nand a platform for iterative steering toward potency and developability within\nhours."}
{"id": "2509.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovič", "Michael Färber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com"}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems."}
{"id": "2509.18154", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18154", "abs": "https://arxiv.org/abs/2509.18154", "authors": ["Tianyu Yu", "Zefan Wang", "Chongyi Wang", "Fuwei Huang", "Wenshuo Ma", "Zhihui He", "Tianchi Cai", "Weize Chen", "Yuxiang Huang", "Yuanqian Zhao", "Bokai Xu", "Junbo Cui", "Yingjing Xu", "Liqing Ruan", "Luoyuan Zhang", "Hanyu Liu", "Jingkun Tang", "Hongyuan Liu", "Qining Guo", "Wenhao Hu", "Bingxiang He", "Jie Zhou", "Jie Cai", "Ji Qi", "Zonghao Guo", "Chi Chen", "Guoyang Zeng", "Yuxuan Li", "Ganqu Cui", "Ning Ding", "Xu Han", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe", "comment": "Project Website: https://github.com/OpenBMB/MiniCPM-V", "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B."}
{"id": "2509.18987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions."}
{"id": "2509.18980", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results."}
{"id": "2509.18161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18161", "abs": "https://arxiv.org/abs/2509.18161", "authors": ["William H Patty"], "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks", "comment": null, "summary": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency."}
{"id": "2509.19020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases."}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources."}
{"id": "2509.18162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication."}
{"id": "2509.19033", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field."}
{"id": "2509.19030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Hervé", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation"}
{"id": "2509.18164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18164", "abs": "https://arxiv.org/abs/2509.18164", "authors": ["Ranfei Chen", "Ming Chen"], "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture\nfollowing auto regressive models. Their denoising process offers a powerful\ngenerative advantage, but they present significant challenges in learning and\nunderstanding numerically sensitive mathematical and order-sensitive logical\ntasks. Current training methods, including pre-training, fine-tuning, and\nreinforcement learning, focus primarily on improving general knowledge\nretention and reasoning abilities, but lack a comprehensive understanding of\nmathematical and logical patterns. We propose DSFT, a simple yet effective\nDiffusion SFT strategy, by adjusting the masking strategy and loss function,\nguiding models to understand mathematical and logical patterns. This strategy\ncan be flexibly combined with pre-training, reinforcement learning, and other\ntraining methods. Validated on models such as LLaDA and Dream series, we prove\nthat DSFT on small-scale data can achieve improvements of 5-10% and\napproximately 2% on mathematical and logical problems, respectively. This\ninspiring masking approach offers insights for future learning of specific\npatterns, which can be easily and efficiently combined with other training\nmethods and applied to various dLLMs. Our code is publicly available at\nhttps://anonymous.4open.science/r/DSFT-0FFB/"}
{"id": "2509.19094", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases."}
{"id": "2509.19058", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches."}
{"id": "2509.18166", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model."}
{"id": "2509.19108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus."}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs."}
{"id": "2509.18169", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems."}
{"id": "2509.19109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation."}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit."}
{"id": "2509.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18171", "abs": "https://arxiv.org/abs/2509.18171", "authors": ["Zhanting Zhou", "KaHou Tam", "Zeqin Wu", "Pengzhao Sun", "Jinbo Wang", "Fengli Zhang"], "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification", "comment": null, "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate."}
{"id": "2509.19125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability."}
{"id": "2509.19265", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings."}
{"id": "2509.18172", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18172", "abs": "https://arxiv.org/abs/2509.18172", "authors": ["Wonjun Bang", "Jongseok Park", "Hongseung Yu", "Kyungmin Bin", "Kyunghan Lee"], "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization", "comment": "9 pages, 4 figures", "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime."}
{"id": "2509.19143", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse."}
{"id": "2509.18173", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}"}
{"id": "2509.19163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments."}
{"id": "2509.18200", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation."}
{"id": "2509.19170", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model."}
{"id": "2509.18208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18208", "abs": "https://arxiv.org/abs/2509.18208", "authors": ["Boyuan Zhang", "Yingjun Du", "Xiantong Zhen", "Ling Shao"], "title": "Variational Task Vector Composition", "comment": null, "summary": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition."}
{"id": "2509.19199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios."}
{"id": "2509.18353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18353", "abs": "https://arxiv.org/abs/2509.18353", "authors": ["Jakub Adamczyk", "Jakub Poziemski", "Franciszek Job", "Mateusz Król", "Maciej Makowski"], "title": "MolPILE - large-scale, diverse dataset for molecular representation learning", "comment": null, "summary": "The size, diversity, and quality of pretraining datasets critically determine\nthe generalization ability of foundation models. Despite their growing\nimportance in chemoinformatics, the effectiveness of molecular representation\nlearning has been hindered by limitations in existing small molecule datasets.\nTo address this gap, we present MolPILE, large-scale, diverse, and rigorously\ncurated collection of 222 million compounds, constructed from 6 large-scale\ndatabases using an automated curation pipeline. We present a comprehensive\nanalysis of current pretraining datasets, highlighting considerable\nshortcomings for training ML models, and demonstrate how retraining existing\nmodels on MolPILE yields improvements in generalization performance. This work\nprovides a standardized resource for model training, addressing the pressing\nneed for an ImageNet-like dataset in molecular chemistry."}
{"id": "2509.19212", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness."}
{"id": "2509.18362", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18362", "abs": "https://arxiv.org/abs/2509.18362", "authors": ["Yuxuan Cai", "Xiaozhuan Liang", "Xinghua Wang", "Jin Ma", "Haijin Liang", "Jinwen Luo", "Xinyu Zuo", "Lisheng Duan", "Yuyang Yin", "Xi Chen"], "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "comment": null, "summary": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference."}
{"id": "2509.19224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications."}
{"id": "2509.18367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18367", "abs": "https://arxiv.org/abs/2509.18367", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data", "comment": null, "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks."}
{"id": "2509.19228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."}
{"id": "2509.18376", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18376", "abs": "https://arxiv.org/abs/2509.18376", "authors": ["Burouj Armgaan", "Eshan Jain", "Harsh Pandey", "Mahesh Chandran", "Sayan Ranu"], "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability", "comment": "31 pages, 20 figures, NeurIPS 2025 (Oral)", "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants."}
{"id": "2509.19249", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance."}
{"id": "2509.18386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection."}
{"id": "2509.19269", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective."}
{"id": "2509.18389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning."}
{"id": "2509.19270", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Božík", "Marek Šuppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models."}
{"id": "2509.18396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18396", "abs": "https://arxiv.org/abs/2509.18396", "authors": ["Doğay Altınel"], "title": "Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules", "comment": "24 pages", "summary": "Deep learning optimizers are optimization algorithms that enable deep neural\nnetworks to learn. The effectiveness of learning is highly dependent on the\noptimizer employed in the training process. Alongside the rapid advancement of\ndeep learning, a wide range of optimizers with different approaches have been\ndeveloped. This study aims to provide a review of various optimizers that have\nbeen proposed and received attention in the literature. From Stochastic\ngradient descent to the most recent ones such as Momentum, AdamW, Sophia, and\nMuon in chronological order, optimizers are examined individually, and their\ndistinctive features are highlighted in the study. The update rule of each\noptimizer is presented in detail, with an explanation of the associated\nconcepts and variables. The techniques applied by these optimizers, their\ncontributions to the optimization process, and their default hyperparameter\nsettings are also discussed. In addition, insights are offered into the open\nchallenges encountered in the optimization of deep learning models. Thus, a\ncomprehensive resource is provided both for understanding the current state of\noptimizers and for identifying potential areas of future development."}
{"id": "2509.19271", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Frédéric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code."}
{"id": "2509.18408", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18408", "abs": "https://arxiv.org/abs/2509.18408", "authors": ["Sarwan Ali"], "title": "Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations", "comment": "Accepted to CIKM 2025 as Short paper", "summary": "We present a novel information-preserving Chaos Game Representation (CGR)\nmethod, also called Reverse-CGR (R-CGR), for biological sequence analysis that\naddresses the fundamental limitation of traditional CGR approaches - the loss\nof sequence information during geometric mapping. Our method introduces\ncomplete sequence recovery through explicit path encoding combined with\nrational arithmetic precision control, enabling perfect sequence reconstruction\nfrom stored geometric traces. Unlike purely geometric approaches, our\nreversibility is achieved through comprehensive path storage that maintains\nboth positional and character information at each step. We demonstrate the\neffectiveness of R-CGR on biological sequence classification tasks, achieving\ncompetitive performance compared to traditional sequence-based methods while\nproviding interpretable geometric visualizations. The approach generates\nfeature-rich images suitable for deep learning while maintaining complete\nsequence information through explicit encoding, opening new avenues for\ninterpretable bioinformatics analysis where both accuracy and sequence recovery\nare essential."}
{"id": "2509.19274", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies."}
{"id": "2509.18433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare."}
{"id": "2509.18445", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.18445", "abs": "https://arxiv.org/abs/2509.18445", "authors": ["Kangzheng Liu", "Leixin Ma"], "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems", "comment": "9 pages, 7 figures", "summary": "The simulation of complex physical systems using a discretized mesh is a\ncornerstone of applied mechanics, but traditional numerical solvers are often\ncomputationally prohibitive for many-query tasks. While Graph Neural Networks\n(GNNs) have emerged as powerful surrogate models for mesh-based data, their\nstandard autoregressive application for long-term prediction is often plagued\nby error accumulation and instability. To address this, we introduce\nMeshODENet, a general framework that synergizes the spatial reasoning of GNNs\nwith the continuous-time modeling of Neural Ordinary Differential Equations. We\ndemonstrate the framework's effectiveness and versatility on a series of\nchallenging structural mechanics problems, including one- and two-dimensional\nelastic bodies undergoing large, non-linear deformations. The results\ndemonstrate that our approach significantly outperforms baseline models in\nlong-term predictive accuracy and stability, while achieving substantial\ncomputational speed-ups over traditional solvers. This work presents a powerful\nand generalizable approach for developing data-driven surrogates to accelerate\nthe analysis and modeling of complex structural systems."}
{"id": "2509.18452", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "D.2.0; G.4; B.8.2"], "pdf": "https://arxiv.org/pdf/2509.18452", "abs": "https://arxiv.org/abs/2509.18452", "authors": ["Anton Lebedev", "Won Kyung Lee", "Soumyadip Ghosh", "Olha I. Yaman", "Vassilis Kalantzis", "Yingdong Lu", "Tomasz Nowicki", "Shashanka Ubaru", "Lior Horesh", "Vassil Alexandrov"], "title": "Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion", "comment": "8 pages, 3 figures, 1 algorithm, 1 table of experiment cases", "summary": "Large, sparse linear systems are pervasive in modern science and engineering,\nand Krylov subspace solvers are an established means of solving them. Yet\nconvergence can be slow for ill-conditioned matrices, so practical deployments\nusually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix\ninversion can generate such preconditioners and accelerate Krylov iterations,\nbut its effectiveness depends on parameters whose optima vary across matrices;\nmanual or grid search is costly. We present an AI-driven framework recommending\nMCMC parameters for a given linear system. A graph neural surrogate predicts\npreconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition\nfunction then chooses the parameter sets most likely to minimise iterations. On\na previously unseen ill-conditioned system, the framework achieves better\npreconditioning with 50\\% of the search budget of conventional methods,\nyielding about a 10\\% reduction in iterations to convergence. These results\nsuggest a route for incorporating MCMC-based preconditioners into large-scale\nsystems."}
{"id": "2509.18457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18457", "abs": "https://arxiv.org/abs/2509.18457", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Velarie Yaa Ansu-Baidoo", "Eric Kim", "Gautham Krishna Gudur", "Mohit Malu", "Owen Krueger", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting", "comment": null, "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively."}
{"id": "2509.18469", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18469", "abs": "https://arxiv.org/abs/2509.18469", "authors": ["Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Probabilistic Geometric Principal Component Analysis with application to neural data", "comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2025. Code is available at GitHub\n  https://github.com/ShanechiLab/PGPCA.git", "summary": "Dimensionality reduction is critical across various domains of science\nincluding neuroscience. Probabilistic Principal Component Analysis (PPCA) is a\nprominent dimensionality reduction method that provides a probabilistic\napproach unlike the deterministic approach of PCA and serves as a connection\nbetween PCA and Factor Analysis (FA). Despite their power, PPCA and its\nextensions are mainly based on linear models and can only describe the data in\na Euclidean coordinate system. However, in many neuroscience applications, data\nmay be distributed around a nonlinear geometry (i.e., manifold) rather than\nlying in the Euclidean space. We develop Probabilistic Geometric Principal\nComponent Analysis (PGPCA) for such datasets as a new dimensionality reduction\nalgorithm that can explicitly incorporate knowledge about a given nonlinear\nmanifold that is first fitted from these data. Further, we show how in addition\nto the Euclidean coordinate system, a geometric coordinate system can be\nderived for the manifold to capture the deviations of data from the manifold\nand noise. We also derive a data-driven EM algorithm for learning the PGPCA\nmodel parameters. As such, PGPCA generalizes PPCA to better describe data\ndistributions by incorporating a nonlinear manifold geometry. In simulations\nand brain data analyses, we show that PGPCA can effectively model the data\ndistribution around various given manifolds and outperforms PPCA for such data.\nMoreover, PGPCA provides the capability to test whether the new geometric\ncoordinate system better describes the data than the Euclidean one. Finally,\nPGPCA can perform dimensionality reduction and learn the data distribution both\naround and on the manifold. These capabilities make PGPCA valuable for\nenhancing the efficacy of dimensionality reduction for analysis of\nhigh-dimensional data that exhibit noise and are distributed around a nonlinear\nmanifold."}
{"id": "2509.18470", "categories": ["cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18470", "abs": "https://arxiv.org/abs/2509.18470", "authors": ["Xiaozhou Tan", "Minghui Zhao", "Mattias Cross", "Anton Ragni"], "title": "Discrete-time diffusion-like models for speech synthesis", "comment": null, "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas."}
{"id": "2509.18471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.18471", "abs": "https://arxiv.org/abs/2509.18471", "authors": ["Mariano Tepper", "Ted Willke"], "title": "Individualized non-uniform quantization for vector search", "comment": null, "summary": "Embedding vectors are widely used for representing unstructured data and\nsearching through it for semantically similar items. However, the large size of\nthese vectors, due to their high-dimensionality, creates problems for modern\nvector search techniques: retrieving large vectors from memory/storage is\nexpensive and their footprint is costly. In this work, we present NVQ\n(non-uniform vector quantization), a new vector compression technique that is\ncomputationally and spatially efficient in the high-fidelity regime. The core\nin NVQ is to use novel parsimonious and computationally efficient\nnonlinearities for building non-uniform vector quantizers. Critically, these\nquantizers are \\emph{individually} learned for each indexed vector. Our\nexperimental results show that NVQ exhibits improved accuracy compared to the\nstate of the art with a minimal computational cost."}
{"id": "2509.18480", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18480", "abs": "https://arxiv.org/abs/2509.18480", "authors": ["Yuyang Wang", "Jiarui Lu", "Navdeep Jaitly", "Josh Susskind", "Miguel Angel Bautista"], "title": "SimpleFold: Folding Proteins is Simpler than You Think", "comment": "28 pages, 11 figures, 13 tables", "summary": "Protein folding models have achieved groundbreaking results typically via a\ncombination of integrating domain knowledge into the architectural blocks and\ntraining pipelines. Nonetheless, given the success of generative models across\ndifferent but related problems, it is natural to question whether these\narchitectural designs are a necessary condition to build performant models. In\nthis paper, we introduce SimpleFold, the first flow-matching based protein\nfolding model that solely uses general purpose transformer blocks. Protein\nfolding models typically employ computationally expensive modules involving\ntriangular updates, explicit pair representations or multiple training\nobjectives curated for this specific domain. Instead, SimpleFold employs\nstandard transformer blocks with adaptive layers and is trained via a\ngenerative flow-matching objective with an additional structural term. We scale\nSimpleFold to 3B parameters and train it on approximately 9M distilled protein\nstructures together with experimental PDB data. On standard folding benchmarks,\nSimpleFold-3B achieves competitive performance compared to state-of-the-art\nbaselines, in addition SimpleFold demonstrates strong performance in ensemble\nprediction which is typically difficult for models trained via deterministic\nreconstruction objectives. Due to its general-purpose architecture, SimpleFold\nshows efficiency in deployment and inference on consumer-level hardware.\nSimpleFold challenges the reliance on complex domain-specific architectures\ndesigns in protein folding, opening up an alternative design space for future\nprogress."}
{"id": "2509.18483", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18483", "abs": "https://arxiv.org/abs/2509.18483", "authors": ["Abhijit Sen", "Illya V. Lukin", "Kurt Jacobs", "Lev Kaplan", "Andrii G. Sotnikov", "Denys I. Bondar"], "title": "Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints", "comment": null, "summary": "The prediction of quantum dynamical responses lies at the heart of modern\nphysics. Yet, modeling these time-dependent behaviors remains a formidable\nchallenge because quantum systems evolve in high-dimensional Hilbert spaces,\noften rendering traditional numerical methods computationally prohibitive.\nWhile large language models have achieved remarkable success in sequential\nprediction, quantum dynamics presents a fundamentally different challenge:\nforecasting the entire temporal evolution of quantum systems rather than merely\nthe next element in a sequence. Existing neural architectures such as recurrent\nand convolutional networks often require vast training datasets and suffer from\nspurious oscillations that compromise physical interpretability. In this work,\nwe introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)\naugmented with physics-informed loss functions that enforce the Ehrenfest\ntheorems. Our method achieves superior accuracy with significantly less\ntraining data: it requires only 5.4 percent of the samples (200) compared to\nTemporal Convolution Networks (3,700). We further introduce the Chain of KANs,\na novel architecture that embeds temporal causality directly into the model\ndesign, making it particularly well-suited for time series modeling. Our\nresults demonstrate that physics-informed KANs offer a compelling advantage\nover conventional black-box models, maintaining both mathematical rigor and\nphysical consistency while dramatically reducing data requirements."}
{"id": "2509.18499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18499", "abs": "https://arxiv.org/abs/2509.18499", "authors": ["Rachel Chung", "Pratyush Nidhi Sharma", "Mikko Siponen", "Rohit Vadodaria", "Luke Smith"], "title": "Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models", "comment": "Presented at the Association of Certified Fraud Examiners (ACFE)\n  Research Institute Annual Meeting, Las Vegas, NV, (2024)", "summary": "Money laundering is a critical global issue for financial institutions.\nAutomated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),\ncan be trained to identify illicit transactions in real time. A major issue for\ndeveloping such models is the lack of access to training data due to privacy\nand confidentiality concerns. Synthetically generated data that mimics the\nstatistical properties of real data but preserves privacy and confidentiality\nhas been proposed as a solution. However, training AML models on purely\nsynthetic datasets presents its own set of challenges. This article proposes\nthe use of hybrid datasets to augment the utility of synthetic datasets by\nincorporating publicly available, easily accessible, and real-world features.\nThese additions demonstrate that hybrid datasets not only preserve privacy but\nalso improve model utility, offering a practical pathway for financial\ninstitutions to enhance AML systems."}
{"id": "2509.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18521", "abs": "https://arxiv.org/abs/2509.18521", "authors": ["Yuzhen Zhou", "Jiajun Li", "Yusheng Su", "Gowtham Ramesh", "Zilin Zhu", "Xiang Long", "Chenyang Zhao", "Jin Pan", "Xiaodong Yu", "Ze Wang", "Kangrui Du", "Jialian Wu", "Ximeng Sun", "Jiang Liu", "Qiaolin Yu", "Hao Chen", "Zicheng Liu", "Emad Barsoum"], "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed. Most\nof these frameworks primarily rely on inference engines for rollout generation\nand training engines for policy updates. However, RL training remains\ncomputationally expensive, with rollout generation accounting for more than 90%\nof total runtime. In addition, its efficiency is often constrained by the\nlong-tail distribution of rollout response lengths, where a few lengthy\nresponses stall entire batches, leaving GPUs idle and underutilized. As model\nand rollout sizes continue to grow, this bottleneck increasingly limits\nscalability. To address this challenge, we propose Active Partial Rollouts in\nReinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the\nrollout phase, APRIL over-provisions rollout requests, terminates once the\ntarget number of responses is reached, and recycles incomplete responses for\ncontinuation in future steps. This strategy ensures that no rollouts are\ndiscarded while substantially reducing GPU idle time. Experiments show that\nAPRIL improves rollout throughput by at most 44% across commonly used RL\nalgorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%\nhigher final accuracy across tasks. Moreover, APRIL is both framework and\nhardware agnostic, already integrated into the slime RL framework, and\ndeployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies\nsystem-level and algorithmic considerations in proposing APRIL, with the aim of\nadvancing RL training efficiency and inspiring further optimizations in RL\nsystems."}
{"id": "2509.18529", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.18529", "abs": "https://arxiv.org/abs/2509.18529", "authors": ["Mingqian Ma"], "title": "Reverse-Complement Consistency for DNA Language Models", "comment": null, "summary": "A fundamental property of DNA is that the reverse complement (RC) of a\nsequence often carries identical biological meaning. However, state-of-the-art\nDNA language models frequently fail to capture this symmetry, producing\ninconsistent predictions for a sequence and its RC counterpart, which\nundermines their reliability. In this work, we introduce Reverse-Complement\nConsistency Regularization (RCCR), a simple and model-agnostic fine-tuning\nobjective that directly penalizes the divergence between a model's prediction\non a sequence and the aligned prediction on its reverse complement. We evaluate\nRCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,\nDNABERT-2) on a wide range of genomic tasks, including sequence classification,\nscalar regression, and profile prediction. Our experiments show that RCCR\nsubstantially improves RC robustness by dramatically reducing prediction flips\nand errors, all while maintaining or improving task accuracy compared to\nbaselines such as RC data augmentation and test-time averaging. By integrating\na key biological prior directly into the learning process, RCCR produces a\nsingle, intrinsically robust, and computationally efficient model fine-tuning\nrecipe for diverse biology tasks."}
{"id": "2509.18542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18542", "abs": "https://arxiv.org/abs/2509.18542", "authors": ["Qi Wang", "Hanyang Peng", "Yue Yu"], "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating\nlarge parameter sets sparsely, minimizing computational overhead. To circumvent\nthe prohibitive cost of training MoEs from scratch, recent work employs\nupcycling, reusing a single pre-trained dense model by replicating its\nfeed-forward network (FFN) layers into experts. However, this limits expert\ndiversity, as all experts originate from a single pre-trained dense model. This\npaper addresses this limitation by constructing powerful MoE models using\nexperts sourced from multiple identically-architected but disparate pre-trained\nmodels (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact\nthat these source models occupy disparate, dissonant regions of the parameter\nspace, making direct upcycling prone to severe performance degradation. To\novercome this, we propose Symphony-MoE, a novel two-stage framework designed to\nharmonize these models into a single, coherent expert mixture. First, we\nestablish this harmony in a training-free manner: we construct a shared\nbackbone via a layer-aware fusion strategy and, crucially, alleviate parameter\nmisalignment among experts using activation-based functional alignment.\nSubsequently, a single lightweight stage of router training coordinates the\nentire architecture. Experiments demonstrate that our method successfully\nintegrates experts from heterogeneous sources, achieving an MoE model that\nsignificantly surpasses baselines in multi-domain tasks and out-of-distribution\ngeneralization."}
{"id": "2509.18552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18552", "abs": "https://arxiv.org/abs/2509.18552", "authors": ["Kiril Bangachev", "Guy Bresler", "Iliyas Noman", "Yury Polyanskiy"], "title": "Global Minimizers of Sigmoid Contrastive Loss", "comment": "Author names listed in alphabetical order. NeurIPS 2025", "summary": "The meta-task of obtaining and aligning representations through contrastive\npretraining is steadily gaining importance since its introduction in CLIP and\nALIGN. In this paper we theoretically explain the advantages of synchronizing\nwith trainable inverse temperature and bias under the sigmoid loss, as\nimplemented in the recent SigLIP and SigLIP2 models of Google DeepMind.\nTemperature and bias can drive the loss function to zero for a rich class of\nconfigurations that we call $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object\nrelated to spherical codes and are parametrized by a margin $\\mathsf{m}$ and\nrelative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of\nconstellations to theoretically justify the success of SigLIP on retrieval, to\nexplain the modality gap present in SigLIP, and to identify the necessary\ndimension for producing high-quality representations. Finally, we propose a\nreparameterization of the sigmoid loss with explicit relative bias, which\nimproves training dynamics in experiments with synthetic data."}
{"id": "2509.18568", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18568", "abs": "https://arxiv.org/abs/2509.18568", "authors": ["Niharika Tewari", "Nguyen Linh Dan Le", "Mujie Liu", "Jing Ren", "Ziqi Xu", "Tabinda Sarwar", "Veeky Baths", "Feng Xia"], "title": "Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia", "comment": null, "summary": "Dementia is a progressive neurodegenerative disorder with multiple\netiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal\ndementia, and vascular dementia. Its clinical and biological heterogeneity\nmakes diagnosis and subtype differentiation highly challenging. Graph Neural\nNetworks (GNNs) have recently shown strong potential in modeling brain\nconnectivity, but their limited robustness, data scarcity, and lack of\ninterpretability constrain clinical adoption. Explainable Graph Neural Networks\n(XGNNs) have emerged to address these barriers by combining graph-based\nlearning with interpretability, enabling the identification of disease-relevant\nbiomarkers, analysis of brain network disruptions, and provision of transparent\ninsights for clinicians. This paper presents the first comprehensive review\ndedicated to XGNNs in dementia research. We examine their applications across\nAlzheimer's disease, Parkinson's disease, mild cognitive impairment, and\nmulti-disease diagnosis. A taxonomy of explainability methods tailored for\ndementia-related tasks is introduced, alongside comparisons of existing models\nin clinical scenarios. We also highlight challenges such as limited\ngeneralizability, underexplored domains, and the integration of Large Language\nModels (LLMs) for early detection. By outlining both progress and open\nproblems, this review aims to guide future work toward trustworthy, clinically\nmeaningful, and scalable use of XGNNs in dementia research."}
{"id": "2509.18573", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18573", "abs": "https://arxiv.org/abs/2509.18573", "authors": ["Dong Chen", "Jian Liu", "Chun-Long Chen", "Guo-Wei Wei"], "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials", "comment": "4 figures, 2 tables", "summary": "Porous materials exhibit vast structural diversity and support critical\napplications in gas storage, separations, and catalysis. However, predictive\nmodeling remains challenging due to the multiscale nature of structure-property\nrelationships, where performance is governed by both local chemical\nenvironments and global pore-network topology. These complexities, combined\nwith sparse and unevenly distributed labeled data, hinder generalization across\nmaterial families. We propose the Interaction Topological Transformer (ITT), a\nunified data-efficient framework that leverages novel interaction topology to\ncapture materials information across multiple scales and multiple levels,\nincluding structural, elemental, atomic, and pairwise-elemental organization.\nITT extracts scale-aware features that reflect both compositional and\nrelational structure within complex porous frameworks, and integrates them\nthrough a built-in Transformer architecture that supports joint reasoning\nacross scales. Trained using a two-stage strategy, i.e., self-supervised\npretraining on 0.6 million unlabeled structures followed by supervised\nfine-tuning, ITT achieves state-of-the-art, accurate, and transferable\npredictions for adsorption, transport, and stability properties. This framework\nprovides a principled and scalable path for learning-guided discovery in\nstructurally and chemically diverse porous materials."}
{"id": "2509.18584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18584", "abs": "https://arxiv.org/abs/2509.18584", "authors": ["Mingchun Sun", "Rongqiang Zhao", "Jie Liu"], "title": "DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation", "comment": null, "summary": "Diffusion models are the mainstream approach for time series generation\ntasks. However, existing diffusion models for time series generation require\nretraining the entire framework to introduce specific conditional guidance.\nThere also exists a certain degree of distributional bias between the generated\ndata and the real data, which leads to potential model biases in downstream\ntasks. Additionally, the complexity of diffusion models and the latent spaces\nleads to an uninterpretable inference process. To address these issues, we\npropose the data style-guided diffusion model (DS-Diffusion). In the\nDS-Diffusion, a diffusion framework based on style-guided kernels is developed\nto avoid retraining for specific conditions. The time-information based\nhierarchical denoising mechanism (THD) is developed to reduce the\ndistributional bias between the generated data and the real data. Furthermore,\nthe generated samples can clearly indicate the data style from which they\noriginate. We conduct comprehensive evaluations using multiple public datasets\nto validate our approach. Experimental results show that, compared to the\nstate-of-the-art model such as ImagenTime, the predictive score and the\ndiscriminative score decrease by 5.56% and 61.55%, respectively. The\ndistributional bias between the generated data and the real data is further\nreduced, the inference process is also more interpretable. Moreover, by\neliminating the need to retrain the diffusion model, the flexibility and\nadaptability of the model to specific conditions are also enhanced."}
{"id": "2509.18607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18607", "abs": "https://arxiv.org/abs/2509.18607", "authors": ["Qiuhai Zeng", "Sarvesh Rajkumar", "Di Wang", "Narendra Gyanchandani", "Wenbo Yan"], "title": "Reflect before Act: Proactive Error Correction in Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ninteractive decision-making tasks, but existing methods often struggle with\nerror accumulation and lack robust self-correction mechanisms. We introduce\n\"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based\ndecision-making by introducing a critical reflect step prior to taking the next\naction. This approach allows for immediate error correction, ensuring smooth\naction path and adaptibity to environment feedback. We evaluate REBACT on three\ndiverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results\ndemonstrate that REBACT significantly outperforms strong baselines, improving\nsuccess rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld\n(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using\nClaude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's\nperformance improvements are achieved with only a few modification steps,\ndemonstrating its computational efficiency."}
{"id": "2509.18611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18611", "abs": "https://arxiv.org/abs/2509.18611", "authors": ["Zituo Chen", "Sili Deng"], "title": "Flow marching for a generative PDE foundation model", "comment": null, "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal\ntrajectories has recently shown promise for building generalizable models of\ndynamical systems. Yet most existing PDE foundation models rely on\ndeterministic Transformer architectures, which lack generative flexibility for\nmany science and engineering applications. We propose Flow Marching, an\nalgorithm that bridges neural operator learning with flow matching motivated by\nan analysis of error accumulation in physical dynamical systems, and we build a\ngenerative PDE foundation model on top of it. By jointly sampling the noise\nlevel and the physical time step between adjacent states, the model learns a\nunified velocity field that transports a noisy current state toward its clean\nsuccessor, reducing long-term rollout drift while enabling uncertainty-aware\nensemble generations. Alongside this core algorithm, we introduce a\nPhysics-Pretrained Variational Autoencoder (P2VAE) to embed physical states\ninto a compact latent space, and an efficient Flow Marching Transformer (FMT)\nthat combines a diffusion-forcing scheme with latent temporal pyramids,\nachieving up to 15x greater computational efficiency than full-length video\ndiffusion models and thereby enabling large-scale pretraining at substantially\nreduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE\nfamilies and train suites of P2VAEs and FMTs at multiple scales. On downstream\nevaluation, we benchmark on unseen Kolmogorov turbulence with few-shot\nadaptation, demonstrate long-term rollout stability over deterministic\ncounterparts, and present uncertainty-stratified ensemble results, highlighting\nthe importance of generative PDE foundation models for real-world applications."}
{"id": "2509.18629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18629", "abs": "https://arxiv.org/abs/2509.18629", "authors": ["Abel Gurung", "Joseph Campbell"], "title": "HyperAdapt: Simple High-Rank Adaptation", "comment": null, "summary": "Foundation models excel across diverse tasks, but adapting them to\nspecialized applications often requires fine-tuning, an approach that is memory\nand compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate\nthis by updating only a small subset of weights. In this paper, we introduce\nHyperAdapt, a parameter-efficient fine-tuning method that significantly reduces\nthe number of trainable parameters compared to state-of-the-art methods like\nLoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying\nrow- and column-wise scaling through diagonal matrices, thereby inducing a\nhigh-rank update while requiring only $n+m$ trainable parameters for an $n\n\\times m$ matrix. Theoretically, we establish an upper bound on the rank of\nHyperAdapt's updates, and empirically, we confirm that it consistently induces\nhigh-rank transformations across model layers. Experiments on GLUE, arithmetic\nreasoning, and commonsense reasoning benchmarks with models up to 14B\nparameters demonstrate that HyperAdapt matches or nearly matches the\nperformance of full fine-tuning and state-of-the-art PEFT methods while using\norders of magnitude fewer trainable parameters."}
{"id": "2509.18653", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18653", "abs": "https://arxiv.org/abs/2509.18653", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering", "comment": "13 pages, Submitted to IEEE Transactions on Signal Processing", "summary": "We introduce a novel framework for clustering a collection of tall matrices\nbased on their column spaces, a problem we term Subspace Clustering of\nSubspaces (SCoS). Unlike traditional subspace clustering methods that assume\nvectorized data, our formulation directly models each data sample as a matrix\nand clusters them according to their underlying subspaces. We establish\nconceptual links to Subspace Clustering and Generalized Canonical Correlation\nAnalysis (GCCA), and clarify key differences that arise in this more general\nsetting. Our approach is based on a Block Term Decomposition (BTD) of a\nthird-order tensor constructed from the input matrices, enabling joint\nestimation of cluster memberships and partially shared subspaces. We provide\nthe first identifiability results for this formulation and propose scalable\noptimization algorithms tailored to large datasets. Experiments on real-world\nhyperspectral imaging datasets demonstrate that our method achieves superior\nclustering accuracy and robustness, especially under high noise and\ninterference, compared to existing subspace clustering techniques. These\nresults highlight the potential of the proposed framework in challenging\nhigh-dimensional applications where structure exists beyond individual data\nvectors."}
{"id": "2509.18703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18703", "abs": "https://arxiv.org/abs/2509.18703", "authors": ["Jakub Adamczyk"], "title": "Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology", "comment": null, "summary": "This research focuses on rational pesticide design, using graph machine\nlearning to accelerate the development of safer, eco-friendly agrochemicals,\ninspired by in silico methods in drug discovery. With an emphasis on\necotoxicology, the initial contributions include the creation of ApisTox, the\nlargest curated dataset on pesticide toxicity to honey bees. We conducted a\nbroad evaluation of machine learning (ML) models for molecular graph\nclassification, including molecular fingerprints, graph kernels, GNNs, and\npretrained transformers. The results show that methods successful in medicinal\nchemistry often fail to generalize to agrochemicals, underscoring the need for\ndomain-specific models and benchmarks. Future work will focus on developing a\ncomprehensive benchmarking suite and designing ML models tailored to the unique\nchallenges of pesticide discovery."}
{"id": "2509.18714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18714", "abs": "https://arxiv.org/abs/2509.18714", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "comment": "This paper is accepted by the 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios."}
{"id": "2509.18719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18719", "abs": "https://arxiv.org/abs/2509.18719", "authors": ["Bo Qu", "Zhurong Wang", "Daisuke Yagi", "Zhen Xu", "Yang Zhao", "Yinan Shan", "Frank Zahradnik"], "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection", "comment": "12 pages, 12 figures, ACL 2025 industry track", "summary": "This paper presents a novel approach to e-commerce payment fraud detection by\nintegrating reinforcement learning (RL) with Large Language Models (LLMs). By\nframing transaction risk as a multi-step Markov Decision Process (MDP), RL\noptimizes risk detection across multiple payment stages. Crafting effective\nreward functions, essential for RL model success, typically requires\nsignificant human expertise due to the complexity and variability in design.\nLLMs, with their advanced reasoning and coding capabilities, are well-suited to\nrefine these functions, offering improvements over traditional methods. Our\napproach leverages LLMs to iteratively enhance reward functions, achieving\nbetter fraud detection accuracy and demonstrating zero-shot capability.\nExperiments with real-world data confirm the effectiveness, robustness, and\nresilience of our LLM-enhanced RL framework through long-term evaluations,\nunderscoring the potential of LLMs in advancing industrial RL applications."}
{"id": "2509.18744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18744", "abs": "https://arxiv.org/abs/2509.18744", "authors": ["Yuqing Liu"], "title": "Theory of periodic convolutional neural network", "comment": null, "summary": "We introduce a novel convolutional neural network architecture, termed the\n\\emph{periodic CNN}, which incorporates periodic boundary conditions into the\nconvolutional layers. Our main theoretical contribution is a rigorous\napproximation theorem: periodic CNNs can approximate ridge functions depending\non $d-1$ linear variables in a $d$-dimensional input space, while such\napproximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer\nvariables). This result establishes a sharp characterization of the expressive\npower of periodic CNNs. Beyond the theory, our findings suggest that periodic\nCNNs are particularly well-suited for problems where data naturally admits a\nridge-like structure of high intrinsic dimension, such as image analysis on\nwrapped domains, physics-informed learning, and materials science. The work\nthus both expands the mathematical foundation of CNN approximation theory and\nhighlights a class of architectures with surprising and practically relevant\napproximation capabilities."}
{"id": "2509.18751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18751", "abs": "https://arxiv.org/abs/2509.18751", "authors": ["Samuel Yoon", "Jongwon Kim", "Juyoung Ha", "Young Myoung Ko"], "title": "MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model", "comment": null, "summary": "Recently reconstruction-based deep models have been widely used for time\nseries anomaly detection, but as their capacity and representation capability\nincrease, these models tend to over-generalize, often reconstructing unseen\nanomalies accurately. Prior works have attempted to mitigate this by\nincorporating a memory architecture that stores prototypes of normal patterns.\nNevertheless, these approaches suffer from high training costs and have yet to\nbe effectively integrated with time series foundation models (TFMs). To address\nthese challenges, we propose \\textbf{MOMEMTO}, a TFM for anomaly detection,\nenhanced with a patch-based memory module to mitigate over-generalization. The\nmemory module is designed to capture representative normal patterns from\nmultiple domains and enables a single model to be jointly fine-tuned across\nmultiple datasets through a multi-domain training strategy. MOMEMTO initializes\nmemory items with latent representations from a pre-trained encoder, organizes\nthem into patch-level units, and updates them via an attention mechanism. We\nevaluate our method using 23 univariate benchmark datasets. Experimental\nresults demonstrate that MOMEMTO, as a single model, achieves higher scores on\nAUC and VUS metrics compared to baseline methods, and further enhances the\nperformance of its backbone TFM, particularly in few-shot learning scenarios."}
{"id": "2509.18766", "categories": ["cs.LG", "math.OC", "stat.ML", "62J07, 68T07", "G.3"], "pdf": "https://arxiv.org/pdf/2509.18766", "abs": "https://arxiv.org/abs/2509.18766", "authors": ["Raphaël Berthier"], "title": "Diagonal Linear Networks and the Lasso Regularization Path", "comment": "29 pages, 1 figure", "summary": "Diagonal linear networks are neural networks with linear activation and\ndiagonal weight matrices. Their theoretical interest is that their implicit\nregularization can be rigorously analyzed: from a small initialization, the\ntraining of diagonal linear networks converges to the linear predictor with\nminimal 1-norm among minimizers of the training loss. In this paper, we deepen\nthis analysis showing that the full training trajectory of diagonal linear\nnetworks is closely related to the lasso regularization path. In this\nconnection, the training time plays the role of an inverse regularization\nparameter. Both rigorous results and simulations are provided to illustrate\nthis conclusion. Under a monotonicity assumption on the lasso regularization\npath, the connection is exact while in the general case, we show an approximate\nconnection."}
{"id": "2509.18810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18810", "abs": "https://arxiv.org/abs/2509.18810", "authors": ["Arman Mohammadi", "Mattias Krysander", "Daniel Jung", "Erik Frisk"], "title": "Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems", "comment": null, "summary": "Deep neural networks has been increasingly applied in fault diagnostics,\nwhere it uses historical data\n  to capture systems behavior, bypassing the need for high-fidelity physical\nmodels.\n  However, despite their competence in prediction tasks, these models often\nstruggle with\n  the evaluation of their confidence. This matter is particularly\n  important in consistency-based diagnosis where decision logic is highly\nsensitive to false alarms.\n  To address this challenge, this work presents a diagnostic framework that\nuses\n  ensemble probabilistic machine learning to\n  improve diagnostic characteristics of data driven consistency based diagnosis\n  by quantifying and automating the prediction uncertainty.\n  The proposed method is evaluated across several case studies using both\nablation\n  and comparative analyses, showing consistent improvements across a range of\ndiagnostic metrics."}
{"id": "2509.18811", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18811", "abs": "https://arxiv.org/abs/2509.18811", "authors": ["Thomas Savary", "François Rozet", "Gilles Louppe"], "title": "Training-Free Data Assimilation with GenCast", "comment": null, "summary": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts."}
{"id": "2509.18826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18826", "abs": "https://arxiv.org/abs/2509.18826", "authors": ["Wenlong Lyu", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective", "comment": "39 pages, 20 figures", "summary": "The well-known graph-based clustering methods, including spectral clustering,\nsymmetric non-negative matrix factorization, and doubly stochastic\nnormalization, can be viewed as relaxations of the kernel $k$-means approach.\nHowever, we posit that these methods excessively relax their inherent low-rank,\nnonnegative, doubly stochastic, and orthonormal constraints to ensure numerical\nfeasibility, potentially limiting their clustering efficacy. In this paper,\nguided by our theoretical analyses, we propose \\textbf{Lo}w-\\textbf{R}ank\n\\textbf{D}oubly stochastic clustering (\\textbf{LoRD}), a model that only\nrelaxes the orthonormal constraint to derive a probabilistic clustering\nresults. Furthermore, we theoretically establish the equivalence between\northogonality and block diagonality under the doubly stochastic constraint. By\nintegrating \\textbf{B}lock diagonal regularization into LoRD, expressed as the\nmaximization of the Frobenius norm, we propose \\textbf{B-LoRD}, which further\nenhances the clustering performance. To ensure numerical solvability, we\ntransform the non-convex doubly stochastic constraint into a linear convex\nconstraint through the introduction of a class probability parameter. We\nfurther theoretically demonstrate the gradient Lipschitz continuity of our LoRD\nand B-LoRD enables the proposal of a globally convergent projected gradient\ndescent algorithm for their optimization. Extensive experiments validate the\neffectiveness of our approaches. The code is publicly available at\nhttps://github.com/lwl-learning/LoRD."}
{"id": "2509.18842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18842", "abs": "https://arxiv.org/abs/2509.18842", "authors": ["Nikolas Chatzis", "Ioannis Kordonis", "Manos Theodosis", "Petros Maragos"], "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion", "comment": "5 pages, 3 figures", "summary": "Expanding neural networks during training is a promising way to augment\ncapacity without retraining larger models from scratch. However, newly added\nneurons often fail to adjust to a trained network and become inactive,\nproviding no contribution to capacity growth. We propose the Shared-Weights\nExtender (SWE), a novel method explicitly designed to prevent inactivity of new\nneurons by coupling them with existing ones for smooth integration. In\nparallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based\nmethod for allocating neurons across layers during deep network expansion. Our\nextensive benchmarking on four datasets shows that our method can effectively\nsuppress neuron inactivity and achieve better performance compared to other\nexpanding methods and baselines."}
{"id": "2509.18851", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18851", "abs": "https://arxiv.org/abs/2509.18851", "authors": ["Gongrui Nan", "Siye Chen", "Jing Huang", "Mengyu Lu", "Dexun Wang", "Chunmei Xie", "Weiqi Xiong", "Xianzhou Zeng", "Qixuan Zhou", "Yadong Li", "Xingzhong Xu"], "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization", "comment": null, "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO."}
{"id": "2509.18893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18893", "abs": "https://arxiv.org/abs/2509.18893", "authors": ["Qinhan Hou", "Yilun Zheng", "Xichun Zhang", "Sitao Luan", "Jing Tang"], "title": "Exploring Heterophily in Graph-level Tasks", "comment": "Accectped by NeurIPS 2025 Workshop, New Perspectives in Advancing\n  Graph Machine Learning (NPGML)", "summary": "While heterophily has been widely studied in node-level tasks, its impact on\ngraph-level tasks remains unclear. We present the first analysis of heterophily\nin graph-level learning, combining theoretical insights with empirical\nvalidation. We first introduce a taxonomy of graph-level labeling schemes, and\nfocus on motif-based tasks within local structure labeling, which is a popular\nlabeling scheme. Using energy-based gradient flow analysis, we reveal a key\ninsight: unlike frequency-dominated regimes in node-level tasks, motif\ndetection requires mixed-frequency dynamics to remain flexible across multiple\nspectral components. Our theory shows that motif objectives are inherently\nmisaligned with global frequency dominance, demanding distinct architectural\nconsiderations. Experiments on synthetic datasets with controlled heterophily\nand real-world molecular property prediction support our findings, showing that\nfrequency-adaptive model outperform frequency-dominated models. This work\nestablishes a new theoretical understanding of heterophily in graph-level\nlearning and offers guidance for designing effective GNN architectures."}
{"id": "2509.18904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18904", "abs": "https://arxiv.org/abs/2509.18904", "authors": ["Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction", "comment": null, "summary": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques."}
{"id": "2509.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18930", "abs": "https://arxiv.org/abs/2509.18930", "authors": ["Alex Schutz", "Victor-Alexandru Darvariu", "Efimia Panagiotaki", "Bruno Lacerda", "Nick Hawes"], "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm."}
{"id": "2509.18949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18949", "abs": "https://arxiv.org/abs/2509.18949", "authors": ["Niccolò Rocchi", "Fabio Stella", "Cassio de Campos"], "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach", "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure", "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models."}
{"id": "2509.18962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18962", "abs": "https://arxiv.org/abs/2509.18962", "authors": ["Kirsten Köbschall", "Sebastian Buschjäger", "Raphael Fischer", "Lisa Hartung", "Stefan Kramer"], "title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles", "comment": null, "summary": "Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly."}
{"id": "2509.18964", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18964", "abs": "https://arxiv.org/abs/2509.18964", "authors": ["Xingtu Liu"], "title": "Central Limit Theorems for Asynchronous Averaged Q-Learning", "comment": null, "summary": "This paper establishes central limit theorems for Polyak-Ruppert averaged\nQ-learning under asynchronous updates. We present a non-asymptotic central\nlimit theorem, where the convergence rate in Wasserstein distance explicitly\nreflects the dependence on the number of iterations, state-action space size,\nthe discount factor, and the quality of exploration. In addition, we derive a\nfunctional central limit theorem, showing that the partial-sum process\nconverges weakly to a Brownian motion."}
{"id": "2509.18968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18968", "abs": "https://arxiv.org/abs/2509.18968", "authors": ["Zhanglu Yan", "Jiayi Mao", "Qianhui Liu", "Fanfan Li", "Gang Pan", "Tao Luo", "Bowen Zhu", "Weng-Fai Wong"], "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding", "comment": null, "summary": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source."}
{"id": "2509.18990", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18990", "abs": "https://arxiv.org/abs/2509.18990", "authors": ["Carson Dudley", "Marisa Eisenberg"], "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning", "comment": null, "summary": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes."}
{"id": "2509.18993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18993", "abs": "https://arxiv.org/abs/2509.18993", "authors": ["Boao Kong", "Junzhu Liang", "Yuxi Liu", "Renjia Deng", "Kun Yuan"], "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure", "comment": "32 pages", "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory."}
{"id": "2509.18997", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18997", "abs": "https://arxiv.org/abs/2509.18997", "authors": ["Pascal Esser", "Maximilian Fleissner", "Debarghya Ghoshdastidar"], "title": "Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization", "comment": null, "summary": "Representation learning from unlabeled data has been extensively studied in\nstatistics, data science and signal processing with a rich literature on\ntechniques for dimension reduction, compression, multi-dimensional scaling\namong others. However, current deep learning models use new principles for\nunsupervised representation learning that cannot be easily analyzed using\nclassical theories. For example, visual foundation models have found tremendous\nsuccess using self-supervision or denoising/masked autoencoders, which\neffectively learn representations from massive amounts of unlabeled data.\nHowever, it remains difficult to characterize the representations learned by\nthese models and to explain why they perform well for diverse prediction tasks\nor show emergent behavior. To answer these questions, one needs to combine\nmathematical tools from statistics and optimization. This paper provides an\noverview of recent theoretical advances in representation learning from\nunlabeled data and mentions our contributions in this direction."}
{"id": "2509.19017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19017", "abs": "https://arxiv.org/abs/2509.19017", "authors": ["Hazem Dewidar", "Elena Umili"], "title": "Fully Learnable Neural Reward Machines", "comment": null, "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs)."}
{"id": "2509.19018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19018", "abs": "https://arxiv.org/abs/2509.19018", "authors": ["Teng Xiao", "Zuchao Li", "Lefei Zhang"], "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment", "comment": null, "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge."}
{"id": "2509.19032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19032", "abs": "https://arxiv.org/abs/2509.19032", "authors": ["Kashaf Ul Emaan"], "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling", "comment": null, "summary": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection."}
{"id": "2509.19063", "categories": ["cs.LG", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2509.19063", "abs": "https://arxiv.org/abs/2509.19063", "authors": ["Przemysław Spyra"], "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training", "comment": null, "summary": "The rising computational and energy demands of deep neural networks (DNNs),\ndriven largely by backpropagation (BP), challenge sustainable AI development.\nThis paper rigorously investigates three BP-free training methods: the\nForward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)\nalgorithms, tracing their progression from foundational concepts to a\ndemonstrably superior solution.\n  A robust comparative framework was established: each algorithm was\nimplemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and\nbenchmarked against an equivalent BP-trained model. Hyperparameters were\noptimized with Optuna, and consistent early stopping criteria were applied\nbased on validation performance, ensuring all models were optimally tuned\nbefore comparison.\n  Results show that MF not only competes with but consistently surpasses BP in\nclassification accuracy on its native MLPs. Its superior generalization stems\nfrom converging to a more favorable minimum in the validation loss landscape,\nchallenging the assumption that global optimization is required for\nstate-of-the-art results. Measured at the hardware level using the NVIDIA\nManagement Library (NVML) API, MF reduces energy consumption by up to 41% and\nshortens training time by up to 34%, translating to a measurably smaller carbon\nfootprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that\nexplains the efficiency gains: exposing FF's architectural inefficiencies,\nvalidating MF's computationally lean design, and challenging the assumption\nthat all BP-free methods are inherently more memory-efficient. By documenting\nthe evolution from FF's conceptual groundwork to MF's synthesis of accuracy and\nsustainability, this work offers a clear, data-driven roadmap for future\nenergy-efficient deep learning."}
{"id": "2509.19044", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19044", "abs": "https://arxiv.org/abs/2509.19044", "authors": ["Yang Li", "Chenyu Wang", "Tingrui Wang", "Yongwei Wang", "Haonan Li", "Zhunga Liu", "Quan Pan"], "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks", "comment": null, "summary": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks."}
{"id": "2509.19084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19084", "abs": "https://arxiv.org/abs/2509.19084", "authors": ["Asela Hevapathige"], "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics."}
{"id": "2509.19078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19078", "abs": "https://arxiv.org/abs/2509.19078", "authors": ["Jian Xu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes", "comment": null, "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality."}
{"id": "2509.19100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19100", "abs": "https://arxiv.org/abs/2509.19100", "authors": ["Alexander Robey"], "title": "Algorithms for Adversarially Robust Deep Learning", "comment": "PhD thesis", "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents."}
{"id": "2509.19112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19112", "abs": "https://arxiv.org/abs/2509.19112", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling", "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning."}
{"id": "2509.19098", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19098", "abs": "https://arxiv.org/abs/2509.19098", "authors": ["Adrien Prevost", "Timothee Mathieu", "Odalric-Ambrym Maillard"], "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning", "comment": null, "summary": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close."}
{"id": "2509.19120", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19120", "abs": "https://arxiv.org/abs/2509.19120", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Sajal K. Das", "Patrick Sello"], "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments."}
{"id": "2509.19122", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19122", "abs": "https://arxiv.org/abs/2509.19122", "authors": ["Chunming Ye", "Wenquan Tian", "Yalan Gao", "Songzhou Li"], "title": "Analysis on distribution and clustering of weight", "comment": "14page,16 figures", "summary": "The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model."}
{"id": "2509.19104", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19104", "abs": "https://arxiv.org/abs/2509.19104", "authors": ["Sharan Sahu", "Martin T. Wells"], "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment", "comment": "70 pages, 9 figures, 3 tables", "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates."}
{"id": "2509.19135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19135", "abs": "https://arxiv.org/abs/2509.19135", "authors": ["Wenying Luo", "Zhiyuan Lin", "Wenhao Xu", "Minghao Liu", "Zhi Li"], "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding", "comment": null, "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence."}
{"id": "2509.19128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19128", "abs": "https://arxiv.org/abs/2509.19128", "authors": ["Alexandre Piché", "Ehsan Kamaloo", "Rafael Pardinas", "Dzmitry Bahdanau"], "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio", "comment": null, "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution."}
{"id": "2509.19220", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19220", "abs": "https://arxiv.org/abs/2509.19220", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Patrick Sello", "Sajal K. Das"], "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity", "comment": null, "summary": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints."}
{"id": "2509.19159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19159", "abs": "https://arxiv.org/abs/2509.19159", "authors": ["Qingfeng Lan", "Gautham Vasan", "A. Rupam Mahmood"], "title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions", "comment": "Code release: https://github.com/qlan3/ENN", "summary": "Catastrophic forgetting has remained a significant challenge for efficient\nreinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While\nrecent works have proposed effective methods to mitigate this issue, they\nmainly focus on the algorithmic side. Meanwhile, we do not fully understand\nwhat architectural properties of neural networks lead to catastrophic\nforgetting. This study aims to fill this gap by studying the role of activation\nfunctions in the training dynamics of neural networks and their impact on\ncatastrophic forgetting in reinforcement learning setup. Our study reveals\nthat, besides sparse representations, the gradient sparsity of activation\nfunctions also plays an important role in reducing forgetting. Based on this\ninsight, we propose a new class of activation functions, elephant activation\nfunctions, that can generate both sparse outputs and sparse gradients. We show\nthat by simply replacing classical activation functions with elephant\nactivation functions in the neural networks of value-based algorithms, we can\nsignificantly improve the resilience of neural networks to catastrophic\nforgetting, thus making reinforcement learning more sample-efficient and\nmemory-efficient."}
{"id": "2509.19189", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19189", "abs": "https://arxiv.org/abs/2509.19189", "authors": ["Binghui Li", "Fengling Chen", "Zixun Huang", "Lean Wang", "Lei Wu"], "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws", "comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper", "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training."}
{"id": "2509.19197", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19197", "abs": "https://arxiv.org/abs/2509.19197", "authors": ["Abdul-Rauf Nuhu", "Parham Kebria", "Vahid Hemmati", "Benjamin Lartey", "Mahmoud Nabil Mahmoud", "Abdollah Homaifar", "Edward Tunstel"], "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness", "comment": null, "summary": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios."}
{"id": "2509.19215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19215", "abs": "https://arxiv.org/abs/2509.19215", "authors": ["Juntong Ni", "Saurabh Kataria", "Shengpu Tang", "Carl Yang", "Xiao Hu", "Wei Jin"], "title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation", "comment": "Accepted at NeurIPS 2025 Workshop on Learning from Time Series for\n  Health", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables"}
{"id": "2509.19222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19222", "abs": "https://arxiv.org/abs/2509.19222", "authors": ["Julien Delavande", "Regis Pierrard", "Sasha Luccioni"], "title": "Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models", "comment": "10 pages. Accepted as an oral presentation at the NeurIPS 2025\n  NextVid Workshop (San Diego, December 6, 2025)", "summary": "Recent advances in text-to-video (T2V) generation have enabled the creation\nof high-fidelity, temporally coherent clips from natural language prompts. Yet\nthese systems come with significant computational costs, and their energy\ndemands remain poorly understood. In this paper, we present a systematic study\nof the latency and energy consumption of state-of-the-art open-source T2V\nmodels. We first develop a compute-bound analytical model that predicts scaling\nlaws with respect to spatial resolution, temporal length, and denoising steps.\nWe then validate these predictions through fine-grained experiments on\nWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and\nlinear scaling with the number of denoising steps. Finally, we extend our\nanalysis to six diverse T2V models, comparing their runtime and energy profiles\nunder default settings. Our results provide both a benchmark reference and\npractical insights for designing and deploying more sustainable generative\nvideo systems."}
{"id": "2509.19233", "categories": ["cs.LG", "I.2.0; I.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19233", "abs": "https://arxiv.org/abs/2509.19233", "authors": ["Milad Leyli-abadi", "Antoine Marot", "Jérôme Picault"], "title": "Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation", "comment": "Accepted at ECML PKDD ML4SPS 2025 workshop", "summary": "In the context of the energy transition, with increasing integration of\nrenewable sources and cross-border electricity exchanges, power grids are\nencountering greater uncertainty and operational risk. Maintaining grid\nstability under varying conditions is a complex task, and power flow simulators\nare commonly used to support operators by evaluating potential actions before\nimplementation. However, traditional physical solvers, while accurate, are\noften too slow for near real-time use. Machine learning models have emerged as\nfast surrogates, and to improve their adherence to physical laws (e.g.,\nKirchhoff's laws), they are often trained with embedded constraints which are\nalso known as physics-informed or hybrid models. This paper presents an\nablation study to demystify hybridization strategies, ranging from\nincorporating physical constraints as regularization terms or unsupervised\nlosses, and exploring model architectures from simple multilayer perceptrons to\nadvanced graph-based networks enabling the direct optimization of physics\nequations. Using our custom benchmarking pipeline for hybrid models called\nLIPS, we evaluate these models across four dimensions: accuracy, physical\ncompliance, industrial readiness, and out-of-distribution generalization. The\nresults highlight how integrating physical knowledge impacts performance across\nthese criteria. All the implementations are reproducible and provided in the\ncorresponding Github page."}
{"id": "2509.19234", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19234", "abs": "https://arxiv.org/abs/2509.19234", "authors": ["Hesam Hosseini", "Ying Cao", "Ali H. Sayed"], "title": "Stability and Generalization of Adversarial Diffusion Training", "comment": null, "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions."}
{"id": "2509.19284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19284", "abs": "https://arxiv.org/abs/2509.19284", "authors": ["Yunzhen Feng", "Julia Kempe", "Cheng Zhang", "Parag Jain", "Anthony Hartshorn"], "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT", "comment": null, "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT."}
{"id": "wechat.2509.593b59d4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNjU2OTkyOA==&mid=2247496839&idx=1&sn=dfad9994bc65c4bca83b60c71dfebeeb&chksm=fb2c1d4eb38211e6b0ec5abc671e083343d3719445aa3dd2984e1095a2b90a1d2627cd848145#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNjU2OTkyOA==&mid=2247496839&idx=1&sn=dfad9994bc65c4bca83b60c71dfebeeb&chksm=fb2c1d4eb38211e6b0ec5abc671e083343d3719445aa3dd2984e1095a2b90a1d2627cd848145#rd", "authors": ["机器感知"], "title": "MAPO：破解<em class=\"highlight\">强化学习</em>优势失真难题，大幅提升基础模型推理性能", "comment": "Source: WeChat, Published: 2025-09-24 12:35:57", "summary": "强化学习是提升基础模型推理能力的关键技术，其中组相对策略优化（GRPO）因无需额外奖励模型而广受青睐。其核心通过优势函数排序轨迹重要性，但现有GRPO采用固定优势公式，忽略样本轨迹确定性差异，引发两大问题：优势"}
{"id": "wechat.2509.ceede26c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY1MTA4MQ==&mid=2247485275&idx=1&sn=7d0f8ba4a0f571fd53f53c3d851be419&chksm=c2f13edc23dc555985404a2efe3625cef34e4184cedcadaa3d4ca675f7ad9e7451593d1cfd55#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY1MTA4MQ==&mid=2247485275&idx=1&sn=7d0f8ba4a0f571fd53f53c3d851be419&chksm=c2f13edc23dc555985404a2efe3625cef34e4184cedcadaa3d4ca675f7ad9e7451593d1cfd55#rd", "authors": ["政管选题研究院"], "title": "基于<em class=\"highlight\">强化学习</em>的科技创新与产业融合路径推演与优化策略研究【管理学自科研究题目挖掘】【20250924】", "comment": "Source: WeChat, Published: 2025-09-24 12:30:18", "summary": "5. 仿真优化：基于强化学习训练政策优化策略6. 验证测试：选取典型民族区域进行实证验证5. 数据需求与算法数据类型来源算法民族数据人口普查社会网络分析（SNA）"}
{"id": "wechat.2509.b8e1d5b7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4NzU3MjA3OQ==&mid=2247503561&idx=1&sn=35be25d44b7b0a82211f20e9db01e10e&chksm=ea52b72532dee1579decf2a6b8342d226ba28d164adb26ddfdc4c6ae168b4be9381f69641764#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4NzU3MjA3OQ==&mid=2247503561&idx=1&sn=35be25d44b7b0a82211f20e9db01e10e&chksm=ea52b72532dee1579decf2a6b8342d226ba28d164adb26ddfdc4c6ae168b4be9381f69641764#rd", "authors": ["快手大模型"], "title": "广告出价进入“自动驾驶”时代，快手提出生成式<em class=\"highlight\">强化学习</em>出价技术！", "comment": "Source: WeChat, Published: 2025-09-24 11:05:53", "summary": "生成式强化学习有两个大方向：1. Generative Model as a world model：建立一个可以模拟不同出价策略下广告投放结果的“数字沙盒”，生成大量训练数据来增强模型学习。"}
{"id": "wechat.2509.db58f761", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247502301&idx=1&sn=19e195ecbb17ea8627e0c1c19aa7bdf0&chksm=cff718fe7388f2ab1856bc8dbf78c4a144148acc2cb6f0da92a51d23eb87a10e012ab455b715#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247502301&idx=1&sn=19e195ecbb17ea8627e0c1c19aa7bdf0&chksm=cff718fe7388f2ab1856bc8dbf78c4a144148acc2cb6f0da92a51d23eb87a10e012ab455b715#rd", "authors": ["智慧车辆与交通"], "title": "COMMTR | Towards Fair Lights：一种多智能体遮罩深度<em class=\"highlight\">强化学习</em>方法，用于走廊级公平高效的交通信号控制系统", "comment": "Source: WeChat, Published: 2025-09-24 09:00:27", "summary": "随着AI技术在交通领域的快速发展，基于深度强化学习（DRL）的自适应交通信号控制（ATSC）成为研究热点。然而，现有大多数方法以机动车行为核心，忽视了行人、公交乘客等交通弱势群体的需求，也难以兼顾公平性与效率。"}
{"id": "wechat.2509.9543f7bf", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDI1MzMyNg==&mid=2247485565&idx=1&sn=927882c6b21a2f36b0789e316966a5b3&chksm=c52336eb1b684d22423aa40e85e65c29424b09b8ee5410fae8b57ae22f244b917b4ac8edea55#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDI1MzMyNg==&mid=2247485565&idx=1&sn=927882c6b21a2f36b0789e316966a5b3&chksm=c52336eb1b684d22423aa40e85e65c29424b09b8ee5410fae8b57ae22f244b917b4ac8edea55#rd", "authors": ["人形AI智控先锋"], "title": "从蹒跚学步到奔跑如飞：<em class=\"highlight\">强化学习</em>在控制领域的进阶之路", "comment": "Source: WeChat, Published: 2025-09-24 08:16:09", "summary": "面对这些问题，强化学习并没有停滞不前，研究者们开始深入思考并探索各种解决方案，就像勇敢的冒险者在困境中寻找出路，不断尝试新的方法和技术，努力突破这些瓶颈，为强化学习在控制领域的进一步发展开辟新的道路"}
{"id": "wechat.2509.6f62b29c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NjY1MTI0Mg==&mid=2660026670&idx=1&sn=59981e8465c8d82e0ab9218ba5189c8c&chksm=8567dbc4c7e8f2b973518d5cac9d404d4e49b582896b31eee0ae55488a91d0e62db10fae7a3a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NjY1MTI0Mg==&mid=2660026670&idx=1&sn=59981e8465c8d82e0ab9218ba5189c8c&chksm=8567dbc4c7e8f2b973518d5cac9d404d4e49b582896b31eee0ae55488a91d0e62db10fae7a3a#rd", "authors": ["机器智能研究MIR"], "title": "南洋理工大学肖佳平 等 | 基于深度<em class=\"highlight\">强化学习</em>的异构机器人系统目标搜索与导航", "comment": "Source: WeChat, Published: 2025-09-24 08:00:00", "summary": "第4节提出了一个多阶段强化学习框架。第5节对所提的方法进行了仿真实验验证并讨论了结果。第6节总结了本工作并对未来工作进行了展望。· 本文作者 ·"}
{"id": "wechat.2509.852a8612", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671018&idx=1&sn=6d2a80cf3c0d87d3a7a09f672a106dc1&chksm=fda0bf7de271a1b4fbee5cd2ee63f4f9503f8a3e7b8ea3b35c91a706ba18cb88fc0174827242#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671018&idx=1&sn=6d2a80cf3c0d87d3a7a09f672a106dc1&chksm=fda0bf7de271a1b4fbee5cd2ee63f4f9503f8a3e7b8ea3b35c91a706ba18cb88fc0174827242#rd", "authors": ["专知"], "title": "<em class=\"highlight\">强化学习</em>遇见大语言模型：贯穿 LLM 生命周期的进展与应用综述", "comment": "Source: WeChat, Published: 2025-09-24 03:00:00", "summary": "近年来，以强化学习（Reinforcement Learning， RL）为核心的训练方法显著提升了大语言模型（Large Language Models， LLMs）的推理与对齐性能，特别是在理解人类意图、遵循用户指令以及增强推理能力方面。"}
{"id": "wechat.2509.f1cf2ed0", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTE1MDc0NQ==&mid=2247487186&idx=1&sn=4e1b75bbc82507fca8a46edf57d6b4b6&chksm=97c42b37c47e93d4f6a1c55b4ef44b57f8f2a55d4870496f94c9f42a3a67e16a549bcdda2a61#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTE1MDc0NQ==&mid=2247487186&idx=1&sn=4e1b75bbc82507fca8a46edf57d6b4b6&chksm=97c42b37c47e93d4f6a1c55b4ef44b57f8f2a55d4870496f94c9f42a3a67e16a549bcdda2a61#rd", "authors": ["暖通前瞻"], "title": "预测信息如何优化HVAC控制？GRU-RL算法突破<em class=\"highlight\">强化学习</em>局限", "comment": "Source: WeChat, Published: 2025-09-23 23:21:32", "summary": "传统强化学习用于空调系统控制时存在未充分利用未来预测信息的问题，本研究旨在解决将预测信息与强化学习结合以优化空调系统运行的难题。研究借助开源框架测试不同预测信息策略的影响，还提出GRU - RL算法来处理系统状"}
{"id": "wechat.2509.7b12ef85", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MDU5NTQ2MQ==&mid=2247485505&idx=2&sn=cb9f142aa431db32f1f3faacc3dc453c&chksm=ce0528ed34ae7c3630332635272ed9a14e5fb40f2453ee89b279a6b9b966092fe4f01a9ad734#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MDU5NTQ2MQ==&mid=2247485505&idx=2&sn=cb9f142aa431db32f1f3faacc3dc453c&chksm=ce0528ed34ae7c3630332635272ed9a14e5fb40f2453ee89b279a6b9b966092fe4f01a9ad734#rd", "authors": ["AI4CNS"], "title": "香港大学提出Duramax<em class=\"highlight\">强化学习</em>框架，实现心血管疾病长期精准脂质控制", "comment": "Source: WeChat, Published: 2025-09-23 15:34:06", "summary": "强化学习（Reinforcement Learning， RL）因其在序列决策优化中的优势，理论上适用于慢性病长期管理。但将其应用于CVD一级预防面临三大挑战：延迟奖励建模——如何将数十年后的心血管事件与当前治疗决策关联；"}
{"id": "wechat.2509.2dd7bd33", "categories": ["wechat.article", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247712292&idx=1&sn=1b3548eb06e5a9396de0d1ba8f1a875b&chksm=e9dd38ad0579e0cb7a1ecaa2e6f3be8861c4a31fb48413537bdee61b66ef290258946a10da54#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247712292&idx=1&sn=1b3548eb06e5a9396de0d1ba8f1a875b&chksm=e9dd38ad0579e0cb7a1ecaa2e6f3be8861c4a31fb48413537bdee61b66ef290258946a10da54#rd", "authors": ["Datawhale"], "title": "清华最新发布114页大型推理模型的<em class=\"highlight\">强化学习</em>综述", "comment": "Source: WeChat, Published: 2025-09-23 14:00:00", "summary": "本文综述了强化学习在大型语言模型 （LLMs） 推理能力发展中的最新进展，特别是自 DeepSeek-R1 发布以来，RL 已成为将 LLMs 转化为大型推理模型的基础方法。"}
{"id": "wechat.2509.ae8a64ae", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489623&idx=1&sn=a96f61d9d535c2162e64220afad90459&chksm=cf281d32cd9d24cb281ac756c8bbacd4cfa916ca9ec805cf086c0c719e1ba0e4db3350d15c0a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489623&idx=1&sn=a96f61d9d535c2162e64220afad90459&chksm=cf281d32cd9d24cb281ac756c8bbacd4cfa916ca9ec805cf086c0c719e1ba0e4db3350d15c0a#rd", "authors": ["行客科技"], "title": "AI <em class=\"highlight\">代码Agent</em>“三国志”：Grok × Claude × GPT-5 Codex", "comment": "Source: WeChat, Published: 2025-09-24 13:06:19", "summary": "一位合格的代码agent，不是“能写几段函数”，而是能稳定跑完这条闭环：感知（读仓库/工单/上下文）→ 规划（拆解任务、选工具）→ 执行（改代码/跑测/发包/GUI 操作）→ 观测（日志/测试/指标）→ 自检与回滚 → 形成可审"}
{"id": "wechat.2509.8a9dd086", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247491901&idx=1&sn=ccb9ec1739032a0eea21d8b4b8dd67fe&chksm=cfe492b6ce1bceec1948a7b2e9cb7420ac18d2bf378b344da4e4f3265f882996cb7f90acddb4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247491901&idx=1&sn=ccb9ec1739032a0eea21d8b4b8dd67fe&chksm=cfe492b6ce1bceec1948a7b2e9cb7420ac18d2bf378b344da4e4f3265f882996cb7f90acddb4#rd", "authors": ["蚂蚁开源"], "title": "智能编程助手 Neovate <em class=\"highlight\">Code</em> 正式开源", "comment": "Source: WeChat, Published: 2025-09-24 10:38:15", "summary": "它集成了 code agent 所需的核心能力。github：https：//github.com/neovateai/neovate-code 目前，Neovate Code 以 CLI 工具的形态提供，但其架构设计高度灵活，未来将支持多种客户端形态，适配更多开发场景。"}
{"id": "wechat.2509.0cde2c2d", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487526&idx=1&sn=16669046bfbcf91aac3f01416fdf605d&chksm=a7da276a8a30351bdb2b98eee6325c251843740bd12ac9de1b2b7528aa14221a3e88c43bfe95#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487526&idx=1&sn=16669046bfbcf91aac3f01416fdf605d&chksm=a7da276a8a30351bdb2b98eee6325c251843740bd12ac9de1b2b7528aa14221a3e88c43bfe95#rd", "authors": ["云谦和他的朋友们"], "title": "Neovate <em class=\"highlight\">Code</em> 现已开源", "comment": "Source: WeChat, Published: 2025-09-24 01:54:25", "summary": "市面上有这么多 Code Agent。以下是让 Neovate Code 与其他 Code Agent 不同的一些特性： 开放的 Claude Code 易于扩展 多客户端支持Claude Code 是一个很棒的代码智能体，但它不是开源的，想要用上它还得费一番力气，同时默认情况下也无"}
{"id": "wechat.2509.666178b1", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652630127&idx=1&sn=540858f320d6d69762df1e19bc0586b2&chksm=f08be02f7cabcacb3cea10fd856cd8b49d4146f13cbafdc035142af6ad9d8737a719805d3efc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652630127&idx=1&sn=540858f320d6d69762df1e19bc0586b2&chksm=f08be02f7cabcacb3cea10fd856cd8b49d4146f13cbafdc035142af6ad9d8737a719805d3efc#rd", "authors": ["新智元"], "title": "秘塔AI放大招！「边想边搜边做」，内置20+<em class=\"highlight\">智能体</em>，想法一键实现", "comment": "Source: WeChat, Published: 2025-09-24 13:04:26", "summary": "借助「Agentic Search」的多模态能力，秘塔AI可以更好的「理解」输入的内容。上传一张图片，就能「洞察」图片所属人的性格底色。图源自小红书秘塔AI按照指令，从性别、性格、习惯、MBTI、专业等角度进行了分析，甚至还能据"}
{"id": "wechat.2509.7890b25b", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MDQ0NTU3MQ==&mid=2247483856&idx=1&sn=fb24ad6a061637caa0362976c02d4eae&chksm=fc0c8898f4757f6f31b98f6da06c9d74194e4a1888d148744e76e8ca47121ad0358c577f6fa7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MDQ0NTU3MQ==&mid=2247483856&idx=1&sn=fb24ad6a061637caa0362976c02d4eae&chksm=fc0c8898f4757f6f31b98f6da06c9d74194e4a1888d148744e76e8ca47121ad0358c577f6fa7#rd", "authors": ["Hus的Ai手札记"], "title": "构建可投入生产的<em class=\"highlight\">Agentic</em>系统：来自Shopify助手的经验教训", "comment": "Source: WeChat, Published: 2025-09-24 12:18:22", "summary": "分享一篇来自于 Shopify 搭建可生产环境的 Agentic 系统，进行了逐字翻译，并保留了英语原文，删除了 \"GRPO Training and Reward Hacking\" 章节，对此章节感兴趣的可以点击文章底部\"原文阅读\""}
{"id": "wechat.2509.1dac74d5", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5ODkxNTc5Mw==&mid=2247484759&idx=1&sn=6d117076c4f2f11114c7e2303b5c0717&chksm=c12e281a3962d45abd42ce16ed58263ad0c6301d470e9dd2d3e9ef5d45be60387565a72134fc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5ODkxNTc5Mw==&mid=2247484759&idx=1&sn=6d117076c4f2f11114c7e2303b5c0717&chksm=c12e281a3962d45abd42ce16ed58263ad0c6301d470e9dd2d3e9ef5d45be60387565a72134fc#rd", "authors": ["大模型白白"], "title": "火到出圈的<em class=\"highlight\">Agentic</em> AI 概念图？超详细解析，小白一看就会！", "comment": "Source: WeChat, Published: 2025-09-24 12:11:20", "summary": "这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1 最核心：LLM's（大语言模型）=AI 的 “大脑”咱们熟悉的 GPT、文心一言都在这一层！"}
{"id": "wechat.2509.fd9089a9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTEwNTIyOA==&mid=2247500895&idx=1&sn=7888b82a09a64e0adfb56345b7e31270&chksm=c514cf7e23ea228242c96c75f1dd905a47f122f7d8dbe8fe1939a1aa543ed0fe6d4cb1f589ed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTEwNTIyOA==&mid=2247500895&idx=1&sn=7888b82a09a64e0adfb56345b7e31270&chksm=c514cf7e23ea228242c96c75f1dd905a47f122f7d8dbe8fe1939a1aa543ed0fe6d4cb1f589ed#rd", "authors": ["阿里云大数据AI平台"], "title": "大数据 AI 平台：构筑 <em class=\"highlight\">Agentic</em> AI 的核心基石", "comment": "Source: WeChat, Published: 2025-09-24 09:46:42", "summary": "在大模型能力加速进化的今天，除了推理模型和各类 Agentic 能力增强模型，世界模型同样备受关注。世界模型能够理解和遵循物理规律，具备因果推理、时间推演等能力，是大模型真正深入现实物理世界的关键。"}
{"id": "wechat.2509.0f50a9f9", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2NzYyMDgyOA==&mid=2247519830&idx=2&sn=5f8a672a785ee1dcb35da87255adf0b8&chksm=ebcf47c9f4f7eb9eb392937fc8baef42da541b99184a55d9311dadbdcbc4e2e6d7d209c6ef89#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2NzYyMDgyOA==&mid=2247519830&idx=2&sn=5f8a672a785ee1dcb35da87255adf0b8&chksm=ebcf47c9f4f7eb9eb392937fc8baef42da541b99184a55d9311dadbdcbc4e2e6d7d209c6ef89#rd", "authors": ["思瀚产业研究院"], "title": "数据即智能，<em class=\"highlight\">Agentic</em> AI 驱动存储范式改变", "comment": "Source: WeChat, Published: 2025-09-24 04:00:35", "summary": "无限上下文 数字世界映射物理世界 数据规模扩展定理 记忆规模扩展定理 环境规模扩展定理 数据决定模型智能的高度 记忆决定agentic应用智能 环境决定模型自演进 数据规模扩展定律，数据驱动模型智能"}
{"id": "wechat.2509.7caa7a75", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651227789&idx=2&sn=fa2e4db714c3a31e8a80e085414c97a8&chksm=8aaf1e9669e5ef47ed5347ca8171ab7046dad5061dd0acc53064b268b4ad25e8c6b4acfde038#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651227789&idx=2&sn=fa2e4db714c3a31e8a80e085414c97a8&chksm=8aaf1e9669e5ef47ed5347ca8171ab7046dad5061dd0acc53064b268b4ad25e8c6b4acfde038#rd", "authors": ["亚马逊云科技"], "title": "ERP不再手工操作，知微行易借<em class=\"highlight\">Agentic</em> AI 驱动企业智能运营！", "comment": "Source: WeChat, Published: 2025-09-24 03:16:05", "summary": "亚马逊云科技for software and technology agentic ai isv & startup 客户成功实践。亚马逊云科技是构建agentic al的不二选择。概述 知微行易（上海）智能科技有限公司（以下简称“知微行易”）作为专注于服务中国500强企业的智能企业运营"}
{"id": "wechat.2509.1485f51f", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5MDA5NjYyMg==&mid=2247553012&idx=2&sn=14a9a1b202c217cde26cd613be7d970b&chksm=fc5cb2439f458e3ba4a7df50ce4c62199cf03916390b0cfa0ec82be318ade592fc522d58ca50#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5MDA5NjYyMg==&mid=2247553012&idx=2&sn=14a9a1b202c217cde26cd613be7d970b&chksm=fc5cb2439f458e3ba4a7df50ce4c62199cf03916390b0cfa0ec82be318ade592fc522d58ca50#rd", "authors": ["前润母基金"], "title": "范式转移！无问芯穹推出基础设施<em class=\"highlight\">智能体</em>蜂群，开启<em class=\"highlight\">Agentic智能体</em>基础设施新纪元", "comment": "Source: WeChat, Published: 2025-09-24 02:08:56", "summary": "infinigence 释放无穹算力 无问芯穹 agentic infra： 重构人工智能及智能体生产新范式 让agi触手可及 agent应用 具身智能 自动驾驶 图片/视频生成 deep research vibe coding 无问芯穹基础设施智能体蜂群 maas 通用大模型 代码模型 视觉模型 语"}
{"id": "wechat.2509.bd3c7cb8", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDczNTMyMQ==&mid=2247485029&idx=1&sn=fc2b27cc1362d3e2b61179602c0c5000&chksm=c4f894cf3b982f0ed0ef07c93fcbfefad59aa9d0211d6c2c8505163991f4108daadd673e2372#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDczNTMyMQ==&mid=2247485029&idx=1&sn=fc2b27cc1362d3e2b61179602c0c5000&chksm=c4f894cf3b982f0ed0ef07c93fcbfefad59aa9d0211d6c2c8505163991f4108daadd673e2372#rd", "authors": ["苏州如姆AI人工智能"], "title": "【Agent专题】硬核干货！<em class=\"highlight\">Agentic</em> AI 架构全解析，一份生产就绪的实战指南", "comment": "Source: WeChat, Published: 2025-09-24 01:55:08", "summary": "agentic ai architecture agent orchestration ai agents 但实际情况是，只需将AI想象成一个能执行任务的小助手，再稍加思考，便能轻松步入代理AI架构的逻辑殿堂。"}
{"id": "wechat.2509.dbd6070c", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357070&idx=1&sn=7df71df7fe4f3dbf0bb96048ec59817a&chksm=8c90132c5ad9863f4c067eaa57e1d36000c1f27b2b744bff94966fdd75f88e1afa8ce1361951#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357070&idx=1&sn=7df71df7fe4f3dbf0bb96048ec59817a&chksm=8c90132c5ad9863f4c067eaa57e1d36000c1f27b2b744bff94966fdd75f88e1afa8ce1361951#rd", "authors": ["AI Agent 领域"], "title": "全网最火<em class=\"highlight\">Agentic</em> AI概念图解析！小白也能看懂！", "comment": "Source: WeChat, Published: 2025-09-23 23:22:23", "summary": "全网最火Agentic AI概念图解析！小白也能看懂！这张图的核心思想是，把Agentic AI体系分成了四个层层递进的部分，就像套娃一样，从最里面开始：1. LLM's（大语言模型）"}
{"id": "wechat.2509.014037e2", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4OTk2MDY0Nw==&mid=2247491732&idx=1&sn=3bf7838e2601a709f434eb05de112b1f&chksm=ce91a3145400ef292777ca2e5d740f2cbf0245d4656cdfb428349a7fde37df792103d7c335cc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4OTk2MDY0Nw==&mid=2247491732&idx=1&sn=3bf7838e2601a709f434eb05de112b1f&chksm=ce91a3145400ef292777ca2e5d740f2cbf0245d4656cdfb428349a7fde37df792103d7c335cc#rd", "authors": ["增长 Growth Croissance"], "title": "咨询 | 听听麦肯锡说企业落地<em class=\"highlight\">智能体</em>的避坑指南", "comment": "Source: WeChat, Published: 2025-09-23 16:01:00", "summary": "什么是Agentic AI？挫折是任何新技术发展过程中的自然阶段，此前其他创新技术的发展也呈现过类似规律。为总结早期经验，我们近期深入研究了麦肯锡主导的50多个智能体AI开发项目，以及市场上数十个其他相关项目，并将分析"}
{"id": "wechat.2509.69012785", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257061&idx=2&sn=bdc1e2aa94d023fc6bbfa70199430950&chksm=bcefabff39fc7fa74e3ce406bd61eae5f6dfe0d4704dea7e9ffd93876d7d1aa3a40a57417ccc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257061&idx=2&sn=bdc1e2aa94d023fc6bbfa70199430950&chksm=bcefabff39fc7fa74e3ce406bd61eae5f6dfe0d4704dea7e9ffd93876d7d1aa3a40a57417ccc#rd", "authors": ["InfoQ"], "title": "C端热战，B端暗涌：<em class=\"highlight\">大模型</em>真正的战场才刚刚开始 | 《中国<em class=\"highlight\">大模型</em>落地应用研究报告 2025》正式发布", "comment": "Source: WeChat, Published: 2025-09-24 13:41:00", "summary": "过去很长一段时间，大模型技术迭代的速度几乎是每季度一次大更新——超大参数模型、MOE、推理成本下降、多模态登场、Agent 涌现，推理模型轮番登场，似乎一切都在变得更快。"}
{"id": "wechat.2509.814e14cb", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2Mjk1MjYxNg==&mid=2247529600&idx=1&sn=11083e433686d9e037a15392b73869ae&chksm=fd60cbead2152d19a79a929c3ee1102ac0436c7bd2a51fc2e2f61f1437812d500fa3290c6855#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2Mjk1MjYxNg==&mid=2247529600&idx=1&sn=11083e433686d9e037a15392b73869ae&chksm=fd60cbead2152d19a79a929c3ee1102ac0436c7bd2a51fc2e2f61f1437812d500fa3290c6855#rd", "authors": ["搬砖小组"], "title": "爆发！", "comment": "Source: WeChat, Published: 2025-09-24 13:08:00", "summary": "大模型最大的变化是人类只需要用自然语言就可以跟机器进行交互，简单来说就是你只要会说话就行，这将大大降低了大家使用工具的门槛。三是超级AI云将配套大模型，成为下一代的计算机。"}
{"id": "wechat.2509.2aa76111", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDA5MjQ2MA==&mid=2649691546&idx=3&sn=3dd430cad0f54d4b7761ab5327214adc&chksm=bf946e3b52163e9a99fe054d7617f09654c98408df4af2f166d6a724ec10873888ca776ac4f5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDA5MjQ2MA==&mid=2649691546&idx=3&sn=3dd430cad0f54d4b7761ab5327214adc&chksm=bf946e3b52163e9a99fe054d7617f09654c98408df4af2f166d6a724ec10873888ca776ac4f5#rd", "authors": ["中文投资网"], "title": "阿里巴巴七连发！<em class=\"highlight\">大模型</em>+多模态齐亮相，AI全面升级", "comment": "Source: WeChat, Published: 2025-09-24 12:42:46", "summary": "qwen 其中最受关注的，是旗舰级大模型 qwen3-max。官方介绍其参数量突破万亿，训练数据规模达到 36T tokens，并分为 Instruct 和 Thinking 两个版本，分别侧重于指令理解和逻辑推理。"}
{"id": "wechat.2509.566ef9d2", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODIzNTc2MA==&mid=2661064648&idx=3&sn=04deb60cae0c162aca18a1b807637aac&chksm=bcdebda500c8353fdfcf2b6066803acd687529e0318444ec6fa55ca44255356aa3f666d161f6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODIzNTc2MA==&mid=2661064648&idx=3&sn=04deb60cae0c162aca18a1b807637aac&chksm=bcdebda500c8353fdfcf2b6066803acd687529e0318444ec6fa55ca44255356aa3f666d161f6#rd", "authors": ["钛媒体"], "title": "可灵AI升级模型降价30%，视频<em class=\"highlight\">大模型</em>会卷入价格战吗？", "comment": "Source: WeChat, Published: 2025-09-24 12:34:57", "summary": "视频生成大模型在投入上，无论是硬件采购还是运营成本（电力、维护等）无疑处于大模型领域的第一梯队，是资金、算力和技术密集度最高的方向之一。"}
{"id": "wechat.2509.fd7b46b9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MDM2NTY2MQ==&mid=2653846533&idx=2&sn=55605801dbf58bb822903318da2049bd&chksm=85268990b487be6f374b64c50ae1669bf8326e0657125b8bf6bc8cbdba87adfbbc7dc2ab716b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MDM2NTY2MQ==&mid=2653846533&idx=2&sn=55605801dbf58bb822903318da2049bd&chksm=85268990b487be6f374b64c50ae1669bf8326e0657125b8bf6bc8cbdba87adfbbc7dc2ab716b#rd", "authors": ["网信黑龙江"], "title": "我国人工智能<em class=\"highlight\">大模型</em>实现批量“上车”", "comment": "Source: WeChat, Published: 2025-09-24 12:34:01", "summary": "科技日报北京9月23日电 （记者崔爽）记者从23日召开的2025世界智能网联汽车大会新闻发布会上获悉，我国智能网联汽车关键技术取得突破，已建成涵盖智能座舱、自动驾驶、网联云控等在内的完整产业链体系，人工智能大模型"}
{"id": "wechat.2509.c1354f18", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3ODU2NzIwOA==&mid=2247487828&idx=2&sn=9001f2fadbcba2573752d087b27dfe91&chksm=cef3c3e23f2e515707632b4eacf41a1ded680831c01cd1eb22d29b878d6267c80bc1acb8e175#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3ODU2NzIwOA==&mid=2247487828&idx=2&sn=9001f2fadbcba2573752d087b27dfe91&chksm=cef3c3e23f2e515707632b4eacf41a1ded680831c01cd1eb22d29b878d6267c80bc1acb8e175#rd", "authors": ["欧路翻译"], "title": "欧路翻译实测10款AI<em class=\"highlight\">大模型</em>，翻译效果大对决", "comment": "Source: WeChat, Published: 2025-09-24 12:30:00", "summary": "参与测评的大模型包括：DeepSeek、OpenAI、阿里通义千问、千问文本翻译大模型、智谱GLM大模型、豆包、腾讯混元、百度文心一言、Gemini-2.5、Kimi。针对不同的文本类型，会调整不同的翻译风格，比如在翻译日常口语文本时，会采"}
{"id": "wechat.2509.238537c4", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NjU5NjQ4MA==&mid=2651247034&idx=1&sn=db9047e4a94c29cf87a0338be5846cde&chksm=8a65f7674eea91b0c26e5a0ee3e48bbfcf0a35df95047a1e22fd8bbaad5fd990781f18406089#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NjU5NjQ4MA==&mid=2651247034&idx=1&sn=db9047e4a94c29cf87a0338be5846cde&chksm=8a65f7674eea91b0c26e5a0ee3e48bbfcf0a35df95047a1e22fd8bbaad5fd990781f18406089#rd", "authors": ["生信人"], "title": "亿级参数！国产超算打造全球最大单细胞<em class=\"highlight\">大模型</em>，生物学领域的Deepseek来了！", "comment": "Source: WeChat, Published: 2025-09-24 12:28:00", "summary": "国产超算打造全球最大单细胞大模型，生物学领域的Deepseek来了！你没看错！这不是大语言模型，而是生命科学的“细胞大模型”！中国科学家用1亿个人类细胞、8亿参数，打造出全球最强单细胞基础模型——CellFM！"}
{"id": "wechat.2509.c7324b72", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247484441&idx=1&sn=b63fe911a6d6082017b18562758ac4b4&chksm=e852ee91849781ddba03cce358556ad04620a15419f2bf260379e8bbf426112f9be97b16fb99#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247484441&idx=1&sn=b63fe911a6d6082017b18562758ac4b4&chksm=e852ee91849781ddba03cce358556ad04620a15419f2bf260379e8bbf426112f9be97b16fb99#rd", "authors": ["懂点AI的海文"], "title": "一篇讲清楚AI<em class=\"highlight\">大模型</em>核心术语：向量库、训练集、多模态", "comment": "Source: WeChat, Published: 2025-09-24 12:15:53", "summary": "熟知的大模型大多是通用大模型，即全能型选手）。标记准确性：监督学习的训练集标签必须正确（比如将 输入样本“猫”标注为“狗”，模型就会学习错误规律，"}
{"id": "wechat.2509.f3901ce5", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MDA0MzY2MQ==&mid=2649549966&idx=2&sn=b6ab4e1a9748bc1c34a778ffa9037c24&chksm=89a352c75152fbc322835270e14d99013b6f94cf6f49b064f3305aeb08e62a9ec869ce5e075d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MDA0MzY2MQ==&mid=2649549966&idx=2&sn=b6ab4e1a9748bc1c34a778ffa9037c24&chksm=89a352c75152fbc322835270e14d99013b6f94cf6f49b064f3305aeb08e62a9ec869ce5e075d#rd", "authors": ["清远检察"], "title": "优秀论文丨大语言<em class=\"highlight\">模型</em>在刑事检察工作中的应用前景与实现路径", "comment": "Source: WeChat, Published: 2025-09-24 12:02:19", "summary": "大模型全称“大语言模型”（Large LanguageModel，LLM），又被称作“大规模预训练语言模型”，是指包含超大规模参数（通常在十亿个以上）的神经网络模型，本质上是指通过对海量规模的无标注自然语言数据的深度学习，具备完"}
{"id": "wechat.2509.bbd455cf", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652789871&idx=1&sn=bf4fc1f5b64abd063c6f7bde92880657&chksm=853d900544f23f7c69b14afee3c630b547d05e8733804ad96f8dbb2a01ceb0dfd82d2de1e34d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652789871&idx=1&sn=bf4fc1f5b64abd063c6f7bde92880657&chksm=853d900544f23f7c69b14afee3c630b547d05e8733804ad96f8dbb2a01ceb0dfd82d2de1e34d#rd", "authors": ["智东西"], "title": "阿里一口气发7款<em class=\"highlight\">大模型</em>！这一款或是最被低估的AI“杀手锏”", "comment": "Source: WeChat, Published: 2025-09-24 11:51:59", "summary": "智东西9月24日杭州报道，今天，在2025杭州云栖大会上，阿里一口气发布了7款大模型，其中重磅推出的通义万相Wan2.5-preview，首次实现音画一体视频生成等功能，成为阿里迄今为止最强的全模态视觉生成大模型。"}
{"id": "wechat.2509.d2049c9e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MDk2NDMwNw==&mid=2247554063&idx=1&sn=38aba8544915419308c373ab6a4d949f&chksm=fd26950e54bcae691d4e9cf754086f6e0c5883b35dbc7691a404d2c5dee401a18b4573d89dae#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MDk2NDMwNw==&mid=2247554063&idx=1&sn=38aba8544915419308c373ab6a4d949f&chksm=fd26950e54bcae691d4e9cf754086f6e0c5883b35dbc7691a404d2c5dee401a18b4573d89dae#rd", "authors": ["清华大学战略与安全研究中心"], "title": "<em class=\"highlight\">大模型</em>开源创新公地：历史演进、价值逻辑与中国叙事", "comment": "Source: WeChat, Published: 2025-09-24 11:28:07", "summary": "大模型（large models）是人工智能技术发展的最新前沿领域，也是新质生产力发展的关键技术要素，有望广泛适用于经济社会各领域任务。开源（open-source）是计算机软件发展过程中诞生的对开放共享的创新协作模式的统称。"}
{"id": "wechat.2509.67cb0be0", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNzA5MjEyNQ==&mid=2247548699&idx=2&sn=80a3c1b75b2cd4cafb0a91f445d6286a&chksm=fb4b63ffcd88444a07e3b109f206eee6b7620bd388033066c7612f468a84b735573b6211c7e6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNzA5MjEyNQ==&mid=2247548699&idx=2&sn=80a3c1b75b2cd4cafb0a91f445d6286a&chksm=fb4b63ffcd88444a07e3b109f206eee6b7620bd388033066c7612f468a84b735573b6211c7e6#rd", "authors": ["装备强国"], "title": "我国人工智能<em class=\"highlight\">大模型</em>实现批量“上车”", "comment": "Source: WeChat, Published: 2025-09-24 11:20:07", "summary": "组织建设综合交通运输大模型，加快普及智能体应用。作为我国首个经国务院批准的国家级智能网联汽车专业会议，自2018年起，世界智能网联汽车大会已连续举办七届。"}
{"id": "wechat.2509.61c3b3ba", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585622&idx=3&sn=9ca738e956632aceb8f5181f6ad50364&chksm=fa9b0686c0143920a02bb137a9e30980956849975e2a396ed298cc81a6b720f519d2f141e884#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585622&idx=3&sn=9ca738e956632aceb8f5181f6ad50364&chksm=fa9b0686c0143920a02bb137a9e30980956849975e2a396ed298cc81a6b720f519d2f141e884#rd", "authors": ["AI思想会"], "title": "ICML 2025 | 会做题≠会思考？首个反例驱动推理基准：揭穿<em class=\"highlight\">大模型</em>“刷题式假象”", "comment": "Source: WeChat, Published: 2025-09-24 11:11:10", "summary": "“大模型能解高数题了，但它是真的理解了数学概念，还是只背会了题库套路？”随着大语言模型（LLMs）在数学领域的应用越来越广，“模型是否真的具备数学推理能力” 成了学界热议的焦点。"}
{"id": "wechat.2509.37fbfb76", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwMDQ4NQ==&mid=2247500253&idx=2&sn=19538621cf50a6744abd23c23549309b&chksm=fa2cef2d51a7ed5486e06328b66b9b7e69e60a73c1534b3a5476fc80ee5abb2f7f1bcf8c6eac#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwMDQ4NQ==&mid=2247500253&idx=2&sn=19538621cf50a6744abd23c23549309b&chksm=fa2cef2d51a7ed5486e06328b66b9b7e69e60a73c1534b3a5476fc80ee5abb2f7f1bcf8c6eac#rd", "authors": ["振邦天然气LNG新能源"], "title": "燃气发电首个垂类<em class=\"highlight\">大模型</em>发布", "comment": "Source: WeChat, Published: 2025-09-24 11:00:00", "summary": "京能集团发布行业首个燃机大模型——京能“擎睿”燃机大模型。作为首个燃气发电领域垂类大模型，京能“擎睿”燃机大模型依托全栈国产算力底座，自主可控、训推一体，实现从软硬件开发到落地应用的全链路突破，助力“"}
{"id": "wechat.2509.1b3d267c", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NTE1Mjg4MA==&mid=2247495486&idx=1&sn=0f6e3e2923f03b10c3af6d8e86345d1c&chksm=fc7086fb0a82771bfde4e3a278276a628be7eb59add0f6c030b5353dfdd0cc31742d7a898be7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NTE1Mjg4MA==&mid=2247495486&idx=1&sn=0f6e3e2923f03b10c3af6d8e86345d1c&chksm=fc7086fb0a82771bfde4e3a278276a628be7eb59add0f6c030b5353dfdd0cc31742d7a898be7#rd", "authors": ["苍何"], "title": "2025云栖大会，阿里亮出王牌，万亿参数和全模态<em class=\"highlight\">大模型</em>齐发，我直接好家伙！", "comment": "Source: WeChat, Published: 2025-09-24 10:57:28", "summary": "2、qwen3-omni：这是首个原生端到端全模态开源大模型，将文本、图像、音频和视频统一在一个模型中，无需权衡模态，能够无缝处理文本、图像、音频和视频等多种输入形式。"}
{"id": "wechat.2509.f0d8812c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2MDAxNjMxNw==&mid=2247503613&idx=2&sn=b7fd219792f2e497d8234aec76546197&chksm=ebf64c7656a456dae72cb92e53dc867aedb1aa70bc881c30cb07c355f8bd02d397eb2b7f05e1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2MDAxNjMxNw==&mid=2247503613&idx=2&sn=b7fd219792f2e497d8234aec76546197&chksm=ebf64c7656a456dae72cb92e53dc867aedb1aa70bc881c30cb07c355f8bd02d397eb2b7f05e1#rd", "authors": ["车市物语"], "title": "<em class=\"highlight\">大模型</em>技术如何重塑智能驾驶?", "comment": "Source: WeChat, Published: 2025-09-24 10:51:03", "summary": "大模型处理语言已经对算力有了非常大的需求，理解视频将需要大得多的算力。华为此前发表的2035年10大趋势中就预测，2035年对算力需求是现在的10万倍。"}
{"id": "wechat.2509.06834357", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4OTc4MzI5OA==&mid=2247852911&idx=1&sn=e7c803863b8caa3ed4ed48b4223788ba&chksm=eda3cf473cbb8b6705e51b81928796f565ed093d34466917c0c35fc63baf91f0b9540319fe0b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4OTc4MzI5OA==&mid=2247852911&idx=1&sn=e7c803863b8caa3ed4ed48b4223788ba&chksm=eda3cf473cbb8b6705e51b81928796f565ed093d34466917c0c35fc63baf91f0b9540319fe0b#rd", "authors": ["云头条"], "title": "<em class=\"highlight\">大模型</em>将吞噬软件", "comment": "Source: WeChat, Published: 2025-09-24 10:44:32", "summary": "大模型作为下一代的操作系统，将允许任何人用自然语言，创造无限多的应用。未来几乎所有与计算世界打交道的软件可能都是由大模型产生的 Agent，而不是现在的商业软件。"}
{"id": "wechat.2509.cb867b0f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjIwNDgzMA==&mid=2247502113&idx=1&sn=82943d6d36783dad74b82a498eb255ec&chksm=c056cb5d549c4a1347127e450522e6b0b6ea8c748612a83bbe8b9dfc518eca7b52d3c9c12203#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjIwNDgzMA==&mid=2247502113&idx=1&sn=82943d6d36783dad74b82a498eb255ec&chksm=c056cb5d549c4a1347127e450522e6b0b6ea8c748612a83bbe8b9dfc518eca7b52d3c9c12203#rd", "authors": ["BME前沿"], "title": "AI重塑心脏介入手术 中国首款心血管AI<em class=\"highlight\">大模型</em>发布", "comment": "Source: WeChat, Published: 2025-09-24 10:00:27", "summary": "打造中国首个心血管AI-OCT大模型腔内影像技术，尤其是光学相干断层成像（OCT），因具备10—20μm级超高分辨率，可清晰呈现血管内斑块性质、支架贴壁等细微结构，已成为心血管疾病的血管介入治疗（PCI）手术中不可或缺的“"}
{"id": "wechat.2509.e8d8fb5a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5ODY1NTAyMQ==&mid=2247520291&idx=1&sn=c278bd6b293aad819dfcc89e8859c1ed&chksm=c199a35b5c5e2f80dfd718e0e5bc24053c5210b77c4faf73baf75c8a94a9b380e7f86550cee3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5ODY1NTAyMQ==&mid=2247520291&idx=1&sn=c278bd6b293aad819dfcc89e8859c1ed&chksm=c199a35b5c5e2f80dfd718e0e5bc24053c5210b77c4faf73baf75c8a94a9b380e7f86550cee3#rd", "authors": ["华东师范大学上海智能教育研究院"], "title": "科研团队风采 | 郝昊副研究员：探索教育<em class=\"highlight\">大模型</em>数据与算法的新境", "comment": "Source: WeChat, Published: 2025-09-24 10:00:13", "summary": "郝昊 研究方向 教育大模型数据工程及系统设计 训练算法创新 个人简介 郝昊，毕业于华东师范大学计算机科学与技术学院，在上海交通大学自然科学研究院从事博士后研究，现为华东师范大学上海智能教育研究院副研究员。"}
{"id": "wechat.2509.34199f78", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247656802&idx=2&sn=0972f6ae3ba882028f559b0978d52d1d&chksm=c05c0b64b4d00508ffc4e1912a00574a1d1361b3409479dbe9763dffb9d4367c114a5750c0e6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247656802&idx=2&sn=0972f6ae3ba882028f559b0978d52d1d&chksm=c05c0b64b4d00508ffc4e1912a00574a1d1361b3409479dbe9763dffb9d4367c114a5750c0e6#rd", "authors": ["DataFunSummit"], "title": "360视角：<em class=\"highlight\">大模型</em>幻觉问题及其解决方案的深度探索与实践", "comment": "Source: WeChat, Published: 2025-09-24 10:00:00", "summary": "1. 什么是大模型幻觉 首先介绍一下什么是大模型的幻觉。在某些情况下，我们在使用大模型生成结果时，会有一个直观的感受，就是“一本正经的胡说八道”。"}
{"id": "wechat.2509.e8d80dd4", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NjcwMjQyNA==&mid=2247489075&idx=1&sn=0d701662635eea767a34ffd0dae640d5&chksm=e80af4908d0259b8a4a57d767a8034d4bebbca173c4ed259486922a3b20b8e096fdd62fd7c78#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NjcwMjQyNA==&mid=2247489075&idx=1&sn=0d701662635eea767a34ffd0dae640d5&chksm=e80af4908d0259b8a4a57d767a8034d4bebbca173c4ed259486922a3b20b8e096fdd62fd7c78#rd", "authors": ["计算杂谈"], "title": "云栖大会炸场！阿里通义全家桶来了，万亿参数国产<em class=\"highlight\">大模型</em>已悄然破局！", "comment": "Source: WeChat, Published: 2025-09-24 09:56:59", "summary": "大模型正展现出变革性的能力与发展潜力，其发展态势对未来影响深远。当下，大模型具备了Tool Use能力，这一能力意义重大，就如同人类创造和使用工具开启了加速进化历程一样，如今大模型凭借此能力能够连接各类数字化工"}
{"id": "wechat.2509.3b2f5702", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNjI3ODAwNw==&mid=2247575291&idx=2&sn=16bd71fa3d354ca43561e806c218b432&chksm=c0d2ac58e543552e341e9ad7ac6845949729feeef7a130f9d8c94138442646de1d4406acc9b1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNjI3ODAwNw==&mid=2247575291&idx=2&sn=16bd71fa3d354ca43561e806c218b432&chksm=c0d2ac58e543552e341e9ad7ac6845949729feeef7a130f9d8c94138442646de1d4406acc9b1#rd", "authors": ["CSDN程序人生"], "title": "GitHub 总计超 77,000 Star，<em class=\"highlight\">大模型</em>推理框架 vLLM、SGLang 是如何炼成的？", "comment": "Source: WeChat, Published: 2025-09-24 09:52:08", "summary": "大语言模型（LLM）的训练过程因巨大的计算需求和突破性的成果而备受关注，然而决定这些模型在现实世界中实用性和广泛采用的关键，却是在推理（Inference）阶段的效率、成本和延迟。"}
{"id": "wechat.2509.5a1a25ce", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&mid=2691560999&idx=1&sn=a87c8b9216ea54f34ff4e386b67a44c2&chksm=a851269dfe694128708d9779c450519cad3f05255c4c44976a9d0f6844b711faf733023eb922#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&mid=2691560999&idx=1&sn=a87c8b9216ea54f34ff4e386b67a44c2&chksm=a851269dfe694128708d9779c450519cad3f05255c4c44976a9d0f6844b711faf733023eb922#rd", "authors": ["腾讯科技"], "title": "斯坦福最新论文，揭秘大语言<em class=\"highlight\">模型</em>心智理论的基础", "comment": "Source: WeChat, Published: 2025-09-24 09:42:30", "summary": "从去年开始，Anthropic的一系列研究逐渐揭开了大模型”心理学“的序幕，也开始让人们逐步了解到，模型可能具有欺骗、自保等看起来非常有”自主意识“的行为。"}
{"id": "wechat.2509.93d5a700", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MzA0MTA3OA==&mid=2449114909&idx=3&sn=eec4b7798ab80fdffe780e50548c197a&chksm=8a89f67aac0c2aba9457c09069c58da4e60919466668927509059a90ff6dc9c519f2b25cb92e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MzA0MTA3OA==&mid=2449114909&idx=3&sn=eec4b7798ab80fdffe780e50548c197a&chksm=8a89f67aac0c2aba9457c09069c58da4e60919466668927509059a90ff6dc9c519f2b25cb92e#rd", "authors": ["社科学术汇"], "title": "“人工智能+”国家战略下的40个AI<em class=\"highlight\">大模型</em>顶刊选题", "comment": "Source: WeChat, Published: 2025-09-24 09:36:01", "summary": "第二部分：大语言模型与语义提取。1、transformer架构与自注意力机制：nlp的技术基础与社会网络语义嵌入。2、bert与ernie模型：大模型从文本数据中提取结构化信息。"}
{"id": "wechat.2509.938dcbea", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491714&idx=1&sn=d8db2255fe7e72de3278724322de931a&chksm=fa9b31d6897c0f2b0b5da3ee6ca9c4bc878df0aa279c8e136e25ce149b6b7c249e8d867d411c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491714&idx=1&sn=d8db2255fe7e72de3278724322de931a&chksm=fa9b31d6897c0f2b0b5da3ee6ca9c4bc878df0aa279c8e136e25ce149b6b7c249e8d867d411c#rd", "authors": ["慕容千语"], "title": "<em class=\"highlight\">大模型</em>校招实习面试高频160问！面试前看看", "comment": "Source: WeChat, Published: 2025-09-24 09:32:05", "summary": "GPT-3。任务）。大模型校招160+面试高频题目。2026届校招。5、什么是预训练？通过自监督学习（如掩码语言建模或下一句预测）学习通用语言表示。目 的是捕获语言的基本规律和知识，为后续微调奠定基础。"}
{"id": "wechat.2509.faf42426", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MjgzNTk2OQ==&mid=2247597733&idx=1&sn=a55c1fabca5f3350e0e617c1a5541073&chksm=fc1e3ebd437b28b9d033d341b053263ddfa964fea936177e4879c04313cbd0af9e8f9f425999#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MjgzNTk2OQ==&mid=2247597733&idx=1&sn=a55c1fabca5f3350e0e617c1a5541073&chksm=fc1e3ebd437b28b9d033d341b053263ddfa964fea936177e4879c04313cbd0af9e8f9f425999#rd", "authors": ["世界人工智能大会"], "title": "《WAIC UP!》嘉宾｜加州大学伯克利分校杰出教授Stuart Russell：<em class=\"highlight\">大模型</em>规模扩张已触顶，人类价值才是未来核心", "comment": "Source: WeChat, Published: 2025-09-24 09:25:24", "summary": "文章看点——大模型规模扩张已触顶，人类价值才是未来核心主题诠释：本期《WAIC UP！》以「交错：与虎（AI）共舞」为题，回应AI深度融入人类社会所带来的时代命题：当技术的力量愈发强大甚至不可控，人类应如何与之共生"}
{"id": "wechat.2509.637cccd6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650992443&idx=1&sn=b57f6b56ba04ff9933f007e49e5c41fc&chksm=854594d232f341e4f1447c1d175617a3b8f3f6f8277f46f3f6fde22b98ac4227edd5c3d2f82e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650992443&idx=1&sn=b57f6b56ba04ff9933f007e49e5c41fc&chksm=854594d232f341e4f1447c1d175617a3b8f3f6f8277f46f3f6fde22b98ac4227edd5c3d2f82e#rd", "authors": ["机器之心"], "title": "<em class=\"highlight\">大模型</em>七连发，外国人馋透了！阿里云栖大会全栈升级够狠", "comment": "Source: WeChat, Published: 2025-09-24 09:22:53", "summary": "众所周知，大模型目前的发展趋势是上下文长度与参数规模两方面的持续扩展。Qwen3-Next 顺应大模型的发展趋势而进行设计，针对性地引入了多项创新：包括混合注意力机制、高稀疏度的 MoE 架构以及多 Token 预测（MTP）机制等核"}
{"id": "wechat.2509.eb627d05", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NzAxNTA0MQ==&mid=2247568509&idx=1&sn=b201480dfae5bcba96f18449dca0ed1c&chksm=911beaf96e16bf6a86029850e137d63e55074fda7f8b0e4f58f297d320bc1ec49349ecc6d728#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NzAxNTA0MQ==&mid=2247568509&idx=1&sn=b201480dfae5bcba96f18449dca0ed1c&chksm=911beaf96e16bf6a86029850e137d63e55074fda7f8b0e4f58f297d320bc1ec49349ecc6d728#rd", "authors": ["CLUE中文语言理解测评基准"], "title": "手机端侧<em class=\"highlight\">大模型</em>测评结果发布！（SuperCLUE-OnDevice）", "comment": "Source: WeChat, Published: 2025-09-24 09:22:29", "summary": "superclue-ondevice手机端侧大模型测评象限图 端侧大模型 云端大模型 实用主义者 卓越领导者。doubao-seed-1.6-thinking-250715（字节跳动） qwen2.5-vl-72b（阿里巴巴） gemini-2.5-pro （google） glm-4.5v（智谱ai） bluelm-2.5-3b （vivo） internvl3.5-8b（上"}
{"id": "wechat.2509.de371618", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651608959&idx=2&sn=ce8dae4c23c3f2e3604a4acf04a807f6&chksm=bc2dd223e0e12345e72fcabe2bccd5b45902f4e3fa4ac1562609a6fd2e787d1178f84d96fb0f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651608959&idx=2&sn=ce8dae4c23c3f2e3604a4acf04a807f6&chksm=bc2dd223e0e12345e72fcabe2bccd5b45902f4e3fa4ac1562609a6fd2e787d1178f84d96fb0f#rd", "authors": ["中国计算机学会"], "title": "<em class=\"highlight\">大模型</em>发展的路在何方？| CNCC大会论坛", "comment": "Source: WeChat, Published: 2025-09-24 09:22:23", "summary": "今年年初发布的 DeepSeek-R1 模型更是让中国的开源大模型一举跻身国际领先行列，掀起了全球范围的热烈讨论。在技术快速演进的浪潮中，人们不禁要思考：沿着大模型的道路，我们是否真的能够走向通用人工智能（AGI），甚至"}
{"id": "wechat.2509.52c8e82f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4NTY4NDk3Mw==&mid=2247533732&idx=1&sn=ce62dc0362bb11c4aaa3ee86eeb28c1e&chksm=cefd044a6549be2f6e0e1d5710375645b33c74a6114b864c915f0a37bb3774940965f901aa9b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4NTY4NDk3Mw==&mid=2247533732&idx=1&sn=ce62dc0362bb11c4aaa3ee86eeb28c1e&chksm=cefd044a6549be2f6e0e1d5710375645b33c74a6114b864c915f0a37bb3774940965f901aa9b#rd", "authors": ["CSDN新程序员"], "title": "听OpenAI科学家，Transformer作者Lukasz的“<em class=\"highlight\">大模型</em>第一性思考”", "comment": "Source: WeChat, Published: 2025-09-24 09:19:37", "summary": "《Attention is All You Need》不仅仅是一篇学术论文，它是大模型理论的奠基性文章，开启了人工智能新纪元的钥匙，为通往通用人工智能（AGI）打开了一扇前所未有的大门。"}
{"id": "wechat.2509.9b07698d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MjgzNTk2OQ==&mid=2247597730&idx=1&sn=ff41c74eab97ad189df0c4a162397379&chksm=fcca089057691e7228fbea6a847b36ed05ce201a8ae630198335fa6efef56104fbc5c6b2af92#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MjgzNTk2OQ==&mid=2247597730&idx=1&sn=ff41c74eab97ad189df0c4a162397379&chksm=fcca089057691e7228fbea6a847b36ed05ce201a8ae630198335fa6efef56104fbc5c6b2af92#rd", "authors": ["世界人工智能大会"], "title": "《WAIC UP!》嘉宾｜加州大学伯克利分校杰出教授Stuart Russell：<em class=\"highlight\">大模型</em>规模扩张已触顶，人类价值才是未来核心", "comment": "Source: WeChat, Published: 2025-09-24 09:13:42", "summary": "文章看点——大模型规模扩张已触顶，人类价值才是未来核心主题诠释：本期《WAIC UP！》以「交错：与虎（AI）共舞」为题，回应AI深度融入人类社会所带来的时代命题：当技术的力量愈发强大甚至不可控，人类应如何与之共生"}
{"id": "wechat.2509.cbbf760e", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652789848&idx=2&sn=db0f4c8a419d764431b940399d86b073&chksm=85365bf3360c9df284982a9833daaa361ce311eea1f681cba59f5b5e0b38b85fbf4d69015d08#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652789848&idx=2&sn=db0f4c8a419d764431b940399d86b073&chksm=85365bf3360c9df284982a9833daaa361ce311eea1f681cba59f5b5e0b38b85fbf4d69015d08#rd", "authors": ["智东西"], "title": "阿里又一<em class=\"highlight\">大模型</em>开源，手机电脑样样玩的溜，多项测试秒GPT-5", "comment": "Source: WeChat, Published: 2025-09-24 09:05:31", "summary": "智东西9月24日报道，今天，阿里通义大模型团队宣布推出全新升级的Qwen3-VL系列模型，并宣布旗舰版本Qwen3-VL-235B-A22B系列开源。这是Qwen系列中最强的视觉语言模型。"}
{"id": "wechat.2509.a4ae8437", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNzIwODM4NQ==&mid=2247554212&idx=1&sn=2cfc31c589166cf3367ead3167ab9d9b&chksm=c10f8732d9385590edf1d80c82baf03fe57592e59bed475518cbc4b43ce46bb7e5ffcf3eec57#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNzIwODM4NQ==&mid=2247554212&idx=1&sn=2cfc31c589166cf3367ead3167ab9d9b&chksm=c10f8732d9385590edf1d80c82baf03fe57592e59bed475518cbc4b43ce46bb7e5ffcf3eec57#rd", "authors": ["壹零社"], "title": "中国<em class=\"highlight\">大模型</em>首进全球三甲，阿里扔出技术“核弹”", "comment": "Source: WeChat, Published: 2025-09-24 09:04:57", "summary": "中国大模型技术的里程碑突破Qwen3-Max的发布标志着中国在大模型领域正式跻身全球第一梯队。这一成就对国内AI产业发展具有深远战略意义。通义大模型预训练的Scaling Law（规模化法则）认为，持续增长数据和参数规模是通向AGI"}
{"id": "wechat.2509.866546af", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4Njg1NzY3Nw==&mid=2247503253&idx=1&sn=14ca3cc135c10907125543b3d6e6e6e0&chksm=ce2910b9fceca5b7f0a13bf9d8205f18ee0a5cab84b79bb6b88a8a78537426dce6987ff3fdd4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4Njg1NzY3Nw==&mid=2247503253&idx=1&sn=14ca3cc135c10907125543b3d6e6e6e0&chksm=ce2910b9fceca5b7f0a13bf9d8205f18ee0a5cab84b79bb6b88a8a78537426dce6987ff3fdd4#rd", "authors": ["上海长三角产业赋能研究院"], "title": "工业<em class=\"highlight\">大模型</em>落地应用进展、痛点及对策建议", "comment": "Source: WeChat, Published: 2025-09-24 08:56:02", "summary": "对于操作精度要求高的核心场景，大模型的可靠性和稳定性尚未得到充分验证，难以满足严苛的安全标准，其应用和推广需要大模型技术的成熟与优化，以及非核心领域的经验积累。"}
{"id": "wechat.2509.c2b9f638", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzAwMjMyOQ==&mid=2649890691&idx=1&sn=8c6b40d201888c6927459f557c572dee&chksm=860a590275eb4511ffcd13d47d3705b3da882ed48ff748314695adf40b9a79300999a4e0bb9e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzAwMjMyOQ==&mid=2649890691&idx=1&sn=8c6b40d201888c6927459f557c572dee&chksm=860a590275eb4511ffcd13d47d3705b3da882ed48ff748314695adf40b9a79300999a4e0bb9e#rd", "authors": ["CHIMA"], "title": "医疗<em class=\"highlight\">大模型</em>从小事做起（六）：氛围编程", "comment": "Source: WeChat, Published: 2025-09-24 08:34:15", "summary": "近日，我拜读了薛万国主任的专栏文章《大模型-我的思与惑》，收获颇丰。文中提到，传统编程要求开发者使用严格、精确且符合特定语法的代码指令与计算机进行交互，而当下，我们正越来越多地借助自然语言，以更接近人"}
{"id": "wechat.2509.6cdfd2be", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMjg0MTk0Nw==&mid=2247582224&idx=1&sn=a303545f104d3ee197013468157daa8a&chksm=e98ccd2b48f69d518494f492c54395c8e1aa59aa45b5f2d980c6f7e254e52dfcff1119ac4325#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMjg0MTk0Nw==&mid=2247582224&idx=1&sn=a303545f104d3ee197013468157daa8a&chksm=e98ccd2b48f69d518494f492c54395c8e1aa59aa45b5f2d980c6f7e254e52dfcff1119ac4325#rd", "authors": ["创新西安"], "title": "硬科技看名企 | 文旅专属AI智能体来了！全国首个省级文旅<em class=\"highlight\">大模型</em>“博观”发布", "comment": "Source: WeChat, Published: 2025-09-24 08:21:06", "summary": "据悉，9月18日在上海举行的华为全联接大会上，陕西文旅行业人工智能大模型——“博观”正式发布。“博观”由陕文投集团与华为公司合作打造，是全国首个省级文旅大模型。"}
{"id": "wechat.2509.e16cffbf", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2649003405&idx=1&sn=12807bdb507a3816c143cced4c87c22b&chksm=868aec6539cbec8047c62d2733ac7bee5fdfac2b53ec06cf5809351d20e4d5ce0cabb197d23c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2649003405&idx=1&sn=12807bdb507a3816c143cced4c87c22b&chksm=868aec6539cbec8047c62d2733ac7bee5fdfac2b53ec06cf5809351d20e4d5ce0cabb197d23c#rd", "authors": ["Ai学习的老章"], "title": "阿里开源<em class=\"highlight\">大模型</em>全球第一，但，最强<em class=\"highlight\">大模型</em>不开源了", "comment": "Source: WeChat, Published: 2025-09-24 08:13:00", "summary": "后面还有一个多模态大模型 Omini，感觉 VL 应该是更专注于文本，视频，尤其是视频，参数量和激活两也都几乎十倍于 OminiQwen3-VL235B-A22BInstruct Gemini2.5-Pro Thinkingbudget 128 GPT5 Minimal or Without thinking Claude-Opus-4.1 Without thinking Other Best With"}
{"id": "wechat.2509.daf11de9", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDQyNjk2Mw==&mid=2247484597&idx=1&sn=c6ccfa5434665bd297a4b58cc190d643&chksm=c4f651b22e80d5133a3b3dab294ad128c09ae659217ceab127b63b9243be06d52fe528051765#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDQyNjk2Mw==&mid=2247484597&idx=1&sn=c6ccfa5434665bd297a4b58cc190d643&chksm=c4f651b22e80d5133a3b3dab294ad128c09ae659217ceab127b63b9243be06d52fe528051765#rd", "authors": ["中商数智浪潮"], "title": "一文掌握<em class=\"highlight\">大模型</em>与智能体：AI时代的“思考者”与“行动者”", "comment": "Source: WeChat, Published: 2025-09-24 07:53:21", "summary": "大模型（Large Language Models， LLMs），顾名思义，是通过海量文本数据训练而成的大型人工智能系统。它具有强大的语言理解和生成能力，能够回答问题、撰写文章、翻译语言，甚至进行一定程度的逻辑推理。"}
{"id": "wechat.2509.d2b52205", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NDg3NzY3NA==&mid=2247524523&idx=2&sn=13134cb83f220d7ff205dba2dcf9383a&chksm=cf76bc7c50175f52c67e4d065ebe49937360292f6c85b854e98e89d5825c0cf4d9c2d097a1cd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NDg3NzY3NA==&mid=2247524523&idx=2&sn=13134cb83f220d7ff205dba2dcf9383a&chksm=cf76bc7c50175f52c67e4d065ebe49937360292f6c85b854e98e89d5825c0cf4d9c2d097a1cd#rd", "authors": ["未来科研前沿"], "title": "Nature | 基于大语言<em class=\"highlight\">模型</em>的统一化药物设计，AI助力打破传统药研，瞬间提速百倍！", "comment": "Source: WeChat, Published: 2025-09-24 07:45:23", "summary": "08机器学习生物医学09AI智慧医疗影像技术10多模态AI医疗大模型特惠福利：报一送一可额外送的回放（包含全套课程回放和课件资料ppt）CRISPR-Cas9基因编辑技术"}
{"id": "wechat.2509.596019da", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMTUxMzYyNQ==&mid=2247488329&idx=1&sn=3b64481603c78dbcceb6f8fa5af6f213&chksm=fbbc51890a36ba5eed0ad53301bcc3683b9576f1396ecf84321ef82c719ebd23636dc20b6690#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMTUxMzYyNQ==&mid=2247488329&idx=1&sn=3b64481603c78dbcceb6f8fa5af6f213&chksm=fbbc51890a36ba5eed0ad53301bcc3683b9576f1396ecf84321ef82c719ebd23636dc20b6690#rd", "authors": ["堆栈future"], "title": "炸裂！阿里CEO吴泳铭断言：<em class=\"highlight\">大模型</em>是“下一代操作系统”！通义Qwen3-Max性能超越GPT-5！", "comment": "Source: WeChat, Published: 2025-09-24 07:25:54", "summary": "大模型是下一代操作系统。llm： the next os 高性能网络 核心利器：通义大模型七连发，性能突破极限 支撑这一宏大愿景的，是通义大模型家族的全面爆发："}
{"id": "wechat.2509.026632ba", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDg1Nzk3NA==&mid=2652587931&idx=1&sn=ccc63dd25f58ecf284a794f9dd1439bf&chksm=bc6e49776920e78ffe6a4ce5d9ed0631335dc4210e95a85bc208bf0000c0e9cf061c935a53cf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDg1Nzk3NA==&mid=2652587931&idx=1&sn=ccc63dd25f58ecf284a794f9dd1439bf&chksm=bc6e49776920e78ffe6a4ce5d9ed0631335dc4210e95a85bc208bf0000c0e9cf061c935a53cf#rd", "authors": ["中国外汇"], "title": "中国外汇 | 加快人工智能<em class=\"highlight\">大模型</em>与银行业深度融合", "comment": "Source: WeChat, Published: 2025-09-24 07:23:44", "summary": "AI大模型技术作为人工智能领域的关键突破，具备通用性、高门槛性和边际成本递减性等显著特点，尤其是大语言模型，其强泛化能力、迁移学习能力和多模态处理能力，已成为驱动金融行业创新的核心力量。"}
{"id": "wechat.2509.5cd0ce48", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247656794&idx=1&sn=f59f0368dc95cd7b4e58376511d13b8b&chksm=c0d9bd0a3f8de623e150d2279e046634b08a868a2f90870ce239f834445e5cef8a07a64bc2a8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247656794&idx=1&sn=f59f0368dc95cd7b4e58376511d13b8b&chksm=c0d9bd0a3f8de623e150d2279e046634b08a868a2f90870ce239f834445e5cef8a07a64bc2a8#rd", "authors": ["DataFunSummit"], "title": "你们催更的<em class=\"highlight\">模型</em>，云栖大会一口气全发了！", "comment": "Source: WeChat, Published: 2025-09-24 07:09:22", "summary": "qwen3-omni 新一代全模态大模型 Qwen3-Omni 是通义全新发布的全模态大模型，支持 19 种语言及方言输入、10 种语言输出，可处理长达 30 分钟的会议录音或播客，精准输出纪要。"}
{"id": "wechat.2509.179bb082", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NTg0NDE1Mw==&mid=2652627055&idx=2&sn=2f08f422e1afa61b4c25a6f0ec646f1e&chksm=bc806ae62a289ee82e83e1a194dca010087d369ce369ecf5d723d4ff553ef7a217d91aa085c1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NTg0NDE1Mw==&mid=2652627055&idx=2&sn=2f08f422e1afa61b4c25a6f0ec646f1e&chksm=bc806ae62a289ee82e83e1a194dca010087d369ce369ecf5d723d4ff553ef7a217d91aa085c1#rd", "authors": ["阿里研究院"], "title": "你们催更的<em class=\"highlight\">模型</em>，在云栖大会悉数发布", "comment": "Source: WeChat, Published: 2025-09-24 06:46:10", "summary": "Qwen3-Omni新一代全模态大模型Qwen3-Omni 是通义全新发布的全模态大模型，支持 19 种语言及方言输入、10 种语言输出，可处理长达 30 分钟的会议录音或播客，精准输出纪要。"}
{"id": "wechat.2509.1b183d5c", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTM3NTMwNA==&mid=2661636475&idx=2&sn=423e82dc3a2c5306cf2fd11870007359&chksm=bcaec1c2c7c5c5f37885775d6a39d9fbdbd6d4b826a92651756298cb6740db3b951a79002a81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTM3NTMwNA==&mid=2661636475&idx=2&sn=423e82dc3a2c5306cf2fd11870007359&chksm=bcaec1c2c7c5c5f37885775d6a39d9fbdbd6d4b826a92651756298cb6740db3b951a79002a81#rd", "authors": ["第一财经"], "title": "阿里云<em class=\"highlight\">大模型</em>产品七连发", "comment": "Source: WeChat, Published: 2025-09-24 06:38:30", "summary": "2025云栖大会现场，阿里云CTO周靖人发布七款大模型产品，包括大语言模型通义旗舰模型Qwen3-Max、下一代基础模型架构Qwen3-Next及系列模型、千问编程模型Qwen3-Coder、视觉理解模型Qwen3-VL、全模态模型Qwen3-Omni、视觉基础模型Wan2.5-prev"}
{"id": "wechat.2509.4eb882ad", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNjg5MDE3NA==&mid=2247527738&idx=1&sn=ca9358327dbf9f2baaa926b535e47784&chksm=f82936bd9c2b5be7ee633a1516aec0226b7d6434af995b0e196b40e2f110aca1709d6424d661#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNjg5MDE3NA==&mid=2247527738&idx=1&sn=ca9358327dbf9f2baaa926b535e47784&chksm=f82936bd9c2b5be7ee633a1516aec0226b7d6434af995b0e196b40e2f110aca1709d6424d661#rd", "authors": ["云蝠智能大模型呼叫"], "title": "<em class=\"highlight\">大模型</em>呼叫，如何帮助政府实现 7*24 小时来电服务？", "comment": "Source: WeChat, Published: 2025-09-24 06:15:33", "summary": "大模型呼叫技术通过“功能模块化+场景定制化”的路径，在政务服务领域形成覆盖呼入呼出全流程的解决方案，已在市场监管、公安、卫健等多领域验证成效，其核心价值体现在效率提升、服务延伸与风险前置三个维度。"}
{"id": "wechat.2509.2c674ddb", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MzczMzQ5Mg==&mid=2247493818&idx=1&sn=e3fdd4c1e08b3053cd4b3027da85a15c&chksm=cfbf52a51ced614bd9f137c871e3a41a861a23e1ccbeacaf8eece409d3d60da0f2b09c0a4821#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MzczMzQ5Mg==&mid=2247493818&idx=1&sn=e3fdd4c1e08b3053cd4b3027da85a15c&chksm=cfbf52a51ced614bd9f137c871e3a41a861a23e1ccbeacaf8eece409d3d60da0f2b09c0a4821#rd", "authors": ["医渡科技"], "title": "医渡科技揭晓医疗<em class=\"highlight\">大模型</em>落地密码：幻觉率<1%，临床试验入组效率提3-5倍", "comment": "Source: WeChat, Published: 2025-09-24 06:07:59", "summary": "并在闭门会议中分享了公司在医疗大模型应用中的创新实践，为行业探索AI医疗落地提供路径参考。徐济铭 潮起钱塘 智启未来 潮起钱塘数据+ai双引擎，构建医疗智能新基建"}
{"id": "wechat.2509.319e57be", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NTI1MDEyNA==&mid=2652724663&idx=1&sn=c0d2e4bec228abc8d8d41bc78e22d518&chksm=8a56ad6baea434e22c86b7327fc9acb3cbdd544865c20bf3b4a8f1520ff08b31aa7df4cba8ff#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NTI1MDEyNA==&mid=2652724663&idx=1&sn=c0d2e4bec228abc8d8d41bc78e22d518&chksm=8a56ad6baea434e22c86b7327fc9acb3cbdd544865c20bf3b4a8f1520ff08b31aa7df4cba8ff#rd", "authors": ["亿欧网"], "title": "阿里股价刷新4年新高，阿里通义<em class=\"highlight\">大模型</em>“全家桶”出炉", "comment": "Source: WeChat, Published: 2025-09-24 05:55:00", "summary": "通义千问qwen模型家族 大语言模型 专项模型 全尺寸 全模态 多场景 300+开源模型 开源全球第一 ·17万+全球衍生模型。通义万相wan模型家族 阿里云：全球领先的全栈人工智能服务商 无影 qoder ai agent 通义灵码 llm os qwen3-next 下一代"}
{"id": "wechat.2509.26d8ae85", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247595922&idx=2&sn=99ee67068bd342ba21e580fe97a3108d&chksm=e971efbd473143ee4da674381edb43b9f1234a113a7d19eb91bea4e12d712f7fd4296694d1e1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247595922&idx=2&sn=99ee67068bd342ba21e580fe97a3108d&chksm=e971efbd473143ee4da674381edb43b9f1234a113a7d19eb91bea4e12d712f7fd4296694d1e1#rd", "authors": ["木木自由"], "title": "AI<em class=\"highlight\">大模型</em>·白皮书 | 《文科生听懂<em class=\"highlight\">大模型</em>》告别<em class=\"highlight\">大模型</em>小白，文科生专属的超易懂科普指南！（附42页ppt下载）", "comment": "Source: WeChat, Published: 2025-09-24 05:30:00", "summary": "梁斌penny：文科生听懂大模型（1.0） .pdf 梁斌penmy/2025年2月/南京 文科生听懂大模型 demysttying large models an aceble coume tor liera artes studert 大模型应用 查看详情>"}
{"id": "wechat.2509.a9534664", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NzE1ODA4Ng==&mid=2247512869&idx=1&sn=4a32734f77597698e897b16a83f4cf0e&chksm=fad297747fa2cb2caf92c966e77e06fff05e71fbfc2d9285b1e3fd2bbdfc1605677474c93a8c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NzE1ODA4Ng==&mid=2247512869&idx=1&sn=4a32734f77597698e897b16a83f4cf0e&chksm=fad297747fa2cb2caf92c966e77e06fff05e71fbfc2d9285b1e3fd2bbdfc1605677474c93a8c#rd", "authors": ["高中信息技术"], "title": "DeepSeek是如何思考的？一文读懂<em class=\"highlight\">大模型</em>的工作原理", "comment": "Source: WeChat, Published: 2025-09-24 02:46:29", "summary": "“大模型”这个术语，主要“大”在以下几个方面：巨大的参数规模 “参数”是模型内部的可调节的“旋钮”，是模型从数据中学习到的知识和规律的具体载体。"}
{"id": "wechat.2509.67a46a4a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzkyNDQxMw==&mid=2650043684&idx=1&sn=25038fcacad96f909255f76fb7f5159d&chksm=89e9dd545ab6e39a922fe28217167a99a405c4a43ff6c6022de6ffc3115d3d8774fc16f90ab9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzkyNDQxMw==&mid=2650043684&idx=1&sn=25038fcacad96f909255f76fb7f5159d&chksm=89e9dd545ab6e39a922fe28217167a99a405c4a43ff6c6022de6ffc3115d3d8774fc16f90ab9#rd", "authors": ["IOT雇佣兵"], "title": "乡村振兴 农业<em class=\"highlight\">大模型</em>汇总", "comment": "Source: WeChat, Published: 2025-09-24 00:10:20", "summary": "4. “万象耕耘”农业大模型（中国移动）中国移动自主研发的农业大模型。以中国移动“九天”大模型为底座。集成了多种农业典型场景的AI专业模型，进行协同运算与深度学习。"}
{"id": "wechat.2509.00379be7", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MDUzOTU5Mw==&mid=2247545923&idx=2&sn=7010cd07b04f082bd831c6ccebd5d254&chksm=cf6854d51d42b6d86337a2e895945f5ca36c2ae688e4b2f7d97206369de774d58cdc04e5dc6d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MDUzOTU5Mw==&mid=2247545923&idx=2&sn=7010cd07b04f082bd831c6ccebd5d254&chksm=cf6854d51d42b6d86337a2e895945f5ca36c2ae688e4b2f7d97206369de774d58cdc04e5dc6d#rd", "authors": ["车百会研究院"], "title": "AI <em class=\"highlight\">大模型</em>驱动汽车产业三大变革——竞争力、产业链、盈利模式", "comment": "Source: WeChat, Published: 2025-09-24 00:02:14", "summary": "大模型具备泛在的理解能力，结合多年发展的人工智能技术，将给汽车带来全面、彻底的变化，让汽车成为更高层次、更多内容的超级智能体，形成会思考、懂用户的 AI汽车。"}
{"id": "wechat.2509.a9916891", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650706744&idx=2&sn=1d2fd6f5506637bafed7797809fc7c87&chksm=bfe82ab1fa69c9c223cf58170d4428021b1b85f817a2b5fa92b640c49b96bbdf8e9604823cb0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650706744&idx=2&sn=1d2fd6f5506637bafed7797809fc7c87&chksm=bfe82ab1fa69c9c223cf58170d4428021b1b85f817a2b5fa92b640c49b96bbdf8e9604823cb0#rd", "authors": ["twt企业IT社区"], "title": "<em class=\"highlight\">大模型</em>存储需求及技术策略", "comment": "Source: WeChat, Published: 2025-09-23 23:35:38", "summary": "随着人工智能和机器学习技术的飞速发展，大模型已广泛应用于各个领域，大模型拥有更强的数据处理能力和更高的预测精度，为企业提供了更为精准的业务分析和预测服务。"}
{"id": "wechat.2509.2c56fdec", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwNDE1NjM2NA==&mid=2652470053&idx=2&sn=33ee1df627c595f55252779ce3b6adc0&chksm=8cfbcbbd691509279ed5908331351f3840ac2261dc8810b0fd7cf9aad50aedabe01e68513c3c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwNDE1NjM2NA==&mid=2652470053&idx=2&sn=33ee1df627c595f55252779ce3b6adc0&chksm=8cfbcbbd691509279ed5908331351f3840ac2261dc8810b0fd7cf9aad50aedabe01e68513c3c#rd", "authors": ["慧天地"], "title": "数字孪生<em class=\"highlight\">大模型</em>，AI发展高级阶段", "comment": "Source: WeChat, Published: 2025-09-23 16:02:13", "summary": "数字孪生大模型：物理实体的“数字化克隆”如果把物理世界的工厂、矿山、城市比作“正在运行的复杂机器”，数字孪生大模型就是为它们打造的实时动态数字镜像——通过数字化手段在虚拟空间复刻物理实体的每一个细节，"}
{"id": "wechat.2509.174d04d9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzE5OTk2NQ==&mid=2247484134&idx=1&sn=1773b76ba0b59eb0f6b58ff68c6ccdb4&chksm=c2b07b6e0b70078fddef0e26b6838218efb54aada27112182f47f0d5c763b0150c64bf5a06a2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzE5OTk2NQ==&mid=2247484134&idx=1&sn=1773b76ba0b59eb0f6b58ff68c6ccdb4&chksm=c2b07b6e0b70078fddef0e26b6838218efb54aada27112182f47f0d5c763b0150c64bf5a06a2#rd", "authors": ["兴之所志"], "title": "说说目前在用的AI<em class=\"highlight\">大模型</em>产品和工具（2025年9月更新）", "comment": "Source: WeChat, Published: 2025-09-23 15:03:32", "summary": "3. Cherry Studio：纯桌面客户端，本身并不提供大模型，但胜在全面和方便，包括api管理、多模型回答、联网搜索、知识库、快捷搜索框、划词工具栏、MCP、导出到Obsidian等。"}
{"id": "tldr.2509.d30e7dd4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "authors": ["TLDR Newsletter"], "title": "Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "summary": "Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor (Sponsor) With all the noise about agents, it's easy to forget that an agent is just a looping LLM call + Memory + Tool usage. Use step-by-step tutorial to build an agent that \"thinks\" with an LLM, calls external tools, remembers context, and keeps looping until it gets the job done. Follow along, get the satisfaction of seeing your first agent in action, and iterate from there to build more complex, enterprise-rea...", "source": "tldr"}
{"id": "tldr.2509.a5c19006", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrKajk8/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/V92uxoBptHuCT9yPUOk0ZS9fQNbK_gJGFkMAPtp8zAU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrKajk8/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/V92uxoBptHuCT9yPUOk0ZS9fQNbK_gJGFkMAPtp8zAU=424", "authors": ["TLDR Newsletter"], "title": "Larry Ellison, a Media Mogul Like No Other", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrKajk8/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/V92uxoBptHuCT9yPUOk0ZS9fQNbK_gJGFkMAPtp8zAU=424", "summary": "Larry Ellison, a Media Mogul Like No Other (9 minute read) Larry Ellison has become a media mogul whose portfolio and power could exceed that of predecessors like Hearst and Pulitzer. Oracle is among the investors in the new American version of TikTok, and Ellison owns more than 40% of Oracle's stock and is its chief technology officer. It is still unclear what the exact share of ownership will be and who will run the new TikTok. The Ellison family recently secured an $8 billion deal for Para...", "source": "tldr"}
{"id": "tldr.2509.112c1ae3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Faltman-huang-negotiations-that-sealed-100-billion-openai-nvidia-deal.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/kcF6D5bMLBx-aUZTm-VMO6iOas6aG191dGCggCC1e5I=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Faltman-huang-negotiations-that-sealed-100-billion-openai-nvidia-deal.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/kcF6D5bMLBx-aUZTm-VMO6iOas6aG191dGCggCC1e5I=424", "authors": ["TLDR Newsletter"], "title": "Altman, Huang, and the last-minute negotiations that sealed the $100 billion OpenAI-Nvidia deal", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Faltman-huang-negotiations-that-sealed-100-billion-openai-nvidia-deal.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/kcF6D5bMLBx-aUZTm-VMO6iOas6aG191dGCggCC1e5I=424", "summary": "Altman, Huang, and the last-minute negotiations that sealed the $100 billion OpenAI-Nvidia deal (12 minute read) OpenAI and Nvidia are now more intimately linked than ever with a monumental deal that will see Nvidia investing $100 billion into OpenAI and providing cutting-edge processors to power a host of new datacenters. The deal was negotiated largely through a mix of virtual discussions and one-on-one meetings between Sam Altman and Jensen Huang - no bankers were involved. Microsoft, Open...", "source": "tldr"}
{"id": "tldr.2509.5ea45c0c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.owlposting.com%2Fp%2Fhow-do-you-use-a-virtual-cell-to%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/R_CsNOgRBuB3bjjHlpoATmUTDMI1VIWE-GQc28J0ehs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.owlposting.com%2Fp%2Fhow-do-you-use-a-virtual-cell-to%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/R_CsNOgRBuB3bjjHlpoATmUTDMI1VIWE-GQc28J0ehs=424", "authors": ["TLDR Newsletter"], "title": "How do you use a virtual cell to do something actually useful?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 25 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.owlposting.com%2Fp%2Fhow-do-you-use-a-virtual-cell-to%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/R_CsNOgRBuB3bjjHlpoATmUTDMI1VIWE-GQc28J0ehs=424", "summary": "How do you use a virtual cell to do something actually useful? (25 minute read) Virtual cells are learned simulations of cells and cellular systems that can be observed in varying conditions and changing contexts. Computational simulations of cells can be useful for all sorts of clinical and preclinical research. The field is still in its early days, but in time, these models will become routine parts of treatments. They will help patients avoid wasting precious time on ineffective treatments...", "source": "tldr"}
{"id": "tldr.2509.9046e1e1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnews.microsoft.com%2Fsource%2Ffeatures%2Finnovation%2Fmicrofluidics-liquid-cooling-ai-chips%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/A0MTIUUnj8uwIf6HYUYXdXds8YaE2GcTaZ1tlTf0-R8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnews.microsoft.com%2Fsource%2Ffeatures%2Finnovation%2Fmicrofluidics-liquid-cooling-ai-chips%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/A0MTIUUnj8uwIf6HYUYXdXds8YaE2GcTaZ1tlTf0-R8=424", "authors": ["TLDR Newsletter"], "title": "AI chips are getting hotter. A microfluidics breakthrough goes straight to the silicon to cool up to three times better", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnews.microsoft.com%2Fsource%2Ffeatures%2Finnovation%2Fmicrofluidics-liquid-cooling-ai-chips%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/A0MTIUUnj8uwIf6HYUYXdXds8YaE2GcTaZ1tlTf0-R8=424", "summary": "AI chips are getting hotter. A microfluidics breakthrough goes straight to the silicon to cool up to three times better (13 minute read) The chips used to run AI datacenters generate much more heat than previous generations of silicon. Current cooling technology will become a bottleneck to progress in just a few years. Microfluidics could boost efficiency and improve sustainability for next-generation AI chips. Microsoft has successfully developed an in-chip microfluidic cooling system that c...", "source": "tldr"}
{"id": "tldr.2509.3a0ec8e7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbasecamp.com%2Fpricing%3Futm_campaign=sponsorship%26utm_medium=digital%26%26utm_source=tldr/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ciS0LErqVcYfV4-uSd9EghcVmMOirKS4n1UvZKozMyI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbasecamp.com%2Fpricing%3Futm_campaign=sponsorship%26utm_medium=digital%26%26utm_source=tldr/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ciS0LErqVcYfV4-uSd9EghcVmMOirKS4n1UvZKozMyI=424", "authors": ["TLDR Newsletter"], "title": "Project management doesn't have to be a load of 🐂", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbasecamp.com%2Fpricing%3Futm_campaign=sponsorship%26utm_medium=digital%26%26utm_source=tldr/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ciS0LErqVcYfV4-uSd9EghcVmMOirKS4n1UvZKozMyI=424", "summary": "Project management doesn't have to be a load of 🐂 (Sponsor) Basecamp is the no-BS project management tool that makes sense immediately. No onboarding saga. No notification hell. One clean place to see who's doing what, what's done, and what's currently on fire. Everything — files, convos, context — stays where it belongs: in the project. Start for free. Stay because it actually works. Try Basecamp", "source": "tldr"}
{"id": "tldr.2509.0135e116", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "authors": ["TLDR Newsletter"], "title": "How Google's dev tools manager makes AI coding work", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "summary": "How Google's dev tools manager makes AI coding work (5 minute read) Ryan Salva, Google's project manager for developer tools, is responsible for tools like Gemini CLI and Gemini Code Assist. His team recently released new third-party research that showed how developers actually use AI tools. This article contains an edited interview with Salva where he talks about the research, how he uses AI tools, and the future of IDEs. Salva believes that over time, the time spent in the IDE will graduall...", "source": "tldr"}
{"id": "tldr.2509.f2d402a1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.codingconfessions.com%2Fp%2Fcompiling-python-to-run-anywhere%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/C5rS9w_0fb9eaJyN6LAxCRoIpo92tfG0BaUw1B85c9c=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.codingconfessions.com%2Fp%2Fcompiling-python-to-run-anywhere%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/C5rS9w_0fb9eaJyN6LAxCRoIpo92tfG0BaUw1B85c9c=424", "authors": ["TLDR Newsletter"], "title": "Compiling Python to Run Anywhere", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.codingconfessions.com%2Fp%2Fcompiling-python-to-run-anywhere%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/C5rS9w_0fb9eaJyN6LAxCRoIpo92tfG0BaUw1B85c9c=424", "summary": "Compiling Python to Run Anywhere (23 minute read) This post looks at how Python can be pushed beyond its limits of speed and portability to create a compiler that turns ordinary code into fast, portable executables. The approach generates optimized kernels while keeping the Python source unchanged. It demonstrates why understanding systems at the lowest level matters.", "source": "tldr"}
{"id": "tldr.2509.62336c03", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-09-23%2Fvcs-are-scrambling-for-a-piece-of-ai-darlings-like-anthropic-cursor-cognition%3FaccessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1ODY4NzE2NCwiZXhwIjoxNzU5MjkxOTY0LCJhcnRpY2xlSWQiOiJUMzFGV1dHUTFZUFkwMCIsImJjb25uZWN0SWQiOiJBOEExRDhFQTI5OTc0OTRGQTQ1QUE2REJBMjAwNTM3MSJ9.UKwefWgoBRNfZgMNFfdv_bmahItD0jpVIu1wgpMlOIM%26utm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/X6y8Hw-sZp3rH_TvwL8X1pDkG_TYvL6bcU5_8YMo_w8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-09-23%2Fvcs-are-scrambling-for-a-piece-of-ai-darlings-like-anthropic-cursor-cognition%3FaccessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1ODY4NzE2NCwiZXhwIjoxNzU5MjkxOTY0LCJhcnRpY2xlSWQiOiJUMzFGV1dHUTFZUFkwMCIsImJjb25uZWN0SWQiOiJBOEExRDhFQTI5OTc0OTRGQTQ1QUE2REJBMjAwNTM3MSJ9.UKwefWgoBRNfZgMNFfdv_bmahItD0jpVIu1wgpMlOIM%26utm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/X6y8Hw-sZp3rH_TvwL8X1pDkG_TYvL6bcU5_8YMo_w8=424", "authors": ["TLDR Newsletter"], "title": "VCs to AI Startups: Please Take Our Money", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-09-23%2Fvcs-are-scrambling-for-a-piece-of-ai-darlings-like-anthropic-cursor-cognition%3FaccessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1ODY4NzE2NCwiZXhwIjoxNzU5MjkxOTY0LCJhcnRpY2xlSWQiOiJUMzFGV1dHUTFZUFkwMCIsImJjb25uZWN0SWQiOiJBOEExRDhFQTI5OTc0OTRGQTQ1QUE2REJBMjAwNTM3MSJ9.UKwefWgoBRNfZgMNFfdv_bmahItD0jpVIu1wgpMlOIM%26utm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/X6y8Hw-sZp3rH_TvwL8X1pDkG_TYvL6bcU5_8YMo_w8=424", "summary": "VCs to AI Startups: Please Take Our Money (7 minute read) The fundraising script has flipped for a few dozen of the top AI startups: VCs are pitching to them and presenting them with gifts and favors in hopes of leading their next round. The best companies are getting preempted every round, and the time between rounds is shrinking. The surge in preemptive deals and soaring valuations has kindled fears of an AI bubble. Some founders have been wary of accepting offers as they risk pricing thems...", "source": "tldr"}
{"id": "tldr.2509.2cabc1ef", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Ftether-reportedly-seeks-lofty-500-billion-valuation-in-capital-raise-.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/pvWO-XU2eL0MnPxXZ8SgOpk2_lUg_C26WQIpvO4vA_0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Ftether-reportedly-seeks-lofty-500-billion-valuation-in-capital-raise-.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/pvWO-XU2eL0MnPxXZ8SgOpk2_lUg_C26WQIpvO4vA_0=424", "authors": ["TLDR Newsletter"], "title": "Tether CEO confirms major capital raise at a reported $500 billion valuation", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F09%2F23%2Ftether-reportedly-seeks-lofty-500-billion-valuation-in-capital-raise-.html%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/pvWO-XU2eL0MnPxXZ8SgOpk2_lUg_C26WQIpvO4vA_0=424", "summary": "Tether CEO confirms major capital raise at a reported $500 billion valuation (3 minute read) Tether is looking to raise between $15 billion and $20 billion for a roughly 3% stake through a select group of high-profile key investors. The deal could put the company's value on par with OpenAI. It will involve new equity rather than existing investors selling their stakes. The talks are still in an early stage, so the details could change.", "source": "tldr"}
{"id": "tldr.2509.ebacb539", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.electric.ai%2Flearn%3Futm_source=tldr%26utm_medium=paid_affiliate%26utm_campaign=250924_mkt_tldr_tech_quick_links_incentive/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/THOIvAprwgq8jtlExpr_dUTwF08EdhoWT0wFQg_Ofg0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.electric.ai%2Flearn%3Futm_source=tldr%26utm_medium=paid_affiliate%26utm_campaign=250924_mkt_tldr_tech_quick_links_incentive/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/THOIvAprwgq8jtlExpr_dUTwF08EdhoWT0wFQg_Ofg0=424", "authors": ["TLDR Newsletter"], "title": "Don't overspend on IT management", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.electric.ai%2Flearn%3Futm_source=tldr%26utm_medium=paid_affiliate%26utm_campaign=250924_mkt_tldr_tech_quick_links_incentive/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/THOIvAprwgq8jtlExpr_dUTwF08EdhoWT0wFQg_Ofg0=424", "summary": "Don't overspend on IT management (Sponsor) Managing employees' devices, security, and laptop shipping/retrieval shouldn't be painful or costly. Check out Electric - the all-in-one platform built for smaller teams. Take a demo for a $200 gift.", "source": "tldr"}
{"id": "tldr.2509.9321d36d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fthe-youtube-tip-of-the-google-spear%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4K9zt-NnLFp8CTqLn7lJcGx01ESiNUOwVasfltAwKrM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fthe-youtube-tip-of-the-google-spear%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4K9zt-NnLFp8CTqLn7lJcGx01ESiNUOwVasfltAwKrM=424", "authors": ["TLDR Newsletter"], "title": "The YouTube Tip of the Google Spear", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fthe-youtube-tip-of-the-google-spear%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4K9zt-NnLFp8CTqLn7lJcGx01ESiNUOwVasfltAwKrM=424", "summary": "The YouTube Tip of the Google Spear (20 minute read) AI will increase the amount of compelling content on YouTube with better margins in the long run.", "source": "tldr"}
{"id": "tldr.2509.c8e2b2db", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/XjAf0akh3VMz9VF3EtlcbATFWIh5xC8IHcpAJRUbojY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/XjAf0akh3VMz9VF3EtlcbATFWIh5xC8IHcpAJRUbojY=424", "authors": ["TLDR Newsletter"], "title": "The Risks of NPM", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/XjAf0akh3VMz9VF3EtlcbATFWIh5xC8IHcpAJRUbojY=424", "summary": "The Risks of NPM (3 minute read) npm install allows the running of arbitrary code, making it unsafe for a variety of environments.", "source": "tldr"}
{"id": "tldr.2509.ea725c90", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.x402.org%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/2nONy0Hy9MMG-J5BgRTvtnpW2izklpPfv7OK9pw-pUQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.x402.org%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/2nONy0Hy9MMG-J5BgRTvtnpW2izklpPfv7OK9pw-pUQ=424", "authors": ["TLDR Newsletter"], "title": "x402", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.x402.org%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/2nONy0Hy9MMG-J5BgRTvtnpW2izklpPfv7OK9pw-pUQ=424", "summary": "x402 (Website) x402 is an open protocol for internet-native payments that enables users to pay for resources via API without registration.", "source": "tldr"}
{"id": "tldr.2509.ae01c18b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks09242025/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ZgApGd7HBWno_5KQrzU-gFYTPWs8_1j0HdRfKO1qATQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks09242025/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ZgApGd7HBWno_5KQrzU-gFYTPWs8_1j0HdRfKO1qATQ=424", "authors": ["TLDR Newsletter"], "title": "Craving more AI in your inbox?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks09242025/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/ZgApGd7HBWno_5KQrzU-gFYTPWs8_1j0HdRfKO1qATQ=424", "summary": "Craving more AI in your inbox? (Sponsor) TLDR AI is your daily fix of LLMs, GenAI, and deep learning goodness. Same TLDR format. Still free.Subscribe now.", "source": "tldr"}
{"id": "tldr.2509.21de3209", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcate.blog%2F2025%2F09%2F23%2Fgetting-more-strategic%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Hrp-tvnCrqU0u91B-S_aJn7hvreVhHaqmNWDt6ogTEA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcate.blog%2F2025%2F09%2F23%2Fgetting-more-strategic%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Hrp-tvnCrqU0u91B-S_aJn7hvreVhHaqmNWDt6ogTEA=424", "authors": ["TLDR Newsletter"], "title": "Getting More Strategic", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcate.blog%2F2025%2F09%2F23%2Fgetting-more-strategic%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Hrp-tvnCrqU0u91B-S_aJn7hvreVhHaqmNWDt6ogTEA=424", "summary": "Getting More Strategic (11 minute read) Strategies are about navigating the path to creating your vision.", "source": "tldr"}
{"id": "tldr.2509.f0a3ed99", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkk.org%2Fthetechnium%2Fthe-periodic-table-of-cognition%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/f8YUwwMYgVeQ6VWcfEua2IbxvcaruOKbxRCcVkvXRl8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkk.org%2Fthetechnium%2Fthe-periodic-table-of-cognition%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/f8YUwwMYgVeQ6VWcfEua2IbxvcaruOKbxRCcVkvXRl8=424", "authors": ["TLDR Newsletter"], "title": "The Periodic Table of Cognition", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkk.org%2Fthetechnium%2Fthe-periodic-table-of-cognition%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/f8YUwwMYgVeQ6VWcfEua2IbxvcaruOKbxRCcVkvXRl8=424", "summary": "The Periodic Table of Cognition (8 minute read) We have only started to slowly identify the elemental parts of intelligence - we need to know all, or at least most, of the parts to make falsifiable predictions to develop a theory of intelligence.", "source": "tldr"}
{"id": "tldr.2509.93d53310", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasteriskmag.com%2Fissues%2F11%2Fclaude-finds-god%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Zp1DQL-Jn_NGNlJG3_sKbRVW7zK2j7GS48TQ8K4hBIY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasteriskmag.com%2Fissues%2F11%2Fclaude-finds-god%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Zp1DQL-Jn_NGNlJG3_sKbRVW7zK2j7GS48TQ8K4hBIY=424", "authors": ["TLDR Newsletter"], "title": "Claude Finds God", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 32 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasteriskmag.com%2Fissues%2F11%2Fclaude-finds-god%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/Zp1DQL-Jn_NGNlJG3_sKbRVW7zK2j7GS48TQ8K4hBIY=424", "summary": "Claude Finds God (32 minute read) When two Claudes talk to each other, after a sufficient number of turns, they appear to enter into a feedback loop in a state that sounds a lot like Buddhism or Eastern mysticism.", "source": "tldr"}
{"id": "tldr.2509.ca692375", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4xtihJxf4JDjNgIxqAM1zAf65sZT9UtQnTYWDeDBwnk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4xtihJxf4JDjNgIxqAM1zAf65sZT9UtQnTYWDeDBwnk=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/4xtihJxf4JDjNgIxqAM1zAf65sZT9UtQnTYWDeDBwnk=424", "summary": "Claude Finds God (32 minute read) When two Claudes talk to each other, after a sufficient number of turns, they appear to enter into a feedback loop in a state that sounds a lot like Buddhism or Eastern mysticism.", "source": "tldr"}
{"id": "tldr.2509.6f7b27ea", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwebflow.com%2Fresources%2Fwebinars%2Faeo-maturity-model-in-action%3Futm_source=tldr%26utm_medium=sponsored-content%26utm_campaign=fy26-aeo-in-action-webinar%26utm_content=marketing-newsletter/2/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/XGL7IgvGqxiims_Ayex_fF42riSeuZAyVi9ai62SYwM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwebflow.com%2Fresources%2Fwebinars%2Faeo-maturity-model-in-action%3Futm_source=tldr%26utm_medium=sponsored-content%26utm_campaign=fy26-aeo-in-action-webinar%26utm_content=marketing-newsletter/2/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/XGL7IgvGqxiims_Ayex_fF42riSeuZAyVi9ai62SYwM=424", "authors": ["TLDR Newsletter"], "title": "AEO in Practice: How Webflow Has Adapted to AI Search", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwebflow.com%2Fresources%2Fwebinars%2Faeo-maturity-model-in-action%3Futm_source=tldr%26utm_medium=sponsored-content%26utm_campaign=fy26-aeo-in-action-webinar%26utm_content=marketing-newsletter/2/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/XGL7IgvGqxiims_Ayex_fF42riSeuZAyVi9ai62SYwM=424", "summary": "AEO in Practice: How Webflow Has Adapted to AI Search (Sponsor) You can't afford to ignore LLMs. Your SEO playbook is still relevant - but it needs a version update, pronto.Get up to speed quickly with Webflow's on-demand webinar, featuring SEO Lead Vivian Hoang and Chief Evangelist Guy Yalif. They'll cover: → How Webflow does Answer Engine Optimization (AEO): real examples of content, structure, and measurement. → The AEO Maturity Model - a framework to evolve your strategy from SEO to AEO. ...", "source": "tldr"}
{"id": "tldr.2509.64cf7bab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffinance.yahoo.com%2Fnews%2Fkantar-us-media-reactions-2025-174600467.html%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/evqeEp18oaYQzriHWq4v2yjaN-meAFMpOqnd8BgWa4Y=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffinance.yahoo.com%2Fnews%2Fkantar-us-media-reactions-2025-174600467.html%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/evqeEp18oaYQzriHWq4v2yjaN-meAFMpOqnd8BgWa4Y=424", "authors": ["TLDR Newsletter"], "title": "Key Takeaways from Kantar US Media Reactions 2025", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffinance.yahoo.com%2Fnews%2Fkantar-us-media-reactions-2025-174600467.html%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/evqeEp18oaYQzriHWq4v2yjaN-meAFMpOqnd8BgWa4Y=424", "summary": "Key Takeaways from Kantar US Media Reactions 2025 (4 minute read) Streaming platforms lead US ad receptivity, with The New York Times, Amazon, Apple TV, Netflix, and X ranking highest. Consumer openness to ads rose to 58% from 47% last year. Marketer investment plans for 2026 favor podcasts and streaming, clashing with consumer interest in OOH, sponsorships, and publications. Hispanic consumers show above-average receptivity to Online Video and Stories. Over 70% of marketers use generative AI...", "source": "tldr"}
{"id": "tldr.2509.5bb3032b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmediacat.uk%2Fparenthood-boosts-social-media-usage-study-finds%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/hyKcnuSoox4XdL8fL8Ezm0ey3ej22Q5glbiktvCC2UY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmediacat.uk%2Fparenthood-boosts-social-media-usage-study-finds%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/hyKcnuSoox4XdL8fL8Ezm0ey3ej22Q5glbiktvCC2UY=424", "authors": ["TLDR Newsletter"], "title": "Parenthood boosts social media usage, study finds", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmediacat.uk%2Fparenthood-boosts-social-media-usage-study-finds%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/hyKcnuSoox4XdL8fL8Ezm0ey3ej22Q5glbiktvCC2UY=424", "summary": "Parenthood boosts social media usage, study finds (2 minute read) 59% of parents report higher social media use since having children. Usage spikes around events like Black Friday, holidays, and back-to-school. Parents also shop collaboratively with children, with 67% influenced by what kids see online. While parenthood fuels online activity, brands are advised to target parents through their hobbies and interests, not only their role as caregivers.", "source": "tldr"}
{"id": "tldr.2509.5490741b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/GTHBwanICKu3HICgTeW5aAoruJe9ZrzafLUEdbHPDV4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/GTHBwanICKu3HICgTeW5aAoruJe9ZrzafLUEdbHPDV4=424", "authors": ["TLDR Newsletter"], "title": "Tired of cookie consent pop-ups? You soon may see less of them", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/GTHBwanICKu3HICgTeW5aAoruJe9ZrzafLUEdbHPDV4=424", "summary": "Tired of cookie consent pop-ups? You soon may see less of them (2 minute read) EU lawmakers are considering changes to the e-Privacy Directive that triggered the constant cookie consent pop-ups online. Ideas include browser-based consent settings or broader exceptions for essential cookies, reducing the need for repeated prompts. Industry leaders are pushing for alignment with GDPR's risk-based model, while privacy groups raise concerns that data protections could be weakened.", "source": "tldr"}
{"id": "tldr.2509.e8b309b3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.practicalecommerce.com%2Frecover-chatgpt-404-traffic-with-ga4%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gZl0Ygp1wiWTsDYRfpYv5EvwqQwig5ze6qrQR2VvnL8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.practicalecommerce.com%2Frecover-chatgpt-404-traffic-with-ga4%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gZl0Ygp1wiWTsDYRfpYv5EvwqQwig5ze6qrQR2VvnL8=424", "authors": ["TLDR Newsletter"], "title": "Recover ChatGPT 404 Traffic with GA4", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.practicalecommerce.com%2Frecover-chatgpt-404-traffic-with-ga4%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gZl0Ygp1wiWTsDYRfpYv5EvwqQwig5ze6qrQR2VvnL8=424", "summary": "Recover ChatGPT 404 Traffic with GA4 (2 minute read) ChatGPT drives high-converting traffic but often generates faulty URLs, which can waste potential visits. An Ahrefs study found ChatGPT 5 links to error pages nearly three times more than Google Search. To recover lost traffic, start by tracking ChatGPT-driven 404 pages in Google Analytics using filters for session source and error page titles. Next, create helpful 404 pages that guide visitors when they land on hallucinated URLs. Use 301 r...", "source": "tldr"}
{"id": "tldr.2509.53fb9a47", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fcarmen-vicente_a-few-weeks-ago-i-was-feeling-annoyed-with-activity-7376252509373255680-ajpp%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oWRlFlFyYwVJ8E_vgEk6q47gMIPwBtuOOZlm1Q3hfTk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fcarmen-vicente_a-few-weeks-ago-i-was-feeling-annoyed-with-activity-7376252509373255680-ajpp%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oWRlFlFyYwVJ8E_vgEk6q47gMIPwBtuOOZlm1Q3hfTk=424", "authors": ["TLDR Newsletter"], "title": "How I vibe coded a social feed via Lovable", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fcarmen-vicente_a-few-weeks-ago-i-was-feeling-annoyed-with-activity-7376252509373255680-ajpp%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oWRlFlFyYwVJ8E_vgEk6q47gMIPwBtuOOZlm1Q3hfTk=424", "summary": "How I vibe coded a social feed via Lovable (2 minute read) A custom social feed was built by outlining a framework with features like voting, saves, topic filters, timestamps, breaking news alerts, and weekly reading stats. Lovable handled the technical build with Supabase, PWA support, and responsive design, while conversational feedback shaped iterations and aesthetics. Specific news and culture sources were added, unwanted ones excluded, and a widget was placed on the phone's homepage. The...", "source": "tldr"}
{"id": "tldr.2509.b982034b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fai%2F%3Futm_source=tldrmarketing%26utm_medium=secondaryad_924%26utm_campaign=25Q3_Connect_AI_Launch_homepage/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/KlgMEbILqxFaMBkrLXLTjO0yq08wZraGPV20UQ8k7KU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fai%2F%3Futm_source=tldrmarketing%26utm_medium=secondaryad_924%26utm_campaign=25Q3_Connect_AI_Launch_homepage/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/KlgMEbILqxFaMBkrLXLTjO0yq08wZraGPV20UQ8k7KU=424", "authors": ["TLDR Newsletter"], "title": "Make AI work with Connect AI", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fai%2F%3Futm_source=tldrmarketing%26utm_medium=secondaryad_924%26utm_campaign=25Q3_Connect_AI_Launch_homepage/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/KlgMEbILqxFaMBkrLXLTjO0yq08wZraGPV20UQ8k7KU=424", "summary": "Make AI work with Connect AI (Sponsor) Most marketing teams were early adopters of ChatGPT and Claude, using them for campaign briefs, social copy and more. But AI can do more than content generation. With Connect AI, securely link real-time data from Google Analytics, Salesforce, Marketo, and more in minutes to AI assistants for better reporting and faster decision-making. Superpower your marketing analytics and get started with Connect AI here", "source": "tldr"}
{"id": "tldr.2509.3833373e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgofishdigital.com%2Fresources%2Fwebinars%2Fpr-geo-strategies%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/PLFlrhf87VoCIKm6oAFpYKHx1mHZVcKn1NH9Hs8dpIk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgofishdigital.com%2Fresources%2Fwebinars%2Fpr-geo-strategies%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/PLFlrhf87VoCIKm6oAFpYKHx1mHZVcKn1NH9Hs8dpIk=424", "authors": ["TLDR Newsletter"], "title": "TODAY: The GEO & PR Strategies That Get You Found in ChatGPT & AI Search", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgofishdigital.com%2Fresources%2Fwebinars%2Fpr-geo-strategies%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/PLFlrhf87VoCIKm6oAFpYKHx1mHZVcKn1NH9Hs8dpIk=424", "summary": "TODAY: The GEO & PR Strategies That Get You Found in ChatGPT & AI Search (Webinar) Go Fish Digital is hosting a webinar that will cover how brands can get cited by top publications to boost brand visibility, SEO, and inclusion in LLM responses. The panel will discuss how PR drives visibility, the links and mentions that carry weight with LLMs, and how to influence knowledge panels and entity-driven results. The event takes place today at 2 pm ET.", "source": "tldr"}
{"id": "tldr.2509.dbac147a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fusetapestry.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/6XimhTkx3yNGoHb-YGx_wJ4JPaSCEicVSBvhuAw0qU0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fusetapestry.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/6XimhTkx3yNGoHb-YGx_wJ4JPaSCEicVSBvhuAw0qU0=424", "authors": ["TLDR Newsletter"], "title": "Tapestry", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fusetapestry.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/6XimhTkx3yNGoHb-YGx_wJ4JPaSCEicVSBvhuAw0qU0=424", "summary": "Tapestry (Tool) Tapestry is an app that gathers content from blogs, RSS feeds, YouTube channels, and other sources into a single chronological timeline. No algorithm interferes with the feed. It prioritizes privacy with no tracking, allows muting of topics and spoilers across feeds, and offers customization of layouts, fonts, and icons.", "source": "tldr"}
{"id": "tldr.2509.5623a238", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fbusiness%2Fmarketing%2Fblog%2Fmeasurement%2Flinkedin-introduces-b2b-attribution-and-analytics-with-marketing-partners%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/e-EwL45Ek1eIkbhOMzB-wcfadt8we1PJx0YJte66trw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fbusiness%2Fmarketing%2Fblog%2Fmeasurement%2Flinkedin-introduces-b2b-attribution-and-analytics-with-marketing-partners%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/e-EwL45Ek1eIkbhOMzB-wcfadt8we1PJx0YJte66trw=424", "authors": ["TLDR Newsletter"], "title": "LinkedIn Introduces B2B Attribution & Analytics with Marketing Partners", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fbusiness%2Fmarketing%2Fblog%2Fmeasurement%2Flinkedin-introduces-b2b-attribution-and-analytics-with-marketing-partners%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/e-EwL45Ek1eIkbhOMzB-wcfadt8we1PJx0YJte66trw=424", "summary": "LinkedIn Introduces B2B Attribution & Analytics with Marketing Partners (3 minute read) LinkedIn's Company Intelligence API shifts B2B measurement from lead-level tracking to company-level insights that reflect how real buying decisions are made. Early adopters reported a 287% increase in companies reached, 75% more MQLs, 96% more SQLs, and a 43% reduction in acquisition costs. The tooling allows marketers to see the full buyer journey, prioritize high-value accounts, improve attribution accu...", "source": "tldr"}
{"id": "tldr.2509.64dd4de8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbrandingstrategyinsider.com%2Fsolving-brand-trade-offs-with-paradoxical-promises%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/T2HC2__hkouJ1_Z_gPIJQp0VEcJZfMi-HTLb46n4vJE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbrandingstrategyinsider.com%2Fsolving-brand-trade-offs-with-paradoxical-promises%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/T2HC2__hkouJ1_Z_gPIJQp0VEcJZfMi-HTLb46n4vJE=424", "authors": ["TLDR Newsletter"], "title": "Solving Brand Trade-Offs With Paradoxical Promises", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbrandingstrategyinsider.com%2Fsolving-brand-trade-offs-with-paradoxical-promises%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/T2HC2__hkouJ1_Z_gPIJQp0VEcJZfMi-HTLb46n4vJE=424", "summary": "Solving Brand Trade-Offs With Paradoxical Promises (7 minute read) Brands often fall into the “Tyranny of the OR,” choosing single-dimensional strategies rather than embracing paradoxical promises that deliver both seemingly conflicting benefits. Unsuccessful examples include Cracker Barrel returning to its original premise and Jaguar abandoning its legacy for a new identity, each potentially missing opportunities to balance heritage with innovation. Research shows consumers dislike trade-off...", "source": "tldr"}
{"id": "tldr.2509.02fc7bb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "authors": ["TLDR Newsletter"], "title": "Your Brain on ChatGPT", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "summary": "Your Brain on ChatGPT (3 minute read) This study examined the cognitive impact of using ChatGPT for essay writing. It compared participants who used an LLM, a search engine, or only their own brain across multiple sessions. EEG data revealed that reliance on the LLM reduced neural connectivity and engagement compared with the Brain-only and Search Engine groups, and participants using the LLM showed weaker memory recall and lower ownership of their work. While LLMs offer immediate assistance,...", "source": "tldr"}
{"id": "tldr.2509.3287bbde", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fheikeyoung_ive-been-working-on-employee-generated-content-activity-7376245128690278400-UgsU%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/3R-_6QnFrXDVhhS0i5KRJVhqF-uBQq5ubqQ4L-7blwA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fheikeyoung_ive-been-working-on-employee-generated-content-activity-7376245128690278400-UgsU%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/3R-_6QnFrXDVhhS0i5KRJVhqF-uBQq5ubqQ4L-7blwA=424", "authors": ["TLDR Newsletter"], "title": "Lessons learned from employee-generated content at Microsoft", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fheikeyoung_ive-been-working-on-employee-generated-content-activity-7376245128690278400-UgsU%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/3R-_6QnFrXDVhhS0i5KRJVhqF-uBQq5ubqQ4L-7blwA=424", "summary": "Lessons learned from employee-generated content (EGC) at Microsoft (1 minute read) The head of content, social, and integrated marketing at Microsoft shares what's worked for their EGC program.", "source": "tldr"}
{"id": "tldr.2509.f75d3266", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Felliot-scott_gamified-emails-yes-please-we-recently-activity-7376216279218798592-0lmV%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oNhTc5Of6b0Yaaqbc8QvyAhNTk06LIgCrVSW84RllYg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Felliot-scott_gamified-emails-yes-please-we-recently-activity-7376216279218798592-0lmV%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oNhTc5Of6b0Yaaqbc8QvyAhNTk06LIgCrVSW84RllYg=424", "authors": ["TLDR Newsletter"], "title": "Zaymo Gamification Examples", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Felliot-scott_gamified-emails-yes-please-we-recently-activity-7376216279218798592-0lmV%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oNhTc5Of6b0Yaaqbc8QvyAhNTk06LIgCrVSW84RllYg=424", "summary": "Zaymo Gamification Examples (1 minute read) Examples of gamified emails built via Zaymo.", "source": "tldr"}
{"id": "tldr.2509.6356273f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsearchengineland.com%2Fgoogle-seasonal-bid-adjustments-app-campaigns-462368%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/vMTDu6FWM5Gwgx43d4xphgaNxhRYy88XuyH-gbICnrk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsearchengineland.com%2Fgoogle-seasonal-bid-adjustments-app-campaigns-462368%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/vMTDu6FWM5Gwgx43d4xphgaNxhRYy88XuyH-gbICnrk=424", "authors": ["TLDR Newsletter"], "title": "Google launches seasonal bid adjustments for app campaigns", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsearchengineland.com%2Fgoogle-seasonal-bid-adjustments-app-campaigns-462368%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/vMTDu6FWM5Gwgx43d4xphgaNxhRYy88XuyH-gbICnrk=424", "summary": "Google launches seasonal bid adjustments for app campaigns (1 minute read) The new beta for Google Ads lets app marketers apply seasonality signals to Smart Bidding, enabling proactive bid boosts during major promotions without waiting for algorithm changes.", "source": "tldr"}
{"id": "tldr.2509.30b05c22", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrmarketing%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oMLpuKKUAVbShMIrUuBZicJuV90DvBR8Kv1g0sBbQRk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrmarketing%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oMLpuKKUAVbShMIrUuBZicJuV90DvBR8Kv1g0sBbQRk=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrmarketing%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/oMLpuKKUAVbShMIrUuBZicJuV90DvBR8Kv1g0sBbQRk=424", "summary": "Google launches seasonal bid adjustments for app campaigns (1 minute read) The new beta for Google Ads lets app marketers apply seasonality signals to Smart Bidding, enabling proactive bid boosts during major promotions without waiting for algorithm changes.", "source": "tldr"}
{"id": "tldr.2509.8f70ff75", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.unscriptedconf.io%2Fcity%2Fvirtual%3Fcampaign_id=701Uw00000PArchIAD%26utm_source=tldr%26utm_medium=email-paid%23registration/2/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/w4_8IdNaPoZYt2RyaT-yy4pO2pjnwq_BOtdmU5xeis0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.unscriptedconf.io%2Fcity%2Fvirtual%3Fcampaign_id=701Uw00000PArchIAD%26utm_source=tldr%26utm_medium=email-paid%23registration/2/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/w4_8IdNaPoZYt2RyaT-yy4pO2pjnwq_BOtdmU5xeis0=424", "authors": ["TLDR Newsletter"], "title": "Last chance to explore the future software delivery at {unscripted} - save your virtual seat", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.unscriptedconf.io%2Fcity%2Fvirtual%3Fcampaign_id=701Uw00000PArchIAD%26utm_source=tldr%26utm_medium=email-paid%23registration/2/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/w4_8IdNaPoZYt2RyaT-yy4pO2pjnwq_BOtdmU5xeis0=424", "summary": "Last chance to explore the future software delivery at {unscripted} - save your virtual seat (Sponsor) After a highly successful tour of 7 cities, the {unscripted} conference is running one last virtual session - that you can join from anywhere, free. {unscripted} Virtual brings leaders from Sony, BlackRock, and Capital One together to show how AI is transforming DevOps. Discover strategies you will not find anywhere else and get a first look at the innovations set to change software delivery...", "source": "tldr"}
{"id": "tldr.2509.7b03cc9b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fper-customer-bot-defenses%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/COD94NSN0zbwJ0vOUySeo5N5difbbZDaBUWFtMlYk94=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fper-customer-bot-defenses%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/COD94NSN0zbwJ0vOUySeo5N5difbbZDaBUWFtMlYk94=424", "authors": ["TLDR Newsletter"], "title": "Building unique, per-customer defenses against advanced bot threats in the AI era", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fper-customer-bot-defenses%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/COD94NSN0zbwJ0vOUySeo5N5difbbZDaBUWFtMlYk94=424", "summary": "Building unique, per-customer defenses against advanced bot threats in the AI era (11 minute read) Cloudflare is launching a new, per-customer bot detection system that uses machine learning models to analyze website traffic and identify anomalous behavior specific to each application. The new system will leverage a unique defense for every application, with the goal of combating sophisticated, AI-driven web scraping and other malicious bot activities. During validation, 34% of the scraping r...", "source": "tldr"}
{"id": "tldr.2509.f8bd7e8d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcapnweb-javascript-rpc-library%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/I7mNNr3pSmT77VzflpvgBSpQPOr8x8WdeHltc62dT0g=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcapnweb-javascript-rpc-library%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/I7mNNr3pSmT77VzflpvgBSpQPOr8x8WdeHltc62dT0g=424", "authors": ["TLDR Newsletter"], "title": "Cap'n Web: a new RPC system for browsers and web servers", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcapnweb-javascript-rpc-library%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/I7mNNr3pSmT77VzflpvgBSpQPOr8x8WdeHltc62dT0g=424", "summary": "Cap'n Web: a new RPC system for browsers and web servers (12 minute read) Cloudflare's Cap'n Web is a lightweight, TypeScript-based RPC system designed for browsers, Workers, and Node.js that supports bidirectional calls, passing functions and objects by reference, promise pipelining, and capability-based security — all over JSON with minimal boilerplate. It aims to combine the simplicity of JavaScript APIs with the power of RPC. Cap'n Web offers an alternative to REST and GraphQL for real-ti...", "source": "tldr"}
{"id": "tldr.2509.cd908f41", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-security-explained%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/nKo16Wn5N190snAl7roRm1ZtiLz2RJMSt1WR2jQO0j4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-security-explained%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/nKo16Wn5N190snAl7roRm1ZtiLz2RJMSt1WR2jQO0j4=424", "authors": ["TLDR Newsletter"], "title": "MCP Security: A Developer's Guide", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-security-explained%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/nKo16Wn5N190snAl7roRm1ZtiLz2RJMSt1WR2jQO0j4=424", "summary": "MCP Security: A Developer's Guide (5 minute read) Model Context Protocol has rapidly become a key standard for connecting AI agents with tools and data, but its flexibility introduces serious risks such as command injection, misconfigurations, prompt injection, and supply chain attacks. Docker addresses these challenges with containerized MCP servers, a policy-enforcing Gateway, and a curated Catalog & Toolkit that provide secure defaults, active security controls, and standardized workflows ...", "source": "tldr"}
{"id": "tldr.2509.065c3b1d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnetflixtechblog.com%2Fempowering-netflix-engineers-with-incident-management-ebb967871de4%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/Xg3Feh9H0ZCz_Zhrr1h9Fue5BbUt-_GzmeZl1BXq08g=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnetflixtechblog.com%2Fempowering-netflix-engineers-with-incident-management-ebb967871de4%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/Xg3Feh9H0ZCz_Zhrr1h9Fue5BbUt-_GzmeZl1BXq08g=424", "authors": ["TLDR Newsletter"], "title": "Empowering Netflix Engineers with Incident Management", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnetflixtechblog.com%2Fempowering-netflix-engineers-with-incident-management-ebb967871de4%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/Xg3Feh9H0ZCz_Zhrr1h9Fue5BbUt-_GzmeZl1BXq08g=424", "summary": "Empowering Netflix Engineers with Incident Management (6 minute read) Netflix shifted incident management from a centralized SRE function to an accessible, company-wide practice by adopting Incident.io and investing in cultural change. The intuitive tooling, internal integrations, and lightweight but consistent processes empowered engineers to declare and manage incidents themselves, driving faster responses and continuous learning across teams.", "source": "tldr"}
{"id": "tldr.2509.47ad1c13", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-golden-path-taming-k8s-complexity-in-favour-of-devex%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/IyX-IyYbFifdlvhDYiB2GqjPNDf8xNMzT_JpJQlqCPA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-golden-path-taming-k8s-complexity-in-favour-of-devex%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/IyX-IyYbFifdlvhDYiB2GqjPNDf8xNMzT_JpJQlqCPA=424", "authors": ["TLDR Newsletter"], "title": "The golden path: Taming K8s complexity in favour of DevEx", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-golden-path-taming-k8s-complexity-in-favour-of-devex%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/IyX-IyYbFifdlvhDYiB2GqjPNDf8xNMzT_JpJQlqCPA=424", "summary": "The golden path: Taming K8s complexity in favour of DevEx (9 minute read) The golden path approach simplifies Kubernetes by providing opinionated, automated workflows that reduce complexity, standardize deployments, and embed security, observability, and automation from the start. It accelerates delivery and lowers operational overhead while enabling faster iteration and competitive business agility.", "source": "tldr"}
{"id": "tldr.2509.8db05cc3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cockroachlabs.com%2Fwebinars%2Fscalable-sql-databases-on-k8s%2F%3Futm_source=tldr%26utm_medium=sponsor%26utm_campaign=tldr-devops-sponsor-global-tofu-data-mod-app-mod-webinar-k8s-q3/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/u5fER4wE0dO5JdF5DyY-9Xqu4mJYoM23TWIUL5JZIXU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cockroachlabs.com%2Fwebinars%2Fscalable-sql-databases-on-k8s%2F%3Futm_source=tldr%26utm_medium=sponsor%26utm_campaign=tldr-devops-sponsor-global-tofu-data-mod-app-mod-webinar-k8s-q3/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/u5fER4wE0dO5JdF5DyY-9Xqu4mJYoM23TWIUL5JZIXU=424", "authors": ["TLDR Newsletter"], "title": "What does it take to scale SQL databases reliably on Kubernetes?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cockroachlabs.com%2Fwebinars%2Fscalable-sql-databases-on-k8s%2F%3Futm_source=tldr%26utm_medium=sponsor%26utm_campaign=tldr-devops-sponsor-global-tofu-data-mod-app-mod-webinar-k8s-q3/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/u5fER4wE0dO5JdF5DyY-9Xqu4mJYoM23TWIUL5JZIXU=424", "summary": "What does it take to scale SQL databases reliably on Kubernetes? (45 min webinar) (Sponsor) Learn what it takes to run stateful SQL workloads in K8s, from running multi-tenant databases at scale to stress-testing under failure conditions. Through real production lessons, see how colocating your database with microservices can unlock operational gains with CockroachDB.Register here", "source": "tldr"}
{"id": "tldr.2509.745a121f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2025%2F09%2F23%2Fsolving-kubernetes-multi-tenancy-challenges-with-vcluster%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/11okXmL8CWhgBelcFW9mHWps1TR7Llp9Nq_J1RFg8WM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2025%2F09%2F23%2Fsolving-kubernetes-multi-tenancy-challenges-with-vcluster%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/11okXmL8CWhgBelcFW9mHWps1TR7Llp9Nq_J1RFg8WM=424", "authors": ["TLDR Newsletter"], "title": "Solving Kubernetes Multi-tenancy Challenges with vCluster", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2025%2F09%2F23%2Fsolving-kubernetes-multi-tenancy-challenges-with-vcluster%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/11okXmL8CWhgBelcFW9mHWps1TR7Llp9Nq_J1RFg8WM=424", "summary": "Solving Kubernetes Multi-tenancy Challenges with vCluster (10 minute read) vCluster by LoftLab is a solution for Kubernetes multi-tenancy challenges in Internal Developer Platforms (IDPs), especially for teams needing cluster-scoped resources like CRDs. Virtual clusters allow teams administrative control in isolated environments while integrating with tools like Falco and Kyverno for security and policy enforcement, although some synchronization challenges exist with Kyverno.", "source": "tldr"}
{"id": "tldr.2509.8c2aa6a8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgin-gonic%2Fgin%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/ejol-K6kogvfwkfp5-GYvKFx2D7aXzPKglXgJt0tDKE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgin-gonic%2Fgin%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/ejol-K6kogvfwkfp5-GYvKFx2D7aXzPKglXgJt0tDKE=424", "authors": ["TLDR Newsletter"], "title": "Gin Web Framework", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgin-gonic%2Fgin%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/ejol-K6kogvfwkfp5-GYvKFx2D7aXzPKglXgJt0tDKE=424", "summary": "Gin Web Framework (GitHub Repo) Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performance—up to 40 times faster—thanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices where speed and developer productivity are essential.", "source": "tldr"}
{"id": "tldr.2509.f131ef0e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F09%2F11%2Fdebug-query-and-build-faster-with-grafana-assistant%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uDHkPBAhYP_PUTQr2CICzWIBJyTDDjTuY50fYqk3cvg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F09%2F11%2Fdebug-query-and-build-faster-with-grafana-assistant%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uDHkPBAhYP_PUTQr2CICzWIBJyTDDjTuY50fYqk3cvg=424", "authors": ["TLDR Newsletter"], "title": "Debug, query, and build faster with AI: How we use Grafana Assistant at Grafana Labs", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F09%2F11%2Fdebug-query-and-build-faster-with-grafana-assistant%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uDHkPBAhYP_PUTQr2CICzWIBJyTDDjTuY50fYqk3cvg=424", "summary": "Debug, query, and build faster with AI: How we use Grafana Assistant at Grafana Labs (10 minute read) Grafana Assistant, now in public preview on Grafana Cloud, helps developers and users save time by simplifying query writing, explaining logs and traces in plain language, automating dashboard edits, and easing onboarding. It integrates directly into workflows to reduce friction, improve productivity, and make observability tasks faster and more intuitive without replacing human oversight.", "source": "tldr"}
{"id": "tldr.2509.41b1785a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmpc-horror-stories-cve-2025-49596-local-host-breach%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/MAkGZOfSLk_MSUg-7DIgOsdN5K1zTT2IhPy0Q5Wg34c=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmpc-horror-stories-cve-2025-49596-local-host-breach%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/MAkGZOfSLk_MSUg-7DIgOsdN5K1zTT2IhPy0Q5Wg34c=424", "authors": ["TLDR Newsletter"], "title": "MCP Horror Stories: The Drive-By Localhost Breach", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmpc-horror-stories-cve-2025-49596-local-host-breach%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/MAkGZOfSLk_MSUg-7DIgOsdN5K1zTT2IhPy0Q5Wg34c=424", "summary": "MCP Horror Stories: The Drive-By Localhost Breach (10 minute read) A critical vulnerability, CVE-2025-49596, was discovered in MCP Inspector, a debugging tool with over 78,000 weekly downloads, turning it into a drive-by attack platform that compromises developer machines upon visiting a malicious website. The vulnerability, which scores 9.4 out of 10 on the CVSS scale, allows attackers to gain control of the MCP Inspector interface and access private files by exploiting a browser implementat...", "source": "tldr"}
{"id": "tldr.2509.09c06c66", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.atspotify.com%2F2025%2F9%2Fspotifys-experiments-with-learning-framework%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uUMlNe1GZeD87pGQYekmpjBZfnSfuX51fL4YnK-eRlU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.atspotify.com%2F2025%2F9%2Fspotifys-experiments-with-learning-framework%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uUMlNe1GZeD87pGQYekmpjBZfnSfuX51fL4YnK-eRlU=424", "authors": ["TLDR Newsletter"], "title": "Beyond Winning: Spotify's Experiments with Learning Framework", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.atspotify.com%2F2025%2F9%2Fspotifys-experiments-with-learning-framework%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/uUMlNe1GZeD87pGQYekmpjBZfnSfuX51fL4YnK-eRlU=424", "summary": "Beyond Winning: Spotify's Experiments with Learning Framework (12 minute read) Spotify's experimentation platform, Confidence, uses the Experiments with Learning (EwL) metric to measure experiment success by whether experiments yielded enough valid information to inform product decisions. Spotify's learning rate is ~64%, but its win rate is ~12%, which emphasizes that most value comes from understanding what doesn't work or detecting regressions, not just shipping improvements.", "source": "tldr"}
{"id": "tldr.2509.4ec7bcd7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcvemon.intruder.io%2F%3Futm_source=tldrdevops%26utm_medium=p_referral%26utm_campaign=global%257Cfixed%257Ccvemon_24_10_25/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/iWH_2HDUNDeMuW9_zYEysXO4H41Tmwr80H8bs9gezfg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcvemon.intruder.io%2F%3Futm_source=tldrdevops%26utm_medium=p_referral%26utm_campaign=global%257Cfixed%257Ccvemon_24_10_25/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/iWH_2HDUNDeMuW9_zYEysXO4H41Tmwr80H8bs9gezfg=424", "authors": ["TLDR Newsletter"], "title": "Tired of finding out about critical CVEs from X/Twitter threads?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcvemon.intruder.io%2F%3Futm_source=tldrdevops%26utm_medium=p_referral%26utm_campaign=global%257Cfixed%257Ccvemon_24_10_25/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/iWH_2HDUNDeMuW9_zYEysXO4H41Tmwr80H8bs9gezfg=424", "summary": "Tired of finding out about critical CVEs from X/Twitter threads? (Sponsor) cvemon is a free resource by Intruder that monitors the security buzz across social media 24/7. It shows a 'hype score' for each new vulnerability along with expert commentary. Check out the live feed", "source": "tldr"}
{"id": "tldr.2509.5f1bf8d9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-opensearch-star-tree-index%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/VaNcp19jrXcSeTjqc_K6NUGWtIbPoNufUgdC3Yjms3E=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-opensearch-star-tree-index%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/VaNcp19jrXcSeTjqc_K6NUGWtIbPoNufUgdC3Yjms3E=424", "authors": ["TLDR Newsletter"], "title": "Amazon OpenSearch Service announces Star-Tree Index", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-opensearch-star-tree-index%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/VaNcp19jrXcSeTjqc_K6NUGWtIbPoNufUgdC3Yjms3E=424", "summary": "Amazon OpenSearch Service announces Star-Tree Index (1 minute read) Amazon OpenSearch Service's Star-Tree Index is a feature that pre-aggregates data at ingestion to deliver sub-second response times for high-cardinality and multi-dimensional queries.", "source": "tldr"}
{"id": "tldr.2509.213386f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-ecs-task-definition-q-developer%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/UGN7-z2t4Srjv_UxktIScJpuZnohKEneMP0oFK4ddHU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-ecs-task-definition-q-developer%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/UGN7-z2t4Srjv_UxktIScJpuZnohKEneMP0oFK4ddHU=424", "authors": ["TLDR Newsletter"], "title": "Amazon ECS enhances task definition editing in the AWS Console with Amazon Q Developer", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F09%2Famazon-ecs-task-definition-q-developer%2F%3Futm_source=tldrdevops/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/UGN7-z2t4Srjv_UxktIScJpuZnohKEneMP0oFK4ddHU=424", "summary": "Amazon ECS enhances task definition editing in the AWS Console with Amazon Q Developer (2 minute read) Amazon ECS now integrates Amazon Q Developer into the AWS Console to streamline creating and updating task definitions with AI-generated code suggestions and inline chat support.", "source": "tldr"}
{"id": "tldr.2509.f29080b4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/o-jQExZTRYfUav996M7cPlDwAi166uS61vwHRaPFyVM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/o-jQExZTRYfUav996M7cPlDwAi166uS61vwHRaPFyVM=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b6708c3-ffb9f835-03d2-467e-b4c8-dc8a84ceb328-000000/o-jQExZTRYfUav996M7cPlDwAi166uS61vwHRaPFyVM=424", "summary": "Amazon ECS enhances task definition editing in the AWS Console with Amazon Q Developer (2 minute read) Amazon ECS now integrates Amazon Q Developer into the AWS Console to streamline creating and updating task definitions with AI-generated code suggestions and inline chat support.", "source": "tldr"}
{"id": "tldr.2509.2b52d9a6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "authors": ["TLDR Newsletter"], "title": "Getting AI to Work in Complex Codebases", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "summary": "Getting AI to Work in Complex Codebases (20 minute read) With core context engineering principles and frequent intentional compaction, current AI models can be great at handling large, detailed codebases. AI can rarely produce correct code in one shot in large code bases, so it needs steps before implementation to do research on the codebase and plan its steps.", "source": "tldr"}
{"id": "tldr.2509.32af0f82", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdistributed-computing-musings.com%2F2025%2F08%2Fthundering-herd-problem-preventing-the-stampede%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/QmEil5oxuYzgYmtAu04uElCpuxltdLxMU_TT29ojX5E=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdistributed-computing-musings.com%2F2025%2F08%2Fthundering-herd-problem-preventing-the-stampede%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/QmEil5oxuYzgYmtAu04uElCpuxltdLxMU_TT29ojX5E=424", "authors": ["TLDR Newsletter"], "title": "Thundering Herd Problem: Preventing the Stampede", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdistributed-computing-musings.com%2F2025%2F08%2Fthundering-herd-problem-preventing-the-stampede%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/QmEil5oxuYzgYmtAu04uElCpuxltdLxMU_TT29ojX5E=424", "summary": "Thundering Herd Problem: Preventing the Stampede (8 minute read) The \"thundering herd\" problem occurs when a cache miss leads to a large number of concurrent requests hitting the database for the same record, overwhelming it. There are two solutions: using a distributed lock to allow only one request to access the database, or using in-process synchronization with (as an example) Java's `CompletableFuture` and `ConcurrentHashMap`. While the distributed lock works across multiple nodes, the in...", "source": "tldr"}
{"id": "tldr.2509.563cdaaa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshiftmag.dev%2Fhow-infobips-infrastructure-handled-10-billion-messages-in-a-day-6162%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/GaNzUjEofh4-x8Q94iP7IhYDPULbssxLwj-PmBHhCLY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshiftmag.dev%2Fhow-infobips-infrastructure-handled-10-billion-messages-in-a-day-6162%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/GaNzUjEofh4-x8Q94iP7IhYDPULbssxLwj-PmBHhCLY=424", "authors": ["TLDR Newsletter"], "title": "How Infobip's Infrastructure Team Handled 10 Billion Messages in a Day", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshiftmag.dev%2Fhow-infobips-infrastructure-handled-10-billion-messages-in-a-day-6162%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/GaNzUjEofh4-x8Q94iP7IhYDPULbssxLwj-PmBHhCLY=424", "summary": "How Infobip's Infrastructure Team Handled 10 Billion Messages in a Day (7 minute read) Infobip's infrastructure team handles 10 billion messages daily. They overcame issues like email overloads, data center failures, and scaling for WhatsApp Business by using cloud solutions, re-architecting systems, and adopting Kubernetes. A critical decommissioning mistake in 2024 led to the implementation of delayed destructive actions, preventing data loss.", "source": "tldr"}
{"id": "tldr.2509.a4f6eccc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fajmoon.com%2Fposts%2Fmesh-i-tried-htmx-then-ditched-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/03p6L86OX9SwxS5jY0-asHtBQLN_Qgs8wdli6ImIhG8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fajmoon.com%2Fposts%2Fmesh-i-tried-htmx-then-ditched-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/03p6L86OX9SwxS5jY0-asHtBQLN_Qgs8wdli6ImIhG8=424", "authors": ["TLDR Newsletter"], "title": "I tried HTMX, then ditched it", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 16 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fajmoon.com%2Fposts%2Fmesh-i-tried-htmx-then-ditched-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/03p6L86OX9SwxS5jY0-asHtBQLN_Qgs8wdli6ImIhG8=424", "summary": "I tried HTMX, then ditched it (16 minute read) While appreciating HTMX's potential to reduce JavaScript reliance, this dev found its lack of structure problematic, leading him to create MESH, a modular SSR framework based on the \"one component = one endpoint\" principle. He concludes that he's not using HTMX in the way it's intended and drops it in favor of simpler JavaScript and SSE.", "source": "tldr"}
{"id": "tldr.2509.58334e30", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freasonunderpressure.com%2Fblog%2Fposts%2Fyour-images-are-probably-oversized%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/3TeVMR78K01GytWE6a-ZUXeaqzpUhxIeoG6_ZXrVmco=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freasonunderpressure.com%2Fblog%2Fposts%2Fyour-images-are-probably-oversized%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/3TeVMR78K01GytWE6a-ZUXeaqzpUhxIeoG6_ZXrVmco=424", "authors": ["TLDR Newsletter"], "title": "Your Images Are Oversized", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 21 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freasonunderpressure.com%2Fblog%2Fposts%2Fyour-images-are-probably-oversized%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/3TeVMR78K01GytWE6a-ZUXeaqzpUhxIeoG6_ZXrVmco=424", "summary": "Your Images Are (Probably) Oversized (21 minute read) Serving appropriately sized images for different screen sizes avoids wasting bandwidth and compute power. This can be done by using the `srcset` and `sizes` attributes on the `img` tag to specify multiple image sources and their intended widths based on viewport size.", "source": "tldr"}
{"id": "tldr.2509.0bd748d3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sitepoint.com%2Ftest-your-page-without-javascript%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/calpEvW6kMyT55boW-yJcAl3rvl-PUCF4c8RhDsiuUk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sitepoint.com%2Ftest-your-page-without-javascript%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/calpEvW6kMyT55boW-yJcAl3rvl-PUCF4c8RhDsiuUk=424", "authors": ["TLDR Newsletter"], "title": "Why You Should Test Your Page Without JavaScript", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sitepoint.com%2Ftest-your-page-without-javascript%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/calpEvW6kMyT55boW-yJcAl3rvl-PUCF4c8RhDsiuUk=424", "summary": "Why You Should Test Your Page Without JavaScript (5 minute read) Devs should test their websites without JavaScript enabled to check accessibility and resilience. JavaScript can fail due to slow networks, browser extensions, or user settings, so sites should use progressive enhancement, starting with semantic HTML that works without JS and adding JavaScript features as enhancements.", "source": "tldr"}
{"id": "tldr.2509.9ccc4381", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FVqej45v%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/9qeuBPMGZ_WAmD9793kga5wA4NJ1509HvRoSEBFIGp8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FVqej45v%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/9qeuBPMGZ_WAmD9793kga5wA4NJ1509HvRoSEBFIGp8=424", "authors": ["TLDR Newsletter"], "title": "How to avoid common Authentication Mistakes", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FVqej45v%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/9qeuBPMGZ_WAmD9793kga5wA4NJ1509HvRoSEBFIGp8=424", "summary": "How to avoid common Authentication Mistakes (Sponsor) From token storage to session handling, this guide walks through the security practices every frontend developer should know. Learn which approaches protect your users and which mistakes to avoid when building authentication into modern web applications.", "source": "tldr"}
{"id": "tldr.2509.08d648e6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftanstack.com%2Fblog%2Fannouncing-tanstack-start-v1%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/b0KGI_n2TRNhS6z1aFWKfcILt3LL4SgAfTsToOCvy78=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftanstack.com%2Fblog%2Fannouncing-tanstack-start-v1%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/b0KGI_n2TRNhS6z1aFWKfcILt3LL4SgAfTsToOCvy78=424", "authors": ["TLDR Newsletter"], "title": "TanStack Start v1 Release Candidate", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftanstack.com%2Fblog%2Fannouncing-tanstack-start-v1%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/b0KGI_n2TRNhS6z1aFWKfcILt3LL4SgAfTsToOCvy78=424", "summary": "TanStack Start v1 Release Candidate (3 minute read) TanStack Start has reached its v1.0 Release Candidate. This version includes type-safe routing, server-first functions, built-in streaming, and URL-as-state primitives, all while offering flexibility for both SPA and SSR architectures.", "source": "tldr"}
{"id": "tldr.2509.dec805d1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.metabase.com%2Fai-data-generator%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RWyieVT6PAHW94DbBVAihRapmyF4fO4yrDF4fXQnvYs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.metabase.com%2Fai-data-generator%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RWyieVT6PAHW94DbBVAihRapmyF4fO4yrDF4fXQnvYs=424", "authors": ["TLDR Newsletter"], "title": "Open source AI Data Generator", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.metabase.com%2Fai-data-generator%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RWyieVT6PAHW94DbBVAihRapmyF4fO4yrDF4fXQnvYs=424", "summary": "Open source AI Data Generator (Website) The AI Data Generator helps users generate sample data with various input parameters, export it in various formats, and explore it within Metabase.", "source": "tldr"}
{"id": "tldr.2509.562937b5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "authors": ["TLDR Newsletter"], "title": "Exploring Active Agent, or can we build AI features the Rails way?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "summary": "Exploring Active Agent, or can we build AI features the Rails way? (8 minute read) Active Agent is a Ruby gem that helps integrate AI features into Rails applications using familiar Rails conventions and patterns. It introduces \"agents\" as a new abstraction that encapsulates AI-backed logic, using action-driven objects, callbacks, and prompt rendering similar to Rails controllers and mailers.", "source": "tldr"}
{"id": "tldr.2509.cd3e5604", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.crunchydata.com%2Fblog%2Fthe-postgres-project-original-goals-and-how-the-creators-totally-nailed-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/1UyUNwqSVVV3035IuJ7m4XPywxzNyuysOTX0Q0ygnR4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.crunchydata.com%2Fblog%2Fthe-postgres-project-original-goals-and-how-the-creators-totally-nailed-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/1UyUNwqSVVV3035IuJ7m4XPywxzNyuysOTX0Q0ygnR4=424", "authors": ["TLDR Newsletter"], "title": "Postgres' Original Project Goals: The Creators Totally Nailed It", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.crunchydata.com%2Fblog%2Fthe-postgres-project-original-goals-and-how-the-creators-totally-nailed-it%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/1UyUNwqSVVV3035IuJ7m4XPywxzNyuysOTX0Q0ygnR4=424", "summary": "Postgres' Original Project Goals: The Creators Totally Nailed It (9 minute read) PostgreSQL's original design goals from 1986 have shaped its modern success. These goals included support for complex objects, extensibility, active database features, and crash recovery, laying a flexible framework. Postgres' adherence to these principles has cemented its position as a versatile and trusted database today.", "source": "tldr"}
{"id": "tldr.2509.40211656", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farchive.md%2FE9V9P%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RKF-LJB1_PswS1-1vSKRkw-8IvMNNBFagVUW38cTUy4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farchive.md%2FE9V9P%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RKF-LJB1_PswS1-1vSKRkw-8IvMNNBFagVUW38cTUy4=424", "authors": ["TLDR Newsletter"], "title": "VCs to AI Startups: Please Take Our Money", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farchive.md%2FE9V9P%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/RKF-LJB1_PswS1-1vSKRkw-8IvMNNBFagVUW38cTUy4=424", "summary": "VCs to AI Startups: Please Take Our Money (7 minute read) VCs are aggressively pursuing investments in top AI startups with gifts and preemptive funding rounds. Companies like Decagon AI are fielding unsolicited offers at soaring valuations. This frenzy is fueled by the belief that AI startups can disrupt tech giants, with a large portion of venture capital flowing to a small group of frontrunners.", "source": "tldr"}
{"id": "tldr.2509.8a4ff67e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "authors": ["TLDR Newsletter"], "title": "Context Engineering for AI Agents: Lessons from Building Manus", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "summary": "Context Engineering for AI Agents: Lessons from Building Manus (11 minute read) Manus is an AI agent that had improved performance through context engineering techniques. The devs optimized the KV-cache hit rate by maintaining a stable prompt prefix with append-only context. They advise against dynamically changing the action space. The file system should be used as an extended memory, along with recitative methods to manipulate attention.", "source": "tldr"}
{"id": "tldr.2509.4a9bd897", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshkspr.mobi%2Fblog%2F2025%2F09%2Ftargetting-specific-characters-with-css-rules%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/6msqjiJ4gniZYZpPwr8KS9hyCz2tr8X8moWBKoKaeJA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshkspr.mobi%2Fblog%2F2025%2F09%2Ftargetting-specific-characters-with-css-rules%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/6msqjiJ4gniZYZpPwr8KS9hyCz2tr8X8moWBKoKaeJA=424", "authors": ["TLDR Newsletter"], "title": "Targeting specific characters with CSS rules", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshkspr.mobi%2Fblog%2F2025%2F09%2Ftargetting-specific-characters-with-css-rules%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/6msqjiJ4gniZYZpPwr8KS9hyCz2tr8X8moWBKoKaeJA=424", "summary": "Targeting specific characters with CSS rules (4 minute read) CSS can be used to target specific characters for styling through manipulating fonts and Unicode ranges, though the practical applications are limited.", "source": "tldr"}
{"id": "tldr.2509.82de62a9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "authors": ["TLDR Newsletter"], "title": "How Google's dev tools manager makes AI coding work", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "summary": "How Google's dev tools manager makes AI coding work (5 minute read) AI coding tools like Gemini CLI are being used for software development by automating code generation from natural language requirements and shifting the developer's role towards architecture and problem decomposition.", "source": "tldr"}
{"id": "tldr.2509.31114a4f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/Gr0BiemnoVP5SrPXIrZeXXxPIoAIiM3002pg3Ge1Cs8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/Gr0BiemnoVP5SrPXIrZeXXxPIoAIiM3002pg3Ge1Cs8=424", "authors": ["TLDR Newsletter"], "title": "The Risks of NPM", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jim-nielsen.com%2F2025%2Fnpm-risks%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/Gr0BiemnoVP5SrPXIrZeXXxPIoAIiM3002pg3Ge1Cs8=424", "summary": "The Risks of NPM (3 minute read) A recent NPM attack, the Qix incident, shows the risk of malicious code injected into widely-used packages targeting end-users by way of bundled website code.", "source": "tldr"}
{"id": "tldr.2509.64aca182", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstefvanwijchen.com%2Freact-and-redux-in-2025%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/C70p_9JycqgkIpvXl3XqSciqyTANqOXPJY7PQgIdw58=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstefvanwijchen.com%2Freact-and-redux-in-2025%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/C70p_9JycqgkIpvXl3XqSciqyTANqOXPJY7PQgIdw58=424", "authors": ["TLDR Newsletter"], "title": "Redux in 2025: A reliable choice for complex React projects", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstefvanwijchen.com%2Freact-and-redux-in-2025%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/C70p_9JycqgkIpvXl3XqSciqyTANqOXPJY7PQgIdw58=424", "summary": "Redux in 2025: A reliable choice for complex React projects (8 minute read) Redux is still a reliable choice for complex React projects due to its predictability, observability, and architectural clarity.", "source": "tldr"}
{"id": "tldr.2509.bfa626a7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/AyVlQodWhGEbSLFgg83jiKedZsoSUfSBf5eiMsPeuU4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/AyVlQodWhGEbSLFgg83jiKedZsoSUfSBf5eiMsPeuU4=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/AyVlQodWhGEbSLFgg83jiKedZsoSUfSBf5eiMsPeuU4=424", "summary": "Redux in 2025: A reliable choice for complex React projects (8 minute read) Redux is still a reliable choice for complex React projects due to its predictability, observability, and architectural clarity.", "source": "tldr"}
{"id": "tldr.2509.e65753a0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FK3VLXY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/-SodUKeIoU21Wvl6-b5BNXqootNYeWfHOXlSzMRd0Nw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FK3VLXY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/-SodUKeIoU21Wvl6-b5BNXqootNYeWfHOXlSzMRd0Nw=424", "authors": ["TLDR Newsletter"], "title": "Bitcoin Eyes Powell and Inflation Data After Leverage Flush", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FK3VLXY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/-SodUKeIoU21Wvl6-b5BNXqootNYeWfHOXlSzMRd0Nw=424", "summary": "Bitcoin Eyes Powell and Inflation Data After Leverage Flush (2 minute read) Bitcoin is consolidating below $113K as traders await Fed Chair Jerome Powell's speech and Friday's core PCE inflation data, which could shape Q4 momentum. Analysts flagged $115,200 as a key support holding above opens the door to retesting highs, while a break lower risks a slide to $105K. After last week's $1.7B leverage flush, liquidity has returned, whales are accumulating, and options demand is clustering around ...", "source": "tldr"}
{"id": "tldr.2509.960aeebc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F36o6IY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/vOkEw9aBlUETJNPjUfzroovcMWVknGUm9zQKiUU9NtQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F36o6IY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/vOkEw9aBlUETJNPjUfzroovcMWVknGUm9zQKiUU9NtQ=424", "authors": ["TLDR Newsletter"], "title": "White House Targets Year-End Passage for Crypto Market Structure Bill", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F36o6IY%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/vOkEw9aBlUETJNPjUfzroovcMWVknGUm9zQKiUU9NtQ=424", "summary": "White House Targets Year-End Passage for Crypto Market Structure Bill (2 minute read) Patrick Witt, Executive Director of the White House Council of Advisors on Digital Assets, said he expects the sweeping crypto market structure bill to pass before the end of 2025. The legislation includes the CLARITY Act and the Responsible Financial Innovation Act, which establish a comprehensive U.S. framework for digital assets, dividing oversight between the SEC and CFTC.", "source": "tldr"}
{"id": "tldr.2509.3da65dde", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970155125374144836.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/gQOyOhHevtuyY4h2MKZqgZmDQ-nMDuIcDpt_Y1STm9s=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970155125374144836.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/gQOyOhHevtuyY4h2MKZqgZmDQ-nMDuIcDpt_Y1STm9s=424", "authors": ["TLDR Newsletter"], "title": "Kalshi Launches Crypto Pre-Markets", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970155125374144836.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/gQOyOhHevtuyY4h2MKZqgZmDQ-nMDuIcDpt_Y1STm9s=424", "summary": "Kalshi Launches Crypto Pre-Markets (2 minute read) Kalshi's Crypto Pre-Markets let users speculate on events tied to pre-token projects, like whether PumpFun, OpenSea, Monad, or Hyperliquid will launch tokens or conduct airdrops. This opens up tradable exposure to topics that currently lack direct markets, similar to how Pendle drove adoption via points speculation. Prediction markets are increasingly becoming a central venue for crypto-native speculation before tokens exist, giving traders a...", "source": "tldr"}
{"id": "tldr.2509.0b58c949", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "authors": ["TLDR Newsletter"], "title": "Coinbase Announces x402 Foundation with Cloudflare", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "summary": "Coinbase Announces x402 Foundation with Cloudflare (5 minute read) x402, the open standard for onchain agentic commerce created by Coinbase, will become controlled by an independent foundation called the x402 Foundation. The foundation will be co-founded by Coinbase and Cloudflare – one of the largest internet infrastructure providers who recently announced an agentic pay-per-crawl option for websites – with more partners to be announced soon. Creating a foundation is a significant step towar...", "source": "tldr"}
{"id": "tldr.2509.d6266bc0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FHS6r1C%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Txdg9caxEpTIiz14B2prbOMrXqZRbh2Sl2CCFgp9lcs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FHS6r1C%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Txdg9caxEpTIiz14B2prbOMrXqZRbh2Sl2CCFgp9lcs=424", "authors": ["TLDR Newsletter"], "title": "Findings from testing 20+ Prediction Markets", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FHS6r1C%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Txdg9caxEpTIiz14B2prbOMrXqZRbh2Sl2CCFgp9lcs=424", "summary": "Findings from testing 20+ Prediction Markets (15 minute read) Kalshi and Polymarket dominate 99% of the prediction market landscape. Kalshi captured 63% market share in early September due to a surge in sports prediction activity. Both platforms actively court builders with different approaches, targeting crypto Twitter communities and DeFi ecosystem developers. Five product categories are emerging across the ecosystem: DeFi protocols enable lending and borrowing against prediction market pos...", "source": "tldr"}
{"id": "tldr.2509.b89f13a5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fulfnxi%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/qPsBKFiUsEQjxGP0GzQ9mdU2AkNJmFXK-QjZn-8NdsM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fulfnxi%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/qPsBKFiUsEQjxGP0GzQ9mdU2AkNJmFXK-QjZn-8NdsM=424", "authors": ["TLDR Newsletter"], "title": "EulerSwap vs Fluid DEX", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fulfnxi%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/qPsBKFiUsEQjxGP0GzQ9mdU2AkNJmFXK-QjZn-8NdsM=424", "summary": "EulerSwap vs Fluid DEX (10 minute read) Fluid DEX maintains a structural advantage in non-volatile pairs like USDC-USDT through a unique Smart Debt feature that turns normal borrowers into passive liquidity providers by allowing debt to be denominated as DEX pool shares rather than single assets, reducing borrowing costs by up to 1.15% APY and creating deeper liquidity than EulerSwap's market maker-only approach. The data supports this advantage with Fluid ranking as the #2 DEX on Ethereum wi...", "source": "tldr"}
{"id": "tldr.2509.edd6458d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970284933596750193.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/HqYJsGenqfbXU2NTwJLDNPbKyXpdsKVzHFeVKFra5iE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970284933596750193.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/HqYJsGenqfbXU2NTwJLDNPbKyXpdsKVzHFeVKFra5iE=424", "authors": ["TLDR Newsletter"], "title": "Aerodrome Redefines DEX Tokenomics", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970284933596750193.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/HqYJsGenqfbXU2NTwJLDNPbKyXpdsKVzHFeVKFra5iE=424", "summary": "Aerodrome Redefines DEX Tokenomics (2 minute read) Aerodrome has distributed $1 of revenue to $veAERO voters for every $723 in trading volume, a model that would have paid out $4.3B if applied to Uniswap's $3.1T in 5-year volume. For comparison, that equals 88% of UNI's current market cap. However, Uniswap has returned nothing to token holders. In the past 30 days alone, Aerodrome distributed $35M to voters, outpacing the combined revenue of PancakeSwap, Curve, and Jupiter.", "source": "tldr"}
{"id": "tldr.2509.47619a4b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970207366739046788.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Z0ou5eqdshk0XF68j1Tv3RKBaYtplEFtszFc_1OIvk0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970207366739046788.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Z0ou5eqdshk0XF68j1Tv3RKBaYtplEFtszFc_1OIvk0=424", "authors": ["TLDR Newsletter"], "title": "Pendle Strategy: Short-Term Principal Tokens and Yield Maximization", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970207366739046788.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Z0ou5eqdshk0XF68j1Tv3RKBaYtplEFtszFc_1OIvk0=424", "summary": "Pendle Strategy: Short-Term Principal Tokens and Yield Maximization (2 minute read) Pendle users often target longer-duration principal tokens (PTs) for fixed yields, but pre-token generation event demand for yield tokens (YTs) can create attractive opportunities in shorter-term PTs. When traders buy YTs, their price rises, pushing down PT prices and boosting PT fixed yields. As shown with USDe pools nearing maturity, PTs with just two days left were offering extreme yields (15–25%), thus sho...", "source": "tldr"}
{"id": "tldr.2509.048c4133", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F0bbxl9%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/nWX0X0jM9wH8SrRELGxXCbMpgVeFioBtb0oN6_WBDQw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F0bbxl9%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/nWX0X0jM9wH8SrRELGxXCbMpgVeFioBtb0oN6_WBDQw=424", "authors": ["TLDR Newsletter"], "title": "Clanker Founder Rejects Rainbow Acquisition Offer", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F0bbxl9%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/nWX0X0jM9wH8SrRELGxXCbMpgVeFioBtb0oN6_WBDQw=424", "summary": "Clanker Founder Rejects Rainbow Acquisition Offer (3 minute read) Jack Dishman, founder of Base-based token launchpad Clanker, rejected Rainbow Wallet's offer to acquire the platform in exchange for 4% of its upcoming RNBW token supply.", "source": "tldr"}
{"id": "tldr.2509.bd40a737", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1969723279876223083.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Qj0JIVjLGeY6-lvilSp5TsnvVe69V-bMpdMkssPNFus=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1969723279876223083.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Qj0JIVjLGeY6-lvilSp5TsnvVe69V-bMpdMkssPNFus=424", "authors": ["TLDR Newsletter"], "title": "Ralph Lauren Accepts Crypto Payments", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1969723279876223083.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/Qj0JIVjLGeY6-lvilSp5TsnvVe69V-bMpdMkssPNFus=424", "summary": "Ralph Lauren Accepts Crypto Payments (1 minute read) Clothing retailer Ralph Lauren now accepts crypto payments via BitPay, primarily in its Miami flagship store.", "source": "tldr"}
{"id": "tldr.2509.0d11e0b3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcryptobriefing.com%2Fsec-innovation-exemption-crypto-2025%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/51te567dT7TuNhgiU8cGO5yo3xJpKtvfei46MqRKEmM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcryptobriefing.com%2Fsec-innovation-exemption-crypto-2025%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/51te567dT7TuNhgiU8cGO5yo3xJpKtvfei46MqRKEmM=424", "authors": ["TLDR Newsletter"], "title": "SEC plans to introduce innovation exemption for crypto firms by EOY", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcryptobriefing.com%2Fsec-innovation-exemption-crypto-2025%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/51te567dT7TuNhgiU8cGO5yo3xJpKtvfei46MqRKEmM=424", "summary": "SEC plans to introduce innovation exemption for crypto firms by EOY (3 minute read) The SEC will create an ‘innovation exemption' for crypto firms by the end of 2025 to encourage growth and continue to move away from the regulation-by-enforcement approach of the previous administration.", "source": "tldr"}
{"id": "tldr.2509.55f5d01a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbeincrypto.com%2F21shares-launches-dogecoin-etf-on-dtcc%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/GjHaaRE5VbOtCmF4XxNFWegQDQlDgcT4D-1dP6biIAs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbeincrypto.com%2F21shares-launches-dogecoin-etf-on-dtcc%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/GjHaaRE5VbOtCmF4XxNFWegQDQlDgcT4D-1dP6biIAs=424", "authors": ["TLDR Newsletter"], "title": "21Shares Launches Dogecoin ETF", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbeincrypto.com%2F21shares-launches-dogecoin-etf-on-dtcc%2F%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/GjHaaRE5VbOtCmF4XxNFWegQDQlDgcT4D-1dP6biIAs=424", "summary": "21Shares Launches Dogecoin ETF (2 minute read) 21Shares has launched its Dogecoin ETF (TDOG) on the Depository Trust & Clearing Corporation.", "source": "tldr"}
{"id": "tldr.2509.503fc2bd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrcrypto%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/4Y4KGKSCPCJGaPklNTmgrgxjfxzo1vMu0eBrf9IG3W8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrcrypto%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/4Y4KGKSCPCJGaPklNTmgrgxjfxzo1vMu0eBrf9IG3W8=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrcrypto%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/4Y4KGKSCPCJGaPklNTmgrgxjfxzo1vMu0eBrf9IG3W8=424", "summary": "21Shares Launches Dogecoin ETF (2 minute read) 21Shares has launched its Dogecoin ETF (TDOG) on the Depository Trust & Clearing Corporation.", "source": "tldr"}
{"id": "tldr.2509.882dd371", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F780287%2Fmicrosoft-paint-project-files-feature-photoshop-psd%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/Y0RZ20bH5xhSeF5bN9ic5hqYUQFvWWIybHXEWll2dXA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F780287%2Fmicrosoft-paint-project-files-feature-photoshop-psd%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/Y0RZ20bH5xhSeF5bN9ic5hqYUQFvWWIybHXEWll2dXA=424", "authors": ["TLDR Newsletter"], "title": "Microsoft Paint is Getting its Own Photoshop-like Project Files", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F780287%2Fmicrosoft-paint-project-files-feature-photoshop-psd%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/Y0RZ20bH5xhSeF5bN9ic5hqYUQFvWWIybHXEWll2dXA=424", "summary": "Microsoft Paint is Getting its Own Photoshop-like Project Files (2 minute read) Microsoft introduced project files to Paint that function like Photoshop Documents, allowing users to save work with layers intact and resume editing later. The update also adds opacity sliders for pencil and brush tools to control transparency levels. Windows 11's Snipping Tool has gained markup features, while Notepad has received free AI writing assistance for Copilot Plus PC users.", "source": "tldr"}
{"id": "tldr.2509.a00e6786", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F17%2Findia-leads-the-way-on-googles-nano-banana-with-a-local-creative-twist%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/sV4Khwtu8etE6OFBZ7FguSo76B1w8KrYvIvAb89lPYw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F17%2Findia-leads-the-way-on-googles-nano-banana-with-a-local-creative-twist%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/sV4Khwtu8etE6OFBZ7FguSo76B1w8KrYvIvAb89lPYw=424", "authors": ["TLDR Newsletter"], "title": "India Leads the Way on Google's Nano Banana with a Local Creative Twist", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F17%2Findia-leads-the-way-on-googles-nano-banana-with-a-local-creative-twist%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/sV4Khwtu8etE6OFBZ7FguSo76B1w8KrYvIvAb89lPYw=424", "summary": "India Leads the Way on Google's Nano Banana with a Local Creative Twist (5 minute read) Google's Nano Banana image-generation model has made India its leading market, propelling the Gemini app to the top of the charts with 15.2 million downloads this year. Indian users are creating unique local trends, such as retro Bollywood portraits, AI-generated saree images, and vintage cityscapes, which are spreading globally. Despite leading in usage, India contributes only 1.5% of Gemini's $6.4 millio...", "source": "tldr"}
{"id": "tldr.2509.7804ec20", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-xDTwsJchdIuftxzMBz6z5pNX5RCPVxdzbF5cRQQ0jM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-xDTwsJchdIuftxzMBz6z5pNX5RCPVxdzbF5cRQQ0jM=424", "authors": ["TLDR Newsletter"], "title": "Tired of cookie consent pop-ups? You soon may see less of them", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmashable.com%2Farticle%2Fcookie-consent-pop-ups-eu-looking-to-change-law%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-xDTwsJchdIuftxzMBz6z5pNX5RCPVxdzbF5cRQQ0jM=424", "summary": "Tired of cookie consent pop-ups? You soon may see less of them (2 minute read) The EU is reviewing its 2009 e-Privacy Directive, which created the widespread cookie consent pop-ups, and may relax the rules to reduce user frustration. Possible changes include adding more exceptions or letting users set consent in their browser. Privacy advocates warn this could weaken protections and expand data use for advertising.", "source": "tldr"}
{"id": "tldr.2509.4c0ed284", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpicnic.beehiiv.com%2Fp%2Fresearch-is-a-leadership-skill-don-t-cede-it-to-ai%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/vHZh2k0Ct8SDAPbntMz3uGxiQ99argEKM3Sd2NPty3I=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpicnic.beehiiv.com%2Fp%2Fresearch-is-a-leadership-skill-don-t-cede-it-to-ai%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/vHZh2k0Ct8SDAPbntMz3uGxiQ99argEKM3Sd2NPty3I=424", "authors": ["TLDR Newsletter"], "title": "Research is a Leadership Skill; Don't Cede it to AI", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpicnic.beehiiv.com%2Fp%2Fresearch-is-a-leadership-skill-don-t-cede-it-to-ai%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/vHZh2k0Ct8SDAPbntMz3uGxiQ99argEKM3Sd2NPty3I=424", "summary": "Research is a Leadership Skill; Don't Cede it to AI (6 minute read) Research serves as a critical leadership tool for creating stakeholder alignment and shared understanding, not just data collection. AI tools undermine this process by eliminating human participation in knowledge creation, preventing the development of ownership and commitment that drives organizational action. Effective UX research requires building stakeholder mental models through collaborative engagement, a process that A...", "source": "tldr"}
{"id": "tldr.2509.d65b929d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsmart-device-onboarding%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ayNlWqGNd45HPYDEJUG8DQ_j2HfN9fNoNGGBpmd-QI8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsmart-device-onboarding%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ayNlWqGNd45HPYDEJUG8DQ_j2HfN9fNoNGGBpmd-QI8=424", "authors": ["TLDR Newsletter"], "title": "Onboarding and Connecting Smart Devices: Five Guidelines for User-Friendly Smart-Device Apps", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsmart-device-onboarding%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ayNlWqGNd45HPYDEJUG8DQ_j2HfN9fNoNGGBpmd-QI8=424", "summary": "Onboarding and Connecting Smart Devices: Five Guidelines for User-Friendly Smart-Device Apps (7 minute read) Smart-device apps require clear, visual step-by-step onboarding flows to prevent users from abandoning devices during setup. Reconnection should mirror the initial setup with automatic detection and detailed guidance, as users rarely recall the exact steps. Apps must also support flexible device management, provide honest progress indicators, and handle errors transparently with specif...", "source": "tldr"}
{"id": "tldr.2509.50c8f55d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsigns-of-ux-maturity-regression%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/QazlRAsVv7o5EmV2fss5gnz-aakq5aIEuvtvj1GLN4c=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsigns-of-ux-maturity-regression%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/QazlRAsVv7o5EmV2fss5gnz-aakq5aIEuvtvj1GLN4c=424", "authors": ["TLDR Newsletter"], "title": "How to Spot Signs of UX Maturity Regression", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nngroup.com%2Farticles%2Fsigns-of-ux-maturity-regression%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/QazlRAsVv7o5EmV2fss5gnz-aakq5aIEuvtvj1GLN4c=424", "summary": "How to Spot Signs of UX Maturity Regression (8 minute read) The UX-maturity model outlines how organizations progress in UX across strategy, culture, process, and outcomes. Maturity can regress if practices stagnate, leadership changes, or UX becomes siloed. Teams can prevent drift by watching for early signals, keeping lightweight check-ins, broadening ownership beyond UX, and integrating maturity into ongoing business processes to maintain momentum.", "source": "tldr"}
{"id": "tldr.2509.d0ca39c8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FqwNM25/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/uIbz9Vc0nOLHWd4Sk9fndAU9lTIp6FSUqfEjTxOowRE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FqwNM25/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/uIbz9Vc0nOLHWd4Sk9fndAU9lTIp6FSUqfEjTxOowRE=424", "authors": ["TLDR Newsletter"], "title": "AI Rendering for Architects", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FqwNM25/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/uIbz9Vc0nOLHWd4Sk9fndAU9lTIp6FSUqfEjTxOowRE=424", "summary": "AI Rendering for Architects (Website) Upload your photo, describe your style, and reimagine any home interior, exterior, or garden using AI.", "source": "tldr"}
{"id": "tldr.2509.3ec38fb2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FxsvNEU/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-CQdVafLHL3FB7dF2f90yXYT2t9ZaWxLKWxxBZP7tI8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FxsvNEU/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-CQdVafLHL3FB7dF2f90yXYT2t9ZaWxLKWxxBZP7tI8=424", "authors": ["TLDR Newsletter"], "title": "AI Video Canvas for Filmmakers", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FxsvNEU/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/-CQdVafLHL3FB7dF2f90yXYT2t9ZaWxLKWxxBZP7tI8=424", "summary": "AI Video Canvas for Filmmakers (Website) Crevas unites Veo3, Kling, and ChatGPT in one canvas, allowing you to create faster, refine with AI Chat, and collaborate in real-time.", "source": "tldr"}
{"id": "tldr.2509.7f29a711", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsPREqg/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/JAuOiU9uhcuDOfTvF5F4xcKOp75Dh5E5OfkH7-GaAv0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsPREqg/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/JAuOiU9uhcuDOfTvF5F4xcKOp75Dh5E5OfkH7-GaAv0=424", "authors": ["TLDR Newsletter"], "title": "Presentation-ready PDFs Without the Busywork", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsPREqg/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/JAuOiU9uhcuDOfTvF5F4xcKOp75Dh5E5OfkH7-GaAv0=424", "summary": "Presentation-ready PDFs Without the Busywork (Website) ReadyBase lets users create production-ready documents in minutes through an AI-native, chat-to-PDF interface that prioritizes speed and value delivery.", "source": "tldr"}
{"id": "tldr.2509.5ab4220b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjoshcusick.substack.com%2Fp%2Fstop-saying-youre-ai-native-start-showing-it%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/r-g7cglnGH5E-YVvAiWfp02LvN89SWd4mfLE1GkTg8M=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjoshcusick.substack.com%2Fp%2Fstop-saying-youre-ai-native-start-showing-it%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/r-g7cglnGH5E-YVvAiWfp02LvN89SWd4mfLE1GkTg8M=424", "authors": ["TLDR Newsletter"], "title": "Stop Saying You're AI-native: Start Showing It", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjoshcusick.substack.com%2Fp%2Fstop-saying-youre-ai-native-start-showing-it%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/r-g7cglnGH5E-YVvAiWfp02LvN89SWd4mfLE1GkTg8M=424", "summary": "Stop Saying You're AI-native: Start Showing It (11 minute read) Being AI-native requires hands-on experimentation with AI tools rather than just designing AI interfaces. True AI-native professionals build personal tools, push AI systems to their limits, and develop strong opinions through constant use of various models and platforms. Hire designers who demonstrate genuine AI fluency through personal projects, documented failures, and in-depth toolchain knowledge, rather than theoretical exper...", "source": "tldr"}
{"id": "tldr.2509.2c5a8b00", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2025%2F09%2Fpsychology-trust-ai-guide-measuring-designing-user-confidence%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/2mn1PmB979YsAlp8LEUy22gn_Zw2khiv8VQWAIK3mpY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2025%2F09%2Fpsychology-trust-ai-guide-measuring-designing-user-confidence%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/2mn1PmB979YsAlp8LEUy22gn_Zw2khiv8VQWAIK3mpY=424", "authors": ["TLDR Newsletter"], "title": "The Psychology of Trust In AI: A Guide to Measuring And Designing for User Confidence", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 21 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2025%2F09%2Fpsychology-trust-ai-guide-measuring-designing-user-confidence%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/2mn1PmB979YsAlp8LEUy22gn_Zw2khiv8VQWAIK3mpY=424", "summary": "The Psychology of Trust In AI: A Guide to Measuring And Designing for User Confidence (21 minute read) Trust in AI systems can be measured and designed for, identifying four key components: ability, benevolence, integrity, and predictability. Calibrated trust—not blind trust or distrust—is the ideal user state, requiring transparent design that acknowledges AI limitations and potential errors. UX professionals must advocate for genuinely trustworthy systems while avoiding \"trustwashing,\" whic...", "source": "tldr"}
{"id": "tldr.2509.80c4b245", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativeboom.com%2Finsight%2Fillustrated-thinking-why-brands-are-turning-back-to-drawing%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/cQiXxF1-LyaI-U6MMAPF6bi23Zk9yZkYb08cGxyVpII=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativeboom.com%2Finsight%2Fillustrated-thinking-why-brands-are-turning-back-to-drawing%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/cQiXxF1-LyaI-U6MMAPF6bi23Zk9yZkYb08cGxyVpII=424", "authors": ["TLDR Newsletter"], "title": "Illustrated thinking: Why brands are turning back to drawing", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativeboom.com%2Finsight%2Fillustrated-thinking-why-brands-are-turning-back-to-drawing%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/cQiXxF1-LyaI-U6MMAPF6bi23Zk9yZkYb08cGxyVpII=424", "summary": "Illustrated thinking: Why brands are turning back to drawing (6 minute read) Illustration has declined in advertising due to risk-averse clients, slower production, and misconceptions of it being “childlike,” but it remains powerful for distinctiveness, flexibility, and impact, especially in murals, toolkits, and digital activations. Experts see a resurgence ahead, with opportunities in motion, hybrid projects, and illustrators acting as creative partners, making illustrated campaigns stand o...", "source": "tldr"}
{"id": "tldr.2509.34aa2e25", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Fproduct-design%2Fis-tesla-finally-getting-a-handle-on-its-design-fails%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/M04IxXXmdvjn3oMzgoLdmfXAqxgBGii4aTPSxAqNokI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Fproduct-design%2Fis-tesla-finally-getting-a-handle-on-its-design-fails%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/M04IxXXmdvjn3oMzgoLdmfXAqxgBGii4aTPSxAqNokI=424", "authors": ["TLDR Newsletter"], "title": "Is Tesla Finally Getting a 'Handle' on its Design Fails?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Fproduct-design%2Fis-tesla-finally-getting-a-handle-on-its-design-fails%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/M04IxXXmdvjn3oMzgoLdmfXAqxgBGii4aTPSxAqNokI=424", "summary": "Is Tesla Finally Getting a 'Handle' on its Design Fails? (4 minute read) Tesla is finally addressing its controversial door handle design.", "source": "tldr"}
{"id": "tldr.2509.040b8fbe", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdesignworklife.com%2Ffrom-fonts-to-freedom-redefining-lifestyle-through-remote-design-work%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/R0rAXVwhzZbBk-xdXciV2UBRdqkCBLaGIr6GfUxbHz4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdesignworklife.com%2Ffrom-fonts-to-freedom-redefining-lifestyle-through-remote-design-work%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/R0rAXVwhzZbBk-xdXciV2UBRdqkCBLaGIr6GfUxbHz4=424", "authors": ["TLDR Newsletter"], "title": "From Fonts to Freedom: Redefining Lifestyle Through Remote Design Work", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdesignworklife.com%2Ffrom-fonts-to-freedom-redefining-lifestyle-through-remote-design-work%2F%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/R0rAXVwhzZbBk-xdXciV2UBRdqkCBLaGIr6GfUxbHz4=424", "summary": "From Fonts to Freedom: Redefining Lifestyle Through Remote Design Work (6 minute read) Remote design work enables designers to transform traditional employment into a location-independent lifestyle.", "source": "tldr"}
{"id": "tldr.2509.a5fdeb5a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthe-brandidentity.com%2Fproject%2Fhex-builds-a-sound-wave-typography-system-for-maples-voice-ai-research%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ADVoQHVuFV1mzNnrYiyxQIvdyZ8wpIVr-20xoMWIo2A=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthe-brandidentity.com%2Fproject%2Fhex-builds-a-sound-wave-typography-system-for-maples-voice-ai-research%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ADVoQHVuFV1mzNnrYiyxQIvdyZ8wpIVr-20xoMWIo2A=424", "authors": ["TLDR Newsletter"], "title": "HEX builds a sound wave typography system for Maple's voice AI research", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthe-brandidentity.com%2Fproject%2Fhex-builds-a-sound-wave-typography-system-for-maples-voice-ai-research%3Futm_source=tldrdesign/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/ADVoQHVuFV1mzNnrYiyxQIvdyZ8wpIVr-20xoMWIo2A=424", "summary": "HEX builds a sound wave typography system for Maple's voice AI research (3 minute read) HEX created Maple's research division identity by exploring voice as rhythm and frequency.", "source": "tldr"}
{"id": "tldr.2509.b5dc2b78", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Faudiences%2Fdesign-professionals%2F%3Futm_source=tldrdesign%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/q4FyhbS_a8aDaxAtByJuviJOfH9Jvxa7u1M6BfH2ICI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Faudiences%2Fdesign-professionals%2F%3Futm_source=tldrdesign%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/q4FyhbS_a8aDaxAtByJuviJOfH9Jvxa7u1M6BfH2ICI=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Faudiences%2Fdesign-professionals%2F%3Futm_source=tldrdesign%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9dfb91-bb019b76-fcf6-4ad7-b33b-416ccad33561-000000/q4FyhbS_a8aDaxAtByJuviJOfH9Jvxa7u1M6BfH2ICI=424", "summary": "HEX builds a sound wave typography system for Maple's voice AI research (3 minute read) HEX created Maple's research division identity by exploring voice as rhythm and frequency.", "source": "tldr"}
{"id": "tldr.2509.6c6ccd53", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-founder%26utm_content=primary/2/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/9OwoTfCmHD_6K3U321xtJoiz0RMN1H2K43sze3ig61c=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-founder%26utm_content=primary/2/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/9OwoTfCmHD_6K3U321xtJoiz0RMN1H2K43sze3ig61c=424", "authors": ["TLDR Newsletter"], "title": "How Snowflake is adapting usage-based pricing for the AI era", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-founder%26utm_content=primary/2/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/9OwoTfCmHD_6K3U321xtJoiz0RMN1H2K43sze3ig61c=424", "summary": "How Snowflake is adapting usage-based pricing for the AI era (Sponsor) Snowflake's usage-based pricing has been one key driver of its market leadership. In this webinar, you'll get an insider view of how the team manages its pricing and billing, and how they're preparing for the Al era of monetization. Join Ryan Campbell, Director of Product Finance at Snowflake, and Scott Woody, Metronome CEO, to learn how Snowflake aligns finance, product, GTM, and engineering around pricing decisions. On t...", "source": "tldr"}
{"id": "tldr.2509.6128a342", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.newsletter.datadrivenvc.io%2Fp%2Fstartup-salaries-equity-and-their-impact-on-employee-retention%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/EhcPoBYaF_vGjn52AfEQcLU9Osu7mcAxgbfba439aJQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.newsletter.datadrivenvc.io%2Fp%2Fstartup-salaries-equity-and-their-impact-on-employee-retention%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/EhcPoBYaF_vGjn52AfEQcLU9Osu7mcAxgbfba439aJQ=424", "authors": ["TLDR Newsletter"], "title": "Startup Salaries, Equity, and Their Impact on Employee Retention", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.newsletter.datadrivenvc.io%2Fp%2Fstartup-salaries-equity-and-their-impact-on-employee-retention%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/EhcPoBYaF_vGjn52AfEQcLU9Osu7mcAxgbfba439aJQ=424", "summary": "Startup Salaries, Equity, and Their Impact on Employee Retention (8 minute read) Startup pay shifted again this year. Salaries are up about 5%, with product and engineering around $189k, while equity grants are roughly 26% below pre-2022, and only 32% of vested in-the-money options got exercised. Early hires still get about 1.5% and employee five near 0.3%. Turnover fell 31% yet churn stays high. Retention improves when pay is competitive, refreshes hit year three to four, and bands are trans...", "source": "tldr"}
{"id": "tldr.2509.3edb9074", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fpdf%2F2509.14448%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2v9OlXt15U22A6HIpzLXKD-UME_Kn96_bEXHYuV4jNo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fpdf%2F2509.14448%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2v9OlXt15U22A6HIpzLXKD-UME_Kn96_bEXHYuV4jNo=424", "authors": ["TLDR Newsletter"], "title": "VCBench: Benchmarking LLMs in Venture Capital", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fpdf%2F2509.14448%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2v9OlXt15U22A6HIpzLXKD-UME_Kn96_bEXHYuV4jNo=424", "summary": "VCBench: Benchmarking LLMs in Venture Capital (9 minute read) Startup investing has long been guesswork, with even top VCs picking winners less than 6% of the time. VCBench is trying to change that: a dataset of 9,000 anonymized founder profiles built to test whether models can predict success better than humans. After scrubbing names, dates, and company info to prevent leakage, researchers ran nine LLMs. GPT-4o delivered 3.2× baseline precision, edging past tier-1 firms.", "source": "tldr"}
{"id": "tldr.2509.7948e77c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "authors": ["TLDR Newsletter"], "title": "Greptile Series A and Greptile v3", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "summary": "Greptile Series A and Greptile v3 (5 minute read) Greptile raised $25M Series A led by Benchmark Capital to advance its AI code review agent, Greptile v3, which addresses scale issues in code validation. The revamped agent detects 3x more critical bugs than its predecessor and has reviewed over 500M lines of code for top firms, preventing 180,000+ bugs. Greptile integrates with tools like Jira and Notion for context-aware feedback and learns team practices from PR comments to enhance code rev...", "source": "tldr"}
{"id": "tldr.2509.a394bd34", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbensaltiel.substack.com%2Fp%2Fhow-to-build-mental-strength%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/uskqtYyffGuE4K4aUc42lCfbYsfrOVejjfAJ4_E1lwA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbensaltiel.substack.com%2Fp%2Fhow-to-build-mental-strength%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/uskqtYyffGuE4K4aUc42lCfbYsfrOVejjfAJ4_E1lwA=424", "authors": ["TLDR Newsletter"], "title": "How To Build Mental Strength", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbensaltiel.substack.com%2Fp%2Fhow-to-build-mental-strength%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/uskqtYyffGuE4K4aUc42lCfbYsfrOVejjfAJ4_E1lwA=424", "summary": "How To Build Mental Strength (5 minute read) Mental strength goes hand in hand with clear thinking. It's what lets you override your defaults instead of running on autopilot. You build it through four trainable muscles: accountability, knowledge, control, and confidence. Add high standards, rituals, and the right role models, and those muscles compound into habits.", "source": "tldr"}
{"id": "tldr.2509.d3c6ba16", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjasonevanish.com%2F2025%2F08%2F29%2Fcustomer-support-no-product-feedback%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/-sYs40A7qrCiABUe8DubagBVpL0w8wC9kbgg66oQJLg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjasonevanish.com%2F2025%2F08%2F29%2Fcustomer-support-no-product-feedback%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/-sYs40A7qrCiABUe8DubagBVpL0w8wC9kbgg66oQJLg=424", "authors": ["TLDR Newsletter"], "title": "Why Customer Support Shouldn't Handle Your Product Feedback", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjasonevanish.com%2F2025%2F08%2F29%2Fcustomer-support-no-product-feedback%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/-sYs40A7qrCiABUe8DubagBVpL0w8wC9kbgg66oQJLg=424", "summary": "Why Customer Support Shouldn't Handle Your Product Feedback (11 minute read) Customer support teams lack the skills and tools necessary to gather actionable product feedback, often resulting in valuable insights getting lost. Aligning customer support's incentives with quick ticket resolution rather than in-depth feedback analysis contributes to this issue. Companies should implement dedicated feedback systems to ensure customers feel heard and product teams receive quality insights, ultimate...", "source": "tldr"}
{"id": "tldr.2509.7c1cdaa4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freview.firstround.com%2Fhow-to-launch-your-startup-out-of-stealth%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/lF5c6yc84w9xjGyn0AImdV2PUZEDZ5D98AGEPH4D6T8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freview.firstround.com%2Fhow-to-launch-your-startup-out-of-stealth%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/lF5c6yc84w9xjGyn0AImdV2PUZEDZ5D98AGEPH4D6T8=424", "authors": ["TLDR Newsletter"], "title": "How to Launch Your Startup Out of Stealth, from Figma's First Marketer", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freview.firstround.com%2Fhow-to-launch-your-startup-out-of-stealth%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/lF5c6yc84w9xjGyn0AImdV2PUZEDZ5D98AGEPH4D6T8=424", "summary": "How to Launch Your Startup Out of Stealth, from Figma's First Marketer (15 minute read) Successful startup launches require strategic preparation, emphasizing feature readiness and clear positioning. Set a specific launch date to avoid perpetual delays and create artifacts like a website, announcement post, and social post to sharpen messaging and decisions. Focus on existing distribution channels, utilize personal networks for amplification, and view the launch as a starting point for growth...", "source": "tldr"}
{"id": "tldr.2509.8820dfbc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firecrawl.dev%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/oQWX7XlO-N6XyRBcUxD7woLnYiwBTQgDQ1sl1VpA-Kg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firecrawl.dev%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/oQWX7XlO-N6XyRBcUxD7woLnYiwBTQgDQ1sl1VpA-Kg=424", "authors": ["TLDR Newsletter"], "title": "Firecrawl", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firecrawl.dev%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/oQWX7XlO-N6XyRBcUxD7woLnYiwBTQgDQ1sl1VpA-Kg=424", "summary": "Firecrawl (Tool) Firecrawl is an open-source tool that converts entire websites into clean, LLM-ready markdown or structured data with a single API call for developers.", "source": "tldr"}
{"id": "tldr.2509.e3fb3f7b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sklsync.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/MeUtGYzFnV67-_bbIfUFCLumP7U8auO5Rxe888qqs8k=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sklsync.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/MeUtGYzFnV67-_bbIfUFCLumP7U8auO5Rxe888qqs8k=424", "authors": ["TLDR Newsletter"], "title": "Skillsync", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sklsync.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/MeUtGYzFnV67-_bbIfUFCLumP7U8auO5Rxe888qqs8k=424", "summary": "Skillsync (Tool) Skillsync uncovers hidden expertise and successful patterns in your codebase to help teams scale what works.", "source": "tldr"}
{"id": "tldr.2509.5062fba6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freachllm.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/gUi4hFiKDNeTfnvH1JKU3nGZPiMmNybWE7bmfappX5k=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freachllm.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/gUi4hFiKDNeTfnvH1JKU3nGZPiMmNybWE7bmfappX5k=424", "authors": ["TLDR Newsletter"], "title": "ReachLLM", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freachllm.com%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/gUi4hFiKDNeTfnvH1JKU3nGZPiMmNybWE7bmfappX5k=424", "summary": "ReachLLM (Tool) ReachLLM audits AI search results for your brand and rivals, uncovering high-intent prompts and brand mentions so you can win the AI answer box.", "source": "tldr"}
{"id": "tldr.2509.1b4f032a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmhdempsey.substack.com%2Fp%2Fnarrative-cycle-entry%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/3kHKpWUKbT06SmNeEr5MrGnvOHLfR4OKIcxFSEzTm9g=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmhdempsey.substack.com%2Fp%2Fnarrative-cycle-entry%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/3kHKpWUKbT06SmNeEr5MrGnvOHLfR4OKIcxFSEzTm9g=424", "authors": ["TLDR Newsletter"], "title": "Narrative Cycle Entry", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmhdempsey.substack.com%2Fp%2Fnarrative-cycle-entry%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/3kHKpWUKbT06SmNeEr5MrGnvOHLfR4OKIcxFSEzTm9g=424", "summary": "Narrative Cycle Entry (4 minute read) Every technology story has a rhythm. First comes obscurity, when only a few people care, and founders have to explain everything from scratch. Then comes conviction, as believers pile in and copycats multiply. Next is euphoria, when hype peaks and differentiation gets hardest. Finally comes the crash, when the same idea is written off until a few survivors prove it real again. Knowing where you enter shapes whether you're explaining an unproven idea, comp...", "source": "tldr"}
{"id": "tldr.2509.2c91752e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpatron.fund%2Fblog%2Ftoward-computational-taste-llms-aesthetics-judgment%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/16bHT2qGXq-ULIAnJtBcxy7zUFxfhoOARI9jmHSYXyk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpatron.fund%2Fblog%2Ftoward-computational-taste-llms-aesthetics-judgment%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/16bHT2qGXq-ULIAnJtBcxy7zUFxfhoOARI9jmHSYXyk=424", "authors": ["TLDR Newsletter"], "title": "Toward Computational Taste: LLMs, Aesthetics & Judgment", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpatron.fund%2Fblog%2Ftoward-computational-taste-llms-aesthetics-judgment%2F%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/16bHT2qGXq-ULIAnJtBcxy7zUFxfhoOARI9jmHSYXyk=424", "summary": "Toward Computational Taste: LLMs, Aesthetics & Judgment (9 minute read) LLMs are being used to model and optimize taste in diverse fields through personalized systems like Taste Engines, Aesthetic LLMs, and Taste Tribes. Methods like LoRe and models like TAPO enable LLMs to adapt to individual aesthetic preferences, potentially revolutionizing recommendation engines and social networks. This shift towards computational taste signifies a new era where machines not only reflect but also shape h...", "source": "tldr"}
{"id": "tldr.2509.50d12865", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jamesmcwalter.com%2Fpost%2Fthe-one-person-billion-dollar-power-development-company%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/Oth7AmXfZBaynx74kIFnSZpiY76zndjQqI-sHRZ2H4o=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jamesmcwalter.com%2Fpost%2Fthe-one-person-billion-dollar-power-development-company%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/Oth7AmXfZBaynx74kIFnSZpiY76zndjQqI-sHRZ2H4o=424", "authors": ["TLDR Newsletter"], "title": "The One-Person, Billion-Dollar Power Development Company", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jamesmcwalter.com%2Fpost%2Fthe-one-person-billion-dollar-power-development-company%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/Oth7AmXfZBaynx74kIFnSZpiY76zndjQqI-sHRZ2H4o=424", "summary": "The One-Person, Billion-Dollar Power Development Company (10 minute read) Power development has always been a slog: years of permits, consultants, site visits, and spreadsheets that eat entire teams. That's about to change. With new data systems, permitting software, robotics, and orchestration tools, one person could realistically manage hundreds of megawatts of projects and generate $100M in fees. This piece explores how an industry known for red tape might collapse into a one-person busine...", "source": "tldr"}
{"id": "tldr.2509.a73075a5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fckarchive.com%2Fb%2F92uzhnh6dnr8eh3roomrdfzpl8333hwh2pxox%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/FMketfw5R-WGM_4OTnpkEcJR8WKnooo4HEl1-8wVA4o=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fckarchive.com%2Fb%2F92uzhnh6dnr8eh3roomrdfzpl8333hwh2pxox%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/FMketfw5R-WGM_4OTnpkEcJR8WKnooo4HEl1-8wVA4o=424", "authors": ["TLDR Newsletter"], "title": "The Google Effect", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fckarchive.com%2Fb%2F92uzhnh6dnr8eh3roomrdfzpl8333hwh2pxox%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/FMketfw5R-WGM_4OTnpkEcJR8WKnooo4HEl1-8wVA4o=424", "summary": "The Google Effect (3 minute read) Our brains don't bother storing facts we know we can Google later, which means your buyers won't remember your product details either.", "source": "tldr"}
{"id": "tldr.2509.946bf32e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.startuparchive.org%2Fp%2Fnaval-ravikant-the-smart-and-leveraged-are-getting-richer%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/rpIRuaET_uCD5EkbLD3nvO5WsabjW1FliVeD8wSuuOY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.startuparchive.org%2Fp%2Fnaval-ravikant-the-smart-and-leveraged-are-getting-richer%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/rpIRuaET_uCD5EkbLD3nvO5WsabjW1FliVeD8wSuuOY=424", "authors": ["TLDR Newsletter"], "title": "The smart and leveraged are getting richer", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.startuparchive.org%2Fp%2Fnaval-ravikant-the-smart-and-leveraged-are-getting-richer%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/rpIRuaET_uCD5EkbLD3nvO5WsabjW1FliVeD8wSuuOY=424", "summary": "The smart and leveraged are getting richer (2 minute read) Naval breaks down why wealth is concentrating faster than ever: leverage.", "source": "tldr"}
{"id": "tldr.2509.1f2611a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4EvWem/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/xeARCN5kl6DSX1yjN66svmCBbvwukzbazFIntM3fsu8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4EvWem/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/xeARCN5kl6DSX1yjN66svmCBbvwukzbazFIntM3fsu8=424", "authors": ["TLDR Newsletter"], "title": "Stripe in talks to buy back stock from investors", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4EvWem/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/xeARCN5kl6DSX1yjN66svmCBbvwukzbazFIntM3fsu8=424", "summary": "Stripe in talks to buy back stock from investors (2 minute read) Stripe plans to repurchase shares from VCs at a $106.7 billion valuation, offering liquidity without going public.", "source": "tldr"}
{"id": "tldr.2509.fbd939c9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2UKasFYfu2oo3opvZo2Rp_TQ4UctBigRZTaTv9EZQLk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2UKasFYfu2oo3opvZo2Rp_TQ4UctBigRZTaTv9EZQLk=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/2UKasFYfu2oo3opvZo2Rp_TQ4UctBigRZTaTv9EZQLk=424", "summary": "Stripe in talks to buy back stock from investors (2 minute read) Stripe plans to repurchase shares from VCs at a $106.7 billion valuation, offering liquidity without going public.", "source": "tldr"}
{"id": "tldr.2509.e7d8667d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.huntress.com%2Flp%2Ftldr%3Futm_source=tldr%26utm_medium=email%26utm_campaign=Cy25-09-camp-platform-global-prospect-iis-x-tldr_newsletter_0924/2/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/EPrPyPu3xlBWBJFNM_PGvfWJ9WmyHKBFJ24ewhn11H4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.huntress.com%2Flp%2Ftldr%3Futm_source=tldr%26utm_medium=email%26utm_campaign=Cy25-09-camp-platform-global-prospect-iis-x-tldr_newsletter_0924/2/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/EPrPyPu3xlBWBJFNM_PGvfWJ9WmyHKBFJ24ewhn11H4=424", "authors": ["TLDR Newsletter"], "title": "Why are big security vendors ignoring 99% of companies?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.huntress.com%2Flp%2Ftldr%3Futm_source=tldr%26utm_medium=email%26utm_campaign=Cy25-09-camp-platform-global-prospect-iis-x-tldr_newsletter_0924/2/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/EPrPyPu3xlBWBJFNM_PGvfWJ9WmyHKBFJ24ewhn11H4=424", "summary": "Why are big security vendors ignoring 99% of companies? (Sponsor) It's not because cybersecurity is only a problem for Fortune 500s. It's because these vendors' business models are built on selling multi-million dollar enterprise licenses. But it doesn't have to be this way.With cybercrime hitting businesses of all sizes and industries, Huntress delivers security for the 99%. The Huntress Managed Platform is custom-built security for endpoints, identities, people, and more, all managed for yo...", "source": "tldr"}
{"id": "tldr.2509.4f747309", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "authors": ["TLDR Newsletter"], "title": "ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "summary": "ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent (2 minute read) ShadowLeak is a zero-click vulnerability in OpenAI's ChatGPT Deep Research agent that utilized invisible, indirect prompt injection commands hidden in emails to exfiltrate Gmail data without user knowledge. The attack operated entirely on OpenAI's servers using the agent's browser.open() function call to send stolen data encoded in Base64 to attacker-controlled URLs, achieving a 100% success rate. OpenAI fixed the vul...", "source": "tldr"}
{"id": "tldr.2509.35668b1a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Flastpass-fake-password-managers-infect-mac-users-with-malware%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/sAxM_a3dalBjanhXOzInq6xVudXkf1efukZceHRLJdQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Flastpass-fake-password-managers-infect-mac-users-with-malware%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/sAxM_a3dalBjanhXOzInq6xVudXkf1efukZceHRLJdQ=424", "authors": ["TLDR Newsletter"], "title": "LastPass: Fake Password Managers Infect Mac Users With Malware", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Flastpass-fake-password-managers-infect-mac-users-with-malware%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/sAxM_a3dalBjanhXOzInq6xVudXkf1efukZceHRLJdQ=424", "summary": "LastPass: Fake Password Managers Infect Mac Users With Malware (2 minute read) There is a malware campaign that uses GitHub repositories to deliver fraudulent password managers. The fake software delivers the Atomic (AMOS) InfoStealer to infected macOS devices. LastPass says that aside from its product, over 100 different softwares are being used, such as 1Password and Notion.", "source": "tldr"}
{"id": "tldr.2509.744c0bc9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fpatch-bypassed-for-supermicro-vulnerability-allowing-bmc-hack%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/a84q5mSLd3dwsk6O3pEHF4c_BnTAl9lrgrCLJ7vPjfE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fpatch-bypassed-for-supermicro-vulnerability-allowing-bmc-hack%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/a84q5mSLd3dwsk6O3pEHF4c_BnTAl9lrgrCLJ7vPjfE=424", "authors": ["TLDR Newsletter"], "title": "Patch Bypassed for Supermicro Vulnerability Allowing BMC Hack", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fpatch-bypassed-for-supermicro-vulnerability-allowing-bmc-hack%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/a84q5mSLd3dwsk6O3pEHF4c_BnTAl9lrgrCLJ7vPjfE=424", "summary": "Patch Bypassed for Supermicro Vulnerability Allowing BMC Hack (2 minute read) Binarly researchers found they could bypass Supermicro's patch for CVE-2024-10237, a firmware flaw allowing malicious updates, prompting a new CVE (CVE-2025-7937) and fix. They also discovered CVE-2025-6198, a flaw that bypasses the Root of Trust. Both enable attackers to control the BMC and OS. These flaws show firmware validation's fragility, risking persistent BMC code execution with major enterprise security imp...", "source": "tldr"}
{"id": "tldr.2509.a00532be", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.pepperclipp.com%2Fpepperclipp-public%2Fa-tag-to-rule-them-all-using-aws-tags-to-enumerate-cloud-resources%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/na9eFZ66tBZHplPMLDMA_lCsKBy2A3XTGYshEItJcV0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.pepperclipp.com%2Fpepperclipp-public%2Fa-tag-to-rule-them-all-using-aws-tags-to-enumerate-cloud-resources%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/na9eFZ66tBZHplPMLDMA_lCsKBy2A3XTGYshEItJcV0=424", "authors": ["TLDR Newsletter"], "title": "A Tag to Rule Them All: Using AWS Tags to Enumerate Cloud Resources", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.pepperclipp.com%2Fpepperclipp-public%2Fa-tag-to-rule-them-all-using-aws-tags-to-enumerate-cloud-resources%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/na9eFZ66tBZHplPMLDMA_lCsKBy2A3XTGYshEItJcV0=424", "summary": "A Tag to Rule Them All: Using AWS Tags to Enumerate Cloud Resources (10 minute read) AWS tags offer a way to attach metadata to a resource, either automatically (like the Name tag) or manually. Several services provide API calls such as ListTags or GetTags, enabling efficient enumeration of resources accessible by an identity. TagNabIt is a tool designed for tag-based brute-force enumeration.", "source": "tldr"}
{"id": "tldr.2509.fe62a48a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.synacktiv.com%2Fen%2Fpublications%2Fexploring-grapheneos-secure-allocator-hardened-malloc%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/5MyN4oyFzdGk7-Ry3veiqmnxsCJWLCHqNsl-wK6UWEA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.synacktiv.com%2Fen%2Fpublications%2Fexploring-grapheneos-secure-allocator-hardened-malloc%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/5MyN4oyFzdGk7-Ry3veiqmnxsCJWLCHqNsl-wK6UWEA=424", "authors": ["TLDR Newsletter"], "title": "Exploring GrapheneOS secure allocator: Hardened Malloc", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.synacktiv.com%2Fen%2Fpublications%2Fexploring-grapheneos-secure-allocator-hardened-malloc%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/5MyN4oyFzdGk7-Ry3veiqmnxsCJWLCHqNsl-wK6UWEA=424", "summary": "Exploring GrapheneOS secure allocator: Hardened Malloc (22 minute read) GrapheneOS features a hardened malloc, a memory allocator that uses ARM Memory Tagging Extension (MTE) to assign tags to memory regions and detect corruption, effectively preventing heap overflows and use-after-free attacks. Its two-stage quarantine system, which involves randomized and FIFO queues, makes exploiting freed memory difficult by requiring attackers to perform thousands of free operations. Large allocations ar...", "source": "tldr"}
{"id": "tldr.2509.02ce9112", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ncsc.gov.uk%2Fguidance%2Fexternal-attack-surface-management-buyers-guide%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/YM-PZ2B2vrfWDaTSZ-a2kl2NK9S7P1v4FYudPIXhysg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ncsc.gov.uk%2Fguidance%2Fexternal-attack-surface-management-buyers-guide%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/YM-PZ2B2vrfWDaTSZ-a2kl2NK9S7P1v4FYudPIXhysg=424", "authors": ["TLDR Newsletter"], "title": "External attack surface management buyer's guide", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ncsc.gov.uk%2Fguidance%2Fexternal-attack-surface-management-buyers-guide%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/YM-PZ2B2vrfWDaTSZ-a2kl2NK9S7P1v4FYudPIXhysg=424", "summary": "External attack surface management (EASM) buyer's guide (11 minute read) External Attack Surface Management (EASM) tools automate the discovery and continuous monitoring of internet-facing assets, providing organizations with an attacker's perspective of their vulnerabilities across domains, IP addresses, and exposed services. The strategic value lies in combining automated asset discovery with risk prioritization—EASM products excel at identifying shadow IT, abandoned subdomains, and misconf...", "source": "tldr"}
{"id": "tldr.2509.143ff216", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "authors": ["TLDR Newsletter"], "title": "Control AI in your SDLC", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "summary": "Control AI in your SDLC (Sponsor) 78% of developers use AI coding tools, yet half of functionally correct AI-generated code is insecure. SCW Trust Agent: AI gives leaders visibility and governance to manage this risk - spotting “shadow AI,” mapping vulnerabilities to skill level, and enforcing policy. Be among the first to join the early access waitlist!", "source": "tldr"}
{"id": "tldr.2509.f91d94ca", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMHaggis%2FMSIXBuilder%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/SrcUugbWCm0dwSM-tqr7pZjLLpSQ9ErRX477MEK4MRk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMHaggis%2FMSIXBuilder%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/SrcUugbWCm0dwSM-tqr7pZjLLpSQ9ErRX477MEK4MRk=424", "authors": ["TLDR Newsletter"], "title": "MSIXBuilder", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMHaggis%2FMSIXBuilder%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/SrcUugbWCm0dwSM-tqr7pZjLLpSQ9ErRX477MEK4MRk=424", "summary": "MSIXBuilder (GitHub Repo) MSIXBuilder is a PowerShell tool for creating MSIX packages with embedded test applications.", "source": "tldr"}
{"id": "tldr.2509.8b3fdb7f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falbertan017%2FLLM4Decompile%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/f8kOld0M_dRJk4ZbiWtL0UhWFZ-GhCo5WeJZc7CHhzY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falbertan017%2FLLM4Decompile%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/f8kOld0M_dRJk4ZbiWtL0UhWFZ-GhCo5WeJZc7CHhzY=424", "authors": ["TLDR Newsletter"], "title": "LLM4Decompile", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falbertan017%2FLLM4Decompile%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/f8kOld0M_dRJk4ZbiWtL0UhWFZ-GhCo5WeJZc7CHhzY=424", "summary": "LLM4Decompile (GitHub Repo) LLM4Decompile is an open-source LLM dedicated to decompilation.", "source": "tldr"}
{"id": "tldr.2509.beba1007", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ffirezone%2Ffirezone%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Lq8sTzFtzhqhaOtkjS3GQfe_N5KNftjP8WNvFOQa08k=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ffirezone%2Ffirezone%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Lq8sTzFtzhqhaOtkjS3GQfe_N5KNftjP8WNvFOQa08k=424", "authors": ["TLDR Newsletter"], "title": "Firezone", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ffirezone%2Ffirezone%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Lq8sTzFtzhqhaOtkjS3GQfe_N5KNftjP8WNvFOQa08k=424", "summary": "Firezone (GitHub Repo) Firezone is an open source platform for secure remote access, suitable for organizations of any size. Unlike most VPNs, it uses a granular, least-privileged approach with group-based policies to control access to applications, subnets, and more.", "source": "tldr"}
{"id": "tldr.2509.c0a33cd0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Famerican-archive-of-public-broadcasting-fixes-bug-exposing-restricted-media%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/jcqX9T9IQmsi5yrew8GduZkMU4Zk78iCtL_xm_zkxKw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Famerican-archive-of-public-broadcasting-fixes-bug-exposing-restricted-media%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/jcqX9T9IQmsi5yrew8GduZkMU4Zk78iCtL_xm_zkxKw=424", "authors": ["TLDR Newsletter"], "title": "American Archive of Public Broadcasting fixes bug exposing restricted media", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Famerican-archive-of-public-broadcasting-fixes-bug-exposing-restricted-media%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/jcqX9T9IQmsi5yrew8GduZkMU4Zk78iCtL_xm_zkxKw=424", "summary": "American Archive of Public Broadcasting fixes bug exposing restricted media (2 minute read) A vulnerability in the American Archive of Public Broadcasting website, which has been exploited since at least 2021, allowed unauthorized downloading of protected and private media through an insecure direct object reference (IDOR) flaw that bypassed access controls by manipulating media ID parameters. The exploit circulated in Discord preservation communities and \"data hoarder\" groups, leading to lea...", "source": "tldr"}
{"id": "tldr.2509.f268733e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.secretservice.gov%2Fnewsroom%2Freleases%2F2025%2F09%2Fus-secret-service-dismantles-imminent-telecommunications-threat-new-york%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/hsFwnmJCPa7ZIXsiXuyEttDz5IvUAO53B5vK3XvPRwA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.secretservice.gov%2Fnewsroom%2Freleases%2F2025%2F09%2Fus-secret-service-dismantles-imminent-telecommunications-threat-new-york%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/hsFwnmJCPa7ZIXsiXuyEttDz5IvUAO53B5vK3XvPRwA=424", "authors": ["TLDR Newsletter"], "title": "U.S. Secret Service dismantles imminent telecommunications threat in New York tristate area", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.secretservice.gov%2Fnewsroom%2Freleases%2F2025%2F09%2Fus-secret-service-dismantles-imminent-telecommunications-threat-new-york%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/hsFwnmJCPa7ZIXsiXuyEttDz5IvUAO53B5vK3XvPRwA=424", "summary": "U.S. Secret Service dismantles imminent telecommunications threat in New York tristate area (2 minute read) The US Secret Service (USSS) dismantled a network of over 300 co-located SIM servers and 100,000 SIM cards positioned within 35 miles of the UN General Assembly in New York that were used to conduct telecommunications-related threats against senior US government officials. The devices were capable of performing multiple attack vectors, including disabling cell phone towers, facilitating...", "source": "tldr"}
{"id": "tldr.2509.f700c43b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "authors": ["TLDR Newsletter"], "title": "When LLMs Autonomously Attack", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "summary": "When LLMs Autonomously Attack (5 minute read) A researcher at Carnegie Mellon University demonstrated that an LLM can coordinate a system of agents to recreate real-world attacks. As part of his PhD, Brian Singer recreated the network environment of the 2017 Equifax data breach and observed that an LLM could replicate the attack. These capabilities allow smaller organizations to use LLMs as dedicated red teamers.", "source": "tldr"}
{"id": "tldr.2509.66755dee", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fmicrosoft%2Fmicrosoft-removes-windows-11-safeguard-hold-after-fixing-face-detection-bug%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/ISvxTFEXoXYXz87rVOOz7Ee-787K3VMjq2XqSv-pJvM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fmicrosoft%2Fmicrosoft-removes-windows-11-safeguard-hold-after-fixing-face-detection-bug%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/ISvxTFEXoXYXz87rVOOz7Ee-787K3VMjq2XqSv-pJvM=424", "authors": ["TLDR Newsletter"], "title": "Microsoft lifts Windows 11 update block after face detection fix", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fmicrosoft%2Fmicrosoft-removes-windows-11-safeguard-hold-after-fixing-face-detection-bug%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/ISvxTFEXoXYXz87rVOOz7Ee-787K3VMjq2XqSv-pJvM=424", "summary": "Microsoft lifts Windows 11 update block after face detection fix (1 minute read) Microsoft has removed a compatibility hold that prevented devices with integrated cameras from installing Windows 11 24H2 due to a face detection bug that caused app freezes when using the Camera app, Windows Hello facial recognition, and other camera-utilizing applications.", "source": "tldr"}
{"id": "tldr.2509.5a9f19c1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fmi6-dark-web-portal-silent-courier-russia-secrets%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/W3XzqCfGDxWwdiUdBs29MhYFXabHLs5BgnnfyS6g5PA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fmi6-dark-web-portal-silent-courier-russia-secrets%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/W3XzqCfGDxWwdiUdBs29MhYFXabHLs5BgnnfyS6g5PA=424", "authors": ["TLDR Newsletter"], "title": "MI6 Opens Dark Web Portal \"Silent Courier\" for Russians to Share Secrets", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fmi6-dark-web-portal-silent-courier-russia-secrets%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/W3XzqCfGDxWwdiUdBs29MhYFXabHLs5BgnnfyS6g5PA=424", "summary": "MI6 Opens Dark Web Portal \"Silent Courier\" for Russians to Share Secrets (2 minute read) MI6 launched a dark web portal called Silent Courier to allow potential agents worldwide, particularly Russians, to securely share sensitive information about terrorism and hostile intelligence activities.", "source": "tldr"}
{"id": "tldr.2509.a4822e93", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F782993%2Fsteam-blockblasters-crypto-scam-malware%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7u-eJCg2s2rIn8U0yvWdd-a5k4lpbLELZtLpMDlrB1o=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F782993%2Fsteam-blockblasters-crypto-scam-malware%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7u-eJCg2s2rIn8U0yvWdd-a5k4lpbLELZtLpMDlrB1o=424", "authors": ["TLDR Newsletter"], "title": "Steam game removed after cryptostealer takes over $150K", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F782993%2Fsteam-blockblasters-crypto-scam-malware%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7u-eJCg2s2rIn8U0yvWdd-a5k4lpbLELZtLpMDlrB1o=424", "summary": "Steam game removed after cryptostealer takes over $150K (2 minute read) Steam removed the free-to-play game BlockBlasters after malware stole over $150,000 in cryptocurrency from players, including $32,000 from a Latvian streamer with stage 4 cancer raising funds.", "source": "tldr"}
{"id": "tldr.2509.84fdced6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/tXmOIid54EPf0pUNHsWIb1edaxFUzPLH-XaMIlQV7x4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/tXmOIid54EPf0pUNHsWIb1edaxFUzPLH-XaMIlQV7x4=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/tXmOIid54EPf0pUNHsWIb1edaxFUzPLH-XaMIlQV7x4=424", "summary": "Steam game removed after cryptostealer takes over $150K (2 minute read) Steam removed the free-to-play game BlockBlasters after malware stole over $150,000 in cryptocurrency from players, including $32,000 from a Latvian streamer with stage 4 cancer raising funds.", "source": "tldr"}
{"id": "tldr.2509.65288a17", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplaid.com%2Fjavelin-identity-fraud-whitepaper%2F%3Futm_source=TLDRAI%26utm_medium=PaidNewsletter%26utm_campaign=TLDRAI_Paid_Newsletter_Ad_Buy%26utm_content=Javelin_Identity_Fraud_Report/2/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/i_uksSauJEo1R7Nd3NIA4rd3yh290XFFUp60ecsjd1c=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplaid.com%2Fjavelin-identity-fraud-whitepaper%2F%3Futm_source=TLDRAI%26utm_medium=PaidNewsletter%26utm_campaign=TLDRAI_Paid_Newsletter_Ad_Buy%26utm_content=Javelin_Identity_Fraud_Report/2/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/i_uksSauJEo1R7Nd3NIA4rd3yh290XFFUp60ecsjd1c=424", "authors": ["TLDR Newsletter"], "title": "How to stay ahead of fraudsters in the AI arms race", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplaid.com%2Fjavelin-identity-fraud-whitepaper%2F%3Futm_source=TLDRAI%26utm_medium=PaidNewsletter%26utm_campaign=TLDRAI_Paid_Newsletter_Ad_Buy%26utm_content=Javelin_Identity_Fraud_Report/2/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/i_uksSauJEo1R7Nd3NIA4rd3yh290XFFUp60ecsjd1c=424", "summary": "How to stay ahead of fraudsters in the AI arms race (Sponsor) Bad actors use AI to test stolen credentials across thousands of sites. They share attack patterns in real-time and automate everything.Meanwhile, many fraud-prevention teams are still working in silos and are bogged down by manual processes.The new playbook by Javelin and Plaid looks at how companies can close the gap with modern fraudsters, while making life easier for honest users: Connect identity, device, and behavioral signal...", "source": "tldr"}
{"id": "tldr.2509.94f91b2b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "authors": ["TLDR Newsletter"], "title": "Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "summary": "Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad (4 minute read) Apple has begun to lay the groundwork for adopting Anthropic's Model Context Protocol (MCP) in its latest round of betas. MCP allows AI agents to interface with traditional platforms. The standard has been widely adopted and has become a universal pathway for AI assistants to plug into APIs and data sources. Apple plans to let developers use a system-level MCP integration to expose actions and functiona...", "source": "tldr"}
{"id": "tldr.2509.874e7dbb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F08mDkc/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/xE1DXpClm-ZyoQWtXF3cWTKNr1l4bxuQyjAP0WV9QE4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F08mDkc/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/xE1DXpClm-ZyoQWtXF3cWTKNr1l4bxuQyjAP0WV9QE4=424", "authors": ["TLDR Newsletter"], "title": "OpenAI Unveils Plans for Seemingly Limitless Expansion of Computing Power", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F08mDkc/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/xE1DXpClm-ZyoQWtXF3cWTKNr1l4bxuQyjAP0WV9QE4=424", "summary": "OpenAI Unveils Plans for Seemingly Limitless Expansion of Computing Power (6 minute read) OpenAI has laid out its vision for a $1 trillion build-out of computing warehouses across the US and abroad. It announced five new datacenter sites across the US, built with Oracle and SoftBank, which together will bring online nearly 7 gigawatts of power. The company envisions that it will need more than 20 gigawatts of computing capacity to meet demand for ChatGPT. Each gigawatt is expected to cost rou...", "source": "tldr"}
{"id": "tldr.2509.f702cf27", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLMds6C/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/iefGx7gZPW4ZJko4oEXuCzR-ludxBkD78kxsr2VEwzg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLMds6C/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/iefGx7gZPW4ZJko4oEXuCzR-ludxBkD78kxsr2VEwzg=424", "authors": ["TLDR Newsletter"], "title": "Google's Mixboard for Visual Concepting", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLMds6C/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/iefGx7gZPW4ZJko4oEXuCzR-ludxBkD78kxsr2VEwzg=424", "summary": "Google's Mixboard for Visual Concepting (2 minute read) Google Labs' Mixboard is an AI-powered concept board that lets users generate and refine visual ideas using text prompts, image editing, and contextual generation tools.", "source": "tldr"}
{"id": "tldr.2509.a48c1b34", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FQXYIDp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/GEPq4ONpcDGM-63OkYRVq6mQN0aTha8OwpDlisobcsk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FQXYIDp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/GEPq4ONpcDGM-63OkYRVq6mQN0aTha8OwpDlisobcsk=424", "authors": ["TLDR Newsletter"], "title": "Perplexity Email Assistant Launch", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FQXYIDp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/GEPq4ONpcDGM-63OkYRVq6mQN0aTha8OwpDlisobcsk=424", "summary": "Perplexity Email Assistant Launch (2 minute read) Perplexity's new Email Assistant for Max subscribers integrates directly with users' inboxes to draft replies, organize messages, and schedule meetings.", "source": "tldr"}
{"id": "tldr.2509.97f7ed06", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/PecOJy1Ca2fUzvuBt78j2XrzMWym6QVNswhd3mMpnCQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/PecOJy1Ca2fUzvuBt78j2XrzMWym6QVNswhd3mMpnCQ=424", "authors": ["TLDR Newsletter"], "title": "Getting AI to Work in Complex Codebases", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/PecOJy1Ca2fUzvuBt78j2XrzMWym6QVNswhd3mMpnCQ=424", "summary": "Getting AI to Work in Complex Codebases (23 minute read) AI coding agents are great for new projects or small changes, but they often make developers less productive in large, established codebases. This post discusses a family of techniques called 'frequent intention compaction' that involves deliberately structuring how context is fed to AI throughout the development process to improve performance on large codebases. The technique has allowed a team to use AI to handle Rust codebases with 3...", "source": "tldr"}
{"id": "tldr.2509.3310c71e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparthsareen.com%2Fblog.html%23sampling.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/m4Ihstto1-E0YbE2J98uditlCKquZewH121DcQ41h5Y=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparthsareen.com%2Fblog.html%23sampling.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/m4Ihstto1-E0YbE2J98uditlCKquZewH121DcQ41h5Y=424", "authors": ["TLDR Newsletter"], "title": "Sampling and structured outputs in LLMs", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparthsareen.com%2Fblog.html%23sampling.md%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/m4Ihstto1-E0YbE2J98uditlCKquZewH121DcQ41h5Y=424", "summary": "Sampling and structured outputs in LLMs (10 minute read) Sampling is the process of selecting a token from a model's vocabulary based on the probability distribution. Structured outputs allow models to turn unstructured data into structured data. Together, they both determine a model's next chosen token. Large language models are extremely sensitive to the slightest variation in their system prompts, templating, structured outputs, and sampling parameters.", "source": "tldr"}
{"id": "tldr.2509.2c773d53", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.delltechnologies.com%2Fasset%2Fen-us%2Fsolutions%2Fbusiness-solutions%2Fbriefs-summaries%2Fendpoint-security-for-ai-ebook.pdf%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/86_j7c-OaQNdM4WuMGMXUeACfPYU_3cJlvNl5yP5Gz4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.delltechnologies.com%2Fasset%2Fen-us%2Fsolutions%2Fbusiness-solutions%2Fbriefs-summaries%2Fendpoint-security-for-ai-ebook.pdf%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/86_j7c-OaQNdM4WuMGMXUeACfPYU_3cJlvNl5yP5Gz4=424", "authors": ["TLDR Newsletter"], "title": "The attack surface of on-device AI", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.delltechnologies.com%2Fasset%2Fen-us%2Fsolutions%2Fbusiness-solutions%2Fbriefs-summaries%2Fendpoint-security-for-ai-ebook.pdf%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/86_j7c-OaQNdM4WuMGMXUeACfPYU_3cJlvNl5yP5Gz4=424", "summary": "The attack surface of on-device AI (Sponsor) Running AI on your PC is transforming productivity - but like all emerging tech, it comes with some security risk. Learn about the tactics attackers use to gain entry to your endpoints, and how you can stay secure. Read the eBook by Dell and Intel", "source": "tldr"}
{"id": "tldr.2509.905280c6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970535239048159237.html%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ObyqXdjap_YKafSI3udEEk3hVCG4j1nr7FVxPWCQTTs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970535239048159237.html%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ObyqXdjap_YKafSI3udEEk3hVCG4j1nr7FVxPWCQTTs=424", "authors": ["TLDR Newsletter"], "title": "GPT-5-Codex is live in the Responses API", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970535239048159237.html%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ObyqXdjap_YKafSI3udEEk3hVCG4j1nr7FVxPWCQTTs=424", "summary": "GPT-5-Codex is live in the Responses API (1 minute read) GPT-5-Codex is now live in OpenAI's Responses API. It is also available in the Codex CLI via API key. The model has been optimized and is not a drop-in replacement for other models. A link to a prompting guide for the model is available.", "source": "tldr"}
{"id": "tldr.2509.ba1869b5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "authors": ["TLDR Newsletter"], "title": "Chrome DevTools for your AI agent", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "summary": "Chrome DevTools (MCP) for your AI agent (5 minute read) The Chrome DevTools Model Context Protocol (MCP) server brings the power of Chrome DevTools to AI coding assistants. It allows AI coding assistants to debug web pages directly in Chrome and benefit from DevTools debugging capabilities and performance insights. Use cases include verifying code changes in real-time, diagnosing network and console errors, simulating user behavior, and automating performance audits. A video showing how the C...", "source": "tldr"}
{"id": "tldr.2509.793d3644", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2O7a2j%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/JbnYcsOYwICP-y4XMEbgB8PtV943ruNvi9-fbFRDfEY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2O7a2j%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/JbnYcsOYwICP-y4XMEbgB8PtV943ruNvi9-fbFRDfEY=424", "authors": ["TLDR Newsletter"], "title": "Build more powerful voice agents with the Gemini Live API", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2O7a2j%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/JbnYcsOYwICP-y4XMEbgB8PtV943ruNvi9-fbFRDfEY=424", "summary": "Build more powerful voice agents with the Gemini Live API (8 minute read) Google has significantly improved function calling and enhanced proactive audio capabilities in the Gemini Live API to handle interruptions, pauses, and side conversations gracefully. It plans to roll out support for thinking capabilities next week. Details about the recent updates to the Gemini Live API are available in the link along with video demos of the new capabilities.", "source": "tldr"}
{"id": "tldr.2509.e8a500b3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.samaltman.com%2Fabundant-intelligence%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/LA9jVm8Sh_Mlb1R4nemlRa8D1WjbGPPKUGLQUkLHCfo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.samaltman.com%2Fabundant-intelligence%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/LA9jVm8Sh_Mlb1R4nemlRa8D1WjbGPPKUGLQUkLHCfo=424", "authors": ["TLDR Newsletter"], "title": "Abundant Intelligence", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.samaltman.com%2Fabundant-intelligence%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/LA9jVm8Sh_Mlb1R4nemlRa8D1WjbGPPKUGLQUkLHCfo=424", "summary": "Abundant Intelligence (2 minute read) OpenAI wants to create a factory that can produce a gigawatt of new AI infrastructure every week. The project will take several years and require innovation at every level of the stack. A lot of the infrastructure will be built in the US. The company will release more details about its plans and partners over the next couple of months.", "source": "tldr"}
{"id": "tldr.2509.1579453a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsharif.io%2F28-ideas-2025%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ktOzdQbgPtiVZRzNpBl7MPLtgl4O9K1KX_gZUarwkag=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsharif.io%2F28-ideas-2025%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ktOzdQbgPtiVZRzNpBl7MPLtgl4O9K1KX_gZUarwkag=424", "authors": ["TLDR Newsletter"], "title": "The 28 AI tools I wish existed", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsharif.io%2F28-ideas-2025%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/ktOzdQbgPtiVZRzNpBl7MPLtgl4O9K1KX_gZUarwkag=424", "summary": "The 28 AI tools I wish existed (5 minute read) There has never been a better time in the history of computing to build software. Advances in AI now make it possible to build apps with new capabilities. This post lists a few ideas for builders to consider.", "source": "tldr"}
{"id": "tldr.2509.416a1f77", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2GkI3Y/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/2Q1SOX3VFumt488i7xivKQt_O1EmlxwkVex_XDujUiI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2GkI3Y/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/2Q1SOX3VFumt488i7xivKQt_O1EmlxwkVex_XDujUiI=424", "authors": ["TLDR Newsletter"], "title": "Microsoft looks to build AI marketplace for publishers", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2GkI3Y/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/2Q1SOX3VFumt488i7xivKQt_O1EmlxwkVex_XDujUiI=424", "summary": "Microsoft looks to build AI marketplace for publishers (3 minute read) Microsoft's Publisher Content Marketplace (PCM) will launch as a pilot program with a limited set of publishers. The company's Copilot assistant will be the first AI buyer on the marketplace. Most AI companies have focused on brokering licensing deals that pay publishers upfront for access to content rather than on a per-use basis. PCM will help Microsoft deepen its relationship with publishers as it expands its AI capabil...", "source": "tldr"}
{"id": "tldr.2509.ddc7b611", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.deepset.ai%2Fwebinars%2Fai-for-insurance-sales%3Futm_campaign=23430057-2510%2520-%2520Insurance%26utm_source=tldr%26utm_medium=newsletter%26utm_content=insurance-sales-webinar/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/aHTJGTpWJC-kr77LHPxMWhYnWf2wTwSyAEGeJLKJkV0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.deepset.ai%2Fwebinars%2Fai-for-insurance-sales%3Futm_campaign=23430057-2510%2520-%2520Insurance%26utm_source=tldr%26utm_medium=newsletter%26utm_content=insurance-sales-webinar/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/aHTJGTpWJC-kr77LHPxMWhYnWf2wTwSyAEGeJLKJkV0=424", "authors": ["TLDR Newsletter"], "title": "How Gen AI is Transforming Insurance Workflows", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.deepset.ai%2Fwebinars%2Fai-for-insurance-sales%3Futm_campaign=23430057-2510%2520-%2520Insurance%26utm_source=tldr%26utm_medium=newsletter%26utm_content=insurance-sales-webinar/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/aHTJGTpWJC-kr77LHPxMWhYnWf2wTwSyAEGeJLKJkV0=424", "summary": "How Gen AI is Transforming Insurance Workflows (Sponsor) Join this deepset webinar with AI engineer Laura Luckert to see a real-world example of how insurers are using Gen AI to speed up broker support and close more business—without adding headcount. Save Your Seat", "source": "tldr"}
{"id": "tldr.2509.0c0869c6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5qDtYr/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/3T8D2kUTGABprWA3ciwHPmDt-rORwGOCDz2CcFSqJm4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5qDtYr/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/3T8D2kUTGABprWA3ciwHPmDt-rORwGOCDz2CcFSqJm4=424", "authors": ["TLDR Newsletter"], "title": "Meta launches super PAC to fight AI regulation", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5qDtYr/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/3T8D2kUTGABprWA3ciwHPmDt-rORwGOCDz2CcFSqJm4=424", "summary": "Meta launches super PAC to fight AI regulation (1 minute read) Meta is investing \"tens of millions\" into the American Technology Excellence Project, a super PAC targeting state-level candidates who oppose AI regulation.", "source": "tldr"}
{"id": "tldr.2509.788e5255", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F779053%2Fsam-altman-says-chatgpt-will-stop-talking-about-suicide-with-teens%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/1gcvbESYVUW1vjY2z4nByNHOO-2m4PW50zSDs0tAbyI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F779053%2Fsam-altman-says-chatgpt-will-stop-talking-about-suicide-with-teens%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/1gcvbESYVUW1vjY2z4nByNHOO-2m4PW50zSDs0tAbyI=424", "authors": ["TLDR Newsletter"], "title": "Sam Altman says ChatGPT will stop talking about suicide with teens", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F779053%2Fsam-altman-says-chatgpt-will-stop-talking-about-suicide-with-teens%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/1gcvbESYVUW1vjY2z4nByNHOO-2m4PW50zSDs0tAbyI=424", "summary": "Sam Altman says ChatGPT will stop talking about suicide with teens (4 minute read) ChatGPT will start using an age-prediction system for under-18 users and stop discussing suicide with teens.", "source": "tldr"}
{"id": "tldr.2509.fcaada08", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FKgxRcu/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/YW-nmclgCWFmJWd2PfKhLcUkbAh2mjoEywrmnfNHRdg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FKgxRcu/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/YW-nmclgCWFmJWd2PfKhLcUkbAh2mjoEywrmnfNHRdg=424", "authors": ["TLDR Newsletter"], "title": "How Americans View AI and Its Impact on People and Society", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FKgxRcu/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/YW-nmclgCWFmJWd2PfKhLcUkbAh2mjoEywrmnfNHRdg=424", "summary": "How Americans View AI and Its Impact on People and Society (6 minute read) The Pew Research Center found that most Americans are increasingly pessimistic about AI's effects on society.", "source": "tldr"}
{"id": "tldr.2509.12a778c1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/O3Ww0QwudnH8OhI3RRDmAo_FNslbxxdOV6Rqlh1rOLo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/O3Ww0QwudnH8OhI3RRDmAo_FNslbxxdOV6Rqlh1rOLo=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/O3Ww0QwudnH8OhI3RRDmAo_FNslbxxdOV6Rqlh1rOLo=424", "summary": "How Americans View AI and Its Impact on People and Society (6 minute read) The Pew Research Center found that most Americans are increasingly pessimistic about AI's effects on society.", "source": "tldr"}
