<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
*Anu Pradhan,Alexandra Ortan,Apurv Verma,Madhavan Seshadri*

Main category: cs.CL

TL;DR: 本文研究使用LLM作为评估者来评估法律领域的检索增强生成系统，发现传统评估指标在AI系统评估中存在误导性，提出了更稳健的评估指标和统计方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，传统推荐系统评估指标在专业领域（如法律研究）中无法捕捉细微的质量维度，需要探索LLM作为评估者的可靠性。

Method: 通过系统实验研究两个关键问题：哪种评估者间可靠性指标最能捕捉LLM与人类评估的一致性，以及如何进行统计上可靠的系统比较。

Result: 发现传统协议指标如Krippendorff's alpha在AI系统评估的偏斜分布中具有误导性，Gwet's AC2和等级相关系数更适合评估者选择，Wilcoxon Signed-Rank Test与Benjamini-Hochberg校正提供了可靠的系统比较统计严谨性。

Conclusion: 研究为法律应用提供了可扩展、成本效益高的评估方法，将人力密集型瓶颈转变为自动化且统计原则化的评估框架。

Abstract: The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.

</details>


### [2] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
*Sumanta Bhattacharyya,Sara Riaz,Pedram Rooshenas*

Main category: cs.CL

TL;DR: R2tA方法通过提炼LLM的推理轨迹来训练任务特定的小型推理模型，解决了高质量标签稀缺的问题，在数据库EERD评估任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当直接人工监督或高质量标签稀缺时，训练任务特定的小型推理模型具有挑战性。LLM产生的丰富中间推理轨迹可以被系统提炼，为训练提供有效的监督信号。

Method: 提出Reason-Refine-then-Align (R2tA)方法：1) 从开源基础模型生成初始推理和响应；2) 提炼这些轨迹，修复幻觉和不一致性，形成高质量数据集；3) 进行两阶段对齐：监督微调(SFT)和直接偏好优化(DPO)，校准模型的中间推理。

Result: 在600个EERD变体数据集（450训练/150测试）上评估，涵盖11个错误类别。R2tA在结构复杂的数据库系统设计任务中表现优异，相比仅提示方法能更好地检测错误而不产生幻觉。

Conclusion: R2tA为数据稀缺领域的可扩展LLM适应提供了一条实用、经济高效的路径，使教育和其它领域的可重现AI工具成为可能。

Abstract: Training a task-specific small reasoning model is challenging when direct
human supervision or high-quality labels are scarce. However, LLMs with
reasoning capabilities produce abundant intermediate reasoning traces that can
be systematically refined to create effective supervision signals. We propose
Reason-Refine-then-Align (R2tA), which turns refined model rationales into
supervision for training task-specific reasoning models. Our method generates
initial reasoning and responses from an open-source base model on task-specific
inputs, then refines these traces, fixing hallucinations and inconsistencies,
to form a high-fidelity dataset. We perform a two-stage alignment, supervised
fine-tuning (SFT), followed by direct preference optimization (DPO) to
calibrate the model's intermediate reasoning with human-validated conceptual
preferences and then condition the final output on that aligned reasoning. As a
case study, we apply R2tA to evaluate extended entity relationship diagrams
(EERDs) in database system design, a structurally complex task where
prompt-only methods miss or hallucinate errors. We curated a dataset of 600
EERD variants (train/test split of 450/150, respectively) with induced mistakes
spanning 11 categories. Empirical evaluation suggests R2tA provides a
practical, cost-effective path to scalable LLM adaptation in data-scarce
domains, enabling reproducible AI tools for education and beyond.

</details>


### [3] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出EconProver方法，通过动态CoT切换机制和多样化并行强化学习，显著降低自动定理证明的计算成本，仅用12%的计算开销就能达到基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在自动定理证明中采用反射式CoT推理和增加采样次数等策略，虽然性能提升但计算开销巨大，且现有成本分析忽略了不同扩展策略带来的采样成本差异。

Method: 提出两种互补方法：1）动态CoT切换机制减少不必要的token消耗；2）多样化并行强化学习与可训练前缀，在受限采样次数下提高通过率。这些方法可集成到统一的EconRL流程中。

Result: 在miniF2F和ProofNet上的实验表明，EconProver仅用基线方法12%的计算成本就达到了相当的性能。

Conclusion: 这项工作为部署轻量级自动定理证明模型提供了可行方案，在不牺牲性能的前提下显著降低计算成本。

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [4] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 本文提出了一种检测LLM意识形态偏见的方法，通过分析模型输出在相关主题提示下的分布变化来识别潜在的意识形态操控，适用于黑盒系统审计。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在数百万用户产品中的广泛应用，其输出可能影响个人信念和公众舆论。需要开发方法来检测LLM是否被有意引导向特定意识形态立场，以防止对公共话语的过度影响。

Method: 采用先前提出的统计方法，适应意识形态偏见审计的新背景。该方法具有模型无关性，不需要访问语言模型内部结构，通过分析主题相关提示下模型输出的分布变化来识别潜在意识形态操控。

Result: 通过一系列实验验证了该方法的实用性，证明其能够有效支持对LLM行为的独立事后审计。

Conclusion: 该方法为检测LLM意识形态操控提供了有效的工具，特别适合对专有黑盒系统进行审计，有助于维护公共话语的公平性。

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [5] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
*Yougen Zhou,Qin Chen,Ningning Zhou,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的双奖励函数方法，通过识别LLM在策略规划中的知识边界来缓解情感支持对话中的策略偏好偏差问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在情感支持对话中存在策略规划准确性低和特定策略偏好偏差的问题，现有方法未能深入分析偏差的根本原因。

Method: 首先识别LLM在策略规划中的知识边界，然后提出基于强化学习的双奖励函数方法，通过准确性和基于熵的置信度来优化策略规划。

Result: 在ESCov和ExTES数据集上的实验表明，该方法在多个LLM骨干网络上均优于基线方法。

Conclusion: 该方法有效缓解了LLM在情感支持对话中的策略偏好偏差问题，提高了策略规划的准确性。

Abstract: Emotional support conversation (ESC) aims to alleviate distress through
empathetic dialogue, yet large language models (LLMs) face persistent
challenges in delivering effective ESC due to low accuracy in strategy
planning. Moreover, there is a considerable preference bias towards specific
strategies. Prior methods using fine-tuned strategy planners have shown
potential in reducing such bias, while the underlying causes of the preference
bias in LLMs have not well been studied. To address these issues, we first
reveal the fundamental causes of the bias by identifying the knowledge
boundaries of LLMs in strategy planning. Then, we propose an approach to
mitigate the bias by reinforcement learning with a dual reward function, which
optimizes strategy planning via both accuracy and entropy-based confidence for
each region according to the knowledge boundaries. Experiments on the ESCov and
ExTES datasets with multiple LLM backbones show that our approach outperforms
the baselines, confirming the effectiveness of our approach.

</details>


### [6] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 提出了一种基于LLM隐藏表示的问题难度估计方法，无需生成输出token即可准确评估问题难度，并在自适应推理策略中提高效率


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖重复采样、辅助模型或微调目标模型，计算成本高且可能损害通用性，需要更高效的难度估计方法

Method: 将token级生成过程建模为马尔可夫链，定义价值函数来估计给定隐藏状态的预期输出质量，仅基于初始隐藏状态进行难度估计

Result: 在文本和多模态任务上的实验表明，该方法在难度估计方面始终优于现有基线，自适应推理策略使用更少的生成token实现了更高的推理效率

Conclusion: 该方法提供了一种高效准确的难度估计方案，无需额外计算成本，可有效指导自适应推理策略

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [7] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
*Sijia Cui,Shuai Xu,Aiyao He,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: PLAP框架通过语言规划和参数化技能执行，解决了LLM智能体在长视野对抗环境中的落地问题，在MicroRTS游戏中表现出色，超越了多个基线智能体。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法存在两个主要问题：直接生成底层动作不可靠，而高层任务规划又依赖专家经验。需要一种能够有效在复杂长视野环境中落地的智能体框架。

Method: 提出PLAP（Plan with Language, Act with Parameter）框架，包含三个核心组件：参数化技能库、LLM驱动的技能规划器、以及将参数化技能转换为可执行动作序列的技能执行器。

Result: 在MicroRTS游戏中，GPT-4o驱动的PLAP在零样本设置下超越了80%的基线智能体，Qwen2-72B驱动的PLAP在少量样本设置下超越了顶级脚本智能体CoacAI。

Conclusion: PLAP框架有效解决了LLM智能体在长视野环境中的落地挑战，并通过综合评估指标建立了LLM长视野技能规划能力的排行榜。

Abstract: Recent advancements in Large Language Models(LLMs) have led to the
development of LLM-based AI agents. A key challenge is the creation of agents
that can effectively ground themselves in complex, adversarial long-horizon
environments. Existing methods mainly focus on (1) using LLMs as policies to
interact with the environment through generating low-level feasible actions,
and (2) utilizing LLMs to generate high-level tasks or language guides to
stimulate action generation. However, the former struggles to generate reliable
actions, while the latter relies heavily on expert experience to translate
high-level tasks into specific action sequences. To address these challenges,
we introduce the Plan with Language, Act with Parameter (PLAP) planning
framework that facilitates the grounding of LLM-based agents in long-horizon
environments. The PLAP method comprises three key components: (1) a skill
library containing environment-specific parameterized skills, (2) a skill
planner powered by LLMs, and (3) a skill executor converting the parameterized
skills into executable action sequences. We implement PLAP in MicroRTS, a
long-horizon real-time strategy game that provides an unfamiliar and
challenging environment for LLMs. The experimental results demonstrate the
effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting
outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully
crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.
Additionally, we design comprehensive evaluation metrics and test 6
closed-source and 2 open-source LLMs within the PLAP framework, ultimately
releasing an LLM leaderboard ranking long-horizon skill planning ability. Our
code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [8] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
*Yongjian Tang,Doruk Tuncel,Christian Koerner,Thomas Runkler*

Main category: cs.CL

TL;DR: 论文研究发现过度提示（over-prompting）现象：在提示中加入过多领域特定示例反而会降低大语言模型性能，挑战了传统少样本学习的认知。通过实验确定了不同LLM的最佳示例数量，在软件需求分类任务上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中过度提示现象，即过多的上下文示例反而导致性能下降的问题，挑战传统少样本学习认为更多相关示例总是有益的认知。

Method: 使用三种标准少样本选择方法（随机采样、语义嵌入、TF-IDF向量）构建提示框架，在多个LLM上进行评估，通过逐步增加TF-IDF选择和分层少样本示例数量来确定每个LLM的最佳示例量。

Result: 实验发现过度添加领域特定示例会降低某些LLM性能，确定了每个LLM的最佳示例数量，在软件需求分类任务上比现有最优方法提升1%的准确率。

Conclusion: 过度提示确实存在，需要为不同LLM找到最佳示例数量，避免性能下降。结合TF-IDF和分层选择的方法能用更少示例获得更好性能。

Abstract: Over-prompting, a phenomenon where excessive examples in prompts lead to
diminished performance in Large Language Models (LLMs), challenges the
conventional wisdom about in-context few-shot learning. To investigate this
few-shot dilemma, we outline a prompting framework that leverages three
standard few-shot selection methods - random sampling, semantic embedding, and
TF-IDF vectors - and evaluate these methods across multiple LLMs, including
GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.
Our experimental results reveal that incorporating excessive domain-specific
examples into prompts can paradoxically degrade performance in certain LLMs,
which contradicts the prior empirical conclusion that more relevant few-shot
examples universally benefit LLMs. Given the trend of LLM-assisted software
engineering and requirement analysis, we experiment with two real-world
software requirement classification datasets. By gradually increasing the
number of TF-IDF-selected and stratified few-shot examples, we identify their
optimal quantity for each LLM. This combined approach achieves superior
performance with fewer examples, avoiding the over-prompting problem, thus
surpassing the state-of-the-art by 1% in classifying functional and
non-functional requirements.

</details>


### [9] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: LLM在人格特质评估方面表现有限，与真实人格特质的Pearson相关系数低于0.26，提示当前模型在心理理解方面存在挑战


<details>
  <summary>Details</summary>
Motivation: 探索LLM在自然对话环境中对人类人格特质的理解能力，填补现有研究在连续真实人格评估方面的空白

Method: 使用包含半结构化访谈记录和连续Big Five特质分数的基准数据集，评估三种方法：零样本和思维链提示、LoRA微调、预训练嵌入回归

Result: 所有模型预测与真实人格特质的相关性都低于0.26，思维链提示相比零样本提示提升有限

Conclusion: 当前LLM与验证心理构念的对齐程度有限，人格推断更多依赖潜在语义表示而非显式推理，需要特质特定提示和上下文感知建模的进一步研究

Abstract: Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.

</details>


### [10] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
*Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出Agentic CPT方法，通过持续预训练构建强大的智能体基础模型AgentFounder-30B，在10个基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于通用基础模型的训练方法在智能体任务中表现不佳，主要原因是缺乏强大的智能体基础模型，导致模型需要同时学习多种智能体行为并与专家演示对齐，产生优化冲突

Method: 提出Agentic Continual Pre-training (Agentic CPT)方法，将其整合到深度研究智能体训练流程中，构建智能体基础模型AgentFounder

Result: AgentFounder-30B在10个基准测试中达到最先进性能：BrowseComp-en 39.9%、BrowseComp-zh 43.3%、HLE Pass@1 31.5%，同时保持强大的工具使用能力

Conclusion: Agentic CPT方法有效解决了智能体基础模型缺失问题，显著提升了智能体在复杂任务中的表现

Abstract: Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating Agentic Continual Pre-training (Agentic CPT) into the
deep research agents training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [11] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
*Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了一个可扩展的环境构建框架AgentScaler，通过自动生成异构模拟环境来增强大语言模型的函数调用能力，采用两阶段微调策略，在多个基准测试中显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现实世界API需要精确、鲁棒的函数调用智能，而智能体能力的广度与其训练环境的多样性密切相关。为了解决环境规模化构建和有效训练的核心挑战

Method: 设计可扩展框架自动构建完全模拟的异构环境，采用两阶段微调策略：先赋予基础智能体能力，再进行领域专业化

Result: 在tau-bench、tau2-Bench和ACEBench等智能体基准测试中，训练的AgentScaler模型显著增强了模型的函数调用能力

Conclusion: 通过规模化环境构建和两阶段训练策略，能够有效提升大语言模型的通用智能体智能和函数调用能力

Abstract: Advanced agentic intelligence is a prerequisite for deploying Large Language
Models in practical, real-world applications. Diverse real-world APIs demand
precise, robust function-calling intelligence, which needs agents to develop
these capabilities through interaction in varied environments. The breadth of
function-calling competence is closely tied to the diversity of environments in
which agents are trained. In this work, we scale up environments as a step
towards advancing general agentic intelligence. This gives rise to two central
challenges: (i) how to scale environments in a principled manner, and (ii) how
to effectively train agentic capabilities from experiences derived through
interactions with these environments. To address these, we design a scalable
framework that automatically constructs heterogeneous environments that are
fully simulated, systematically broadening the space of function-calling
scenarios. We further adapt a two-phase agent fine-tuning strategy: first
endowing agents with fundamental agentic capabilities, then specializing them
for domain-specific contexts. Extensive experiments on agentic benchmarks,
tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,
AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [12] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
*Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou*

Main category: cs.CL

TL;DR: ReSum是一种新的web agent范式，通过周期性上下文摘要克服LLM上下文窗口限制，实现无限探索，比ReAct平均提升4.5%性能


<details>
  <summary>Details</summary>
Motivation: 解决LLM-based web agent在复杂查询任务中因上下文窗口限制而无法完成多轮搜索的问题

Method: 提出ReSum范式进行周期性上下文摘要，并开发ReSum-GRPO训练方法进行摘要条件推理训练

Result: 在三个基准测试中平均比ReAct提升4.5%，经过训练后进一步提升8.2%，WebResummer-30B在BrowseComp基准上达到33.3%和18.3%的Pass@1

Conclusion: ReSum通过摘要机制有效解决了上下文限制问题，显著提升了web agent的性能表现

Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce ReSum, a novel paradigm that enables indefinite
exploration through periodic context summarization. ReSum converts growing
interaction histories into compact reasoning states, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and
advantage broadcasting to familiarize agents with summary-conditioned
reasoning. Extensive experiments on web agents of varying scales across three
benchmarks demonstrate that ReSum delivers an average absolute improvement of
4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO
training. Notably, with only 1K training samples, our WebResummer-30B (a
ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on
BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web
agents.

</details>


### [13] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 该论文批判性评估了LLM激活解释方法，发现现有基准测试存在缺陷，解释结果往往反映了解释器LLM的参数知识而非目标模型的真实激活模式。


<details>
  <summary>Details</summary>
Motivation: 验证LLM内部表示解释方法的有效性，探究这些方法是否真正揭示了目标模型的内部工作机制，还是仅仅反映了输入信息或解释器模型的知识。

Method: 通过对比实验评估流行解释方法，在现有基准测试上进行测试，并设计控制实验来分析解释结果的信息来源。

Result: 发现解释方法在无需访问目标模型内部的情况下也能在基准测试上成功，且解释结果往往反映了解释器LLM的参数知识而非目标模型的激活模式。

Conclusion: 需要开发更有针对性的基准测试和实验控制来严格评估解释方法是否真正提供了对LLM操作的有意义洞察。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究发现SFT+RL两阶段微调中，SFT早期OOD性能最佳但随后下降，RL主要起恢复作用而非创造新能力，奇异向量旋转是性能变化的关键机制


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL在两阶段微调中的协同机制，挑战"SFT记忆、RL泛化"的简单说法

Method: 使用SVD分析参数矩阵，手动编辑参数并观察性能影响，分析不同训练阶段的OOD表现

Result: 发现OOD性能在SFT早期达到峰值后下降，RL主要恢复SFT丢失的能力而非创造新能力，奇异向量旋转而非奇异值变化是主要机制

Conclusion: 重新定义了SFT和RL在两阶段微调中的角色，发现奇异向量旋转是关键机制，RL的恢复能力有限制

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [15] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文对带有辅助回归头的JEPA变体进行理论分析，证明在确定性MDP中，当训练使潜在转移一致性损失和辅助回归损失趋近于零时，非等价观测必须映射到不同的潜在表示，辅助任务锚定了表示必须保留的区分性。


<details>
  <summary>Details</summary>
Motivation: Joint-Embedding Predictive Architecture (JEPA) 在视觉表示学习和基于模型的强化学习中应用日益广泛，但其行为机制仍缺乏深入理解，需要理论分析来指导改进。

Method: 提出一个简单的实用JEPA变体，包含与潜在动力学联合训练的辅助回归头，在确定性MDP中进行理论分析，并通过计数环境中的控制消融实验验证理论。

Result: 证明了"无病态表示坍缩"定理：在训练损失趋零的条件下，非等价观测必须映射到不同的潜在表示，辅助任务决定了表示需要保持的区分性。实验表明联合训练比单独训练产生更丰富的表示。

Conclusion: 通过辅助函数与转移动力学的结合，可以编码正确的等价关系，这为提高JEPA编码器性能指明了路径。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [16] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 提出了一个统一的评估框架，比较PPO、DPO、ORPO、KTO等LLM对齐方法在事实性、安全性、简洁性、主动性和多样性五个维度上的表现，发现不同方法在不同维度各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个技术或特定维度，缺乏对LLM对齐方法内在权衡的整体评估，需要建立一个全面的评估框架来指导更平衡可靠的LLM开发。

Method: 使用统一的评估框架，比较四种主流对齐方法（PPO、DPO、ORPO、KTO），在分布内和分布外数据集上进行评估，采用经过人工研究验证的LLM-as-Judge提示方法。

Result: DPO和KTO在事实准确性方面表现最佳，PPO和DPO在安全性方面领先，PPO在简洁性和主动性之间取得最佳平衡。

Conclusion: 研究揭示了常见对齐方法的内在权衡关系，为开发更平衡可靠的LLM提供了指导，不同方法在不同应用场景下各有优势。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [17] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor是一种后训练方法，通过生成高不确定性任务、RFT冷启动和DUPO算法，使开源模型在复杂信息搜索任务中达到专有代理的性能水平。


<details>
  <summary>Details</summary>
Motivation: 解决LLM训练中超越人类认知限制的问题，专有代理系统在复杂信息搜索基准上表现出超人类能力，而开源模型缺乏系统化处理极端不确定性的推理模式。

Method: 通过结构化采样和信息模糊化生成新颖的高不确定性任务，采用RFT冷启动和高效的代理强化学习训练算法DUPO（重复采样策略优化）。

Result: WebSailor在复杂信息搜索任务中显著优于所有开源代理，匹配专有代理的性能，缩小了能力差距。

Conclusion: 该方法成功地将专有代理的关键能力移植到开源模型中，证明了系统化不确定性处理能力的重要性。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [18] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文对两种基于投影的安全RL方法（SE-RL和SP-RL）进行了理论比较，重点分析了动作别名现象对策略梯度的影响，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 尽管基于投影的安全过滤器在安全关键RL中广泛应用，但对其两种主要集成策略（SE-RL和SP-RL）的理论差异缺乏正式理解，需要系统分析它们的行为和性能差异。

Method: 提出了统一的形式化框架，理论分析了SE-RL和SP-RL的策略梯度估计，重点研究了动作别名现象的影响，并比较了不同的缓解策略，包括为SP-RL提出的新型基于惩罚的改进方法。

Result: 实证结果支持理论预测，显示动作别名对SP-RL的损害更大，但通过适当的改进策略，SP-RL可以在各种环境中匹配或超越改进的SE-RL。

Conclusion: 研究结果为根据任务特性选择和优化基于投影的安全RL方法提供了可操作的见解，表明两种方法在适当改进后都能取得良好性能。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [19] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1是一个基于强化学习的框架，通过生成可执行Python代码使大语言模型能够进行通用、组合式和多步骤的工具使用，在GAIA基准测试中比强基线提升约10%的准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语言理解和推理方面表现强大，但在处理需要最新知识、精确操作或专用工具使用的现实任务时仍有限制。

Method: 提出强化学习框架Tool-R1，支持用户自定义工具和标准库集成，通过基于结果的奖励函数（结合LLM答案判断和代码执行成功）指导策略优化，并使用动态样本队列缓存和重用高质量轨迹以提高训练效率。

Result: 在GAIA基准测试中，Tool-R1显著提高了准确性和鲁棒性，比强基线提升约10%，在复杂多步骤任务上提升更大。

Conclusion: Tool-R1在现实应用中具有实现可靠高效工具增强推理的潜力。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [20] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp是首个开源的金融搜索智能体基准测试，包含三个真实金融分析师工作流程任务，由70位金融专家标注，评估了21个模型在复杂金融搜索推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有金融数据集缺乏对端到端智能体数据搜索能力的评估，金融领域需要处理时效性强、领域特定的复杂多步搜索任务，是评估搜索能力和知识推理的理想场景。

Method: 构建包含三个任务（时效数据获取、简单历史查询、复杂历史调查）的基准测试，涵盖全球和大中华区市场635个问题，采用多阶段质量保证流程，由70位金融专家进行标注。

Result: Grok 4在全球化子集上表现最佳，接近专家级准确率；DouBao在大中华区子集领先；实验显示配备网络搜索和金融插件能显著提升性能，模型和工具的国家来源对性能有显著影响。

Conclusion: FinSearchComp通过与真实分析师任务对齐并提供端到端评估，为复杂金融搜索和推理提供了专业、高难度的测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [21] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO是一种单流策略优化方法，通过持久化KL自适应值跟踪器和全局优势归一化，解决了传统分组方法GRPO的退化组问题和同步障碍，在数学推理任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分组的策略梯度方法（如GRPO）存在退化组消除学习信号和同步障碍限制可扩展性的问题，需要一种更稳定高效的优化方法。

Method: 提出单流策略优化（SPO），使用持久化的KL自适应值跟踪器替代每组的基线，并在整个批次中全局归一化优势，实现无分组的高吞吐量优化。

Result: 在五个数学基准测试中，SPO比GRPO平均maj@32提高了3.4个百分点，在BRUMO 25上提升7.3pp，AIME 25上提升4.4pp，HMMT 25上提升3.3pp。

Conclusion: SPO通过基本原理而非架构变通来推动LLM推理进步，证明了简化复杂性可以带来更好的性能和效率。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [22] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 该论文提出了一种将重复推理片段转换为可重用"行为"的方法，通过LLM的元认知分析创建"行为手册"，在推理时提供相关行为指导，显著减少token使用并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: LLMs在解决多步问题时经常重复推导相同的中间步骤，导致token使用和延迟增加，上下文窗口饱和限制了探索能力。需要一种机制来避免重复推理，提高效率。

Method: 通过LLM的元认知分析将重复推理片段转换为简洁可重用的"行为"（名称+指令），存储在"行为手册"中，在推理时提供上下文指导或通过监督微调蒸馏到参数中。

Result: 1) 行为条件推理：减少推理token达46%，同时保持或提高准确率；2) 行为引导自改进：无参数更新下准确率提高10%；3) 行为条件SFT：比普通SFT更有效地将非推理模型转换为推理模型。

Conclusion: 将缓慢推导转换为快速程序提示使LLMs能够记住如何推理，而不仅仅记住结论，显著提高了推理效率和效果。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math是一个面向越南高中数学考试的自主代理框架，集成三个AI代理：题目生成器、解题解释器和个性化导师，支持学生自主练习和教师生成标准化试题。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生准备国家高中数学毕业考试，减轻教师手动出题负担，提供标准化且多样化的练习材料。

Method: 开发包含三个专门AI代理的框架：基于规范矩阵的题目生成器、提供详细步骤推理的解题器/解释器、以及根据学生表现自适应的个性化导师。

Result: 初步评估显示V-Math能生成符合矩阵标准的考试题目，具有高解题准确率，提供连贯的解释，并丰富了练习材料的多样性。

Conclusion: V-Math有潜力支持符合国家标准的可扩展、公平的数学备考，同时通过AI辅助的考试创建赋能教师。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [24] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant是一个开源的人类-AI协作框架，用于简化科学工作流的端到端创建，在机器学习领域的综述和视角论文写作中表现出色，提高了起草效率和主题一致性，但仍需要人类监督来确保准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助研究工具虽然强大但分散，缺乏以人为中心的工作流程，需要开发一个集成化的协作框架来简化科学工作流的创建过程。

Method: 开发AIssistant框架，集成模块化工具和智能体进行文献综合、分段实验、引用管理和自动LaTeX论文生成，同时在每个阶段保持人类监督。采用三层评估：独立人类评审、自动化LLM评审和程序主席监督。

Result: AIssistant提高了起草效率和主题一致性，但存在引用幻觉、难以适应动态论文结构以及多模态内容整合不完全等局限性。人类-AI协作对于保持事实正确性、方法合理性和伦理合规性仍然至关重要。

Conclusion: AIssistant是一个有前景的人类-AI协作框架，能够有效支持科学工作流的创建，但需要进一步解决引用准确性和结构适应性等挑战。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [25] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 提出一种分解方法，先进行结构化交互摘要，再用微调模型进行意图提取，提升资源受限模型在UI交互轨迹中的意图理解能力


<details>
  <summary>Details</summary>
Motivation: 解决小型设备端模型在UI交互意图理解方面的困难，提供隐私保护、低成本和低延迟的用户体验

Method: 采用两阶段方法：1) 结构化交互摘要捕获每个用户动作的关键信息；2) 使用微调模型对聚合摘要进行意图提取

Result: 该方法提高了资源受限模型的意图理解能力，甚至超越大型多模态语言模型的基准性能

Conclusion: 分解方法能有效提升小型模型在UI交互意图理解方面的表现，为设备端智能代理发展提供可行方案

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [26] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 提出Reasoning-Aware Compression (RAC)方法，通过在剪枝过程中同时重构输入和思维链激活，显著提升推理语言模型的压缩性能


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在推理时产生长思维链导致部署成本高，传统剪枝方法在推理任务上性能损失较大且可能使模型更慢

Method: 在剪枝过程中联合重构输入和模型策略内思维链的激活，与现有剪枝工作流（如SparseGPT）无缝集成

Result: RAC方法显著提升了现有剪枝方法的性能，解决了传统方法在推理任务上的性能损失问题

Conclusion: RAC是一种简单有效的drop-in修复方案，能够显著改善推理语言模型的压缩效果

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [27] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 论文介绍了Minerva CQ，一种基于Agentic AI的实时客服助手系统，通过主动工作流和持续上下文构建，显著提升了客服效率和客户体验。


<details>
  <summary>Details</summary>
Motivation: 传统客服系统存在处理时间长、首次解决率低、客户满意度差的问题，现有AI辅助工具多为被动响应，缺乏深度上下文推理能力。

Method: 采用Agentic AI方法，结合实时转录、意图情感检测、实体识别、上下文检索、动态客户画像和部分对话摘要，实现主动工作流触发和动态适应。

Result: 在真实生产环境中部署的Minerva CQ作为AI副驾驶，在多轮部署中实现了客服效率和客户体验的可衡量改进。

Conclusion: Agentic AI驱动的实时客服助手系统能够有效解决传统客服痛点，通过主动智能辅助提升整体服务质量。

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [28] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: GBV-SQL是一个多代理框架，通过SQL2Text回译验证来解决Text2SQL中的语义鸿沟问题，同时揭示了基准测试中普遍存在的黄金错误问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在Text2SQL生成方面取得显著进展，但语法有效的查询经常误解用户意图，存在关键语义鸿沟。此外，当前评估受到基准测试质量问题的严重影响

Method: 提出GBV-SQL多代理框架，引入带SQL2Text回译验证的引导生成机制。使用专门代理将生成的SQL翻译回自然语言，验证其与原始问题的逻辑一致性。还建立了黄金错误的正式分类法

Result: 在BIRD基准上达到63.23%的执行准确率，绝对提升5.8%。在去除有缺陷样本后，在Spider基准上达到96.5%（开发集）和97.6%（测试集）的执行准确率

Conclusion: 该工作提供了语义验证的鲁棒框架，并对基准测试完整性提出了批判性视角，强调了更严格数据集策展的必要性

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [29] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 提出了分层记忆架构H²R，通过将高级规划记忆与低级执行记忆解耦，实现细粒度知识迁移，提高LLM智能体在多任务场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将先验经验和知识视为单一整体，导致知识迁移效率低下且粒度粗，需要更细粒度的知识迁移机制。

Method: 提出分层记忆架构，包含高级规划记忆和低级执行记忆；引入分层后见反思机制(H²R)，从过往交互中提炼可重用的分层知识；测试时分别检索高低级记忆。

Result: 在两个基准测试中，H²R能够提高泛化能力和决策性能，优于Expel等基线方法。

Conclusion: 分层记忆架构和H²R机制能够有效实现细粒度知识迁移，提升LLM智能体在多任务场景中的表现。

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [30] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: Planning Copilot是一个集成多种规划工具的聊天机器人，通过自然语言指令调用工具，显著提升LLMs在长时程规划任务中的表现，甚至超越大型商业模型GPT-5。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主执行复杂任务时缺乏可靠的长期规划能力，需要外部工具支持来弥补这一缺陷。

Method: 基于Model Context Protocol (MCP)标准，集成多种规划工具（语法检查、规划器选择、计划验证、执行模拟等），允许任何支持MCP的LLM无需领域特定微调即可使用。

Result: 实验表明Planning Copilot在使用三个开源LLM时显著优于无工具支持的相同模型，在有限定性比较中甚至显著超越GPT-5。

Conclusion: 专用规划工具是使LLMs有效执行规划任务的有效途径，MCP标准为实现工具集成提供了可行方案。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [31] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 本文提出了一个用于金融犯罪合规的智能AI代理系统，通过行动设计研究方法开发，强调可解释性、可追溯性和合规性设计，实现了自动化工作流程。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规成本不断上升但效果有限，现有AI解决方案不透明且难以满足监管要求，需要开发更有效的合规系统。

Method: 采用行动设计研究(ADR)方法，与金融科技公司和监管机构合作，使用基于工件的建模，为自主代理分配明确角色，实现任务特定模型路由和审计日志。

Result: 开发了参考架构和实际原型，展示了智能AI如何重构金融犯罪合规工作流程，支持透明度和机构信任。

Conclusion: 当自动化嵌入到负责任的治理结构中时，可以在高风险监管环境中支持透明度和机构信任，扩展了信息系统文献中关于AI支持合规的研究。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [32] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: RepIt是一个简单高效的概念向量提取框架，能够通过少量数据在LLM中隔离特定概念表示，实现精准的行为干预，同时避免对模型其他功能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法往往产生比预期更广泛的影响，需要更纯净的概念向量来实现针对性干预，从而更精细地理解LLM行为。

Method: 提出RepIt框架，通过数据高效的方式提取概念特定表示，能够在少量示例（仅需十几个）和有限计算资源（单张A6000）下实现精准干预。

Result: 在五个前沿LLM上验证，RepIt能够选择性抑制目标概念的拒绝行为，同时保持其他安全基准得分；纠正信号仅定位到100-200个神经元。

Conclusion: RepIt展示了针对性干预可以抵消过度泛化问题，为更精细的模型行为控制奠定了基础，但也引发了关于使用有限计算和数据就能操纵模型的担忧。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [33] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition是一个新的计算认知建模范式，通过记忆感官、语言、概念、情景和程序知识的星座模式，让智能代理能够像人类一样处理现实生活的复杂性。


<details>
  <summary>Details</summary>
Motivation: 为了解决智能代理在复杂现实环境中高效认知处理的问题，通过模拟人类认知模式来降低认知负荷，同时处理异常情况。

Method: 基于形状的建模方法，包括特定目标、假设、建模策略、知识库和实际模型，在特定认知架构中实现。使用形状识别模式、习惯行动、类比推理和满意度原则。

Result: 提出了一个具体的LEIA（语言赋能的智能代理）建模框架，能够实现可解释、可扩展且值得信赖的代理系统。

Conclusion: 形状认知范式不仅适用于LEIA建模，其原则可以更广泛地应用于知识型和混合型AI，为这些领域注入新的活力。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [34] [Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML](https://arxiv.org/abs/2509.12395)
*Yash Mundhra,Max Valk,Maliheh Izadi*

Main category: cs.SE

TL;DR: LLM在工业专有代码库中的代码生成性能研究，提出了build@k评估指标，发现提示技术和模型大小对输出质量有显著影响


<details>
  <summary>Details</summary>
Motivation: 探索LLM在工业专有环境中的代码生成能力，解决领域特定约束和代码依赖性问题

Method: 开发针对ASML专有代码库的评估框架和新基准，提出build@k评估指标，比较不同提示技术、通用与代码专用LLM的性能

Result: 提示技术和模型大小对输出质量影响显著，few-shot和chain-of-thought提示获得最高构建成功率，代码专用与通用LLM性能差异不明显

Conclusion: LLM在工业专有环境中具有应用潜力，但需要针对性的评估框架和提示技术

Abstract: Large language models have shown impressive performance in various domains,
including code generation across diverse open-source domains. However, their
applicability in proprietary industrial settings, where domain-specific
constraints and code interdependencies are prevalent, remains largely
unexplored. We present a case study conducted in collaboration with the
leveling department at ASML to investigate the performance of LLMs in
generating functional, maintainable code within a closed, highly specialized
software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase
and introduced a new benchmark. Additionally, we proposed a new evaluation
metric, build@k, to assess whether LLM-generated code successfully compiles and
integrates within real industrial repositories. We investigate various
prompting techniques, compare the performance of generic and code-specific
LLMs, and examine the impact of model size on code generation capabilities,
using both match-based and execution-based metrics. The findings reveal that
prompting techniques and model size have a significant impact on output
quality, with few-shot and chain-of-thought prompting yielding the highest
build success rates. The difference in performance between the code-specific
LLMs and generic LLMs was less pronounced and varied substantially across
different model families.

</details>


### [35] [From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出了一个基于LLM代理的AI工作流，用于将传统Fortran代码自动转换为性能可移植的Kokkos C++程序，实现了跨硬件平台的代码现代化。


<details>
  <summary>Details</summary>
Motivation: 随着HPC向GPU加速架构转变，传统Fortran代码缺乏原生GPU绑定，需要现代化改造以实现性能可移植性。手动移植工作量大且需要专业知识，因此探索AI驱动的自动化解决方案。

Method: 使用专门的LLM代理协作工作流，包括翻译、验证、编译、运行、测试、调试和优化等多个阶段，将Fortran内核转换为可移植的Kokkos C++程序。

Result: 工作流成功现代化了多个基准内核，生成的Kokkos代码在不同硬件分区上实现性能可移植。付费OpenAI模型（如GPT-5）仅需几美元就能生成优化代码并超越Fortran基线，而开源模型经常失败。

Conclusion: 证明了基于代理的AI在Fortran到Kokkos转换中的可行性，为自主现代化传统科学应用提供了途径，展示了LLM驱动系统在科学计算领域的结构化推理潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases
originally developed for homogeneous, CPU-based systems. As High-Performance
Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many
accelerators lack native Fortran bindings, creating an urgent need to modernize
legacy codes for portability. Frameworks like Kokkos provide performance
portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos
porting demands significant expertise and time. Large language models (LLMs)
have shown promise in source-to-source code generation, yet their use in fully
autonomous workflows for translating and optimizing parallel code remains
largely unexplored, especially for performance portability across diverse
hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents"
collaborate to translate, validate, compile, run, test, debug, and optimize
Fortran kernels into portable Kokkos C++ programs. Results show the pipeline
modernizes a range of benchmark kernels, producing performance-portable Kokkos
codes across hardware partitions. Paid OpenAI models such as GPT-5 and
o4-mini-high executed the workflow for only a few U.S. dollars, generating
optimized codes that surpassed Fortran baselines, whereas open-source models
like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos
transformation and offers a pathway for autonomously modernizing legacy
scientific applications to run portably and efficiently on diverse
supercomputers. It further highlights the potential of LLM-driven agentic
systems to perform structured, domain-specific reasoning tasks in scientific
and systems-oriented applications.

</details>


### [36] [Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding](https://arxiv.org/abs/2509.12491)
*Veronica Pimenova,Sarah Fakhoury,Christian Bird,Margaret-Anne Storey,Madeline Endres*

Main category: cs.SE

TL;DR: 本文首次对vibe coding进行了系统性定性研究，通过分析访谈和社交媒体内容，揭示了这种AI辅助编程范式的本质、使用动机、实践方式、痛点及最佳实践。


<details>
  <summary>Details</summary>
Motivation: vibe coding作为一种新兴的自然语言编程范式，虽然已有初步研究，但缺乏对开发者实际体验和认知的实证理解，需要系统性的定性调查。

Method: 采用定性研究方法，分析超过19万字的半结构化访谈、Reddit讨论和LinkedIn帖子内容，构建基于实证的vibe coding理论框架。

Result: 提出了以对话式AI交互、共同创造和开发者心流体验为核心的vibe coding理论；发现AI信任在委托到共同创造的连续体中起调节作用；识别了规范、可靠性、调试等方面的痛点和风险。

Conclusion: 研究为AI开发工具的未来发展提供了启示，并为vibe coding的进一步研究指明了方向，强调了开发者体验和心流维持的重要性。

Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly
become a compelling and controversial natural language programming paradigm in
AI-assisted software development. Centered on iterative co-design with an AI
assistant, vibe coding emphasizes flow and experimentation over strict upfront
specification. While initial studies have begun to explore this paradigm, most
focus on analyzing code artifacts or proposing theories with limited empirical
backing. There remains a need for a grounded understanding of vibe coding as it
is perceived and experienced by developers. We present the first systematic
qualitative investigation of vibe coding perceptions and practice. Drawing on
over 190,000 words from semi-structured interviews, Reddit threads, and
LinkedIn posts, we characterize what vibe coding is, why and how developers use
it, where it breaks down, and which emerging practices aim to support it. We
propose a qualitatively grounded theory of vibe coding centered on
conversational interaction with AI, co-creation, and developer flow and joy. We
find that AI trust regulates movement along a continuum from delegation to
co-creation and supports the developer experience by sustaining flow. We
surface recurring pain points and risks in areas including specification,
reliability, debugging, latency, code review burden, and collaboration. We also
present best practices that have been discovered and shared to mitigate these
challenges. We conclude with implications for the future of AI dev tools and
directions for researchers investigating vibe coding.

</details>


### [37] [Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design](https://arxiv.org/abs/2509.12973)
*Aamer Aljagthami,Mohammed Banabila,Musab Alshehri,Mohammed Kabini,Mohammad D. Alahmadi*

Main category: cs.SE

TL;DR: 本文系统评估了大型语言模型在C++、Java、Python和C#之间的代码翻译性能，发现详细提示和英语提示能显著提升翻译质量，所有LLM均优于传统TransCoder基线。


<details>
  <summary>Details</summary>
Motivation: 评估不同LLM在代码翻译中的表现，研究提示设计和提示语言对翻译质量的影响，为软件现代化和跨语言互操作性提供实践指导。

Method: 使用BLEU和CodeBLEU指标，在C++、Java、Python、C#四种语言间进行双向翻译评估，比较两种提示风格（简洁指令和详细规范）和两种提示语言（英语和阿拉伯语）。

Result: 详细提示在所有模型和翻译方向上均带来一致提升，英语提示比阿拉伯语提示性能高13-15%，最佳模型在Java到C#和Python到C++等挑战性语言对上获得最高CodeBLEU分数。

Conclusion: 精心设计的提示工程和提示语言选择对代码翻译质量至关重要，LLM在代码翻译任务上显著优于传统方法。

Abstract: Large language models (LLMs) have shown promise for automated source-code
translation, a capability critical to software migration, maintenance, and
interoperability. Yet comparative evidence on how model choice, prompt design,
and prompt language shape translation quality across multiple programming
languages remains limited. This study conducts a systematic empirical
assessment of state-of-the-art LLMs for code translation among C++, Java,
Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and
CodeBLEU, we quantify syntactic fidelity and structural correctness under two
prompt styles (concise instruction and detailed specification) and two prompt
languages (English and Arabic), with direction-aware evaluation across language
pairs. Experiments show that detailed prompts deliver consistent gains across
models and translation directions, and English prompts outperform Arabic by
13-15%. The top-performing model attains the highest CodeBLEU on challenging
pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM
outperforms TransCoder across the benchmark. These results demonstrate the
value of careful prompt engineering and prompt language choice, and provide
practical guidance for software modernization and cross-language
interoperability.

</details>


### [38] [Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models](https://arxiv.org/abs/2509.13023)
*Ştefan-Claudiu Susan,Andrei Arusoaie,Dorel Lucanu*

Main category: cs.SE

TL;DR: 提出了一种结合Slither检测器、LLMs、Kontrol和Forge的智能合约漏洞检测管道，能可靠检测缺陷并生成证明，有效减少误报和人工验证负担。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具和大型语言模型在Solidity智能合约漏洞检测中存在高误报率，需要能够正式或经验证明缺陷存在的方法。

Method: 集成定制Slither检测器、LLMs、Kontrol和Forge的检测管道，通过符号执行或具体执行来正确分类代码故障。

Result: 对七种关键缺陷类型进行了实验，展示了在重入、复杂回调和错误访问控制策略等漏洞检测方面的有效性。

Conclusion: 建立了一个将启发式分析与形式验证相结合的强大框架，实现了更可靠和自动化的智能合约审计。

Abstract: The high rate of false alarms from static analysis tools and Large Language
Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts,
demanding methods that can formally or empirically prove the presence of
defects. This paper introduces a novel detection pipeline that integrates
custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is
designed to reliably detect defects and generate proofs. We currently perform
experiments with promising results for seven types of critical defects. We
demonstrate the pipeline's efficacy by presenting our findings for three
vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control
Policies -- that are challenging for current verification solutions, which
often generate false alarms or fail to detect them entirely. We highlight the
potential of either symbolic or concrete execution in correctly classifying
such code faults. By chaining these instruments, our method effectively
validates true positives, significantly reducing the manual verification
burden. Although we identify potential limitations, such as the inconsistency
and the cost of LLMs, our findings establish a robust framework for combining
heuristic analysis with formal verification to achieve more reliable and
automated smart contract auditing.

</details>


### [39] [Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs](https://arxiv.org/abs/2509.13055)
*Youngkyoung Kim,Sanghyeok Park,Misoo Kim,Gangho Yoon,Eunseok Lee,Simon S. Woo*

Main category: cs.SE

TL;DR: 提出Progressive Knowledge Enhancement (PKE)框架，通过多阶段提示逐步提取LLM的潜在知识，显著提升半导体设备语言ALPG的代码生成准确率


<details>
  <summary>Details</summary>
Motivation: 半导体设备编程语言如ALPG具有低层语法和陡峭学习曲线，现有LLM在处理这类专业低级语言时效果有限，需要新的方法来提升代码生成能力

Method: 提出渐进式知识增强(PKE)框架，通过多阶段提示策略，从简单到复杂的示例逐步激活LLM的潜在知识，无需大量微调

Result: 在工业ALPG数据集上，PKE显著优于标准提示方法，比次优技术高出11.1%和15.2%的精确匹配分数，渐进式知识提取有效提升准确性

Conclusion: PKE为提升LLM在专业低级编程语言方面的能力提供了实用方法，支持半导体软件开发生产力的提升

Abstract: Semiconductors form the backbone of modern electronics, with their
manufacturing and testing relying on highly specialized equipment and
domain-specific programming languages. Equipment languages such as the
Algorithmic Pattern Generator (ALPG) are critical for precise hardware control
but are challenging to program due to their low-level syntax and steep learning
curve. While large language models (LLMs) have shown promise in generating
high-level code from natural language, their effectiveness on low-level
equipment languages remains limited. To address this, we propose Progressive
Knowledge Enhancement (PKE), a novel multi-stage prompting framework that
progressively extracts and activates the latent knowledge within LLMs, guiding
them from simple to complex examples without extensive fine-tuning. Empirical
evaluation on an industrial ALPG dataset shows that PKE significantly
outperforms standard prompting and surpasses state-of-the-art methods in
generating correct ALPG code, achieving 11.1\% and 15.2\% higher exact match
scores compared to the second-best technique. Further analysis of individual
components confirms that progressive knowledge extraction based on difficulty
enhances accuracy. Our study offer a practical approach to boosting LLM
capabilities for specialized low-level programming, supporting greater
productivity in semiconductor software development.

</details>
