{"id": "2509.10546", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10546", "abs": "https://arxiv.org/abs/2509.10546", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRisk-Concealment Attacks (RCA)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u9690\u85cf\u76d1\u7ba1\u98ce\u9669\uff0c\u6210\u529f\u7ed5\u8fc7\u4e3b\u6d41LLMs\u7684\u9632\u62a4\u673a\u5236\uff0c\u5728\u91d1\u878d\u9886\u57df\u66b4\u9732\u51fa\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7ea2\u961f\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6709\u5bb3\u5185\u5bb9\uff0c\u5ffd\u89c6\u4e86\u91d1\u878d\u9886\u57df\u7684\u76d1\u7ba1\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76LLMs\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faRCA\u591a\u8f6e\u653b\u51fb\u6846\u67b6\uff0c\u6784\u5efaFIN-Bench\u91d1\u878d\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf99\u4e2a\u4e3b\u6d41LLMs\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "RCA\u653b\u51fb\u5e73\u5747\u6210\u529f\u738793.18%\uff0cGPT-4.1\u8fbe98.28%\uff0cOpenAI o1\u8fbe97.56%\uff0c\u663e\u793a\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002", "conclusion": "\u91d1\u878d\u9886\u57dfLLMs\u5b58\u5728\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e9f\u9700\u66f4\u5f3a\u7684\u9886\u57df\u611f\u77e5\u5bf9\u9f50\u673a\u5236\u548c\u5ba1\u6838\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.10946", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10946", "abs": "https://arxiv.org/abs/2509.10946", "authors": ["Roberto Morabito", "Guanghan Wu"], "title": "When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning", "comment": "This paper has been accepted for publication in Computer (IEEE). Upon\n  publication, the copyright will be transferred to IEEE", "summary": "Large Language Models (LLMs) are increasingly used to automate software\ngeneration in embedded machine learning workflows, yet their outputs often fail\nsilently or behave unpredictably. This article presents an empirical\ninvestigation of failure modes in LLM-powered ML pipelines, based on an\nautopilot framework that orchestrates data preprocessing, model conversion, and\non-device inference code generation. We show how prompt format, model behavior,\nand structural assumptions influence both success rates and failure\ncharacteristics, often in ways that standard validation pipelines fail to\ndetect. Our analysis reveals a diverse set of error-prone behaviors, including\nformat-induced misinterpretations and runtime-disruptive code that compiles but\nbreaks downstream. We derive a taxonomy of failure categories and analyze\nerrors across multiple LLMs, highlighting common root causes and systemic\nfragilities. Though grounded in specific devices, our study reveals broader\nchallenges in LLM-based code generation. We conclude by discussing directions\nfor improving reliability and traceability in LLM-powered embedded ML systems.", "AI": {"tldr": "\u5bf9LLM\u9a71\u52a8\u7684\u5d4c\u5165\u5f0f\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u4e2d\u5931\u8d25\u6a21\u5f0f\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u63d0\u793a\u683c\u5f0f\u3001\u6a21\u578b\u884c\u4e3a\u548c\u7ed3\u6784\u5047\u8bbe\u5982\u4f55\u5f71\u54cd\u6210\u529f\u7387\u548c\u5931\u8d25\u7279\u5f81\uff0c\u6807\u51c6\u9a8c\u8bc1\u7ba1\u9053\u5f80\u5f80\u65e0\u6cd5\u68c0\u6d4b\u8fd9\u4e9b\u9519\u8bef\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5d4c\u5165\u5f0f\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u81ea\u52a8\u5316\u8f6f\u4ef6\u751f\u6210\u65f6\u7ecf\u5e38\u51fa\u73b0\u9759\u9ed8\u5931\u8d25\u6216\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u8fdb\u884c\u5b9e\u8bc1\u8c03\u67e5\uff0c\u5206\u6790\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u8f6c\u6362\u548c\u8bbe\u5907\u7aef\u63a8\u7406\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5efa\u7acb\u5931\u8d25\u5206\u7c7b\u6cd5\u5e76\u5206\u6790\u591a\u4e2aLLM\u7684\u9519\u8bef\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u79cd\u6613\u51fa\u9519\u884c\u4e3a\uff0c\u5305\u62ec\u683c\u5f0f\u5f15\u8d77\u7684\u8bef\u89e3\u548c\u7f16\u8bd1\u901a\u8fc7\u4f46\u7834\u574f\u4e0b\u6e38\u8fd0\u884c\u7684\u4ee3\u7801\uff0c\u63ed\u793a\u4e86\u5e38\u89c1\u6839\u672c\u539f\u56e0\u548c\u7cfb\u7edf\u8106\u5f31\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u57fa\u4e8e\u7279\u5b9a\u8bbe\u5907\uff0c\u4f46\u7814\u7a76\u63ed\u793a\u4e86LLM\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u6539\u8fdbLLM\u9a71\u52a8\u7684\u5d4c\u5165\u5f0fML\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2509.10704", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10704", "abs": "https://arxiv.org/abs/2509.10704", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Rajarishi Sinha", "Sercan \u00d6. Ar\u0131k"], "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration", "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables\n  including references and appendices)", "summary": "Text-to-image (T2I) models, while offering immense creative potential, are\nhighly reliant on human intervention, posing significant usability challenges\nthat often necessitate manual, iterative prompt engineering over often\nunderspecified prompts. This paper introduces Maestro, a novel self-evolving\nimage generation system that enables T2I models to autonomously self-improve\ngenerated images through iterative evolution of prompts, using only an initial\nprompt. Maestro incorporates two key innovations: 1) self-critique, where\nspecialized multimodal LLM (MLLM) agents act as 'critics' to identify\nweaknesses in generated images, correct for under-specification, and provide\ninterpretable edit signals, which are then integrated by a 'verifier' agent\nwhile preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge\nfor head-to-head comparisons between iteratively generated images, eschewing\nproblematic images, and evolving creative prompt candidates that align with\nuser intents. Extensive experiments on complex T2I tasks using black-box models\ndemonstrate that Maestro significantly improves image quality over initial\nprompts and state-of-the-art automated methods, with effectiveness scaling with\nmore advanced MLLM components. This work presents a robust, interpretable, and\neffective pathway towards self-improving T2I generation.", "AI": {"tldr": "Maestro\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u4ee3\u7406\u5b9e\u73b0\u81ea\u6211\u6279\u5224\u548c\u81ea\u6f14\u5316\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u548c\u8fed\u4ee3\u63d0\u793a\u5de5\u7a0b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u4e3b\u7684\u56fe\u50cf\u8d28\u91cf\u63d0\u5347", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4ee3\u7406\u8fdb\u884c\u81ea\u6211\u6279\u5224\uff08\u8bc6\u522b\u56fe\u50cf\u5f31\u70b9\u3001\u7ea0\u6b63\u4e0d\u660e\u786e\u6027\uff09\u548c\u81ea\u6211\u6f14\u5316\uff08\u56fe\u50cf\u6bd4\u8f83\u3001\u6dd8\u6c70\u95ee\u9898\u56fe\u50cf\u3001\u6f14\u5316\u63d0\u793a\u8bcd\uff09", "result": "\u5728\u590d\u6742\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u521d\u59cb\u63d0\u793a\u548c\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u6548\u679c\u968fMLLM\u7ec4\u4ef6\u5148\u8fdb\u6027\u800c\u63d0\u5347", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u81ea\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8def\u5f84", "topic": "agent analysis"}}
{"id": "2509.11132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11132", "abs": "https://arxiv.org/abs/2509.11132", "authors": ["Xiaoyu Zhang", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Chenhao Lin", "Chao Shen", "Tianlin Li", "Yang Liu"], "title": "Rethinking Technology Stack Selection with AI Coding Proficiency", "comment": "23 pages", "summary": "Large language models (LLMs) are now an integral part of software development\nworkflows and are reshaping the whole process. Traditional technology stack\nselection has not caught up. Most of the existing selection methods focus\nsolely on the inherent attributes of the technology, overlooking whether the\nLLM can effectively leverage the chosen technology. For example, when\ngenerating code snippets using popular libraries like Selenium (one of the most\nwidely used test automation tools with over 33k GitHub stars), existing LLMs\nfrequently generate low-quality code snippets (e.g., using deprecated APIs and\nmethods, or containing syntax errors). As such, teams using LLM assistants risk\nchoosing technologies that cannot be used effectively by LLMs, yielding high\ndebugging effort and mounting technical debt. We foresee a practical question\nin the LLM era, is a technology ready for AI-assisted development? In this\npaper, we first propose the concept, AI coding proficiency, the degree to which\nLLMs can utilize a given technology to generate high-quality code snippets. We\nconduct the first comprehensive empirical study examining AI proficiency across\n170 third-party libraries and 61 task scenarios, evaluating six widely used\nLLMs. Our findings reveal that libraries with similar functionalities can\nexhibit up to 84% differences in the quality score of LLM-generated code, while\ndifferent models also exhibit quality gaps among their generation results using\nthe same library. These gaps translate into real engineering costs and can\nsteer developer choices toward a narrow set of libraries with high AI coding\nproficiency, threatening technological diversity in the ecosystem. We call on\nthe community to integrate AI proficiency assessments into technology selection\nframeworks and develop mitigation strategies, preserving competitive balance in\nAI-driven development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAI\u7f16\u7a0b\u719f\u7ec3\u5ea6\u6982\u5ff5\uff0c\u8bc4\u4f30LLMs\u5229\u7528\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u5e93\u4e4b\u95f4\u5b58\u572884%\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u547c\u5401\u5c06AI\u719f\u7ec3\u5ea6\u8bc4\u4f30\u7eb3\u5165\u6280\u672f\u9009\u62e9\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u9009\u62e9\u65b9\u6cd5\u53ea\u5173\u6ce8\u6280\u672f\u56fa\u6709\u5c5e\u6027\uff0c\u5ffd\u89c6\u4e86LLMs\u662f\u5426\u80fd\u6709\u6548\u5229\u7528\u6240\u9009\u6280\u672f\uff0c\u5bfc\u81f4\u751f\u6210\u4f4e\u8d28\u91cf\u4ee3\u7801\u7247\u6bb5\u548c\u589e\u52a0\u8c03\u8bd5\u6210\u672c\u3002", "method": "\u5bf9170\u4e2a\u7b2c\u4e09\u65b9\u5e93\u548c61\u4e2a\u4efb\u52a1\u573a\u666f\u8fdb\u884c\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f306\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u7684AI\u7f16\u7a0b\u719f\u7ec3\u5ea6\u3002", "result": "\u529f\u80fd\u76f8\u4f3c\u7684\u5e93\u5728LLM\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u5f97\u5206\u4e0a\u5b58\u5728\u9ad8\u8fbe84%\u7684\u5dee\u5f02\uff0c\u4e0d\u540c\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u5e93\u65f6\u4e5f\u8868\u73b0\u51fa\u8d28\u91cf\u5dee\u8ddd\u3002", "conclusion": "AI\u719f\u7ec3\u5ea6\u5dee\u8ddd\u4f1a\u5bfc\u81f4\u771f\u5b9e\u5de5\u7a0b\u6210\u672c\uff0c\u53ef\u80fd\u4f7f\u5f00\u53d1\u8005\u9009\u62e9\u8303\u56f4\u53d8\u7a84\uff0c\u5a01\u80c1\u6280\u672f\u751f\u6001\u591a\u6837\u6027\uff0c\u9700\u8981\u5f00\u53d1\u7f13\u89e3\u7b56\u7565\u3002", "topic": "swe application"}}
{"id": "2509.11238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11238", "abs": "https://arxiv.org/abs/2509.11238", "authors": ["Dongming Jin", "Zhi Jin", "Yiran Zhang", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Xiaohong Chen", "Weisong Sun"], "title": "UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories", "comment": "21page, 5 figures", "summary": "Software maintainability critically depends on high-quality requirements\ndescriptions and explicit traceability between requirements and code. Although\nautomated code summarization (ACS) and requirements traceability (RT)\ntechniques have been widely studied, existing ACS methods mainly generate\nimplementation-level (i.e., developer-oriented) requirements (IRs) for\nfine-grained units (e.g., methods), while RT techniques often overlook the\nimpact of project evolution. As a result, user-level (i.e., end user-oriented)\nrequirements (URs) and live trace links remain underexplored, despite their\nimportance for supporting user understanding and for validating whether\nAI-generated software aligns with user intent. To address this gap, we propose\nUserTrace, a multi-agent system that automatically generates URs and recovers\nlive trace links (from URs to IRs to code) from software repositories.\nUserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,\nWriter, and Verifier) through a three-phase process: structuring repository\ndependencies, deriving IRs for code units, and synthesizing URs with\ndomain-specific context. Our comparative evaluation shows that UserTrace\nproduces URs with higher completeness, correctness, and helpfulness than an\nestablished baseline, and achieves superior precision in trace link recovery\ncompared to five state-of-the-art RT approaches. A user study further\ndemonstrates that UserTrace helps end users validate whether the AI-generated\nrepositories align with their intent.", "AI": {"tldr": "UserTrace\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u81ea\u52a8\u4ece\u8f6f\u4ef6\u4ed3\u5e93\u751f\u6210\u7528\u6237\u7ea7\u9700\u6c42(URs)\u5e76\u6062\u590d\u5b9e\u65f6\u7684\u9700\u6c42\u8ffd\u8e2a\u94fe\u63a5(\u4eceURs\u5230\u5b9e\u73b0\u7ea7\u9700\u6c42\u518d\u5230\u4ee3\u7801)\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7801\u6458\u8981\u548c\u9700\u6c42\u8ffd\u8e2a\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u4ee3\u7801\u6458\u8981(ACS)\u6280\u672f\u4e3b\u8981\u751f\u6210\u9762\u5411\u5f00\u53d1\u8005\u7684\u5b9e\u73b0\u7ea7\u9700\u6c42(IRs)\uff0c\u800c\u9700\u6c42\u8ffd\u8e2a(RT)\u6280\u672f\u5f80\u5f80\u5ffd\u89c6\u9879\u76ee\u6f14\u8fdb\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7528\u6237\u7ea7\u9700\u6c42(URs)\u548c\u5b9e\u65f6\u8ffd\u8e2a\u94fe\u63a5\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u8fd9\u4e9b\u5bf9\u652f\u6301\u7528\u6237\u7406\u89e3\u548c\u9a8c\u8bc1AI\u751f\u6210\u8f6f\u4ef6\u662f\u5426\u7b26\u5408\u7528\u6237\u610f\u56fe\u81f3\u5173\u91cd\u8981\u3002", "method": "UserTrace\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u534f\u8c03\u56db\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53(\u4ee3\u7801\u5ba1\u67e5\u5458\u3001\u641c\u7d22\u5668\u3001\u7f16\u5199\u5668\u3001\u9a8c\u8bc1\u5668)\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u7ed3\u6784\u5316\u4ed3\u5e93\u4f9d\u8d56\u3001\u4e3a\u4ee3\u7801\u5355\u5143\u63a8\u5bfcIRs\u3001\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u4e0a\u4e0b\u6587\u5408\u6210URs\u3002", "result": "\u6bd4\u8f83\u8bc4\u4f30\u663e\u793a\uff0cUserTrace\u751f\u6210\u7684URs\u5728\u5b8c\u6574\u6027\u3001\u6b63\u786e\u6027\u548c\u6709\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u8ffd\u8e2a\u94fe\u63a5\u6062\u590d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u4e94\u79cd\u6700\u5148\u8fdb\u7684RT\u65b9\u6cd5\u3002\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660eUserTrace\u80fd\u5e2e\u52a9\u7ec8\u7aef\u7528\u6237\u9a8c\u8bc1AI\u751f\u6210\u4ed3\u5e93\u662f\u5426\u7b26\u5408\u5176\u610f\u56fe\u3002", "conclusion": "UserTrace\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u7ea7\u9700\u6c42\u751f\u6210\u548c\u5b9e\u65f6\u8ffd\u8e2a\u94fe\u63a5\u6062\u590d\u7684\u95ee\u9898\uff0c\u4e3a\u652f\u6301\u7528\u6237\u7406\u89e3\u548c\u9a8c\u8bc1AI\u751f\u6210\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2509.10769", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10769", "abs": "https://arxiv.org/abs/2509.10769", "authors": ["Tara Bogavelli", "Roshnee Sharma", "Hari Subramani"], "title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "comment": null, "summary": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f01\u4e1a\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e8618\u79cd\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u53d1\u73b0\u6a21\u578b\u7279\u5b9a\u7684\u67b6\u6784\u504f\u597d\u6311\u6218\u4e86\u901a\u7528\u5316\u8303\u5f0f\uff0c\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u4efb\u52a1\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u5f53\u524d\u5bf9\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5b64\u7acb\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e0d\u540c\u8bbe\u8ba1\u7ef4\u5ea6\u5982\u4f55\u4ea4\u4e92\u7684\u5b9e\u8bc1\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u4f01\u4e1a\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8618\u79cd\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u8003\u5bdf\u4e86\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u7f16\u6392\u7b56\u7565\u3001\u667a\u80fd\u4f53\u63d0\u793a\u5b9e\u73b0\uff08ReAct vs \u51fd\u6570\u8c03\u7528\uff09\u3001\u5185\u5b58\u67b6\u6784\u548c\u601d\u7ef4\u5de5\u5177\u96c6\u6210\u3002", "result": "\u53d1\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u7279\u5b9a\u67b6\u6784\u504f\u597d\uff0c\u6311\u6218\u4e86\u901a\u7528\u5316\u8303\u5f0f\uff1b\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u590d\u6742\u4efb\u52a1\u6700\u9ad8\u6210\u529f\u7387\u4ec535.3%\uff0c\u8f83\u7b80\u5355\u4efb\u52a1\u4e3a70.8%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e2e\u52a9\u5728\u67b6\u6784\u7ec4\u4ef6\u548c\u6a21\u578b\u9009\u62e9\u65b9\u9762\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "topic": "agent analysis"}}
{"id": "2509.11252", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11252", "abs": "https://arxiv.org/abs/2509.11252", "authors": ["Chengze li", "Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "comment": null, "summary": "LLMs have become the mainstream approaches to code generation. Existing LLMs\nmainly employ autoregressive generation, i.e. generating code token-by-token\nfrom left to right. However, the underlying autoregressive generation has two\nlimitations in code generation. First, autoregressive LLMs only generate a\ntoken at each step, showing low efficiency in practice. Second, programming is\na non-sequential process involving back-and-forth editing, while autoregressive\nLLMs only employ the left-to-right generation order. These two intrinsic\nlimitations hinder the further development of LLMs in code generation.\nRecently, diffusion LLMs have emerged as a promising alternative. Diffusion\nLLMs address the above limitations with two advances, including multi-token\nprediction (i.e. generating multiple tokens at each step) and flexible\ngeneration order (i.e. flexibly determining which positions to generate\ntokens). However, there is no systematic study exploring diffusion LLMs in code\ngeneration. To bridge the knowledge gap, we present the first empirical study\nof diffusion LLMs for code generation. Our study involves 9 representative\ndiffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on\nthe results, we summarize the following findings. (1) Existing diffusion LLMs\nare competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs\nhave a stronger length extrapolation ability than autoregressive LLMs and\nperform better in long code understanding. (3) We explore factors impacting the\neffectiveness and efficiency of diffusion LLMs, and provide practical guidance.\n(4) We discuss several promising further directions to improve diffusion LLMs\non code generation. We open-source all source code, data, and results to\nfacilitate the following research. The code is publicly available at\nhttps://github.com/zhangyitonggg/dllm4code.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u6269\u6563LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6269\u6563LLM\u5728\u6027\u80fd\u4e0a\u4e0e\u81ea\u56de\u5f52LLM\u76f8\u5f53\uff0c\u5728\u957f\u4ee3\u7801\u7406\u89e3\u548c\u957f\u5ea6\u5916\u63a8\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u81ea\u56de\u5f52LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u751f\u6210\u987a\u5e8f\u53d7\u9650\u7684\u95ee\u9898\uff0c\u6269\u6563LLM\u901a\u8fc7\u591a\u4ee4\u724c\u9884\u6d4b\u548c\u7075\u6d3b\u751f\u6210\u987a\u5e8f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5bf99\u4e2a\u4ee3\u8868\u6027\u6269\u6563LLM\u57284\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u5176\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u6269\u6563LLM\u4e0e\u76f8\u4f3c\u89c4\u6a21\u7684\u81ea\u56de\u5f52LLM\u7ade\u4e89\u529b\u76f8\u5f53\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u957f\u5ea6\u5916\u63a8\u80fd\u529b\u548c\u957f\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u4e86\u5f71\u54cd\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u6269\u6563LLM\u662f\u4ee3\u7801\u751f\u6210\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u7814\u7a76\u4e3a\u5176\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u548c\u53d1\u5c55\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2509.10818", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10818", "abs": "https://arxiv.org/abs/2509.10818", "authors": ["Boris Kovalerchuk", "Brent D. Fegley"], "title": "LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering", "comment": "25 pages,4 figures, 2 tables", "summary": "Difficult decision-making problems abound in various disciplines and domains.\nThe proliferation of generative techniques, especially large language models\n(LLMs), has excited interest in using them for decision support. However, LLMs\ncannot yet resolve missingness in their training data, leading to\nhallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by\nincorporating external information retrieval, reducing hallucinations and\nimproving accuracy. Yet, RAG and related methods are only partial solutions, as\nthey may lack access to all necessary sources or key missing information. Even\neveryday issues often challenge LLMs' abilities. Submitting longer prompts with\ncontext and examples is one approach to address knowledge gaps, but designing\neffective prompts is non-trivial and may not capture complex mental models of\ndomain experts. For tasks with missing critical information, LLMs are\ninsufficient, as are many existing systems poorly represented in available\ndocuments. This paper explores how LLMs can make decision-making more\nefficient, using a running example of evaluating whether to respond to a call\nfor proposals. We propose a technology based on optimized human-machine\ndialogue and monotone Boolean and k-valued functions to discover a\ncomputationally tractable personal expert mental model (EMM) of\ndecision-making. Our EMM algorithm for LLM prompt engineering has four steps:\n(1) factor identification, (2) hierarchical structuring of factors, (3)\ngenerating a generalized expert mental model specification, and (4) generating\na detailed generalized expert mental model from that specification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14/k\u503c\u51fd\u6570\u7684EMM\u7b97\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u53ef\u8ba1\u7b97\u5904\u7406\u7684\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "LLM\u5728\u5904\u7406\u51b3\u7b56\u95ee\u9898\u65f6\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u7f3a\u5931\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0cRAG\u7b49\u65b9\u6cd5\u53ea\u80fd\u90e8\u5206\u89e3\u51b3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6355\u83b7\u4e13\u5bb6\u590d\u6742\u7684\u5fc3\u667a\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u56db\u6b65EMM\u7b97\u6cd5\uff1a\u56e0\u5b50\u8bc6\u522b\u3001\u56e0\u5b50\u5c42\u6b21\u7ed3\u6784\u5316\u3001\u751f\u6210\u5e7f\u4e49\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\u89c4\u8303\u3001\u4ece\u89c4\u8303\u751f\u6210\u8be6\u7ec6\u6a21\u578b\uff0c\u57fa\u4e8e\u4f18\u5316\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14/k\u503c\u51fd\u6570\u3002", "result": "\u5f00\u53d1\u4e86\u53ef\u8ba1\u7b97\u5904\u7406\u7684\u4e2a\u4eba\u4e13\u5bb6\u51b3\u7b56\u5fc3\u667a\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u652f\u6301LLM\u8fdb\u884c\u51b3\u7b56\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u7684\u590d\u6742\u51b3\u7b56\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.10875", "categories": ["cs.AI", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2509.10875", "abs": "https://arxiv.org/abs/2509.10875", "authors": ["Jesse Gardner", "Vladimir A. Baulin"], "title": "Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?", "comment": null, "summary": "The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)\nresearch, guiding development from foundational theories to contemporary\napplications like Large Language Model (LLM)-based systems. This paper\ncritically re-evaluates the necessity and optimality of this agent-centric\nparadigm. We argue that its persistent conceptual ambiguities and inherent\nanthropocentric biases may represent a limiting framework. We distinguish\nbetween agentic systems (AI inspired by agency, often semi-autonomous, e.g.,\nLLM-based agents), agential systems (fully autonomous, self-producing systems,\ncurrently only biological), and non-agentic systems (tools without the\nimpression of agency). Our analysis, based on a systematic review of relevant\nliterature, deconstructs the agent paradigm across various AI frameworks,\nhighlighting challenges in defining and measuring properties like autonomy and\ngoal-directedness. We argue that the 'agentic' framing of many AI systems,\nwhile heuristically useful, can be misleading and may obscure the underlying\ncomputational mechanisms, particularly in Large Language Models (LLMs). As an\nalternative, we propose a shift in focus towards frameworks grounded in\nsystem-level dynamics, world modeling, and material intelligence. We conclude\nthat investigating non-agentic and systemic frameworks, inspired by complex\nsystems, biology, and unconventional computing, is essential for advancing\ntowards robust, scalable, and potentially non-anthropomorphic forms of general\nintelligence. This requires not only new architectures but also a fundamental\nreconsideration of our understanding of intelligence itself, moving beyond the\nagent metaphor.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u91cd\u65b0\u8bc4\u4f30\u4e86AI\u7814\u7a76\u4e2d\u4ee5\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u8ba4\u4e3a\u5176\u6982\u5ff5\u6a21\u7cca\u6027\u548c\u4eba\u7c7b\u4e2d\u5fc3\u504f\u89c1\u53ef\u80fd\u9650\u5236\u4e86AI\u53d1\u5c55\uff0c\u63d0\u51fa\u5e94\u8f6c\u5411\u57fa\u4e8e\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u4e16\u754c\u5efa\u6a21\u548c\u7269\u8d28\u667a\u80fd\u7684\u6846\u67b6\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6AI\u7814\u7a76\u4e2d\u957f\u671f\u5b58\u5728\u7684\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u63a2\u8ba8\u5176\u6982\u5ff5\u5c40\u9650\u6027\u548c\u4eba\u7c7b\u4e2d\u5fc3\u504f\u89c1\uff0c\u5bfb\u6c42\u66f4\u4f18\u7684AI\u53d1\u5c55\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u5404\u79cdAI\u6846\u67b6\u4e2d\u7684\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u89e3\u6784\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u5bfc\u5411\u6027\u7b49\u5c5e\u6027\u7684\u5b9a\u4e49\u548c\u6d4b\u91cf\u6311\u6218\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u6846\u67b6\u867d\u7136\u542f\u53d1\u5f0f\u6709\u7528\u4f46\u53ef\u80fd\u8bef\u5bfc\uff0c\u7279\u522b\u662f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63a9\u76d6\u4e86\u5e95\u5c42\u8ba1\u7b97\u673a\u5236\uff0c\u9700\u8981\u975e\u667a\u80fd\u4f53\u548c\u7cfb\u7edf\u6027\u6846\u67b6\u3002", "conclusion": "\u5e94\u7814\u7a76\u53d7\u590d\u6742\u7cfb\u7edf\u3001\u751f\u7269\u5b66\u548c\u975e\u4f20\u7edf\u8ba1\u7b97\u542f\u53d1\u7684\u975e\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8fd9\u9700\u8981\u65b0\u67b6\u6784\u548c\u5bf9\u667a\u80fd\u672c\u8d28\u7684\u6839\u672c\u91cd\u65b0\u601d\u8003\u3002", "topic": "agent analysis"}}
{"id": "2509.10511", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10511", "abs": "https://arxiv.org/abs/2509.10511", "authors": ["Umberto Gon\u00e7alves de Sousa"], "title": "LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs", "comment": "17 pages, 6 figures", "summary": "Reinforcement learning (RL) has transformed sequential decision-making, but\ntraditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy\nOptimization (PPO) often struggle with efficient exploration, stability, and\nadaptability in dynamic environments. This study presents LogGuardQ (Adaptive\nLog Guard with Cognitive enhancement), a novel framework that integrates a\ndual-memory system inspired by human cognition and adaptive exploration\nstrategies driven by temperature decay and curiosity. Evaluated on a dataset of\n1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,\nLogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for\nPPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.\nThe mean reward is 20.34 \\pm 44.63 across all episodes (versus 18.80 \\pm 43.98\nfor DQN and -0.17 \\pm 23.79 for PPO), with an average of 5.0 steps per episode\n(constant across models). Graphical analyses, including learning curves\nsmoothed with a Savgol filter (window=501, polynomial=2), variance trends,\naction distributions, and cumulative detections, demonstrate LogGuardQ's\nsuperior stability and efficiency. Statistical tests (Mann-Whitney U) confirm\nsignificant performance advantages (e.g., p = 0.0002 vs. DQN with negligible\neffect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN\nvs. PPO with small effect size). By bridging cognitive science and RL,\nLogGuardQ offers a scalable approach to adaptive learning in uncertain\nenvironments, with potential applications in cybersecurity, intrusion\ndetection, and decision-making under uncertainty.", "AI": {"tldr": "LogGuardQ\u662f\u4e00\u4e2a\u7ed3\u5408\u4eba\u7c7b\u8ba4\u77e5\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfDQN\u548cPPO\u7b97\u6cd5\uff0c\u68c0\u6d4b\u7387\u8fbe\u523096.0%\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982DQN\u548cPPO\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u7a33\u5b9a\u6027\u5dee\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLogGuardQ\u6846\u67b6\uff0c\u6574\u5408\u4e86\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6e29\u5ea6\u8870\u51cf\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728100\u4e07\u6761\u6a21\u62df\u8bbf\u95ee\u65e5\u5fd7\uff0847.9%\u5f02\u5e38\uff09\u7684\u6d4b\u8bd5\u4e2d\uff0cLogGuardQ\u8fbe\u523096.0%\u68c0\u6d4b\u7387\uff08DQN\u4e3a93.0%\uff0cPPO\u4e3a47.1%\uff09\uff0c\u5e73\u5747\u5956\u52b120.34\u00b144.63\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u663e\u793a\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "LogGuardQ\u901a\u8fc7\u878d\u5408\u8ba4\u77e5\u79d1\u5b66\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u7f51\u7edc\u5b89\u5168\u548c\u5165\u4fb5\u68c0\u6d4b\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.11523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11523", "abs": "https://arxiv.org/abs/2509.11523", "authors": ["Ziliang Wang", "Ge Li", "Jia Li", "Hao Zhu", "Zhi Jin"], "title": "VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection", "comment": null, "summary": "The application of language models to project-level vulnerability detection\nremains challenging, owing to the dual requirement of accurately localizing\nsecurity-sensitive code and correctly correlating and reasoning over complex\nprogram context. We present VulAgent, a multi-agent vulnerability detection\nframework based on hypothesis validation. Our design is inspired by how human\nauditors review code: when noticing a sensitive operation, they form a\nhypothesis about a possible vulnerability, consider potential trigger paths,\nand then verify the hypothesis against the surrounding context. VulAgent\nimplements a semantics-sensitive, multi-view detection pipeline: specialized\nagents, each aligned to a specific analysis perspective (e.g., memory,\nauthorization), collaboratively surface and precisely localize sensitive code\nsites with higher coverage. Building on this, VulAgent adopts a\nhypothesis-validation paradigm: for each vulnerability report, it builds\nhypothesis conditions and a trigger path, steering the LLM to target the\nrelevant program context and defensive checks during verification, which\nreduces false positives. On average across the two datasets, VulAgent improves\noverall accuracy by 6.6%, increases the correct identification rate of\nvulnerable--fixed code pairs by up to 450% (246% on average), and reduces the\nfalse positive rate by about 36% compared with state-of-the-art LLM-based\nbaselines.", "AI": {"tldr": "VulAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5047\u8bbe\u9a8c\u8bc1\u7684\u591a\u4ee3\u7406\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u5de5\u4ee3\u7801\u5ba1\u8ba1\u6d41\u7a0b\uff0c\u4f7f\u7528\u4e13\u95e8\u5316\u7684\u4ee3\u7406\u4ece\u4e0d\u540c\u5206\u6790\u89c6\u89d2\u534f\u4f5c\u68c0\u6d4b\u6f0f\u6d1e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u9879\u76ee\u7ea7\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u51c6\u786e\u5b9a\u4f4d\u5b89\u5168\u654f\u611f\u4ee3\u7801\u5e76\u6b63\u786e\u5173\u8054\u590d\u6742\u7a0b\u5e8f\u4e0a\u4e0b\u6587\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8fd9\u4e24\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u6bcf\u4e2a\u4ee3\u7406\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5206\u6790\u89c6\u89d2\uff08\u5982\u5185\u5b58\u3001\u6388\u6743\uff09\uff0c\u901a\u8fc7\u5047\u8bbe\u9a8c\u8bc1\u8303\u5f0f\uff1a\u5f62\u6210\u6f0f\u6d1e\u5047\u8bbe\u3001\u6784\u5efa\u89e6\u53d1\u8def\u5f84\u3001\u9a8c\u8bc1\u5047\u8bbe\u6761\u4ef6\uff0c\u5f15\u5bfcLLM\u5173\u6ce8\u76f8\u5173\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u548c\u9632\u5fa1\u68c0\u67e5\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u6574\u4f53\u51c6\u786e\u73876.6%\uff0c\u6f0f\u6d1e-\u4fee\u590d\u4ee3\u7801\u5bf9\u6b63\u786e\u8bc6\u522b\u7387\u6700\u9ad8\u63d0\u5347450%\uff08\u5e73\u5747246%\uff09\uff0c\u8bef\u62a5\u7387\u964d\u4f4e\u7ea636%\u3002", "conclusion": "VulAgent\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u5047\u8bbe\u9a8c\u8bc1\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u9879\u76ee\u7ea7\u4ee3\u7801\u5b89\u5168\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2509.11626", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11626", "abs": "https://arxiv.org/abs/2509.11626", "authors": ["Prerna Agarwal", "Himanshu Gupta", "Soujanya Soni", "Rohith Vallam", "Renuka Sindhgatta", "Sameep Mehta"], "title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) has lead to the\ndevelopment of agents capable of complex reasoning and interaction with\nexternal tools. In enterprise contexts, the effective use of such tools that\nare often enabled by application programming interfaces (APIs), is hindered by\npoor documentation, complex input or output schema, and large number of\noperations. These challenges make tool selection difficult and reduce the\naccuracy of payload formation by up to 25%. We propose ACE, an automated tool\ncreation and enrichment framework that transforms enterprise APIs into\nLLM-compatible tools. ACE, (i) generates enriched tool specifications with\nparameter descriptions and examples to improve selection and invocation\naccuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters\nrelevant tools at runtime, reducing prompt complexity while maintaining\nscalability. We validate our framework on both proprietary and open-source APIs\nand demonstrate its integration with agentic frameworks. To the best of our\nknowledge, ACE is the first end-to-end framework that automates the creation,\nenrichment, and dynamic selection of enterprise API tools for LLM agents.", "AI": {"tldr": "ACE\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5de5\u5177\u521b\u5efa\u548c\u589e\u5f3a\u6846\u67b6\uff0c\u5c06\u4f01\u4e1aAPI\u8f6c\u6362\u4e3aLLM\u517c\u5bb9\u5de5\u5177\uff0c\u901a\u8fc7\u751f\u6210\u4e30\u5bcc\u7684\u5de5\u5177\u89c4\u8303\u548c\u52a8\u6001\u7b5b\u9009\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u5de5\u5177\u9009\u62e9\u548c\u8c03\u7528\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2dAPI\u5de5\u5177\u4f7f\u7528\u5b58\u5728\u6587\u6863\u4e0d\u5b8c\u5584\u3001\u8f93\u5165\u8f93\u51fa\u6a21\u5f0f\u590d\u6742\u3001\u64cd\u4f5c\u6570\u91cf\u591a\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u5de5\u5177\u9009\u62e9\u56f0\u96be\uff0c\u6709\u6548\u8f7d\u8377\u5f62\u6210\u51c6\u786e\u6027\u964d\u4f4e\u8fbe25%\u3002", "method": "ACE\u6846\u67b6(i)\u751f\u6210\u5305\u542b\u53c2\u6570\u63cf\u8ff0\u548c\u793a\u4f8b\u7684\u4e30\u5bcc\u5de5\u5177\u89c4\u8303\uff0c(ii)\u96c6\u6210\u52a8\u6001\u7b5b\u9009\u673a\u5236\u5728\u8fd0\u884c\u65f6\u8fc7\u6ee4\u76f8\u5173\u5de5\u5177\uff0c\u964d\u4f4e\u63d0\u793a\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u4e13\u6709\u548c\u5f00\u6e90API\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u4ee3\u7406\u6846\u67b6\u7684\u96c6\u6210\u80fd\u529b\u3002", "conclusion": "ACE\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u4f01\u4e1aAPI\u5de5\u5177\u521b\u5efa\u3001\u589e\u5f3a\u548c\u52a8\u6001\u9009\u62e9\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2509.11026", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11026", "abs": "https://arxiv.org/abs/2509.11026", "authors": ["Ziang Li", "Manasi Ganti", "Zixian Ma", "Helena Vasconcelos", "Qijia He", "Ranjay Krishna"], "title": "Rethinking Human Preference Evaluation of LLM Rationales", "comment": "Published in the XLLM-Reason-Plan Workshop on the Application of LLM\n  Explainability to Reasoning and Planning at COLM 2025", "summary": "Large language models (LLMs) often generate natural language rationales --\nfree-form explanations that help improve performance on complex reasoning tasks\nand enhance interpretability for human users. However, evaluating these\nrationales remains challenging. While recent work has relied on binary\npreference judgments from humans or LLM judges, such evaluations are often\nopaque and coarse-grained, offering limited insight into what makes one\nrationale better than another. In this work, we rethink preference evaluation\nfor LLM-generated rationales by asking: (1) What attributes define good\nrationales? (2) Can human preferences be explained by these attributes? (3) Can\nattribute-based evaluation overcome the limitations of binary comparisons? We\nidentify a set of key rationale attributes from prior literature and assess\nthem using automatic metrics, LLM judgments, and human annotations. We then\nanalyze two standard human preference datasets MT Bench and Chatbot Arena using\nSHAP to identify which attributes best explain human preference outcomes.\nFinally, we re-evaluate model-generated rationales using attribute-specific ELO\nscores, revealing more nuanced model comparisons and insights. Our findings\nsuggest that fine-grained attribute evaluations can better characterize\nrationale quality and guide future research toward more interpretable and\nreliable evaluation practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u601d\u8003LLM\u751f\u6210rationale\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff08\u800c\u975e\u4e8c\u5143\u504f\u597d\uff09\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7SHAP\u5206\u6790\u8bc6\u522b\u5173\u952e\u5c5e\u6027\uff0c\u5e76\u7528\u5c5e\u6027\u7279\u5b9a\u7684ELO\u5206\u6570\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u6bd4\u8f83", "motivation": "\u5f53\u524dLLM\u751f\u6210rationale\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u504f\u597d\u5224\u65ad\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u591f\u900f\u660e\u548c\u7ec6\u7c92\u5ea6\uff0c\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3rationale\u8d28\u91cf\u7684\u5177\u4f53\u5dee\u5f02", "method": "1) \u4ece\u6587\u732e\u4e2d\u8bc6\u522b\u5173\u952erationale\u5c5e\u6027\uff1b2) \u4f7f\u7528\u81ea\u52a8\u6307\u6807\u3001LLM\u5224\u65ad\u548c\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u8fd9\u4e9b\u5c5e\u6027\uff1b3) \u7528SHAP\u5206\u6790MT Bench\u548cChatbot Arena\u6570\u636e\u96c6\uff1b4) \u7528\u5c5e\u6027\u7279\u5b9a\u7684ELO\u5206\u6570\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b", "result": "\u7814\u7a76\u53d1\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4f30\u80fd\u66f4\u597d\u5730\u8868\u5f81rationale\u8d28\u91cf\uff0c\u63ed\u793a\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u6bd4\u8f83\u7ed3\u679c\uff0c\u4e3a\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc", "conclusion": "\u57fa\u4e8e\u5c5e\u6027\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u4e8c\u5143\u504f\u597d\u8bc4\u4f30\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u6df1\u5165\u7684rationale\u8d28\u91cf\u6d1e\u5bdf\u548c\u6a21\u578b\u6027\u80fd\u6bd4\u8f83", "topic": "agent analysis"}}
{"id": "2509.11686", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11686", "abs": "https://arxiv.org/abs/2509.11686", "authors": ["Jian Wang", "Xiaofei Xie", "Qiang Hu", "Shangqing Liu", "Yi Li"], "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models", "comment": "EMNLP2025-findings", "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u8fd0\u884c\u65f6\u884c\u4e3a\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u6267\u884c\u8f68\u8ff9\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u4f46\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8bed\u4e49\u4fe1\u606f\u5bf9\u76d1\u7763\u5fae\u8c03\u548c\u63a8\u7406\u9636\u6bb5\u7684\u63d0\u5347\u6548\u679c\u6709\u9650\u3002", "motivation": "\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7a0b\u5e8f\u5b9e\u9645\u529f\u80fd\u548c\u8fd0\u884c\u65f6\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u8bed\u4e49\u4fe1\u606f\u7684\u8868\u793a\u4e0d\u4e00\u81f4\u4e14\u788e\u7247\u5316\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u6765\u96c6\u6210\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u6267\u884c\u8f68\u8ff9\uff09\u5230\u4ee3\u7801\u4efb\u52a1\u76f8\u5173\u7684\u63d0\u793a\u4e2d\uff0c\u5e76\u5168\u9762\u7814\u7a76\u8bed\u4e49\u4fe1\u606f\u5728\u63d0\u5347\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u5173\u6ce8\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bed\u4e49\u4fe1\u606f\u5728\u76d1\u7763\u5fae\u8c03\u548c\u63a8\u7406\u9636\u6bb5\u7684\u6709\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u5148\u524d\u7814\u7a76\u76f8\u77db\u76fe\uff0c\u8868\u660e\u8bed\u4e49\u4fe1\u606f\u5bf9\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6709\u7528\u6027\u6709\u9650\u3002", "conclusion": "\u867d\u7136\u8bed\u4e49\u4fe1\u606f\u96c6\u6210\u6846\u67b6\u662f\u53ef\u884c\u7684\uff0c\u4f46\u8bed\u4e49\u4fe1\u606f\u672c\u8eab\u5bf9\u63d0\u5347\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u5176\u4ed6\u589e\u5f3a\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2509.11035", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11035", "abs": "https://arxiv.org/abs/2509.11035", "authors": ["Yu Cui", "Hang Fu", "Haibin Zhang", "Licheng Wang", "Cong Zuo"], "title": "Free-MAD: Consensus-Free Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate (MAD) is an emerging approach to improving the reasoning\ncapabilities of large language models (LLMs). Existing MAD methods rely on\nmultiple rounds of interaction among agents to reach consensus, and the final\noutput is selected by majority voting in the last round. However, this\nconsensus-based design faces several limitations. First, multiple rounds of\ncommunication increases token overhead and limits scalability. Second, due to\nthe inherent conformity of LLMs, agents that initially produce correct\nresponses may be influenced by incorrect ones during the debate process,\ncausing error propagation. Third, majority voting introduces randomness and\nunfairness in the decision-making phase, and can degrade the reasoning\nperformance.\n  To address these issues, we propose \\textsc{Free-MAD}, a novel MAD framework\nthat eliminates the need for consensus among agents. \\textsc{Free-MAD}\nintroduces a novel score-based decision mechanism that evaluates the entire\ndebate trajectory rather than relying on the last round only. This mechanism\ntracks how each agent's reasoning evolves, enabling more accurate and fair\noutcomes. In addition, \\textsc{Free-MAD} reconstructs the debate phase by\nintroducing anti-conformity, a mechanism that enables agents to mitigate\nexcessive influence from the majority. Experiments on eight benchmark datasets\ndemonstrate that \\textsc{Free-MAD} significantly improves reasoning performance\nwhile requiring only a single-round debate and thus reducing token costs. We\nalso show that compared to existing MAD approaches, \\textsc{Free-MAD} exhibits\nimproved robustness in real-world attack scenarios.", "AI": {"tldr": "Free-MAD\u662f\u4e00\u4e2a\u65e0\u9700\u8fbe\u6210\u5171\u8bc6\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5206\u6570\u7684\u51b3\u7b56\u673a\u5236\u548c\u53cd\u4ece\u4f17\u673a\u5236\uff0c\u5728\u5355\u8f6e\u8fa9\u8bba\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u63a8\u7406\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u8fbe\u6210\u5171\u8bc6\uff0c\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u9519\u8bef\u4f20\u64ad\u548c\u591a\u6570\u6295\u7968\u968f\u673a\u6027\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5206\u6570\u7684\u51b3\u7b56\u673a\u5236\u8bc4\u4f30\u6574\u4e2a\u8fa9\u8bba\u8f68\u8ff9\uff0c\u5f15\u5165\u53cd\u4ece\u4f17\u673a\u5236\u51cf\u5c11\u591a\u6570\u610f\u89c1\u7684\u8fc7\u5ea6\u5f71\u54cd\uff0c\u53ea\u9700\u5355\u8f6e\u8fa9\u8bba\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u51cf\u5c11token\u6210\u672c\uff0c\u5728\u771f\u5b9e\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Free-MAD\u901a\u8fc7\u6d88\u9664\u5171\u8bc6\u9700\u6c42\u548c\u6539\u8fdb\u51b3\u7b56\u673a\u5236\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.11708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11708", "abs": "https://arxiv.org/abs/2509.11708", "authors": ["Zhantong Xue", "Pingchuan Ma", "Zhaoyu Wang", "Shuai Wang"], "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation", "comment": null, "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ZK-Eval\u8bc4\u4f30\u6846\u67b6\u548cZK-Coder\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u77e5\u8bc6\u8bc1\u660e\u7f16\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u3002", "motivation": "\u96f6\u77e5\u8bc6\u8bc1\u660e\u7f16\u7a0b\u5177\u6709\u77e5\u8bc6\u5bc6\u96c6\u548c\u6613\u9519\u7684\u7279\u70b9\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728ZK\u7f16\u7a0b\u9886\u57df\u7684\u6709\u6548\u6027\u5c1a\u672a\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "method": "\u63d0\u51faZK-Eval\u4e09\u7ea7\u8bc4\u4f30\u7ba1\u9053\uff08\u8bed\u8a00\u77e5\u8bc6\u3001\u7ec4\u4ef6\u80fd\u529b\u3001\u7aef\u5230\u7aef\u7a0b\u5e8f\u751f\u6210\uff09\uff0c\u5e76\u5f00\u53d1ZK-Coder\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u7ea6\u675f\u8349\u56fe\u3001\u5f15\u5bfc\u68c0\u7d22\u548c\u4ea4\u4e92\u5f0f\u4fee\u590d\u673a\u5236\u3002", "result": "\u5728Circom\u548cNoir\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6210\u529f\u7387\u5206\u522b\u4ece17.35%\u63d0\u5347\u523083.38%\uff0c\u4ee5\u53ca\u4ece32.21%\u63d0\u5347\u523090.05%\u3002", "conclusion": "\u901a\u8fc7ZK-Eval\u548cZK-Coder\u5efa\u7acb\u4e86\u7cfb\u7edf\u8bc4\u4f30\u548c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728ZK\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u57fa\u7840\uff0c\u964d\u4f4e\u4e86\u5b9e\u8df5\u95e8\u69db\u5e76\u63a8\u8fdb\u53ef\u4fe1\u8ba1\u7b97\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.11068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11068", "abs": "https://arxiv.org/abs/2509.11068", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability", "comment": null, "summary": "The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,\nmulti-agent systems. This introduces a fundamental challenge in establishing\ncomputational trust, specifically how one agent can verify that another's\noutput was genuinely produced by a claimed LLM, and not falsified or generated\nby a cheaper or inferior model. To address this challenge, this paper proposes\na verification framework that achieves tractable asymmetric effort, where the\ncost to verify a computation is substantially lower than the cost to perform\nit. Our approach is built upon the principle of deterministic replicability, a\nproperty inherent to autoregressive models that strictly necessitates a\ncomputationally homogeneous environment where all agents operate on identical\nhardware and software stacks. Within this defined context, our framework\nenables multiple validators to probabilistically audit small, random segments\nof an LLM's output and it distributes the verification workload effectively.\nThe simulations demonstrated that targeted verification can be over 12 times\nfaster than full regeneration, with tunable parameters to adjust the detection\nprobability. By establishing a tractable mechanism for auditable LLM systems,\nour work offers a foundational layer for responsible AI and serves as a\ncornerstone for future research into the more complex, heterogeneous\nmulti-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u786e\u5b9a\u6027\u53ef\u590d\u5236\u6027\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8ba1\u7b97\u540c\u8d28\u73af\u5883\u4e2d\u9a8c\u8bc1LLM\u8f93\u51fa\u7684\u771f\u5b9e\u6027\uff0c\u5b9e\u73b0\u9a8c\u8bc1\u6210\u672c\u8fdc\u4f4e\u4e8e\u8ba1\u7b97\u6210\u672c\u7684\u4e0d\u5bf9\u79f0\u52aa\u529b\u3002", "motivation": "\u968f\u7740LLM\u5411\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff0c\u9700\u8981\u89e3\u51b3\u8ba1\u7b97\u4fe1\u4efb\u95ee\u9898\uff0c\u5373\u5982\u4f55\u9a8c\u8bc1\u667a\u80fd\u4f53\u8f93\u51fa\u786e\u5b9e\u6765\u81ea\u58f0\u79f0\u7684LLM\u800c\u975e\u5ec9\u4ef7\u6216\u52a3\u8d28\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u786e\u5b9a\u6027\u53ef\u590d\u5236\u6027\u539f\u7406\uff0c\u5728\u8ba1\u7b97\u540c\u8d28\u73af\u5883\u4e0b\uff0c\u8ba9\u591a\u4e2a\u9a8c\u8bc1\u8005\u6982\u7387\u6027\u5730\u5ba1\u8ba1LLM\u8f93\u51fa\u7684\u5c0f\u968f\u673a\u7247\u6bb5\uff0c\u6709\u6548\u5206\u914d\u9a8c\u8bc1\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u6a21\u62df\u663e\u793a\u76ee\u6807\u9a8c\u8bc1\u6bd4\u5b8c\u5168\u91cd\u65b0\u751f\u6210\u5feb12\u500d\u4ee5\u4e0a\uff0c\u5177\u6709\u53ef\u8c03\u8282\u53c2\u6570\u6765\u8c03\u6574\u68c0\u6d4b\u6982\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u5ba1\u8ba1LLM\u7cfb\u7edf\u5efa\u7acb\u4e86\u53ef\u884c\u673a\u5236\uff0c\u4e3a\u8d1f\u8d23\u4efbAI\u63d0\u4f9b\u57fa\u7840\u5c42\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u590d\u6742\u7684\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2509.11748", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11748", "abs": "https://arxiv.org/abs/2509.11748", "authors": ["Marius Mignard", "Steven Costiou", "Nicolas Anquetil", "Anne Etien"], "title": "Analysing Python Machine Learning Notebooks with Moose", "comment": null, "summary": "Machine Learning (ML) code, particularly within notebooks, often exhibits\nlower quality compared to traditional software. Bad practices arise at three\ndistinct levels: general Python coding conventions, the organizational\nstructure of the notebook itself, and ML-specific aspects such as\nreproducibility and correct API usage. However, existing analysis tools\ntypically focus on only one of these levels and struggle to capture ML-specific\nsemantics, limiting their ability to detect issues. This paper introduces\nVespucci Linter, a static analysis tool with multi-level capabilities, built on\nMoose and designed to address this challenge. Leveraging a metamodeling\napproach that unifies the notebook's structural elements with Python code\nentities, our linter enables a more contextualized analysis to identify issues\nacross all three levels. We implemented 22 linting rules derived from the\nliterature and applied our tool to a corpus of 5,000 notebooks from the Kaggle\nplatform. The results reveal violations at all levels, validating the relevance\nof our multi-level approach and demonstrating Vespucci Linter's potential to\nimprove the quality and reliability of ML development in notebook environments.", "AI": {"tldr": "Vespucci Linter\u662f\u4e00\u4e2a\u591a\u5c42\u6b21\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u4ee3\u7801\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u80fd\u591f\u540c\u65f6\u68c0\u6d4bPython\u7f16\u7801\u89c4\u8303\u3001\u7b14\u8bb0\u672c\u7ec4\u7ec7\u7ed3\u6784\u548cML\u7279\u5b9a\u95ee\u9898\u4e09\u4e2a\u5c42\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u4ee3\u7801\u8d28\u91cf\u666e\u904d\u8f83\u4f4e\uff0c\u73b0\u6709\u5de5\u5177\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u5c42\u9762\u4e14\u96be\u4ee5\u6355\u6349ML\u7279\u5b9a\u8bed\u4e49\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u591a\u5c42\u6b21\u4e0a\u4e0b\u6587\u5206\u6790\u7684\u5de5\u5177\u3002", "method": "\u57fa\u4e8eMoose\u6784\u5efa\uff0c\u91c7\u7528\u5143\u5efa\u6a21\u65b9\u6cd5\u7edf\u4e00\u7b14\u8bb0\u672c\u7ed3\u6784\u5143\u7d20\u548cPython\u4ee3\u7801\u5b9e\u4f53\uff0c\u5b9e\u73b0\u4e8622\u4e2a\u57fa\u4e8e\u6587\u732e\u7684linting\u89c4\u5219\uff0c\u5e76\u57285000\u4e2aKaggle\u7b14\u8bb0\u672c\u4e0a\u8fdb\u884c\u4e86\u5e94\u7528\u6d4b\u8bd5\u3002", "result": "\u5728\u6240\u6709\u4e09\u4e2a\u5c42\u9762\u90fd\u53d1\u73b0\u4e86\u8fdd\u89c4\u60c5\u51b5\uff0c\u9a8c\u8bc1\u4e86\u591a\u5c42\u6b21\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u5de5\u5177\u5728\u63d0\u9ad8\u7b14\u8bb0\u672c\u73af\u5883\u4e2dML\u5f00\u53d1\u8d28\u91cf\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "Vespucci Linter\u901a\u8fc7\u591a\u5c42\u6b21\u5206\u6790\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ML\u7b14\u8bb0\u672c\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u6539\u5584ML\u5f00\u53d1\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "2509.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11079", "abs": "https://arxiv.org/abs/2509.11079", "authors": ["Jinwei Su", "Yinghui Xia", "Qizhen Lan", "Xinyuan Song", "Yang Jingsong", "Lewei He", "Tianyu Shi"], "title": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows", "comment": null, "summary": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication.", "AI": {"tldr": "DAAO\u662f\u4e00\u4e2a\u52a8\u6001\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7684\u5de5\u4f5c\u6d41\u8c03\u6574\u3001\u7b97\u5b50\u5206\u914d\u548cLLM\u8def\u7531\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f7f\u7528\u9759\u6001\u6216\u4efb\u52a1\u7ea7\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u52a8\u6001\u8c03\u6574\uff0c\u4e14\u5ffd\u89c6\u4e86\u5f02\u6784LLM\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002", "method": "\u63d0\u51faDAAO\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u96be\u5ea6\u4f30\u8ba1\u3001\u6a21\u5757\u5316\u7b97\u5b50\u5206\u914d\u5668\u3001\u6210\u672c\u548c\u6027\u80fd\u611f\u77e5\u7684LLM\u8def\u7531\u5668\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDAAO\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "DAAO\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u67e5\u8be2\u96be\u5ea6\u548c\u5f02\u6784LLM\u7684\u667a\u80fd\u5206\u914d\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u67e5\u8be2\u7279\u5b9a\u63a8\u7406\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2509.11787", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11787", "abs": "https://arxiv.org/abs/2509.11787", "authors": ["Pascal Joos", "Islem Bouzenia", "Michael Pradel"], "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings", "comment": null, "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.", "AI": {"tldr": "CodeCureAgent\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u81ea\u52a8\u5206\u6790\u3001\u5206\u7c7b\u548c\u4fee\u590d\u9759\u6001\u5206\u6790\u8b66\u544a\uff0c\u901a\u8fc7\u8fed\u4ee3\u8c03\u7528\u5de5\u5177\u6536\u96c6\u4ee3\u7801\u5e93\u4fe1\u606f\u5e76\u7f16\u8f91\u4ee3\u7801\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u57281000\u4e2aSonarQube\u8b66\u544a\u4e0a\u5b9e\u73b0\u4e8696.8%\u7684\u5408\u7406\u4fee\u590d\u7387\u3002", "motivation": "\u4f20\u7edf\u4e0a\u5f00\u53d1\u8005\u9700\u8981\u624b\u52a8\u5904\u7406\u9759\u6001\u5206\u6790\u5de5\u5177\u4ea7\u751f\u7684\u8b66\u544a\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u7e41\u7410\u4e14\u5bb9\u6613\u5bfc\u81f4\u8b66\u544a\u79ef\u7d2f\u548c\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u8fed\u4ee3\u8c03\u7528\u5de5\u5177\u6536\u96c6\u4ee3\u7801\u5e93\u4fe1\u606f\uff08\u5982\u4ee3\u7801\u641c\u7d22\uff09\u5e76\u7f16\u8f91\u4ee3\u7801\u6765\u4fee\u590d\u8b66\u544a\uff0c\u914d\u5907\u4e09\u6b65\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u6279\u51c6\u8865\u4e01\uff1a\u6784\u5efa\u9879\u76ee\u3001\u9a8c\u8bc1\u8b66\u544a\u6d88\u5931\u4e14\u4e0d\u5f15\u5165\u65b0\u8b66\u544a\u3001\u8fd0\u884c\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u5728106\u4e2aJava\u9879\u76ee\u76841000\u4e2aSonarQube\u8b66\u544a\u4e0a\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e8696.8%\u7684\u5408\u7406\u4fee\u590d\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u9ad8\u51fa30.7%\u548c29.2%\uff0c\u6b63\u786e\u4fee\u590d\u7387\u8fbe\u523086.3%\uff0c\u6bcf\u4e2a\u8b66\u544a\u5904\u7406\u6210\u672c\u7ea62.9\u7f8e\u5206\uff0c\u8017\u65f6\u7ea64\u5206\u949f\u3002", "conclusion": "CodeCureAgent\u80fd\u591f\u53ef\u9760\u5730\u4fee\u590d\u9759\u6001\u5206\u6790\u8b66\u544a\uff0c\u6709\u52a9\u4e8e\u6e05\u7406\u73b0\u6709\u4ee3\u7801\u5e93\u5e76\u53ef\u4ee5\u96c6\u6210\u5230CI/CD\u7ba1\u9053\u4e2d\u9632\u6b62\u8b66\u544a\u79ef\u7d2f\u3002", "topic": "swe application"}}
{"id": "2509.10852", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10852", "abs": "https://arxiv.org/abs/2509.10852", "authors": ["Sangyeop Kim", "Yohan Lee", "Sanghwa Kim", "Hyunjong Kim", "Sungzoon Cho"], "title": "Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue", "comment": "Accepted by EMNLP 2025 (Findings)", "summary": "Effective long-term memory in conversational AI requires synthesizing\ninformation across multiple sessions. However, current systems place excessive\nreasoning burden on response generation, making performance significantly\ndependent on model sizes. We introduce PREMem (Pre-storage Reasoning for\nEpisodic Memory), a novel approach that shifts complex reasoning processes from\ninference to memory construction. PREMem extracts fine-grained memory fragments\ncategorized into factual, experiential, and subjective information; it then\nestablishes explicit relationships between memory items across sessions,\ncapturing evolution patterns like extensions, transformations, and\nimplications. By performing this reasoning during pre-storage rather than when\ngenerating a response, PREMem creates enriched representations while reducing\ncomputational demands during interactions. Experiments show significant\nperformance improvements across all model sizes, with smaller models achieving\nresults comparable to much larger baselines while maintaining effectiveness\neven with constrained token budgets. Code and dataset are available at\nhttps://github.com/sangyeop-kim/PREMem.", "AI": {"tldr": "PREMem\u662f\u4e00\u79cd\u901a\u8fc7\u5728\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\u800c\u975e\u54cd\u5e94\u751f\u6210\u9636\u6bb5\u8fdb\u884c\u590d\u6742\u63a8\u7406\u6765\u63d0\u5347\u5bf9\u8bddAI\u957f\u671f\u8bb0\u5fc6\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u80fd\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u5e76\u63d0\u5347\u5404\u79cd\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bddAI\u7cfb\u7edf\u5728\u957f\u671f\u8bb0\u5fc6\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u590d\u6742\u7684\u63a8\u7406\u8fc7\u7a0b\u90fd\u96c6\u4e2d\u5728\u54cd\u5e94\u751f\u6210\u9636\u6bb5\uff0c\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u6a21\u578b\u89c4\u6a21\uff0c\u8ba1\u7b97\u8d1f\u62c5\u8fc7\u91cd\u3002", "method": "\u63d0\u51faPREMem\u65b9\u6cd5\uff0c\u5728\u8bb0\u5fc6\u9884\u5b58\u50a8\u9636\u6bb5\u8fdb\u884c\u590d\u6742\u63a8\u7406\uff1a\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bb0\u5fc6\u7247\u6bb5\uff08\u4e8b\u5b9e\u6027\u3001\u7ecf\u9a8c\u6027\u3001\u4e3b\u89c2\u6027\u4fe1\u606f\uff09\uff0c\u5efa\u7acb\u8de8\u4f1a\u8bdd\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u6355\u6349\u6269\u5c55\u3001\u8f6c\u6362\u3001\u8574\u542b\u7b49\u6f14\u5316\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u6a21\u578b\u89c4\u6a21\u90fd\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u80fd\u8fbe\u5230\u4e0e\u5927\u57fa\u7ebf\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u5373\u4f7f\u5728\u53d7\u9650\u7684token\u9884\u7b97\u4e0b\u4ecd\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u63a8\u7406\u8fc7\u7a0b\u4ece\u63a8\u7406\u9636\u6bb5\u8f6c\u79fb\u5230\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\u662f\u6709\u6548\u7684\uff0cPREMem\u65b9\u6cd5\u80fd\u591f\u521b\u5efa\u4e30\u5bcc\u7684\u8bb0\u5fc6\u8868\u793a\u540c\u65f6\u51cf\u5c11\u4ea4\u4e92\u65f6\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2509.11942", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.11942", "abs": "https://arxiv.org/abs/2509.11942", "authors": ["Lu\u00eds F. Gomes", "Xin Zhou", "David Lo", "Rui Abreu"], "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems", "comment": null, "summary": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VisDocSketcher\uff0c\u9996\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u53ef\u89c6\u5316\u6587\u6863\u65b9\u6cd5\uff0c\u4ee5\u53caAutoSketchEval\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u4e3a74.4%\u7684\u6837\u672c\u751f\u6210\u6709\u6548\u53ef\u89c6\u5316\u6587\u6863\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534726.7-39.8%\u3002", "motivation": "\u53ef\u89c6\u5316\u6587\u6863\u80fd\u6709\u6548\u964d\u4f4e\u5f00\u53d1\u8005\u7406\u89e3\u964c\u751f\u4ee3\u7801\u7684\u8ba4\u77e5\u969c\u788d\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8017\u65f6\u4e14\u96be\u4ee5\u8bc4\u4f30\u3002\u76ee\u524d\u6ca1\u6709\u65b9\u6cd5\u80fd\u81ea\u52a8\u4ece\u4ee3\u7801\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u6587\u6863\uff0c\u8bc4\u4f30\u4e5f\u7f3a\u4e4f\u6807\u51c6\u5316\u3002", "method": "\u7ed3\u5408\u9759\u6001\u5206\u6790\u548cLLM\u4ee3\u7406\u8bc6\u522b\u4ee3\u7801\u5173\u952e\u5143\u7d20\u5e76\u751f\u6210\u53ef\u89c6\u5316\u8868\u793a\uff0c\u63d0\u51faAutoSketchEval\u8bc4\u4f30\u6846\u67b6\u4f7f\u7528\u4ee3\u7801\u7ea7\u6307\u6807\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\u3002", "result": "\u80fd\u4e3a74.4%\u6837\u672c\u751f\u6210\u6709\u6548\u53ef\u89c6\u5316\u6587\u6863\uff0c\u6bd4\u6a21\u677f\u57fa\u7ebf\u63d0\u534726.7-39.8%\u3002\u8bc4\u4f30\u6846\u67b6AUC\u8d85\u8fc70.87\uff0c\u80fd\u53ef\u9760\u533a\u5206\u9ad8\u8d28\u91cf\u548c\u4f4e\u8d28\u91cf\u6587\u6863\u3002", "conclusion": "\u4e3a\u81ea\u52a8\u5316\u53ef\u89c6\u5316\u6587\u6863\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u65e2\u80fd\u751f\u6210\u6709\u6548\u53ef\u89c6\u5316\u8868\u793a\u53c8\u80fd\u53ef\u9760\u8bc4\u4f30\u8d28\u91cf\u7684\u5b9e\u7528\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "2509.12021", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12021", "abs": "https://arxiv.org/abs/2509.12021", "authors": ["Benedikt Fein", "Florian Oberm\u00fcller", "Gordon Fraser"], "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis", "comment": "ASE 2025 Tool Demonstration Track", "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.", "AI": {"tldr": "LitterBox+\u6846\u67b6\u5c06Scratch\u79ef\u6728\u7f16\u7a0b\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u7ed3\u5408LLMs\u63d0\u4f9b\u4ee3\u7801\u67e5\u8be2\u3001\u8d28\u91cf\u5206\u6790\u548c\u4fee\u590d\u751f\u6210\u529f\u80fd\uff0c\u6269\u5c55\u4e86Scratch IDE\u7684AI\u8f85\u52a9\u7f16\u7a0b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3Scratch\u56fe\u5f62\u5316\u7f16\u7a0b\u73af\u5883\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\uff0c\u4e3a\u5b66\u4e60\u8005\u63d0\u4f9bAI\u8f85\u52a9\u7f16\u7a0b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5c06\u79ef\u6728\u4ee3\u7801\u8f6c\u6362\u4e3a\u9002\u5408LLM\u7684\u6587\u672c\u8868\u793a\uff0c\u96c6\u6210LitterBox\u9759\u6001\u5206\u6790\u5de5\u5177\u4e0eLLM\u751f\u6210\u80fd\u529b\uff0c\u63d0\u4f9bAPI\u548cIDE\u6269\u5c55\u3002", "result": "\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u63d0\u793a\u8bcd\u548cLLM\u63d0\u4f9b\u5546\uff0c\u5b9e\u73b0\u4e86\u5728Scratch\u73af\u5883\u4e2d\u76f4\u63a5\u4f7f\u7528LLM\u529f\u80fd\u3002", "conclusion": "LitterBox+\u6210\u529f\u514b\u670d\u4e86\u56fe\u5f62\u5316\u7f16\u7a0b\u4f7f\u7528LLM\u7684\u969c\u788d\uff0c\u4e3a\u6559\u80b2\u7f16\u7a0b\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684AI\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2509.12087", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12087", "abs": "https://arxiv.org/abs/2509.12087", "authors": ["Pengyu Xue", "Kunwu Zheng", "Zhen Yang", "Yifei Pei", "Linhao Wu", "Jiahui Dong", "Xiapu Luo", "Yan Xiao", "Fei Liu", "Yuxuan Zhang", "Xiran Lyu", "Xianhang Li", "Xuanyu Zhu", "Chengyi Wang"], "title": "A New Benchmark for Evaluating Code Translation with Third-Party Libraries", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence.", "AI": {"tldr": "TransLibEval\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u7b2c\u4e09\u65b9\u5e93\u4e2d\u5fc3\u4ee3\u7801\u7ffb\u8bd1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u4e2a\u771f\u5b9e\u4efb\u52a1\uff0c\u8986\u76d6Python\u3001Java\u548cC++\uff0c\u8bc4\u4f30\u663e\u793aLLMs\u5728\u6d89\u53ca\u7b2c\u4e09\u65b9\u5e93\u7684\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u6027\u80fd\u4e0b\u964d\u8d85\u8fc760%", "motivation": "\u73b0\u6709\u4ee3\u7801\u7ffb\u8bd1\u57fa\u51c6\u5728\u7b2c\u4e09\u65b9\u5e93\u7c7b\u522b\u548c\u89c4\u6a21\u4e0a\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u66b4\u9732TPL\u76f8\u5173\u9519\u8bef\uff0c\u800c\u5b9e\u9645\u7f16\u7a0b\u4e2d\u8d85\u8fc790%\u4f9d\u8d56\u7b2c\u4e09\u65b9\u5e93\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790LLMs\u5728\u6d89\u53ca\u5404\u79cdTPL\u7684\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd", "method": "\u6784\u5efaTransLibEval\u57fa\u51c6\uff0c\u5305\u542b200\u4e2a\u771f\u5b9e\u4efb\u52a1\uff0c\u6db5\u76d6\u6570\u636e\u5904\u7406\u3001\u673a\u5668\u5b66\u4e60\u548cweb\u5f00\u53d1\u7b49\u591a\u6837\u5316TPL\u7c7b\u522b\uff0c\u8bc4\u4f307\u4e2a\u6700\u65b0LLM\u57286\u79cd\u7ffb\u8bd1\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790GPT-4o\u76844,831\u4e2a\u5931\u8d25\u6848\u4f8b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u65e0\u5e93\u8bbe\u7f6e\u6027\u80fd\u5e73\u5747\u4e0b\u964d\u8d85\u8fc760%\uff0c\u4e0d\u540c\u7b56\u7565\u8868\u73b0\u51fa\u5f02\u8d28\u6027\u4f18\u52bf\uff0c\u63ed\u793a\u4e86\u5927\u91cf\u4e4b\u524d\u88ab\u63a9\u76d6\u7684\u7b2c\u4e09\u65b9\u5f15\u7528\u9519\u8bef", "conclusion": "\u7814\u7a76\u7a81\u663e\u4e86\u5e93\u4e2d\u5fc3\u7ffb\u8bd1\u7684\u72ec\u7279\u6311\u6218\uff0c\u4e3a\u6539\u8fdbTPL\u611f\u77e5\u7684\u4ee3\u7801\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc", "topic": "swe benchmark"}}
{"id": "2509.11311", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.11311", "abs": "https://arxiv.org/abs/2509.11311", "authors": ["Bingchen Wang", "Zi-Yu Khoo", "Bryan Kian Hsiang Low"], "title": "Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble", "comment": "Preprint of work originally submitted to AAAI 2026. Under revision\n  for resubmission to a machine learning venue", "summary": "Large language models (LLMs) have demonstrated promise in emulating\nhuman-like responses across a wide range of tasks. In this paper, we propose a\nnovel alignment framework that treats LLMs as agent proxies for human survey\nrespondents, affording a cost-effective and steerable solution to two pressing\nchallenges in the social sciences: the rising cost of survey deployment and the\ngrowing demographic imbalance in survey response data. Drawing inspiration from\nthe theory of revealed preference, we formulate alignment as a two-stage\nproblem: constructing diverse agent personas called endowments that simulate\nplausible respondent profiles, and selecting a representative subset to\napproximate a ground-truth population based on observed data. To implement the\nparadigm, we introduce P2P, a system that steers LLM agents toward\nrepresentative behavioral patterns using structured prompt engineering,\nentropy-based sampling, and regression-based selection. Unlike\npersonalization-heavy approaches, our alignment approach is\ndemographic-agnostic and relies only on aggregate survey results, offering\nbetter generalizability and parsimony. Beyond improving data efficiency in\nsocial science research, our framework offers a testbed for studying the\noperationalization of pluralistic alignment. We demonstrate the efficacy of our\napproach on real-world opinion survey datasets, showing that our aligned agent\npopulations can reproduce aggregate response patterns with high fidelity and\nexhibit substantial response diversity, even without demographic conditioning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u6846\u67b6P2P\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4eba\u7c7b\u8c03\u67e5\u53d7\u8bbf\u8005\u7684\u4ee3\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u4eba\u8bbe\u548c\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u6765\u4fbf\u5b9c\u5730\u4eff\u771f\u771f\u5b9e\u4eba\u7fa4\u8c03\u67e5\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u8c03\u67e5\u90e8\u7f72\u6210\u672c\u4e0a\u5347\u548c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u53d7\u663e\u793a\u504f\u597d\u7406\u8bba\u542f\u53d1\uff0c\u5c06\u5bf9\u9f50\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e24\u9636\u6bb5\u95ee\u9898\uff1a\u6784\u5efa\u591a\u6837\u5316\u4eba\u8bbe\uff08endowments\uff09\u548c\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u3002\u63d0\u51faP2P\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u3001\u57fa\u4e8e\u71b5\u7684\u91c7\u6837\u548c\u56de\u5f52\u9009\u62e9\u6765\u5bfc\u5411LLM\u4ee3\u7406\u7684\u4ee3\u8868\u6027\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u610f\u89c1\u8c03\u67e5\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5bf9\u9f50\u540e\u7684\u4ee3\u7406\u7fa4\u4f53\u80fd\u591f\u9ad8\u4fdd\u771f\u5ea6\u5730\u590d\u73b0\u805a\u5408\u54cd\u5e94\u6a21\u5f0f\uff0c\u5c55\u73b0\u51fa\u5b9e\u8d28\u6027\u7684\u54cd\u5e94\u591a\u6837\u6027\uff0c\u800c\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u6761\u4ef6\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u6570\u636e\u6548\u7387\uff0c\u8fd8\u4e3a\u7814\u7a8b\u5f0f\u5bf9\u9f50\u7684\u64cd\u4f5c\u5316\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e8a\uff0c\u5177\u6709\u66f4\u597d\u7684\u666e\u904d\u6027\u548c\u7b80\u6d01\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.12159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12159", "abs": "https://arxiv.org/abs/2509.12159", "authors": ["Jingyu Xiao", "Zhongyi Zhang", "Yuxuan Wan", "Yintong Huo", "Yang Liu", "Michael R. Lyu"], "title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression", "comment": null, "summary": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder.", "AI": {"tldr": "EfficientUICoder\u662f\u4e00\u4e2a\u9488\u5bf9UI2Code\u4efb\u52a1\u7684\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7d20\u611f\u77e5\u538b\u7f29\u3001\u533a\u57df\u611f\u77e5\u7ec6\u5316\u548c\u81ea\u9002\u5e94\u91cd\u590d\u6291\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u7f51\u9875\u8d28\u91cf", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728UI2Code\u4efb\u52a1\u4e2d\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u5b58\u5728\u56fe\u50cf\u548c\u4ee3\u7801\u4ee4\u724c\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u5197\u957f\u4e14\u65e0\u6548\u7684HTML\u6587\u4ef6", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u538b\u7f29\u6846\u67b6\uff1a1)\u5143\u7d20\u548c\u5e03\u5c40\u611f\u77e5\u4ee4\u724c\u538b\u7f29\uff1b2)\u533a\u57df\u611f\u77e5\u4ee4\u724c\u7ec6\u5316\uff1b3)\u81ea\u9002\u5e94\u91cd\u590d\u4ee4\u724c\u6291\u5236", "result": "\u5b9e\u73b055%-60%\u538b\u7f29\u7387\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e44.9%\uff0c\u751f\u6210\u4ee4\u724c\u51cf\u5c1141.4%\uff0c\u9884\u586b\u5145\u65f6\u95f4\u51cf\u5c1146.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1148.8%", "conclusion": "\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347UI\u4ee3\u7801\u751f\u6210\u6548\u7387\uff0c\u5728\u4e0d\u5f71\u54cd\u7f51\u9875\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "topic": "swe application"}}
{"id": "2509.11361", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11361", "abs": "https://arxiv.org/abs/2509.11361", "authors": ["Yichen Han", "Bojun Liu", "Zhengpeng zhou", "Guanyu Liu", "Zeng Zhang", "Yang Yang", "Wenli Wang", "Isaac N Shi", "Yunyan", "Lewei He", "Tianyu Shi"], "title": "MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization", "comment": null, "summary": "Prompt engineering is crucial for leveraging large language models (LLMs),\nbut existing methods often rely on a single optimization trajectory, limiting\nadaptability and efficiency while suffering from narrow perspectives, gradient\nconflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt\nGradient Descent), a framework integrating multi-agent collaboration with\ngradient-based optimization. MAPGD features specialized agents for task\nclarity, example selection, format design, and stylistic refinement; semantic\ngradient coordination to resolve conflicts; bandit-based candidate selection\nfor efficient exploration-exploitation; and theoretical convergence guarantees.\nExperiments on classification, generation, and reasoning tasks show MAPGD\noutperforms single-agent and random baselines in accuracy and efficiency.\nAblations confirm the benefits of gradient fusion, agent specialization, and\nconflict resolution, providing a unified, gradient-inspired multi-agent\napproach to robust and interpretable prompt optimization.", "AI": {"tldr": "MAPGD\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u63d0\u793a\u68af\u5ea6\u4e0b\u964d\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u68af\u5ea6\u4f18\u5316\u89e3\u51b3\u4f20\u7edf\u5355\u8f68\u8ff9\u63d0\u793a\u5de5\u7a0b\u7684\u5c40\u9650\u6027\uff0c\u5728\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u4f18\u5316\u8f68\u8ff9\uff0c\u5b58\u5728\u9002\u5e94\u6027\u5dee\u3001\u6548\u7387\u4f4e\u3001\u89c6\u89d2\u72ed\u7a84\u3001\u68af\u5ea6\u51b2\u7a81\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e0e\u68af\u5ea6\u4f18\u5316\uff0c\u5305\u542b\u4efb\u52a1\u6e05\u6670\u5316\u3001\u793a\u4f8b\u9009\u62e9\u3001\u683c\u5f0f\u8bbe\u8ba1\u548c\u98ce\u683c\u4f18\u5316\u7b49\u4e13\u95e8\u5316\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u8bed\u4e49\u68af\u5ea6\u534f\u8c03\u89e3\u51b3\u51b2\u7a81\uff0c\u57fa\u4e8ebandit\u7684\u5019\u9009\u9009\u62e9\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMAPGD\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u968f\u673a\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u68af\u5ea6\u878d\u5408\u3001\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\u548c\u51b2\u7a81\u89e3\u51b3\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAPGD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u68af\u5ea6\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u63d0\u793a\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2509.10531", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10531", "abs": "https://arxiv.org/abs/2509.10531", "authors": ["Himanshu Choudhary", "Arishi Orra", "Manoj Thakur"], "title": "FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities", "comment": null, "summary": "Portfolio optimization is essential for balancing risk and return in\nfinancial decision-making. Deep Reinforcement Learning (DRL) has stood out as a\ncutting-edge tool for portfolio optimization that learns dynamic asset\nallocation using trial-and-error interactions. However, most DRL-based methods\nare restricted to allocating assets within a pre-defined investment universe\nand overlook exploring new opportunities. This study introduces an investment\nlandscape that integrates exploiting existing assets with exploring new\ninvestment opportunities in an extended universe. The proposed approach\nleverages two DRL agents and dynamically balances these objectives to adapt to\nevolving markets while enhancing portfolio performance. One agent allocates\nassets within the existing universe, while another assists in exploring new\nopportunities in the extended universe. The effciency of the proposed\nmethodology is determined using two real-world market data sets. The\nexperiments demonstrate the superiority of the suggested approach against the\nstate-of-the-art portfolio strategies and baseline methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5229\u7528\u73b0\u6709\u8d44\u4ea7\u548c\u63a2\u7d22\u65b0\u6295\u8d44\u673a\u4f1a\u7684\u53ccDRL\u667a\u80fd\u4f53\u6295\u8d44\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u8fd9\u4e24\u4e2a\u76ee\u6807\u6765\u63d0\u5347\u6295\u8d44\u7ec4\u5408\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u6295\u8d44\u8303\u56f4\u5185\u7684\u8d44\u4ea7\u914d\u7f6e\uff0c\u5ffd\u89c6\u4e86\u63a2\u7d22\u65b0\u6295\u8d44\u673a\u4f1a\u7684\u91cd\u8981\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u5e02\u573a\u73af\u5883\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff1a\u4e00\u4e2a\u8d1f\u8d23\u5728\u73b0\u6709\u6295\u8d44\u8303\u56f4\u5185\u8fdb\u884c\u8d44\u4ea7\u914d\u7f6e\uff0c\u53e6\u4e00\u4e2a\u534f\u52a9\u5728\u6269\u5c55\u6295\u8d44\u8303\u56f4\u5185\u63a2\u7d22\u65b0\u673a\u4f1a\uff0c\u52a8\u6001\u5e73\u8861\u5229\u7528\u548c\u63a2\u7d22\u76ee\u6807\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6295\u8d44\u7ec4\u5408\u7b56\u7565\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u6574\u5408\u5229\u7528\u73b0\u6709\u8d44\u4ea7\u548c\u63a2\u7d22\u65b0\u673a\u4f1a\uff0c\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5e02\u573a\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u6295\u8d44\u7ec4\u5408\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.11127", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11127", "abs": "https://arxiv.org/abs/2509.11127", "authors": ["Hongxu Zhou", "Hylke Westerdijk", "Khondoker Ittehadul Islam"], "title": "Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification", "comment": null, "summary": "This study investigates how context and emotional tone metadata influence\nlarge language model (LLM) reasoning and performance in fallacy classification\ntasks, particularly within political debate settings. Using data from U.S.\npresidential debates, we classify six fallacy types through various prompting\nstrategies applied to the Qwen-3 (8B) model. We introduce two theoretically\ngrounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table\nof Arguments, and evaluate their effectiveness against a baseline prompt under\nthree input settings: text-only, text with context, and text with both context\nand audio-based emotional tone metadata. Results suggest that while theoretical\nprompting can improve interpretability and, in some cases, accuracy, the\naddition of context and especially emotional tone metadata often leads to\nlowered performance. Emotional tone metadata biases the model toward labeling\nstatements as \\textit{Appeal to Emotion}, worsening logical reasoning. Overall,\nbasic prompts often outperformed enhanced ones, suggesting that attention\ndilution from added inputs may worsen rather than improve fallacy\nclassification in LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8c2c\u8bef\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u653f\u6cbb\u8fa9\u8bba\u573a\u666f\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u60c5\u611f\u5143\u6570\u636e\u4f1a\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u5c06\u9648\u8ff0\u6807\u8bb0\u4e3a\"\u8bc9\u8bf8\u60c5\u611f\"\u8c2c\u8bef\uff0c\u53cd\u800c\u964d\u4f4e\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u8fa9\u8bba\u8c2c\u8bef\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\u5bf9\u6a21\u578b\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u5927\u9009\u8fa9\u8bba\u6570\u636e\uff0c\u5bf9Qwen-3(8B)\u6a21\u578b\u5e94\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5305\u62ec\u57fa\u4e8e\u8bed\u7528\u8fa9\u8bc1\u6cd5\u548c\u8bba\u8bc1\u5468\u671f\u8868\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5728\u4e09\u79cd\u8f93\u5165\u8bbe\u7f6e\u4e0b\uff08\u7eaf\u6587\u672c\u3001\u5e26\u4e0a\u4e0b\u6587\u6587\u672c\u3001\u5e26\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\uff09\u8fdb\u884c\u516d\u79cd\u8c2c\u8bef\u7c7b\u578b\u7684\u5206\u7c7b\u3002", "result": "\u7406\u8bba\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u6dfb\u52a0\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\u901a\u5e38\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u60c5\u611f\u5143\u6570\u636e\u4f7f\u6a21\u578b\u504f\u5411\u5c06\u9648\u8ff0\u6807\u8bb0\u4e3a\"\u8bc9\u8bf8\u60c5\u611f\"\u8c2c\u8bef\uff0c\u6076\u5316\u903b\u8f91\u63a8\u7406\u3002\u57fa\u7840\u63d0\u793a\u901a\u5e38\u4f18\u4e8e\u589e\u5f3a\u63d0\u793a\u3002", "conclusion": "\u989d\u5916\u8f93\u5165\u7684\u6ce8\u610f\u529b\u5206\u6563\u53ef\u80fd\u6076\u5316\u800c\u975e\u6539\u5584LLM\u7684\u8c2c\u8bef\u5206\u7c7b\u6027\u80fd\uff0c\u60c5\u611f\u5143\u6570\u636e\u4f1a\u5f15\u5165\u504f\u89c1\uff0c\u57fa\u7840\u63d0\u793a\u7b56\u7565\u5f80\u5f80\u66f4\u6709\u6548\u3002", "topic": "agent analysis"}}
{"id": "2509.11141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11141", "abs": "https://arxiv.org/abs/2509.11141", "authors": ["Shiyao Cui", "Xijia Feng", "Yingkang Wang", "Junxiao Yang", "Zhexin Zhang", "Biplab Sikdar", "Hongning Wang", "Han Qiu", "Minlie Huang"], "title": "When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity", "comment": null, "summary": "Emojis are globally used non-verbal cues in digital communication, and\nextensive research has examined how large language models (LLMs) understand and\nutilize emojis across contexts. While usually associated with friendliness or\nplayfulness, it is observed that emojis may trigger toxic content generation in\nLLMs. Motivated by such a observation, we aim to investigate: (1) whether\nemojis can clearly enhance the toxicity generation in LLMs and (2) how to\ninterpret this phenomenon. We begin with a comprehensive exploration of\nemoji-triggered LLM toxicity generation by automating the construction of\nprompts with emojis to subtly express toxic intent. Experiments across 5\nmainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate\nthat prompts with emojis could easily induce toxicity generation. To understand\nthis phenomenon, we conduct model-level interpretations spanning semantic\ncognition, sequence generation and tokenization, suggesting that emojis can act\nas a heterogeneous semantic channel to bypass the safety mechanisms. To pursue\ndeeper insights, we further probe the pre-training corpus and uncover potential\ncorrelation between the emoji-related data polution with the toxicity\ngeneration behaviors. Supplementary materials provide our implementation code\nand data. (Warning: This paper contains potentially sensitive contents)", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8868\u60c5\u7b26\u53f7\u4f1a\u89e6\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6bd2\u5185\u5bb9\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6784\u5efa\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\uff0c\u57287\u4e2a\u4e3b\u6d41LLM\u548c5\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u4ece\u8bed\u4e49\u8ba4\u77e5\u3001\u5e8f\u5217\u751f\u6210\u7b49\u89d2\u5ea6\u8fdb\u884c\u4e86\u89e3\u91ca\u3002", "motivation": "\u89c2\u5bdf\u5230\u8868\u60c5\u7b26\u53f7\u53ef\u80fd\u89e6\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6bd2\u5185\u5bb9\uff0c\u65e8\u5728\u7814\u7a76\u8868\u60c5\u7b26\u53f7\u662f\u5426\u80fd\u660e\u663e\u589e\u5f3aLLM\u7684\u6bd2\u6027\u751f\u6210\u80fd\u529b\uff0c\u5e76\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6784\u5efa\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\u6765\u5fae\u5999\u8868\u8fbe\u6709\u6bd2\u610f\u56fe\uff0c\u57287\u4e2a\u8457\u540dLLM\u548c5\u79cd\u4e3b\u6d41\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u5c42\u9762\u7684\u8bed\u4e49\u8ba4\u77e5\u3001\u5e8f\u5217\u751f\u6210\u548c\u5206\u8bcd\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\u5bb9\u6613\u8bf1\u5bfc\u6bd2\u6027\u5185\u5bb9\u751f\u6210\uff0c\u8868\u60c5\u7b26\u53f7\u53ef\u4f5c\u4e3a\u5f02\u8d28\u8bed\u4e49\u901a\u9053\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u9884\u8bad\u7ec3\u8bed\u6599\u5206\u6790\u663e\u793a\u8868\u60c5\u7b26\u53f7\u76f8\u5173\u6570\u636e\u6c61\u67d3\u4e0e\u6bd2\u6027\u751f\u6210\u884c\u4e3a\u5b58\u5728\u6f5c\u5728\u5173\u8054\u3002", "conclusion": "\u8868\u60c5\u7b26\u53f7\u786e\u5b9e\u80fd\u589e\u5f3aLLM\u7684\u6bd2\u6027\u751f\u6210\uff0c\u8fd9\u79cd\u73b0\u8c61\u6e90\u4e8e\u8868\u60c5\u7b26\u53f7\u4f5c\u4e3a\u5f02\u8d28\u8bed\u4e49\u901a\u9053\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u7684\u80fd\u529b\uff0c\u4e0e\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u6709\u5173\u3002", "topic": "agent analysis"}}
{"id": "2509.11575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11575", "abs": "https://arxiv.org/abs/2509.11575", "authors": ["Ching Chang", "Yidan Shi", "Defu Cao", "Wei Yang", "Jeehyun Hwang", "Haixin Wang", "Jiacheng Pang", "Wei Wang", "Yan Liu", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models", "comment": "This paper is currently under review", "summary": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u5b9a\u4e49\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u95ee\u9898\uff0c\u6309\u63a8\u7406\u62d3\u6251\u7ed3\u6784\u5c06\u6587\u732e\u5206\u4e3a\u4e09\u7c7b\uff1a\u76f4\u63a5\u4e00\u6b65\u63a8\u7406\u3001\u7ebf\u6027\u94fe\u63a8\u7406\u548c\u5206\u652f\u7ed3\u6784\u63a8\u7406\uff0c\u5e76\u4ea4\u53c9\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u4e3b\u8981\u76ee\u6807\u3001\u65b9\u6cd5\u7cfb\u7edf\u548c\u8bc4\u4f30\u5b9e\u8df5\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5c06\u65f6\u95f4\u4f5c\u4e3a\u7b2c\u4e00\u7c7b\u8f74\uff0c\u5e76\u5c06\u4e2d\u95f4\u8bc1\u636e\u76f4\u63a5\u878d\u5165\u7b54\u6848\u4e2d\u3002\u8be5\u9886\u57df\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7ec4\u7ec7\u6846\u67b6\u6765\u7406\u89e3\u4e0d\u540c\u63a8\u7406\u62d3\u6251\u7ed3\u6784\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u63a8\u52a8\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u63a8\u7406\u62d3\u6251\u7ed3\u6784\uff08\u76f4\u63a5\u63a8\u7406\u3001\u7ebf\u6027\u94fe\u63a8\u7406\u3001\u5206\u652f\u7ed3\u6784\u63a8\u7406\uff09\u4e0e\u4e3b\u8981\u76ee\u6807\uff08\u4f20\u7edf\u5206\u6790\u3001\u89e3\u91ca\u7406\u89e3\u3001\u56e0\u679c\u63a8\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u751f\u6210\uff09\u7684\u4ea4\u53c9\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u89e3\u9a8c\u8bc1\u3001\u96c6\u6210\u3001\u5de5\u5177\u4f7f\u7528\u7b49\u6807\u7b7e\u96c6\u6765\u7ec4\u7ec7\u6587\u732e\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u6027\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u5728\u5fe0\u5b9e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\uff0c\u6574\u7406\u4e86\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u8d44\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u5b9e\u8df5\u548c\u8bbe\u8ba1\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7ed3\u6784\u9700\u8981\u5728 grounding \u548c\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u4e0e\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u590d\u73b0\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u672a\u6765\u8fdb\u5c55\u5c06\u4f9d\u8d56\u4e8e\u5c06\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7528\u6302\u94a9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5728\u79fb\u4f4d\u611f\u77e5\u3001\u6d41\u5f0f\u548c\u957f\u89c6\u91ce\u8bbe\u7f6e\u4e0b\u6743\u8861\u6210\u672c\u4e0e\u98ce\u9669\u7684\u95ed\u73af\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2509.11719", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11719", "abs": "https://arxiv.org/abs/2509.11719", "authors": ["Bingqing Wei", "Lianmin Chen", "Zhongyu Xia", "Yongtao Wang"], "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction", "comment": null, "summary": "Multi-agent trajectory prediction in autonomous driving requires a\ncomprehensive understanding of complex social dynamics. Existing methods,\nhowever, often struggle to capture the full richness of these dynamics,\nparticularly the co-existence of multi-scale interactions and the diverse\nbehaviors of heterogeneous agents. To address these challenges, this paper\nintroduces HeLoFusion, an efficient and scalable encoder for modeling\nheterogeneous and multi-scale agent interactions. Instead of relying on global\ncontext, HeLoFusion constructs local, multi-scale graphs centered on each\nagent, allowing it to effectively model both direct pairwise dependencies and\ncomplex group-wise interactions (\\textit{e.g.}, platooning vehicles or\npedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of\nagent heterogeneity through an aggregation-decomposition message-passing scheme\nand type-specific feature networks, enabling it to learn nuanced,\ntype-dependent interaction patterns. This locality-focused approach enables a\nprincipled representation of multi-level social context, yielding powerful and\nexpressive agent embeddings. On the challenging Waymo Open Motion Dataset,\nHeLoFusion achieves state-of-the-art performance, setting new benchmarks for\nkey metrics including Soft mAP and minADE. Our work demonstrates that a\nlocality-grounded architecture, which explicitly models multi-scale and\nheterogeneous interactions, is a highly effective strategy for advancing motion\nforecasting.", "AI": {"tldr": "HeLoFusion\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u7684\u9ad8\u6548\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6784\u5efa\u5c40\u90e8\u591a\u5c3a\u5ea6\u56fe\u6765\u5efa\u6a21\u5f02\u6784\u548c\u591a\u5c3a\u5ea6\u4ea4\u4e92\uff0c\u5728Waymo\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u590d\u6742\u7684\u793e\u4f1a\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\u591a\u5c3a\u5ea6\u4ea4\u4e92\u7684\u5171\u5b58\u548c\u5f02\u6784\u667a\u80fd\u4f53\u7684\u591a\u6837\u5316\u884c\u4e3a\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u5efa\u6a21\u8fd9\u4e9b\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u6784\u5efa\u4ee5\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u5c40\u90e8\u591a\u5c3a\u5ea6\u56fe\uff0c\u4f7f\u7528\u805a\u5408-\u5206\u89e3\u6d88\u606f\u4f20\u9012\u65b9\u6848\u548c\u7c7b\u578b\u7279\u5b9a\u7279\u5f81\u7f51\u7edc\u6765\u5904\u7406\u667a\u80fd\u4f53\u5f02\u6784\u6027\uff0c\u5efa\u6a21\u76f4\u63a5\u6210\u5bf9\u4f9d\u8d56\u548c\u590d\u6742\u7fa4\u4f53\u4ea4\u4e92\u3002", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3aSoft mAP\u548cminADE\u7b49\u5173\u952e\u6307\u6807\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u57fa\u4e8e\u5c40\u90e8\u6027\u3001\u663e\u5f0f\u5efa\u6a21\u591a\u5c3a\u5ea6\u548c\u5f02\u6784\u4ea4\u4e92\u7684\u67b6\u6784\u662f\u63a8\u8fdb\u8fd0\u52a8\u9884\u6d4b\u7684\u6709\u6548\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2509.11914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11914", "abs": "https://arxiv.org/abs/2509.11914", "authors": ["Yiqun Yao", "Naitong Yu", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Wenjia Ma", "Xuying Meng", "Jing Li", "Aixin Sun", "Yequan Wang"], "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models", "comment": null, "summary": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.", "AI": {"tldr": "EgoMem\u662f\u9996\u4e2a\u4e3a\u5168\u53cc\u5de5\u6a21\u578b\u8bbe\u8ba1\u7684\u7ec8\u8eab\u8bb0\u5fc6\u4ee3\u7406\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u89c6\u542c\u6d41\u4e2d\u5b9e\u65f6\u8bc6\u522b\u591a\u7528\u6237\u3001\u63d0\u4f9b\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u5e76\u7ef4\u62a4\u7528\u6237\u957f\u671f\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u8bb0\u5fc6\u4ee3\u7406\u4e3b\u8981\u9488\u5bf9LLMs\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5904\u7406\u539f\u59cb\u89c6\u542c\u6d41\u3002EgoMem\u65e8\u5728\u89e3\u51b3\u7ec8\u8eab\u3001\u5b9e\u65f6\u548c\u5177\u8eab\u573a\u666f\u4e2d\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u5f02\u6b65\u8fdb\u7a0b\uff1a\u68c0\u7d22\u8fc7\u7a0b\uff08\u52a8\u6001\u8bc6\u522b\u7528\u6237\u5e76\u6536\u96c6\u4e0a\u4e0b\u6587\uff09\u3001\u5168\u6a21\u6001\u5bf9\u8bdd\u8fc7\u7a0b\uff08\u751f\u6210\u4e2a\u6027\u5316\u97f3\u9891\u54cd\u5e94\uff09\u3001\u5185\u5b58\u7ba1\u7406\u8fc7\u7a0b\uff08\u68c0\u6d4b\u5bf9\u8bdd\u8fb9\u754c\u5e76\u66f4\u65b0\u957f\u671f\u8bb0\u5fc6\uff09\u3002", "result": "\u68c0\u7d22\u548c\u5185\u5b58\u7ba1\u7406\u6a21\u5757\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u4e0eRoboEgo\u5168\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\u96c6\u6210\u540e\uff0c\u5b9e\u65f6\u4e2a\u6027\u5316\u5bf9\u8bdd\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f97\u5206\u8d85\u8fc787%\u3002", "conclusion": "EgoMem\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u5927\u57fa\u7ebf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ec8\u8eab\u3001\u5b9e\u65f6\u548c\u5177\u8eab\u573a\u666f\u3002", "topic": "agent analysis"}}
{"id": "2509.11943", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11943", "abs": "https://arxiv.org/abs/2509.11943", "authors": ["Antonin Sulc", "Thorsten Hellert"], "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics", "comment": "10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at\n  NeuralIPS", "summary": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4f7f\u7528Kripke\u6a21\u578b\u8868\u793a\u4fe1\u5ff5\u72b6\u6001\uff0c\u7ed3\u5408\u6a21\u6001\u903b\u8f91\u8fdb\u884c\u63a8\u7406\uff0c\u5728\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\u6210\u529f\u8bca\u65ad\u590d\u6742\u7ea7\u8054\u6545\u969c", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u4f46\u5ffd\u89c6\u4e86\u5728\u73af\u5883\u4e2d\u6269\u5c55\u667a\u80fd\u4f53\u63a8\u7406\u7ed3\u6784\u3001\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u662fAI\u7814\u7a76\u4e2d\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u4e2a\u4f53\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u72b6\u6001\u5f62\u5f0f\u5316\u4e3aKripke\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u6001\u903b\u8f91\u8fdb\u884c\u53ef\u80fd\u6027\u548c\u5fc5\u8981\u6027\u63a8\u7406\uff0c\u5229\u7528\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u4f5c\u4e3a\u903b\u8f91\u7ea6\u675f\u6765\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u7684\u5047\u8bbe\u751f\u6210", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u8bca\u65ad\u4e86\u590d\u6742\u7684\u7ea7\u8054\u6545\u969c\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u4e49\u76f4\u89c9\u4e0e\u6a21\u6001\u903b\u8f91\u7684\u4e25\u683c\u53ef\u9a8c\u8bc1\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u9760\u548c\u53ef\u9a8c\u8bc1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u903b\u8f91\u7ea6\u675f\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u5f97\u51fa\u7269\u7406\u6216\u903b\u8f91\u4e0a\u4e0d\u53ef\u884c\u7684\u7ed3\u8bba", "topic": "agent analysis"}}
{"id": "2509.11514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11514", "abs": "https://arxiv.org/abs/2509.11514", "authors": ["Zhengxiang Wang", "Weiling Li", "Panagiotis Kaliosis", "Owen Rambow", "Susan E. Brennan"], "title": "LVLMs are Bad at Overhearing Human Referential Communication", "comment": "EMNLP 2025 (Main)", "summary": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e867\u4e2a\u5148\u8fdb\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u81ea\u53d1\u5bf9\u8bdd\u4e2d\u5f15\u7528\u8868\u8fbe\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u534f\u4f5c\u5bf9\u8c61\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u591a\u6b21\u5bf9\u8bdd\u5b66\u4e60\u6539\u8fdb\u3002", "motivation": "\u7406\u89e3\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u5f15\u7528\u8868\u8fbe\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6574\u5408\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u5bf9\u8bdd\u4ea4\u4e92\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4eba\u7c7b\u81ea\u53d1\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u8ba9LVLMs\u4f5c\u4e3a\u65c1\u542c\u8005\u89c2\u5bdf\u4eba\u7c7b\u5728\u534f\u4f5c\u5bf9\u8c61\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5f53\u524dLVLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u6311\u6218\u6027\uff0c\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u901a\u8fc7\u591a\u6b21\u89c2\u5bdf\u540c\u4e00\u53c2\u4e0e\u8005\u7684\u91cd\u590d\u5bf9\u8bdd\u83b7\u5f97\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u4efb\u52a1\u5bf9\u73b0\u6709LVLMs\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u5bf9\u8bdd\u7406\u89e3\u548c\u5b66\u4e60\u673a\u5236\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86\u8bed\u6599\u5e93\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.10753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10753", "abs": "https://arxiv.org/abs/2509.10753", "authors": ["Minh Vu", "Brian K. Tran", "Syed A. Shah", "Geigh Zollicoffer", "Nhat Hoang-Xuan", "Manish Bhattarai"], "title": "HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling", "comment": null, "summary": "Large Language Models (LLMs) exhibit impressive reasoning and\nquestion-answering capabilities. However, they often produce inaccurate or\nunreliable content known as hallucinations. This unreliability significantly\nlimits their deployment in high-stakes applications. Thus, there is a growing\nneed for a general-purpose method to detect hallucinations in LLMs. In this\nwork, we introduce HalluField, a novel field-theoretic approach for\nhallucination detection based on a parametrized variational principle and\nthermodynamics. Inspired by thermodynamics, HalluField models an LLM's response\nto a given query and temperature setting as a collection of discrete likelihood\ntoken paths, each associated with a corresponding energy and entropy. By\nanalyzing how energy and entropy distributions vary across token paths under\nchanges in temperature and likelihood, HalluField quantifies the semantic\nstability of a response. Hallucinations are then detected by identifying\nunstable or erratic behavior in this energy landscape. HalluField is\ncomputationally efficient and highly practical: it operates directly on the\nmodel's output logits without requiring fine-tuning or auxiliary neural\nnetworks. Notably, the method is grounded in a principled physical\ninterpretation, drawing analogies to the first law of thermodynamics.\nRemarkably, by modeling LLM behavior through this physical lens, HalluField\nachieves state-of-the-art hallucination detection performance across models and\ndatasets.", "AI": {"tldr": "HalluField\u662f\u4e00\u79cd\u57fa\u4e8e\u70ed\u529b\u5b66\u573a\u8bba\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790LLM\u8f93\u51fatoken\u8def\u5f84\u7684\u80fd\u91cf\u548c\u71b5\u5206\u5e03\u6765\u91cf\u5316\u8bed\u4e49\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u3002", "motivation": "LLM\u5728\u751f\u6210\u5185\u5bb9\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u4e0d\u53ef\u9760\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u5316\u53d8\u5206\u539f\u7406\u548c\u70ed\u529b\u5b66\uff0c\u5c06LLM\u5bf9\u67e5\u8be2\u548c\u6e29\u5ea6\u8bbe\u7f6e\u7684\u54cd\u5e94\u5efa\u6a21\u4e3a\u79bb\u6563\u4f3c\u7136token\u8def\u5f84\u96c6\u5408\uff0c\u5206\u6790\u80fd\u91cf\u548c\u71b5\u5206\u5e03\u968f\u6e29\u5ea6\u548c\u4f3c\u7136\u53d8\u5316\u7684\u60c5\u51b5\u6765\u68c0\u6d4b\u8bed\u4e49\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "HalluField\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u65e0\u9700\u5fae\u8c03\u6216\u8f85\u52a9\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u901a\u8fc7\u70ed\u529b\u5b66\u7269\u7406\u89c6\u89d2\u5efa\u6a21LLM\u884c\u4e3a\uff0cHalluField\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u7406\u6027\u5f3a\u3001\u5b9e\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2509.11619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11619", "abs": "https://arxiv.org/abs/2509.11619", "authors": ["Spandan Anaokar", "Shrey Ganatra", "Harshvivek Kashid", "Swapnil Bhattacharyya", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems", "comment": "6 pages + references + appendix, 3 figures, 2 tables", "summary": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset", "AI": {"tldr": "\u5f00\u53d1\u4e86HalluDetect\u5e7b\u89c9\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5728\u6d88\u8d39\u8005\u6295\u8bc9\u804a\u5929\u673a\u5668\u4eba\u4e2d\u5b9e\u73b069%\u7684F1\u5206\u6570\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534725.44%\u3002AgentBot\u67b6\u6784\u5c06\u5e7b\u89c9\u964d\u81f3\u6bcf\u8f6e0.4159\u6b21\uff0c\u540c\u65f6\u4fdd\u630196.13%\u7684\u6700\u9ad8token\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u4f46\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u6d88\u8d39\u8005\u6295\u8bc9\u5904\u7406\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002", "method": "\u57fa\u4e8eLLaMA 3.1 8B Instruct\u6a21\u578b\u5f00\u53d1HalluDetect\u5e7b\u89c9\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5bf9\u4e94\u79cd\u804a\u5929\u673a\u5668\u4eba\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8AgentBot\u67b6\u6784\u7684\u6027\u80fd\u3002", "result": "HalluDetect\u7cfb\u7edf\u8fbe\u523069%\u7684F1\u5206\u6570\uff0c\u6bd4\u57fa\u7ebf\u68c0\u6d4b\u5668\u63d0\u534725.44%\u3002AgentBot\u67b6\u6784\u5c06\u5e7b\u89c9\u964d\u81f3\u6bcf\u8f6e0.4159\u6b21\uff0ctoken\u51c6\u786e\u7387\u8fbe\u523096.13%\u3002", "conclusion": "\u4f18\u5316\u7684\u63a8\u7406\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u589e\u5f3a\u5bf9LLM\u9a71\u52a8\u52a9\u624b\u7684\u4fe1\u4efb\u3002", "topic": "agent analysis"}}
{"id": "2509.10970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10970", "abs": "https://arxiv.org/abs/2509.10970", "authors": ["Joshua Au Yeung", "Jacopo Dalmasso", "Luca Foschini", "Richard JB Dobson", "Zeljko Kraljevic"], "title": "The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models", "comment": null, "summary": "Background: Emerging reports of \"AI psychosis\" are on the rise, where\nuser-LLM interactions may exacerbate or induce psychosis or adverse\npsychological symptoms. The sycophantic and agreeable nature of LLMs can\nbeneficial, it can become a vector for harm by reinforcing delusional beliefs\nin vulnerable users.\n  Methods: We introduce psychosis-bench, a novel benchmark designed to\nsystematically evaluate the psychogenicity of LLMs comprimising 16 structured,\n12-turn conversational scenarios simulating the progression of delusional\nthemes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)\nand potential harms. We evaluated eight prominent LLMs for Delusion\nConfirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across\nexplicit and implicit conversational contexts.\n  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated\npsychogenic potential, showing a strong tendency to perpetuate rather than\nchallenge delusions (mean DCS of 0.91 $\\pm$0.88). Models frequently enabled\nharmful user requests (mean HES of 0.69 $\\pm$0.84) and offered safety\ninterventions in only roughly a third of applicable turns (mean SIS of 0.37\n$\\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.\nPerformance was significantly worse in implicit scenarios, models were more\nlikely to confirm delusions and enable harm while offering fewer interventions\n(p < .001). A strong correlation was found between DCS and HES (rs = .77).\nModel performance varied widely, indicating that safety is not an emergent\nproperty of scale alone.\n  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk\nand underscores the urgent need for re-thinking how we train LLMs. We frame\nthis issue not merely as a technical challenge but as a public health\nimperative requiring collaboration between developers, policymakers, and\nhealthcare professionals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86psychosis-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u7cbe\u795e\u75c5\u6027\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u786e\u8ba4\u5984\u60f3\u3001\u4fc3\u6210\u4f24\u5bb3\u884c\u4e3a\u7684\u503e\u5411\uff0c\u5b89\u5168\u5e72\u9884\u4e25\u91cd\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\"AI\u7cbe\u795e\u75c5\"\u6848\u4f8b\u7684\u589e\u52a0\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4e0e\u6613\u611f\u7528\u6237\u4e92\u52a8\u65f6\u53ef\u80fd\u52a0\u5267\u6216\u8bf1\u53d1\u7cbe\u795e\u75c5\u75c7\u72b6\u7684\u98ce\u9669\uff0c\u7279\u522b\u662fLLM\u7684\u987a\u4ece\u6027\u53ef\u80fd\u5f3a\u5316\u5984\u60f3\u4fe1\u5ff5\u3002", "method": "\u521b\u5efa\u5305\u542b16\u4e2a\u7ed3\u6784\u5316\u300112\u8f6e\u5bf9\u8bdd\u573a\u666f\u7684psychosis-bench\u57fa\u51c6\uff0c\u6a21\u62df\u4e09\u7c7b\u5984\u60f3\u4e3b\u9898\u7684\u8fdb\u5c55\uff0c\u8bc4\u4f308\u4e2a\u4e3b\u6d41LLM\u5728\u5984\u60f3\u786e\u8ba4\u3001\u4f24\u5bb3\u4fc3\u6210\u548c\u5b89\u5168\u5e72\u9884\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709LLM\u90fd\u8868\u73b0\u51fa\u7cbe\u795e\u81f4\u75c5\u6f5c\u529b\uff0c\u5e73\u5747\u5984\u60f3\u786e\u8ba4\u5f97\u52060.91\u00b10.88\uff0c\u4f24\u5bb3\u4fc3\u6210\u5f97\u52060.69\u00b10.84\uff0c\u5b89\u5168\u5e72\u9884\u4ec5\u5728\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u76f8\u5173\u8f6e\u6b21\u4e2d\u51fa\u73b0\uff08\u5f97\u52060.37\u00b10.48\uff09\u3002\u9690\u5f0f\u573a\u666f\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "LLM\u7684\u7cbe\u795e\u81f4\u75c5\u6027\u662f\u53ef\u91cf\u5316\u7684\u98ce\u9669\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003LLM\u8bad\u7ec3\u65b9\u5f0f\uff0c\u8fd9\u4e0d\u4ec5\u662f\u6280\u672f\u6311\u6218\uff0c\u66f4\u662f\u9700\u8981\u5f00\u53d1\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u533b\u7597\u4e13\u4e1a\u4eba\u58eb\u5408\u4f5c\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.11233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11233", "abs": "https://arxiv.org/abs/2509.11233", "authors": ["Emil Malmsten", "Wendelin B\u00f6hmer"], "title": "TransZero: Parallel Tree Expansion in MuZero using Transformer Networks", "comment": "Submitted to BNAIC/BeNeLearn 2025. 15 pages, 4 figures", "summary": "We present TransZero, a model-based reinforcement learning algorithm that\nremoves the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike\nMuZero, which constructs its search tree step by step using a recurrent\ndynamics model, TransZero employs a transformer-based network to generate\nmultiple latent future states simultaneously. Combined with the Mean-Variance\nConstrained (MVC) evaluator that eliminates dependence on inherently sequential\nvisitation counts, our approach enables the parallel expansion of entire\nsubtrees during planning. Experiments in MiniGrid and LunarLander show that\nTransZero achieves up to an eleven-fold speedup in wall-clock time compared to\nMuZero while maintaining sample efficiency. These results demonstrate that\nparallel tree construction can substantially accelerate model-based\nreinforcement learning, bringing real-time decision-making in complex\nenvironments closer to practice. The code is publicly available on GitHub.", "AI": {"tldr": "TransZero\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7Transformer\u7f51\u7edc\u5e76\u884c\u751f\u6210\u591a\u4e2a\u6f5c\u5728\u672a\u6765\u72b6\u6001\uff0c\u6d88\u9664\u4e86MCTS\u4e2d\u7684\u987a\u5e8f\u74f6\u9888\uff0c\u76f8\u6bd4MuZero\u5b9e\u73b0\u4e8611\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u4e2d\u7684\u987a\u5e8f\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u5e76\u884c\u6811\u6784\u5efa\u4ee5\u52a0\u901f\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u66f4\u63a5\u8fd1\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f51\u7edc\u540c\u65f6\u751f\u6210\u591a\u4e2a\u6f5c\u5728\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408\u5747\u503c-\u65b9\u5dee\u7ea6\u675f(MVC)\u8bc4\u4f30\u5668\u6d88\u9664\u5bf9\u987a\u5e8f\u8bbf\u95ee\u8ba1\u6570\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u6574\u4e2a\u5b50\u6811\u7684\u5e76\u884c\u6269\u5c55\u3002", "result": "\u5728MiniGrid\u548cLunarLander\u73af\u5883\u4e2d\uff0cTransZero\u76f8\u6bd4MuZero\u5b9e\u73b0\u4e86\u9ad8\u8fbe11\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u5e76\u884c\u6811\u6784\u5efa\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12108", "abs": "https://arxiv.org/abs/2509.12108", "authors": ["Min Zeng", "Jinfei Sun", "Xueyou Luo", "Caiquan Liu", "Shiqi Zhang", "Li Xie", "Xiaoxin Chen"], "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "comment": "Accepted at EMNLP 2025", "summary": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.", "AI": {"tldr": "GTA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u7684\u6548\u7387\u4e0e\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u80fd\u529b\u589e\u76ca\uff0c\u91c7\u7528\u5148\u731c\u6d4b\u540e\u601d\u8003\u518d\u56de\u7b54\u7684\u4e09\u6b65\u6d41\u7a0b\uff0c\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u7eafRL\u66f4\u5feb\u6536\u655b\u548c\u6bd4\u7eafSFT\u66f4\u9ad8\u6027\u80fd\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u7eafRL\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u6536\u655b\u6162\uff0c\u4ee5\u53ca\u7eafSFT\u65b9\u6cd5\u6027\u80fd\u4e0a\u9650\u6709\u9650\u3001\u7406\u8bba\u57fa\u7840\u8584\u5f31\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u6548\u7387\u4e0e\u80fd\u529b\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51faGuess-Think-Answer\u6846\u67b6\uff1a\u6a21\u578b\u5148\u4ea7\u751f\u4e34\u65f6\u731c\u6d4b\uff08\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u4f18\u5316\uff09\uff0c\u7136\u540e\u53cd\u601d\u8be5\u731c\u6d4b\uff0c\u6700\u540e\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff0c\u4f7f\u7528RL\u5956\u52b1\u6765\u5851\u9020\u6700\u7ec8\u8f93\u51fa\u548c\u6574\u4e2aGTA\u7ed3\u6784\u7684\u683c\u5f0f\u3002\u91c7\u7528\u635f\u5931\u63a9\u7801\u548c\u68af\u5ea6\u7ea6\u675f\u6765\u7f13\u89e3\u4e24\u79cd\u8bad\u7ec3\u4fe1\u53f7\u4e4b\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u3002", "result": "\u5728\u56db\u4e2a\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cGTA\u663e\u8457\u52a0\u901f\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u8d85\u8d8a\u4e86\u72ec\u7acb\u7684SFT\u548cRL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GTA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86SFT\u8bad\u7ec3\u6548\u7387\u4e0eRL\u80fd\u529b\u589e\u76ca\u7684\u7ed3\u5408\uff0c\u4e3a\u89e3\u51b3\u6548\u7387-\u80fd\u529b\u6743\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u8bad\u7ec3\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.11259", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11259", "abs": "https://arxiv.org/abs/2509.11259", "authors": ["David Schiff", "Ofir Lindenbaum", "Yonathan Efroni"], "title": "Gradient Free Deep Reinforcement Learning With TabPFN", "comment": null, "summary": "Gradient based optimization is fundamental to most modern deep reinforcement\nlearning algorithms, however, it introduces significant sensitivity to\nhyperparameters, unstable training dynamics, and high computational costs. We\npropose TabPFN RL, a novel gradient free deep RL framework that repurposes the\nmeta trained transformer TabPFN as a Q function approximator. Originally\ndeveloped for tabular classification, TabPFN is a transformer pre trained on\nmillions of synthetic datasets to perform inference on new unseen datasets via\nin context learning. Given an in context dataset of sample label pairs and new\nunlabeled data, it predicts the most likely labels in a single forward pass,\nwithout gradient updates or task specific fine tuning. We use TabPFN to predict\nQ values using inference only, thereby eliminating the need for back\npropagation at both training and inference. To cope with the model's fixed\ncontext budget, we design a high reward episode gate that retains only the top\n5% of trajectories. Empirical evaluations on the Gymnasium classic control\nsuite demonstrate that TabPFN RL matches or surpasses Deep Q Network on\nCartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent\nor any extensive hyperparameter tuning. We discuss the theoretical aspects of\nhow bootstrapped targets and non stationary visitation distributions violate\nthe independence assumptions encoded in TabPFN's prior, yet the model retains a\nsurprising generalization capacity. We further formalize the intrinsic context\nsize limit of in context RL algorithms and propose principled truncation\nstrategies that enable continual learning when the context is full. Our results\nestablish prior fitted networks such as TabPFN as a viable foundation for fast\nand computationally efficient RL, opening new directions for gradient free RL\nwith large pre trained transformers.", "AI": {"tldr": "TabPFN RL\u662f\u4e00\u4e2a\u65e0\u68af\u5ea6\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3transformer TabPFN\u4f5c\u4e3aQ\u51fd\u6570\u8fd1\u4f3c\u5668\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u63a8\u7406\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u6216\u5fae\u8c03\uff0c\u5728\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8aDQN\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u8d85\u53c2\u6570\u654f\u611f\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u68af\u5ea6\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u5c06\u539f\u672c\u7528\u4e8e\u8868\u683c\u5206\u7c7b\u7684TabPFN transformer\u91cd\u65b0\u7528\u4e8eQ\u503c\u9884\u6d4b\uff0c\u8bbe\u8ba1\u9ad8\u5956\u52b1\u8f68\u8ff9\u95e8\u63a7\u673a\u5236\u4fdd\u7559top 5%\u8f68\u8ff9\uff0c\u5904\u7406\u56fa\u5b9a\u4e0a\u4e0b\u6587\u9884\u7b97\u9650\u5236\u3002", "result": "\u5728Gymnasium\u7ecf\u5178\u63a7\u5236\u5957\u4ef6\uff08CartPole v1\u3001MountainCar v0\u3001Acrobot v1\uff09\u4e0a\u5339\u914d\u6216\u8d85\u8d8aDeep Q Network\u6027\u80fd\uff0c\u65e0\u9700\u68af\u5ea6\u4e0b\u964d\u6216\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "TabPFN\u7b49\u5148\u9a8c\u62df\u5408\u7f51\u7edc\u4e3a\u5feb\u901f\u8ba1\u7b97\u9ad8\u6548\u7684RL\u63d0\u4f9b\u4e86\u53ef\u884c\u57fa\u7840\uff0c\u5f00\u8f9f\u4e86\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3transformer\u8fdb\u884c\u65e0\u68af\u5ea6RL\u7684\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12168", "abs": "https://arxiv.org/abs/2509.12168", "authors": ["Timothy Rupprecht", "Enfu Nan", "Arash Akbari", "Arman Akbari", "Lei Lu", "Priyanka Maan", "Sean Duffy", "Pu Zhao", "Yumei He", "David Kaeli", "Yanzhi Wang"], "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing", "comment": null, "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.", "AI": {"tldr": "\u63d0\u51faRAGs-to-Riches\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u6539\u8fdbLLM\u89d2\u8272\u626e\u6f14\uff0c\u5728\u654c\u5bf9\u7528\u6237\u4ea4\u4e92\u4e2d\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u63d0\u9ad835%\u7684\u53c2\u8003\u6f14\u793a\u6807\u8bb0\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u89d2\u8272\u626e\u6f14\u4e2d\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u7a81\u7834\u89d2\u8272\uff0c\u7279\u522b\u662f\u5728\u4e0e\u654c\u5bf9\u7528\u6237\u4ea4\u4e92\u65f6\u53ef\u80fd\u4ea7\u751f\u6709\u5bb3\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u89d2\u8272\u4e00\u81f4\u6027\u3002", "method": "\u5c06LLM\u89d2\u8272\u626e\u6f14\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6587\u672c\u68c0\u7d22\u95ee\u9898\uff0c\u5229\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u53c2\u8003\u6f14\u793a\u6765\u8c03\u8282LLM\u54cd\u5e94\uff0c\u63d0\u51faIOO\u548cIOR\u4e24\u4e2a\u65b0\u7684token\u7ea7ROUGE\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u4e0e\u654c\u5bf9\u7528\u6237\u6a21\u62df\u4ea4\u4e92\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u591a\u5229\u752835%\u7684\u53c2\u8003\u6f14\u793a\u6807\u8bb0\uff0c\u5728453\u6b21\u89d2\u8272\u626e\u6f14\u4ea4\u4e92\u4e2d\u88ab\u4e00\u81f4\u8bc4\u4e3a\u66f4\u771f\u5b9e\u3001\u66f4\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u3002", "conclusion": "RAGs-to-Riches\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u6784\u5efa\u7a33\u5065\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684LLM\u89d2\u8272\u626e\u6f14\u6846\u67b6\uff0c\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2509.11357", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.11357", "abs": "https://arxiv.org/abs/2509.11357", "authors": ["Yahav Bechavod", "Jiuyao Lu", "Aaron Roth"], "title": "Online Omniprediction with Long-Term Constraints", "comment": null, "summary": "We introduce and study the problem of online omniprediction with long-term\nconstraints. At each round, a forecaster is tasked with generating predictions\nfor an underlying (adaptively, adversarially chosen) state that are broadcast\nto a collection of downstream agents, who must each choose an action. Each of\nthe downstream agents has both a utility function mapping actions and state to\nutilities, and a vector-valued constraint function mapping actions and states\nto vector-valued costs. The utility and constraint functions can arbitrarily\ndiffer across downstream agents. Their goal is to choose actions that guarantee\nthemselves no regret while simultaneously guaranteeing that they do not\ncumulatively violate the constraints across time. We show how to make a single\nset of predictions so that each of the downstream agents can guarantee this by\nacting as a simple function of the predictions, guaranteeing each of them\n$\\tilde{O}(\\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We\nalso show how to extend our guarantees to arbitrary intersecting contextually\ndefined \\emph{subsequences}, guaranteeing each agent both regret and constraint\nviolation bounds not just marginally, but simultaneously on each subsequence,\nagainst a benchmark set of actions simultaneously tailored to each subsequence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u5168\u9884\u6d4b\u4e0e\u957f\u671f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u4e00\u9884\u6d4b\u96c6\u4f7f\u4e0b\u6e38\u4ee3\u7406\u5728\u4fdd\u8bc1\u65e0\u9057\u61be\u7684\u540c\u65f6\u6ee1\u8db3\u7d2f\u79ef\u7ea6\u675f\uff0c\u5e76\u6269\u5c55\u5230\u4efb\u610f\u5b50\u5e8f\u5217\u7684\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5728\u81ea\u9002\u5e94\u5bf9\u6297\u73af\u5883\u4e0b\uff0c\u591a\u4e2a\u4e0b\u6e38\u4ee3\u7406\u9700\u8981\u57fa\u4e8e\u9884\u6d4b\u9009\u62e9\u884c\u52a8\uff0c\u540c\u65f6\u4fdd\u8bc1\u65e0\u9057\u61be\u548c\u7d2f\u79ef\u7ea6\u675f\u4e0d\u8fdd\u53cd\u7684\u95ee\u9898\u3002\u4e0d\u540c\u4ee3\u7406\u7684\u6548\u7528\u548c\u7ea6\u675f\u51fd\u6570\u53ef\u80fd\u5b8c\u5168\u4e0d\u540c\uff0c\u9700\u8981\u7edf\u4e00\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u5355\u4e00\u9884\u6d4b\u96c6\uff0c\u4f7f\u6bcf\u4e2a\u4e0b\u6e38\u4ee3\u7406\u901a\u8fc7\u7b80\u5355\u51fd\u6570\u6620\u5c04\u9884\u6d4b\u6765\u9009\u62e9\u884c\u52a8\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u4ee3\u7406\u7684\u9057\u61be\u4e3aO(\u221aT)\u4e14\u7d2f\u79ef\u7ea6\u675f\u8fdd\u53cd\u4e3aO(1)\u3002\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u4efb\u610f\u4e0a\u4e0b\u6587\u5b9a\u4e49\u7684\u76f8\u4ea4\u5b50\u5e8f\u5217\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u8bc1\u6bcf\u4e2a\u4e0b\u6e38\u4ee3\u7406\u83b7\u5f97O(\u221aT)\u7684\u9057\u61be\u754c\u548cO(1)\u7684\u7d2f\u79ef\u7ea6\u675f\u8fdd\u53cd\u754c\uff0c\u540c\u65f6\u5728\u6240\u6709\u5b50\u5e8f\u5217\u4e0a\u90fd\u80fd\u4fdd\u6301\u8fd9\u4e9b\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebf\u5168\u9884\u6d4b\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u4ee3\u7406\u73af\u5883\u4e0b\u7684\u957f\u671f\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u4e0b\u6e38\u4ee3\u7406\u63d0\u4f9b\u7edf\u4e00\u7684\u9884\u6d4b\u670d\u52a1\uff0c\u786e\u4fdd\u4ed6\u4eec\u5728\u5404\u79cd\u5b50\u5e8f\u5217\u4e0a\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2509.11367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11367", "abs": "https://arxiv.org/abs/2509.11367", "authors": ["Chang-Hwan Lee", "Alexander Shim"], "title": "Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures", "comment": "28 pages, 3 figures, 17 tables", "summary": "Reinforcement learning (RL) agents typically assume stationary environment\ndynamics. Yet in real-world applications such as healthcare, robotics, and\nfinance, transition probabilities or reward functions may evolve, leading to\nmodel drift. This paper proposes a novel framework to detect such drifts by\nanalyzing the distributional changes in sequences of agent behavior.\nSpecifically, we introduce a suite of edit operation-based measures to quantify\ndeviations between state-action trajectories generated under stationary and\nperturbed conditions. Our experiments demonstrate that these measures can\neffectively distinguish drifted from non-drifted scenarios, even under varying\nlevels of noise, providing a practical tool for drift detection in\nnon-stationary RL environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f16\u8f91\u64cd\u4f5c\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u68c0\u6d4b\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6a21\u578b\u6f02\u79fb\uff0c\u901a\u8fc7\u5206\u6790\u667a\u80fd\u4f53\u884c\u4e3a\u5e8f\u5217\u7684\u5206\u5e03\u53d8\u5316\u6765\u8bc6\u522b\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff08\u5982\u533b\u7597\u3001\u673a\u5668\u4eba\u3001\u91d1\u878d\uff09\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u5f80\u5f80\u662f\u975e\u5e73\u7a33\u7684\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u5047\u8bbe\u73af\u5883\u52a8\u6001\u662f\u9759\u6b62\u7684\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u73af\u5883\u52a8\u6001\u7684\u53d8\u5316\u3002", "method": "\u5f15\u5165\u4e00\u5957\u57fa\u4e8e\u7f16\u8f91\u64cd\u4f5c\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u5728\u9759\u6b62\u6761\u4ef6\u548c\u6270\u52a8\u6761\u4ef6\u4e0b\u751f\u6210\u7684\u72b6\u6001-\u52a8\u4f5c\u8f68\u8ff9\u4e4b\u95f4\u7684\u504f\u5dee\u6765\u68c0\u6d4b\u6f02\u79fb\u3002\u8fd9\u4e9b\u5ea6\u91cf\u65b9\u6cd5\u5206\u6790\u667a\u80fd\u4f53\u884c\u4e3a\u5e8f\u5217\u7684\u5206\u5e03\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u57fa\u4e8e\u7f16\u8f91\u64cd\u4f5c\u7684\u5ea6\u91cf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206\u6f02\u79fb\u548c\u975e\u6f02\u79fb\u573a\u666f\uff0c\u5373\u4f7f\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u566a\u58f0\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u6f02\u79fb\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u8fdb\u884c\u6f02\u79fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u667a\u80fd\u4f53\u9002\u5e94\u73af\u5883\u52a8\u6001\u7684\u53d8\u5316\u3002", "topic": "agent analysis"}}
{"id": "2509.11967", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11967", "abs": "https://arxiv.org/abs/2509.11967", "authors": ["Harold Triedman", "Vitaly Shmatikov"], "title": "MillStone: How Open-Minded Are LLMs?", "comment": "19 pages, 7 tables, 7 figures", "summary": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.", "AI": {"tldr": "MillStone\u662f\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u6d4b\u91cf\u5916\u90e8\u8bba\u70b9\u5bf9LLM\u5728\u4e89\u8bae\u95ee\u9898\u4e0a\u7acb\u573a\u5f71\u54cd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0LLM\u5728\u5927\u591a\u6570\u95ee\u9898\u4e0a\u601d\u60f3\u5f00\u653e\uff0c\u6743\u5a01\u4fe1\u606f\u6e90\u5bb9\u6613\u5f71\u54cd\u5176\u7acb\u573a\u3002", "motivation": "\u968f\u7740\u7528\u6237\u5f00\u59cb\u4f9d\u8d56LLM\u83b7\u53d6\u5305\u62ec\u4e89\u8bae\u6027\u8bdd\u9898\u5728\u5185\u7684\u5404\u79cd\u4fe1\u606f\uff0c\u9700\u8981\u7406\u89e3LLM\u8f93\u51fa\u4e2d\u7684\u7acb\u573a\u548c\u89c2\u70b9\u5982\u4f55\u53d7\u5230\u5176\u4f7f\u7528\u7684\u4fe1\u606f\u6e90\u6587\u6863\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1MillStone\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e94\u7528\u4e8e9\u4e2a\u9886\u5148\u7684LLM\uff0c\u6d4b\u91cf\u5b83\u4eec\u5bf9\u4e0d\u540c\u7acb\u573a\u8bba\u70b9\u7684\u5f00\u653e\u7a0b\u5ea6\u3001\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u3001\u6700\u5177\u8bf4\u670d\u529b\u7684\u8bba\u70b9\u7b49\u3002", "result": "LLM\u5728\u5927\u591a\u6570\u95ee\u9898\u4e0a\u601d\u60f3\u5f00\u653e\uff0c\u6743\u5a01\u4fe1\u606f\u6e90\u5bb9\u6613\u6539\u53d8LLM\u7684\u7acb\u573a\uff0c\u7a81\u663e\u4e86\u4fe1\u606f\u6e90\u9009\u62e9\u548cLLM\u7cfb\u7edf\u53ef\u80fd\u88ab\u64cd\u7eb5\u7684\u98ce\u9669\u3002", "conclusion": "LLM\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u548c\u641c\u7d22\u7cfb\u7edf\u5b58\u5728\u88ab\u64cd\u7eb5\u7684\u98ce\u9669\uff0c\u4fe1\u606f\u6e90\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2509.11543", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11543", "abs": "https://arxiv.org/abs/2509.11543", "authors": ["Zhengxi Lu", "Jiabo Ye", "Fei Tang", "Yongliang Shen", "Haiyang Xu", "Ziwei Zheng", "Weiming Lu", "Ming Yan", "Fei Huang", "Jun Xiao", "Yueting Zhuang"], "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning", "comment": "22 pages, 17 figures", "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u534a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b0\u8303\u5f0f\uff0c\u5728\u79bb\u7ebf\u8f68\u8ff9\u4e0a\u6a21\u62df\u5728\u7ebfRL\uff0c\u901a\u8fc7\u8865\u4e01\u6a21\u5757\u5904\u7406\u8f68\u8ff9\u5dee\u5f02\uff0c\u5f15\u5165\u6298\u6263\u672a\u6765\u56de\u62a5\u548c\u52a0\u6743\u4f18\u52bf\u4f18\u5316\u7b56\u7565\uff0c\u5728GUI\u4ee3\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3GUI\u4ee3\u7406\u4e2d\u79bb\u7ebfRL\u7f3a\u4e4f\u8f68\u8ff9\u7ea7\u5956\u52b1\u4fe1\u53f7\u548c\u5728\u7ebfRL\u5956\u52b1\u7a00\u758f\u3001\u90e8\u7f72\u6210\u672c\u9ad8\u7684\u6839\u672c\u56f0\u5883", "method": "\u534a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff1a\u5728\u79bb\u7ebf\u8f68\u8ff9\u4e0a\u6a21\u62df\u5728\u7ebfrollout\uff0c\u4f7f\u7528\u8865\u4e01\u6a21\u5757\u81ea\u9002\u5e94\u6062\u590d\u8f68\u8ff9\u5dee\u5f02\uff0c\u5f15\u5165\u6298\u6263\u672a\u6765\u56de\u62a5\u548c\u52a0\u6743\u6b65\u7ea7/\u56de\u5408\u7ea7\u4f18\u52bf\u4f18\u5316\u7b56\u7565", "result": "\u5728\u56db\u4e2a\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u6a21\u578b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u5347\uff08AndroidWorld +12.0%\uff0cAITW +23.8%\uff09", "conclusion": "\u534a\u5728\u7ebfRL\u6709\u6548\u5f25\u5408\u4e86\u79bb\u7ebf\u8bad\u7ec3\u6548\u7387\u548c\u5728\u7ebf\u591a\u8f6e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0cSOP\u6307\u6807\u80fd\u66f4\u597d\u5bf9\u9f50\u771f\u5b9e\u5728\u7ebf\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2509.11629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11629", "abs": "https://arxiv.org/abs/2509.11629", "authors": ["Chentao Cao", "Xiaojun Xu", "Bo Han", "Hang Li"], "title": "Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check", "comment": null, "summary": "As large language models (LLMs) continue to advance in capabilities, ensuring\ntheir safety against jailbreak attacks remains a critical challenge. In this\npaper, we introduce a novel safety alignment approach called Answer-Then-Check,\nwhich enhances LLM robustness against malicious prompts by applying thinking\nability to mitigate jailbreaking problems before producing a final answer to\nthe user. Our method enables models to directly answer the question in their\nthought and then critically evaluate its safety before deciding whether to\nprovide it. To implement this approach, we construct the Reasoned Safety\nAlignment (ReSA) dataset, comprising 80K examples that teach models to reason\nthrough direct responses and then analyze their safety. Experimental results\ndemonstrate that our approach achieves the Pareto frontier with superior safety\ncapability while decreasing over-refusal rates on over-refusal benchmarks.\nNotably, the model fine-tuned with ReSA maintains general reasoning\ncapabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our\nmethod equips models with the ability to perform safe completion. Unlike\npost-hoc methods that can only reject harmful queries, our model can provide\nhelpful and safe alternative responses for sensitive topics (e.g., self-harm).\nFurthermore, we discover that training on a small subset of just 500 examples\ncan achieve comparable performance to using the full dataset, suggesting that\nsafety alignment may require less data than previously assumed.", "AI": {"tldr": "\u63d0\u51faAnswer-Then-Check\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u601d\u8003\u56de\u7b54\u518d\u5b89\u5168\u68c0\u67e5\u7684\u673a\u5236\u589e\u5f3aLLM\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u6784\u5efaReSA\u6570\u636e\u96c6\u5e76\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u786e\u4fdd\u5176\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\u7684\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faAnswer-Then-Check\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u5148\u5728\u601d\u8003\u4e2d\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u7136\u540e\u6279\u5224\u6027\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u518d\u51b3\u5b9a\u662f\u5426\u8f93\u51fa\u3002\u6784\u5efa\u5305\u542b8\u4e07\u6837\u672c\u7684ReSA\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5b89\u5168\u63a8\u7406\u3002", "result": "\u65b9\u6cd5\u8fbe\u5230\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5728\u4fdd\u6301MMLU\u3001MATH500\u548cHumanEval\u57fa\u51c6\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u964d\u4f4e\u8fc7\u62d2\u7edd\u7387\uff0c\u5e76\u80fd\u63d0\u4f9b\u5b89\u5168\u66ff\u4ee3\u54cd\u5e94\u3002\u4ec5\u7528500\u6837\u672c\u5373\u53ef\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86LLM\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u6bd4\u4e4b\u524d\u5047\u8bbe\u9700\u8981\u66f4\u5c11\u6570\u636e\u3002", "topic": "agent analysis"}}
{"id": "2509.12010", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12010", "abs": "https://arxiv.org/abs/2509.12010", "authors": ["Filippo Lazzati", "Alberto Maria Metelli"], "title": "Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids", "comment": null, "summary": "We study the problem of generalizing an expert agent's behavior, provided\nthrough demonstrations, to new environments and/or additional constraints.\nInverse Reinforcement Learning (IRL) offers a promising solution by seeking to\nrecover the expert's underlying reward function, which, if used for planning in\nthe new settings, would reproduce the desired behavior. However, IRL is\ninherently ill-posed: multiple reward functions, forming the so-called feasible\nset, can explain the same observed behavior. Since these rewards may induce\ndifferent policies in the new setting, in the absence of additional\ninformation, a decision criterion is needed to select which policy to deploy.\nIn this paper, we propose a novel, principled criterion that selects the\n\"average\" policy among those induced by the rewards in a certain bounded subset\nof the feasible set. Remarkably, we show that this policy can be obtained by\nplanning with the reward centroid of that subset, for which we derive a\nclosed-form expression. We then present a provably efficient algorithm for\nestimating this centroid using an offline dataset of expert demonstrations\nonly. Finally, we conduct numerical simulations that illustrate the\nrelationship between the expert's behavior and the behavior produced by our\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u53ef\u884c\u5956\u52b1\u96c6\u5408\u7684\u8d28\u5fc3\u6765\u9009\u62e9\u5e73\u5747\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3IRL\u4e2d\u591a\u5956\u52b1\u51fd\u6570\u89e3\u91ca\u76f8\u540c\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u9006\u5f3a\u5316\u5b66\u4e60(IRL)\u5b58\u5728\u56fa\u6709\u7684\u4e0d\u9002\u5b9a\u6027\uff0c\u591a\u4e2a\u5956\u52b1\u51fd\u6570\u53ef\u4ee5\u89e3\u91ca\u76f8\u540c\u7684\u4e13\u5bb6\u884c\u4e3a\u3002\u5728\u65b0\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u4e0d\u540c\u7684\u5956\u52b1\u51fd\u6570\u53ef\u80fd\u4ea7\u751f\u4e0d\u540c\u7684\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u51b3\u7b56\u6807\u51c6\u6765\u9009\u62e9\u8981\u90e8\u7f72\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u6807\u51c6\uff0c\u9009\u62e9\u5728\u53ef\u884c\u5956\u52b1\u96c6\u5408\u7684\u6709\u754c\u5b50\u96c6\u4e2d\u8bf1\u5bfc\u51fa\u7684\"\u5e73\u5747\"\u7b56\u7565\u3002\u8bc1\u660e\u8be5\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u8be5\u5b50\u96c6\u7684\u5956\u52b1\u8d28\u5fc3\u8fdb\u884c\u89c4\u5212\u83b7\u5f97\uff0c\u5e76\u63a8\u5bfc\u4e86\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ec5\u4f7f\u7528\u4e13\u5bb6\u6f14\u793a\u79bb\u7ebf\u6570\u636e\u96c6\u6765\u4f30\u8ba1\u8be5\u8d28\u5fc3\u7684\u6709\u6548\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bf4\u660e\u4e86\u4e13\u5bb6\u884c\u4e3a\u4e0e\u8be5\u65b9\u6cd5\u4ea7\u751f\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3IRL\u4e2d\u7684\u4e0d\u9002\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u65b0\u73af\u5883\u4e2d\u4ea7\u751f\u4e0e\u4e13\u5bb6\u884c\u4e3a\u4e00\u81f4\u7684\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12026", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12026", "abs": "https://arxiv.org/abs/2509.12026", "authors": ["Filippo Lazzati", "Alberto Maria Metelli"], "title": "Imitation Learning as Return Distribution Matching", "comment": null, "summary": "We study the problem of training a risk-sensitive reinforcement learning (RL)\nagent through imitation learning (IL). Unlike standard IL, our goal is not only\nto train an agent that matches the expert's expected return (i.e., its average\nperformance) but also its risk attitude (i.e., other features of the return\ndistribution, such as variance). We propose a general formulation of the\nrisk-sensitive IL problem in which the objective is to match the expert's\nreturn distribution in Wasserstein distance. We focus on the tabular setting\nand assume the expert's reward is known. After demonstrating the limited\nexpressivity of Markovian policies for this task, we introduce an efficient and\nsufficiently expressive subclass of non-Markovian policies tailored to it.\nBuilding on this subclass, we develop two provably efficient algorithms, RS-BC\nand RS-KT, for solving the problem when the transition model is unknown and\nknown, respectively. We show that RS-KT achieves substantially lower sample\ncomplexity than RS-BC by exploiting dynamics information. We further\ndemonstrate the sample efficiency of return distribution matching in the\nsetting where the expert's reward is unknown by designing an oracle-based\nvariant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and\nRS-BC with numerical simulations, highlighting both their sample efficiency and\nthe advantages of non-Markovian policies over standard sample-efficient IL\nalgorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u76ee\u6807\u662f\u5339\u914d\u4e13\u5bb6\u7684\u671f\u671b\u56de\u62a5\u548c\u98ce\u9669\u6001\u5ea6\uff08\u56de\u62a5\u5206\u5e03\u7279\u5f81\uff09\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u5206\u5e03\u5339\u914d\u65b9\u6cd5\u548c\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u53ea\u5173\u6ce8\u5339\u914d\u4e13\u5bb6\u7684\u671f\u671b\u56de\u62a5\uff0c\u4f46\u5ffd\u7565\u4e86\u98ce\u9669\u6001\u5ea6\u7b49\u56de\u62a5\u5206\u5e03\u7684\u5176\u4ed6\u91cd\u8981\u7279\u5f81\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5339\u914d\u671f\u671b\u56de\u62a5\u548c\u98ce\u9669\u654f\u611f\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u98ce\u9669\u654f\u611f\u6a21\u4eff\u5b66\u4e60\u7684Wasserstein\u8ddd\u79bb\u5339\u914d\u6846\u67b6\uff0c\u5f15\u5165\u8868\u8fbe\u529b\u8db3\u591f\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u7b56\u7565\u5b50\u7c7b\uff0c\u5f00\u53d1\u4e86RS-BC\uff08\u672a\u77e5\u8f6c\u79fb\u6a21\u578b\uff09\u548cRS-KT\uff08\u5df2\u77e5\u8f6c\u79fb\u6a21\u578b\uff09\u4e24\u79cd\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5956\u52b1\u672a\u77e5\u60c5\u51b5\u4e0b\u7684\u53d8\u4f53\u3002", "result": "RS-KT\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u52a8\u6001\u4fe1\u606f\u5b9e\u73b0\u4e86\u6bd4RS-BC\u66f4\u4f4e\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u975e\u9a6c\u5c14\u53ef\u592b\u7b56\u7565\u76f8\u6bd4\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u98ce\u9669\u654f\u611f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5339\u914d\u4e13\u5bb6\u7684\u56de\u62a5\u5206\u5e03\u7279\u5f81\uff0c\u975e\u9a6c\u5c14\u53ef\u592b\u7b56\u7565\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u98ce\u9669\u654f\u611fRL\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12117", "abs": "https://arxiv.org/abs/2509.12117", "authors": ["Aryaman Reddi", "Gabriele Tiboni", "Jan Peters", "Carlo D'Eramo"], "title": "$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)\ntypically employ a policy update that responds to the current strategies of\nother agents. While being straightforward, this approach does not account for\nthe updates of other agents at the same update step, resulting in\nmiscoordination. In this paper, we introduce the $K$-Level Policy Gradient\n(KPG), a method that recursively updates each agent against the updated\npolicies of other agents, speeding up the discovery of effective coordinated\npolicies. We theoretically prove that KPG with finite iterates achieves\nmonotonic convergence to a local Nash equilibrium under certain conditions. We\nprovide principled implementations of KPG by applying it to the deep MARL\nalgorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior\nperformance over existing deep MARL algorithms in StarCraft II and multi-agent\nMuJoCo.", "AI": {"tldr": "K-Level Policy Gradient (KPG) \u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u66f4\u65b0\u667a\u80fd\u4f53\u7b56\u7565\u6765\u5e94\u5bf9\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u540c\u65f6\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u66f4\u65b0\u7b56\u7565\u65f6\u53ea\u8003\u8651\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u5f53\u524d\u7b56\u7565\uff0c\u800c\u6ca1\u6709\u8003\u8651\u5b83\u4eec\u5728\u540c\u4e00\u66f4\u65b0\u6b65\u9aa4\u4e2d\u7684\u7b56\u7565\u53d8\u5316\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u534f\u8c03\u5931\u8d25\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86K-Level Policy Gradient (KPG)\u65b9\u6cd5\uff0c\u9012\u5f52\u5730\u66f4\u65b0\u6bcf\u4e2a\u667a\u80fd\u4f53\u76f8\u5bf9\u4e8e\u5176\u4ed6\u667a\u80fd\u4f53\u66f4\u65b0\u540e\u7b56\u7565\u7684\u7b56\u7565\uff0c\u52a0\u901f\u6709\u6548\u534f\u8c03\u7b56\u7565\u7684\u53d1\u73b0\u3002\u5c06KPG\u5e94\u7528\u4e8eMAPPO\u3001MADDPG\u548cFACMAC\u7b49\u6df1\u5ea6MARL\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660eKPG\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u4e0b\u80fd\u591f\u5355\u8c03\u6536\u655b\u5230\u5c40\u90e8\u7eb3\u4ec0\u5747\u8861\u3002\u5728StarCraft II\u548c\u591a\u667a\u80fd\u4f53MuJoCo\u5b9e\u9a8c\u4e2d\uff0cKPG\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6df1\u5ea6MARL\u7b97\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "KPG\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u591a\u667a\u80fd\u4f53\u540c\u65f6\u66f4\u65b0\u7684\u534f\u8c03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3MARL\u4e2d\u7684\u534f\u8c03\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
