{"id": "2509.18337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18337", "abs": "https://arxiv.org/abs/2509.18337", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation", "comment": "15 pages, 4 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Commit messages play a key role in documenting the intent behind code\nchanges. However, they are often low-quality, vague, or incomplete, limiting\ntheir usefulness. Commit Message Generation (CMG) aims to automatically\ngenerate descriptive commit messages from code diffs to reduce developers'\neffort and improve message quality. Although recent advances in LLMs have shown\npromise in automating CMG, their performance remains limited. This paper aims\nto enhance CMG performance by retrieving similar diff-message pairs to guide\nLLMs to generate commit messages that are more precise and informative. We\nproposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message\nGeneration, structured in three phases: (1) Retrieve: retrieving the similar\ndiff-message pairs; (2) Augment: combining them with the query diff into a\nstructured prompt; and (3) Generate: generating commit messages corresponding\nto the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific\nterminologies and writing styles from the retrieved diff-message pairs, thereby\nproducing high-quality commit messages. We evaluated our method on various\nLLMs, including closed-source GPT models and open-source DeepSeek models.\nExperimental results show that CoRaCMG significantly boosts LLM performance\nacross four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,\nDeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when\naugmented with a single retrieved example pair. After incorporating the single\nexample pair, GPT-4o achieves the highest improvement rate, with BLEU\nincreasing by 89%. Moreover, performance gains plateau after more than three\nexamples are used, indicating diminishing returns. Further analysis shows that\nthe improvements are attributed to the model's ability to capture the\nterminologies and writing styles of human-written commit messages from the\nretrieved example pairs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoRaCMG\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3c\u7684diff-message\u5bf9\u6765\u589e\u5f3aLLM\u751f\u6210\u63d0\u4ea4\u6d88\u606f\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86BLEU\u3001Rouge-L\u3001METEOR\u548cCIDEr\u7b49\u6307\u6807\u3002", "motivation": "\u63d0\u4ea4\u6d88\u606f\u5728\u8bb0\u5f55\u4ee3\u7801\u53d8\u66f4\u610f\u56fe\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u63d0\u4ea4\u6d88\u606f\u8d28\u91cf\u666e\u904d\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002\u867d\u7136LLM\u5728\u81ea\u52a8\u751f\u6210\u63d0\u4ea4\u6d88\u606f\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u5f85\u63d0\u5347\u3002", "method": "CoRaCMG\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(1)\u68c0\u7d22\u76f8\u4f3c\u7684diff-message\u5bf9\uff1b(2)\u5c06\u68c0\u7d22\u7ed3\u679c\u4e0e\u67e5\u8be2diff\u7ed3\u5408\u6210\u7ed3\u6784\u5316\u63d0\u793a\uff1b(3)\u901a\u8fc7LLM\u751f\u6210\u5bf9\u5e94\u7684\u63d0\u4ea4\u6d88\u606f\u3002\u8be5\u65b9\u6cd5\u4f7fLLM\u80fd\u4ece\u68c0\u7d22\u5230\u7684\u793a\u4f8b\u4e2d\u5b66\u4e60\u9879\u76ee\u7279\u5b9a\u672f\u8bed\u548c\u5199\u4f5c\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoRaCMG\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cdLLM\u7684\u6027\u80fd\uff0cDeepSeek-R1\u5728BLEU\u548cCIDEr\u4e0a\u5206\u522b\u63d0\u5347\u4e8676%\u548c71%\uff0cGPT-4o\u7684BLEU\u63d0\u5347\u4e8689%\u3002\u6027\u80fd\u63d0\u5347\u5728\u8d85\u8fc73\u4e2a\u793a\u4f8b\u540e\u8d8b\u4e8e\u7a33\u5b9a\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u63d0\u4ea4\u6d88\u606f\u751f\u6210\u8d28\u91cf\uff0c\u6539\u8fdb\u4e3b\u8981\u5f52\u56e0\u4e8e\u6a21\u578b\u4ece\u68c0\u7d22\u793a\u4f8b\u4e2d\u5b66\u4e60\u4eba\u7c7b\u7f16\u5199\u63d0\u4ea4\u6d88\u606f\u7684\u672f\u8bed\u548c\u5199\u4f5c\u98ce\u683c\u7684\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "2509.18454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18454", "abs": "https://arxiv.org/abs/2509.18454", "authors": ["Andrzej Bia\u0142ecki", "Piotr Bia\u0142ecki", "Piotr Sowi\u0144ski", "Mateusz Budziak", "Jan Gajewski"], "title": "SC2Tools: StarCraft II Toolset and Dataset API", "comment": null, "summary": "Computer games, as fully controlled simulated environments, have been\nutilized in significant scientific studies demonstrating the application of\nReinforcement Learning (RL). Gaming and esports are key areas influenced by the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) solutions\nat scale. Tooling simplifies scientific workloads and is essential for\ndeveloping the gaming and esports research area.\n  In this work, we present ``SC2Tools'', a toolset containing multiple\nsubmodules responsible for working with, and producing larger datasets. We\nprovide a modular structure of the implemented tooling, leaving room for future\nextensions where needed. Additionally, some of the tools are not StarCraft~2\nexclusive and can be used with other types of data for dataset creation.\n  The tools we present were leveraged in creating one of the largest\nStarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch\nLightning application programming interface (API) for easy access to the data.\n  We conclude that alleviating the burden of data collection, preprocessing,\nand custom code development is essential for less technically proficient\nresearchers to engage in the growing gaming and esports research area. Finally,\nour solution provides some foundational work toward normalizing experiment\nworkflow in StarCraft~2", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SC2Tools\u5de5\u5177\u96c6\uff0c\u7528\u4e8e\u5904\u7406\u661f\u9645\u4e89\u97382\u6e38\u620f\u6570\u636e\uff0c\u7b80\u5316\u6570\u636e\u6536\u96c6\u548c\u9884\u5904\u7406\u5de5\u4f5c\uff0c\u652f\u6301\u6e38\u620f\u548c\u7535\u5b50\u7ade\u6280\u7814\u7a76\u3002", "motivation": "\u6e38\u620f\u73af\u5883\u4f5c\u4e3a\u53d7\u63a7\u6a21\u62df\u73af\u5883\u5728\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u6570\u636e\u6536\u96c6\u548c\u9884\u5904\u7406\u5de5\u4f5c\u7e41\u91cd\uff0c\u963b\u788d\u4e86\u6280\u672f\u80fd\u529b\u8f83\u5f31\u7684\u7814\u7a76\u8005\u53c2\u4e0e\u6e38\u620f\u548c\u7535\u5b50\u7ade\u6280\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86SC2Tools\u5de5\u5177\u96c6\uff0c\u5305\u542b\u591a\u4e2a\u5b50\u6a21\u5757\uff0c\u91c7\u7528\u6a21\u5757\u5316\u7ed3\u6784\u8bbe\u8ba1\uff0c\u652f\u6301\u661f\u9645\u4e89\u97382\u6bd4\u8d5b\u6570\u636e\u96c6\u521b\u5efa\uff0c\u5e76\u63d0\u4f9bPyTorch\u548cPyTorch Lightning API\u63a5\u53e3\u3002", "result": "\u521b\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u661f\u9645\u4e89\u97382\u6bd4\u8d5b\u6570\u636e\u96c6\u4e4b\u4e00\uff0c\u5de5\u5177\u96c6\u4e0d\u4ec5\u9650\u4e8e\u661f\u9645\u4e89\u97382\uff0c\u8fd8\u53ef\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e\u96c6\u521b\u5efa\u3002", "conclusion": "\u51cf\u8f7b\u6570\u636e\u6536\u96c6\u548c\u9884\u5904\u7406\u8d1f\u62c5\u5bf9\u4e8e\u4fc3\u8fdb\u6e38\u620f\u548c\u7535\u5b50\u7ade\u6280\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u5de5\u5177\u4e3a\u661f\u9645\u4e89\u97382\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u7a0b\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u4f5c\u3002", "topic": "swe application"}}
{"id": "2509.18808", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18808", "abs": "https://arxiv.org/abs/2509.18808", "authors": ["Zexun Zhan", "Shuzheng Gao", "Ruida Hu", "Cuiyun Gao"], "title": "SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration. However, existing benchmarks mainly formalize the task as a static,\nsingle-turn problem, overlooking the stepwise requirement changes and iterative\nworkflows in real-world software development. This mismatch limits the\nunderstanding of how well LLMs can support real-world development workflows.\nConstructing such iterative benchmarks is challenging due to the lack of public\ninteraction traces and the difficulty of creating discriminative, turn-specific\ntest cases.\n  To bridge this gap, we present SR-Eval, a benchmark specifically designed to\nassess LLMs on iterative code generation under Stepwise requirements\nRefinement. SR-Eval spans both function-level and repository-level tasks in\nPython and Java, enabling fine-grained and progressive evaluation across\nevolving requirements. The construction of SR-Eval follows a carefully designed\npipeline that first leverages a multi-agent-based requirement generation method\nto simulate the development process and recover the multi-round interaction\nprocess from final requirements, then employs a semantic-aware discriminative\ntest case generation component to ensure discriminative and consistent\nevaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857\nquestions at both function and repository levels. Using SR-Eval, we evaluate 11\nrepresentative LLMs with three prompting strategies that simulate different\nusage patterns. Results show that iterative code generation under stepwise\nrequirement refinement remains highly challenging: the best-performing model\nachieves only 22.67% completion rate on function-level tasks and 20.00% on\nrepository-level tasks. We further observe that prompting strategies\nsubstantially influence performance, highlighting the need for the development\nof advanced methods.", "AI": {"tldr": "SR-Eval\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30LLMs\u5728\u9010\u6b65\u9700\u6c42\u7ec6\u5316\u4e0b\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u51fd\u6570\u7ea7\u548c\u4ed3\u5e93\u7ea7\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c06\u4ee3\u7801\u751f\u6210\u89c6\u4e3a\u9759\u6001\u3001\u5355\u8f6e\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9010\u6b65\u9700\u6c42\u53d8\u5316\u548c\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u7406\u89e3LLMs\u652f\u6301\u771f\u5b9e\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u9700\u6c42\u751f\u6210\u65b9\u6cd5\u6a21\u62df\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4ece\u6700\u7ec8\u9700\u6c42\u4e2d\u6062\u590d\u591a\u8f6e\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u8bed\u4e49\u611f\u77e5\u7684\u5224\u522b\u6027\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u7ec4\u4ef6\u786e\u4fdd\u6bcf\u8f6e\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u548c\u5224\u522b\u6027\u3002", "result": "\u8bc4\u4f3011\u4e2a\u4ee3\u8868\u6027LLMs\uff0c\u6700\u4f73\u6a21\u578b\u5728\u51fd\u6570\u7ea7\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523022.67%\u5b8c\u6210\u7387\uff0c\u4ed3\u5e93\u7ea7\u4efb\u52a1\u4e3a20.00%\uff0c\u63d0\u793a\u7b56\u7565\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u9010\u6b65\u9700\u6c42\u7ec6\u5316\u4e0b\u7684\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u4ecd\u7136\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "topic": "swe benchmark"}}
{"id": "2509.19136", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.5; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.19136", "abs": "https://arxiv.org/abs/2509.19136", "authors": ["S\u00e9bastien Salva", "Redha Taguelmimt"], "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "comment": null, "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u76f4\u63a5\u6267\u884c\u81ea\u7136\u8bed\u8a00\u6d4b\u8bd5\u7528\u4f8b\u7684\u53ef\u884c\u6027\uff0c\u91cd\u70b9\u5173\u6ce8NL\u6d4b\u8bd5\u7528\u4f8b\u7684\u4e0d\u5065\u5168\u6027\u548c\u6267\u884c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u62a4\u673a\u5236\u548c\u4e13\u95e8\u4ee3\u7406\u6765\u9a8c\u8bc1\u6d4b\u8bd5\u6b65\u9aa4\u7684\u6b63\u786e\u6267\u884c\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u5199\u53ef\u6267\u884c\u6d4b\u8bd5\u811a\u672c\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u7ef4\u62a4\uff0c\u800c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6d4b\u8bd5\u7528\u4f8b\u9a8c\u8bc1\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u5e94\u7528\u7a0b\u5e8f\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002LLM\u7684\u8fdb\u5c55\u4f7f\u5f97\u76f4\u63a5\u6267\u884cNL\u6d4b\u8bd5\u7528\u4f8b\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u5b58\u5728\u4e0d\u5065\u5168\u6027\u548c\u6267\u884c\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6267\u884cNL\u6d4b\u8bd5\u7528\u4f8b\u7684\u7b97\u6cd5\uff0c\u91c7\u7528\u9632\u62a4\u673a\u5236\u548c\u4e13\u95e8\u4ee3\u7406\u52a8\u6001\u9a8c\u8bc1\u6bcf\u4e2a\u6d4b\u8bd5\u6b65\u9aa4\u7684\u6b63\u786e\u6267\u884c\u3002\u5f15\u5165\u4e86\u8bc4\u4f30LLM\u6d4b\u8bd5\u6267\u884c\u80fd\u529b\u7684\u6307\u6807\u548c\u4e00\u4e2a\u91cf\u5316\u6267\u884c\u4e00\u81f4\u6027\u7684\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u5f31\u4e0d\u5065\u5168\u6027\u5b9a\u4e49\u6765\u8868\u5f81NL\u6d4b\u8bd5\u7528\u4f8b\u6267\u884c\u53ef\u63a5\u53d7\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e868\u4e2a\u516c\u5f00\u53ef\u7528\u7684LLM\uff08\u53c2\u6570\u4ece3B\u523070B\uff09\uff0c\u7ed3\u679c\u663e\u793aMeta Llama 3.1 70B\u5728NL\u6d4b\u8bd5\u7528\u4f8b\u6267\u884c\u4e2d\u8868\u73b0\u51fa\u53ef\u63a5\u53d7\u7684\u80fd\u529b\uff0c\u6267\u884c\u4e00\u81f4\u6027\u9ad8\uff08\u8d85\u8fc73-sigma\u6c34\u5e73\uff09\u3002", "conclusion": "\u5f53\u524dLLM\u4ee3\u7406\u5728GUI\u6d4b\u8bd5\u65b9\u9762\u65e2\u6709\u6f5c\u529b\u4e5f\u5b58\u5728\u5c40\u9650\u6027\uff0cMeta Llama 3.1 70B\u5728NL\u6d4b\u8bd5\u7528\u4f8b\u6267\u884c\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6574\u4f53\u6280\u672f\u4ecd\u9700\u6539\u8fdb\u3002", "topic": "swe application"}}
{"id": "2509.19185", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.19185", "abs": "https://arxiv.org/abs/2509.19185", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "title": "An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications", "comment": null, "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6d4b\u8bd5\u5b9e\u8df5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e8639\u4e2a\u5f00\u6e90\u4ee3\u7406\u6846\u67b6\u548c439\u4e2a\u4ee3\u7406\u5e94\u7528\uff0c\u53d1\u73b0\u6d4b\u8bd5\u52aa\u529b\u5b58\u5728\u6839\u672c\u6027\u5012\u7f6e\uff1a\u786e\u5b9a\u6027\u7ec4\u4ef6\u6d88\u8017\u4e8670%\u4ee5\u4e0a\u7684\u6d4b\u8bd5\u52aa\u529b\uff0c\u800c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u5212\u4e3b\u4f53\u548c\u89e6\u53d1\u7ec4\u4ef6\uff08\u63d0\u793a\uff09\u88ab\u4e25\u91cd\u5ffd\u89c6\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u7684AI\u4ee3\u7406\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u56fa\u6709\u7684\u975e\u786e\u5b9a\u6027\u548c\u4e0d\u53ef\u91cd\u73b0\u6027\u7ed9\u6d4b\u8bd5\u548c\u8d28\u91cf\u4fdd\u8bc1\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u63d0\u4f9b\u4efb\u52a1\u7ea7\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u5982\u4f55\u9a8c\u8bc1\u4ee3\u7406\u5185\u90e8\u6b63\u786e\u6027\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5bf939\u4e2a\u5f00\u6e90\u4ee3\u7406\u6846\u67b6\u548c439\u4e2a\u4ee3\u7406\u5e94\u7528\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5341\u79cd\u4e0d\u540c\u7684\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6a21\u5f0f\u6620\u5c04\u5230\u4ee3\u7406\u6846\u67b6\u548c\u5e94\u7528\u7684\u89c4\u8303\u67b6\u6784\u7ec4\u4ef6\u4e0a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u6d4b\u8bd5\u6a21\u5f0f\uff08\u5982\u8d1f\u9762\u6d4b\u8bd5\u548c\u6210\u5458\u6d4b\u8bd5\uff09\u88ab\u5e7f\u6cdb\u91c7\u7528\u4ee5\u7ba1\u7406\u57fa\u7840\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u65b0\u578b\u4ee3\u7406\u7279\u5b9a\u65b9\u6cd5\uff08\u5982DeepEval\uff09\u4f7f\u7528\u7387\u4ec5\u4e3a1%\u5de6\u53f3\u3002\u6d4b\u8bd5\u52aa\u529b\u5b58\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\uff1a\u786e\u5b9a\u6027\u7ec4\u4ef6\u536070%\u4ee5\u4e0a\u6d4b\u8bd5\u52aa\u529b\uff0c\u800c\u8ba1\u5212\u4e3b\u4f53\u548c\u89e6\u53d1\u7ec4\u4ef6\u5206\u522b\u53ea\u5360\u4e0d\u52305%\u548c1%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u975e\u786e\u5b9a\u6027\u9002\u5e94\u65b9\u9762\u7684\u7406\u6027\u4f46\u4e0d\u5b8c\u6574\u7684\u8c03\u6574\uff0c\u5efa\u8bae\u6846\u67b6\u5f00\u53d1\u8005\u6539\u8fdb\u5bf9\u65b0\u578b\u6d4b\u8bd5\u65b9\u6cd5\u7684\u652f\u6301\uff0c\u5e94\u7528\u5f00\u53d1\u8005\u5fc5\u987b\u91c7\u7528\u63d0\u793a\u56de\u5f52\u6d4b\u8bd5\uff0c\u7814\u7a76\u4eba\u5458\u5e94\u63a2\u7d22\u91c7\u7528\u969c\u788d\u3002", "topic": "agent analysis"}}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "ZERA\u662f\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u63d0\u793a\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u548c\u4f4e\u5f00\u9500\u8fed\u4ee3\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u5230\u9ad8\u8d28\u91cf\u63d0\u793a\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u7528\u6237\u63d0\u793a\uff0c\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u9700\u8981\u5927\u91cf\u6837\u672c\u548c\u957f\u8fed\u4ee3\u5468\u671f\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u4e14\u8106\u5f31\u3002", "method": "ZERA\u4f7f\u7528\u516b\u4e2a\u53ef\u6cdb\u5316\u6807\u51c6\u5bf9\u63d0\u793a\u8fdb\u884c\u8bc4\u5206\uff0c\u57fa\u4e8e\u7ed3\u6784\u5316\u6279\u8bc4\u4fee\u8ba2\u63d0\u793a\uff0c\u901a\u8fc7\u81ea\u52a8\u63a8\u65ad\u6743\u91cd\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u3002", "result": "\u5728\u4e94\u4e2aLLM\u548c\u4e5d\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cZERA\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u63d0\u793a\u6784\u5efa\u7684\u6709\u6548\u8d21\u732e\u3002", "conclusion": "ZERA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u548c\u8054\u5408\u4f18\u5316\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u81ea\u52a8\u5316\u6574\u4e2aOpenFOAM\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u7f51\u683c\u751f\u6210\u3001HPC\u811a\u672c\u751f\u6210\u548c\u540e\u5904\u7406\u53ef\u89c6\u5316\uff0c\u5728110\u4e2a\u4eff\u771f\u4efb\u52a1\u4e2d\u8fbe\u523088.2%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66(CFD)\u4eff\u771f\u5de5\u5177\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u4e14\u8bbe\u7f6e\u590d\u6742\uff0c\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u529f\u80fd\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u7f51\u683c\u751f\u6210\u4ee3\u7406\u3001HPC\u811a\u672c\u751f\u6210\u4ee3\u7406\u7b49\uff1b\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u66b4\u9732\u6838\u5fc3\u529f\u80fd\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff1b\u91c7\u7528\u5206\u5c42\u591a\u7d22\u5f15RAG\u8fdb\u884c\u7cbe\u786e\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u4f9d\u8d56\u611f\u77e5\u7684\u914d\u7f6e\u751f\u6210\u3002", "result": "\u5728110\u4e2a\u4eff\u771f\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Claude 3.5 Sonnet\u8fbe\u523088.2%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eMetaOpenFOAM\u768455.5%\u3002", "conclusion": "Foam-Agent\u663e\u8457\u964d\u4f4e\u4e86CFD\u7684\u4e13\u4e1a\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u4e13\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u6c11\u4e3b\u5316\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u3002", "topic": "code agent"}}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTEMPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u524d\u7f00\u6811\u7ed3\u6784\u5b9e\u73b0\u65e0critic\u6a21\u578b\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u5728\u6570\u5b66\u548c\u533b\u7597QA\u4efb\u52a1\u4e2d\u4f18\u4e8ePPO\u548cGRPO\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5ef6\u8fdf\u5956\u52b1\u5bfc\u81f4\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\u74f6\u9888\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bbe\u7f6e\u4e0b\uff08\u6700\u7ec8\u7b54\u6848\u53ef\u68c0\u67e5\u4e14\u6bcf\u4e2a\u63d0\u793a\u53ef\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff09\u3002", "method": "\u63d0\u51faPrefix-to-Tree\uff08P2T\uff09\u65b9\u6cd5\u5c06\u54cd\u5e94\u7ec4\u8f6c\u6362\u4e3a\u524d\u7f00\u6811\uff0c\u8ba1\u7b97\u975e\u53c2\u6570\u5316\u524d\u7f00\u503c\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1TEMPO\u7b97\u6cd5\uff0c\u7ed3\u5408GRPO\u7684\u7ec4\u76f8\u5bf9\u7ed3\u679c\u4fe1\u53f7\u548c\u5206\u652f\u95e8\u63a7\u7684\u65f6\u5e8f\u5dee\u5206\u4fee\u6b63\u3002", "result": "\u5728Qwen3-1.7B/4B\u6a21\u578b\u4e0a\uff0cTEMPO\u5728\u5206\u5e03\u5185\uff08MATH\u3001MedQA\uff09\u548c\u5206\u5e03\u5916\uff08GSM-HARD\u3001AMC23\u7b49\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8ePPO\u548cGRPO\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u8bad\u7ec3\u65f6\u95f4\u76f8\u8fd1\u3002", "conclusion": "TEMPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u65e0\u9700\u5b66\u4e60\u4ef7\u503c\u7f51\u7edc\u6216\u989d\u5916\u8bc4\u5224\u5668\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5c06LLM\u4f5c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5224\u65ad\u5019\u9009\u8def\u5f84\u662f\u5426\u80fd\u6b63\u786e\u8bca\u65ad\u60a3\u8005\u8f93\u5165\uff0c\u800c\u975e\u76f4\u63a5\u751f\u6210\u63a8\u7406\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6216\u5fae\u8c03\u5c06\u77e5\u8bc6\u56fe\u8c31\u5185\u5bb9\u63d2\u5165\u63d0\u793a\u4e2d\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u53d7\u5956\u52b1\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u5c06LLM\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u6765\u9a8c\u8bc1\u8bca\u65ad\u8def\u5f84\u7684\u6b63\u786e\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u77e5\u8bc6\u8def\u5f84\u5224\u65ad\u4efb\u52a1\u8868\u8ff0\u548c\u516b\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u6d4b\u8bd5\u8def\u5f84\u5224\u65ad\u80fd\u529b\u662f\u5426\u80fd\u6cdb\u5316\u5230\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\uff08\u5982\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\uff09\u3002\u4f7f\u7528\u4e09\u4e2a\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18LLM\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7279\u5b9a\u5956\u52b1\u4f18\u5316\u548c\u84b8\u998f\u80fd\u5e26\u6765\u5f3a\u5927\u7684\u8def\u5f84\u5224\u65ad\u6027\u80fd\uff0c\u4f46\u5411\u4e0b\u6e38\u4efb\u52a1\u7684\u8fc1\u79fb\u6027\u8f83\u5f31\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\"\u5956\u52b1\u6a21\u578b\u98ce\u683c\"\u63a8\u7406\u7684\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e3a\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u76d1\u7763\u5982\u4f55\u5f71\u54cd\u533b\u7597GenAI\u7cfb\u7edf\u7684\u8bca\u65ad\u63a8\u7406\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u8f85\u52a9\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6821\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u6765\u4fee\u6b63\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u9519\u8bef\uff0c\u663e\u8457\u964d\u4f4eDER\u548c\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u3002", "motivation": "\u5927\u591a\u6570\u81ea\u52a8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u5728\u6ca1\u6709\u7528\u6237\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\uff0c\u800c\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6d41\u5f0fASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u751f\u6210\u7b80\u6d01\u6458\u8981\uff0c\u63a5\u53d7\u7528\u6237\u53e3\u5934\u53cd\u9988\u5e76\u7acb\u5373\u6574\u5408\u3002\u5f00\u53d1\u4e86SWM\u6280\u672f\u68c0\u6d4b\u548c\u5206\u5272\u591a\u8bf4\u8bdd\u4eba\u6bb5\uff0c\u4ee5\u53ca\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\u6280\u672f\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u7cfb\u7edf\u663e\u8457\u964d\u4f4e\u4e869.92%\u7684DER\u548c44.23%\u7684\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6709\u6548\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2509.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}", "AI": {"tldr": "\u63d0\u51fa\u644a\u9500\u6f5c\u5728\u5f15\u5bfc\uff08ALS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u5355\u4e2a\u5411\u91cf\u5728\u63a8\u7406\u65f6\u4ee5\u6052\u5b9a\u6210\u672c\u5e94\u7528\uff0c\u66ff\u4ee3\u6602\u8d35\u7684\u9010\u67e5\u8be2\u4f18\u5316\u5faa\u73af\uff0c\u5b9e\u73b02-5\u500d\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u8d2a\u5a6aCoT\u548c\u81ea\u4e00\u81f4\u6027\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\u56e0\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\u800c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u8fed\u4ee3\u4f18\u5316\u548c\u591a\u6b65\u9a8c\u8bc1\u9700\u898110-100\u500d\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u66f4\u76f4\u63a5\u4f46\u4ecd\u9700\u6602\u8d35\u7684\u9010\u67e5\u8be2\u4f18\u5316\u5faa\u73af\u3002", "method": "ALS\u8ba1\u7b97\u6210\u529f\u4e0e\u4e0d\u6210\u529f\u751f\u6210\u4e4b\u95f4\u9690\u85cf\u72b6\u6001\u7684\u5e73\u5747\u5dee\u5f02\uff0c\u4f7f\u7528\u8be5\u65b9\u5411\u6821\u51c6\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\u3002\u5f53\u89e3\u7801\u504f\u79bb\u6210\u529f\u6d41\u5f62\u65f6\uff0cALS\u5c06\u6fc0\u6d3b\u72b6\u6001\u63a8\u56de\u6b63\u786e\u65b9\u5411\uff0c\u6240\u6709\u4f18\u5316\u8ba1\u7b97\u5728\u79bb\u7ebf\u9636\u6bb5\u5b8c\u6210\u3002", "result": "\u5728GSM8K\u548cMATH-500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALS\u76f8\u6bd4\u8fed\u4ee3\u65b9\u6cd5\u5b9e\u73b02-5\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u8d2a\u5a6aCoT\u548c\u81ea\u4e00\u81f4\u6027\u57fa\u7ebf\uff0c\u6548\u7387-\u51c6\u786e\u7387\u6743\u8861\u63d0\u5347\u9ad8\u8fbe101%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6f5c\u5728\u4f18\u5316\u7684\u4e3b\u8981\u6536\u76ca\u53ef\u4ee5\u901a\u8fc7\u79bb\u7ebf\u65b9\u5f0f\u83b7\u5f97\uff0c\u4f7f\u590d\u6742\u63a8\u7406\u6280\u672f\u80fd\u591f\u5b9e\u9645\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd'N-Plus-1'GPT\u4ee3\u7406\u673a\u6784\uff0c\u7528\u4e8e\u673a\u68b0\u5de5\u7a0b\u95ee\u9898\u7684\u521d\u59cb\u4f4e\u6210\u672c\u5206\u6790\u3002\u8be5\u673a\u6784\u901a\u8fc7\u591a\u4e2a\u72ec\u7acb\u6c42\u89e3\u4ee3\u7406\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u518d\u901a\u8fc7\u6bd4\u8f83\u4ee3\u7406\u8fdb\u884c\u6c47\u603b\u548c\u6bd4\u8f83\uff0c\u4ee5\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "motivation": "GPT\u5728\u89e3\u51b3\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u95ee\u9898\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff08\u6210\u529f\u7387\u4ec585%\uff09\uff0c\u8fd9\u4f7f\u5f97'\u5f00\u7bb1\u5373\u7528'\u7684GPT\u4e0d\u9002\u5408\u5728\u6559\u80b2\u548c\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528'N-Plus-1'\u4ee3\u7406\u67b6\u6784\uff1a\u9996\u5148\u542f\u52a8N\u4e2a\u72ec\u7acb\u6c42\u89e3\u4ee3\u7406\u751f\u6210N\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u8c03\u7528\u6bd4\u8f83\u4ee3\u7406\u5bf9\u8fd9\u4e9b\u65b9\u6848\u8fdb\u884c\u6c47\u603b\u3001\u6bd4\u8f83\uff0c\u5e76\u63a8\u8350\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\uff0c\u5f53\u6bcf\u4e2a\u6c42\u89e3\u4ee3\u7406\u7684\u6210\u529f\u6982\u7387\u5927\u4e8e1/2\u65f6\uff0c\u591a\u6570\u89e3\u51b3\u65b9\u6848\u5f88\u53ef\u80fd\u662f\u6b63\u786e\u7684\u3002", "result": "\u4e0e\u5546\u4e1a\u591a\u4ee3\u7406\u6a21\u578bGrok Heavy\u76f8\u6bd4\uff0c\u8be5\u673a\u6784\u5728\u8bbe\u8ba1\u548c\u6027\u80fd\u4e0a\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46\u66f4\u6ce8\u91cd\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u7406\u673a\u6784\u80fd\u591f\u663e\u8457\u63d0\u9ad8GPT\u5728\u673a\u68b0\u5de5\u7a0b\u95ee\u9898\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u9002\u5408\u6559\u80b2\u548c\u5de5\u7a0b\u5b9e\u8df5\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ComputerAgent\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u684c\u9762\u5e94\u7528\u63a7\u5236\uff0c\u901a\u8fc7\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff08\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\uff09\u89e3\u51b3\u73b0\u6709MLLM\u65b9\u6cd5\u7684\u9ad8\u5ef6\u8fdf\u3001\u4f4e\u6837\u672c\u6548\u7387\u548c\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u684c\u9762\u5e94\u7528\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u6837\u672c\u6548\u7387\u5dee\u3001\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff1b\u4f7f\u7528\u4e09\u91cd\u6a21\u6001\u72b6\u6001\u7f16\u7801\u5668\uff08\u622a\u56fe\u3001\u4efb\u52a1ID\u3001\u6570\u503c\u72b6\u6001\uff09\u5904\u7406\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff1b\u96c6\u6210\u5143\u52a8\u4f5c\u548c\u65e9\u505c\u673a\u5236\u51cf\u5c11\u65e0\u6548\u4ea4\u4e92\uff1b\u4f7f\u7528\u7d27\u51d1\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u5c0f\u578b\u7b56\u7565\u7f51\u7edc\uff081500\u4e07\u53c2\u6570\uff09\u5b9e\u73b0\u8bbe\u5907\u4e0a\u63a8\u7406\u3002", "result": "\u5728135\u4e2a\u771f\u5b9e\u4e16\u754c\u684c\u9762\u4efb\u52a1\u6d4b\u8bd5\u4e2d\uff0cComputerAgent\u5728\u7b80\u5355\u4efb\u52a1\uff08<8\u6b65\uff09\u4e0a\u8fbe\u523092.1%\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u4efb\u52a1\uff08\u22658\u6b65\uff09\u4e0a\u8fbe\u523058.8%\u6210\u529f\u7387\uff0c\u5728\u7b80\u5355\u573a\u666f\u4e2d\u5339\u914d\u6216\u8d85\u8fc7200B\u53c2\u6570MLLM\u57fa\u7ebf\uff0c\u540c\u65f6\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e3a\u8ba1\u7b97\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u5355\u4e00MLLM\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "AI": {"tldr": "MOBILERL\u662f\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u81ea\u9002\u5e94\u7b97\u6cd5\u548c\u5956\u52b1\u8c03\u6574\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u79fb\u52a8GUI\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5728AndroidWorld\u548cAndroidLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u79fb\u52a8GUI\u4ee3\u7406\u9762\u4e34\u4efb\u52a1\u96be\u5ea6\u5206\u5e03\u4e0d\u5747\u548c\u5927\u89c4\u6a21\u73af\u5883\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMOBILERL\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u96be\u5ea6\u81ea\u9002\u5e94GRPO\u7b97\u6cd5\uff08ADAGRPO\uff09\uff0c\u5305\u542b\u96be\u5ea6\u81ea\u9002\u5e94\u6b63\u5411\u56de\u653e\u3001\u5931\u8d25\u8bfe\u7a0b\u8fc7\u6ee4\u548c\u6700\u77ed\u8def\u5f84\u5956\u52b1\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728Qwen2.5-VL-7B\u548cGLM-4.1V-9B\u6a21\u578b\u4e0a\u5e94\u7528\uff0cMOBILERL-9B\u5728AndroidWorld\u8fbe\u523075.8%\u6210\u529f\u7387\uff0cAndroidLab\u8fbe\u523046.8%\u6210\u529f\u7387\uff0c\u5747\u4e3a\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MOBILERL\u6846\u67b6\u80fd\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5728\u591a\u6837\u5316\u79fb\u52a8\u5e94\u7528\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5df2\u5e94\u7528\u4e8eAutoGLM\u4ea7\u54c1\u5e76\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "\u672c\u6587\u5bf9LLM\u5728\u56fe\u6570\u636e\u4e0a\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5728\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u957f\u6587\u672c\u6216\u9ad8\u5bc6\u5ea6\u56fe\u4e2d\u4f18\u52bf\u660e\u663e\uff0c\u4e14\u6240\u6709\u4ea4\u4e92\u7b56\u7565\u5728\u5f02\u8d28\u56fe\u4e2d\u90fd\u6709\u6548\u3002", "motivation": "\u968f\u7740LLM\u5728\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u5bf9LLM\u4e0e\u56fe\u6570\u636e\u4ea4\u4e92\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u9700\u8981\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u4ee5\u6307\u5bfc\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86LLM-\u56fe\u4ea4\u4e92\u6a21\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4ee3\u7801\u751f\u6210\uff09\u3001\u6570\u636e\u96c6\u9886\u57df\u3001\u7ed3\u6784\u673a\u5236\u3001\u7279\u5f81\u7279\u6027\u548c\u6a21\u578b\u914d\u7f6e\u7b49\u591a\u4e2a\u5173\u952e\u53d8\u91cf\u3002", "result": "\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u6700\u5f3a\uff0c\u7279\u522b\u662f\u5728\u957f\u6587\u672c\u6216\u9ad8\u5bc6\u5ea6\u56fe\u4e2d\uff1b\u6240\u6709\u4ea4\u4e92\u7b56\u7565\u5728\u5f02\u8d28\u56fe\u4e2d\u90fd\u6709\u6548\uff1b\u4ee3\u7801\u751f\u6210\u80fd\u7075\u6d3b\u8c03\u6574\u5bf9\u7ed3\u6784\u3001\u7279\u5f81\u6216\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f53\u524dLLM-\u56fe\u4ea4\u4e92\u6a21\u5f0f\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u4f18\u52a3\u52bf\u5206\u6790\uff0c\u5e76\u4e3a\u672a\u6765\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u539f\u5219\u3002", "topic": "agent analysis"}}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "IFEval-FC\u662f\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u53c2\u6570\u63cf\u8ff0\u4e2d\u683c\u5f0f\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5173\u6ce8\u53c2\u6570\u6b63\u786e\u6027\u800c\u5ffd\u7565\u683c\u5f0f\u8981\u6c42\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982BFCL\u3001tau^2-Bench\u3001ACEBench\uff09\u53ea\u8bc4\u4f30\u53c2\u6570\u6b63\u786e\u6027\uff0c\u4e0d\u6d4b\u8bd5\u683c\u5f0f\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u8fd9\u5728\u73b0\u5b9eAI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u7f3a\u9677\u3002", "method": "\u57fa\u4e8eIFEval\u8bbe\u8ba1\uff0c\u5728JSON schema\u63cf\u8ff0\u4e2d\u76f4\u63a5\u7f16\u7801\u53ef\u9a8c\u8bc1\u7684\u683c\u5f0f\u8981\u6c42\uff08\u5982\u5fc5\u987b\u7528\u53cc\u5f15\u53f7\u3001ISO\u65e5\u671f\u683c\u5f0f\u7b49\uff09\uff0c\u5305\u542b750\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6bcf\u4e2a\u7528\u4f8b\u5305\u542b\u4e00\u4e2a\u51fd\u6570\u548c\u5bf9\u5e94\u7684\u7528\u6237\u67e5\u8be2\uff0c\u91c7\u7528\u5168\u81ea\u52a8\u7b97\u6cd5\u8bc4\u4f30\u3002", "result": "\u5373\u4f7f\u662fGPT-5\u548cClaude 4.1 Opus\u7b49\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\u4e5f\u7ecf\u5e38\u65e0\u6cd5\u9075\u5faa\u57fa\u672c\u683c\u5f0f\u89c4\u5219\uff0c\u63ed\u793a\u4e86\u73b0\u5b9e\u4e16\u754c\u4ee3\u7406\u7cfb\u7edf\u7684\u5b9e\u9645\u5c40\u9650\u6027\u3002", "conclusion": "\u683c\u5f0f\u6307\u4ee4\u9075\u5faa\u662f\u51fd\u6570\u8c03\u7528\u7684\u5173\u952e\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0cIFEval-FC\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u8fd9\u4e00\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2509.18127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "Safe-SAIL\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5bf9\u5b89\u5168\u76f8\u5173\u884c\u4e3a\u7684\u673a\u5236\u6027\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bc4\u4f30LLM\u8f93\u51fa\u6216\u7279\u5b9a\u5b89\u5168\u4efb\u52a1\uff0c\u96be\u4ee5\u5e94\u5bf9\u66f4\u5e7f\u6cdb\u7684\u672a\u5b9a\u4e49\u98ce\u9669\u3002\u7a00\u758f\u81ea\u7f16\u7801\u5668\u867d\u7136\u6709\u52a9\u4e8e\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u672a\u80fd\u5145\u5206\u89e3\u91ca\u4e0e\u7ec6\u7c92\u5ea6\u5b89\u5168\u6982\u5ff5\u76f8\u5173\u7684\u7279\u5f81\u3002", "method": "\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u5177\u6709\u6700\u4f73\u6982\u5ff5\u7279\u5b9a\u53ef\u89e3\u91ca\u6027\u7684SAE\uff0c\u89e3\u91ca\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7b56\u7565\u6765\u6269\u5c55\u89e3\u91ca\u8fc7\u7a0b\u3002", "result": "\u5c06\u53d1\u5e03\u5305\u542bSAE\u68c0\u67e5\u70b9\u548c\u4eba\u7c7b\u53ef\u8bfb\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u5168\u9762\u5de5\u5177\u5305\uff0c\u652f\u6301\u5bf9\u5b89\u5168\u98ce\u9669\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u4fc3\u8fdbLLM\u5b89\u5168\u7814\u7a76\uff0c\u901a\u8fc7\u673a\u5236\u6027\u7406\u89e3\u6765\u5e94\u5bf9\u9ad8\u98ce\u9669\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLMZ+\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u767d\u540d\u5355\u673a\u5236\u4fdd\u62a4\u667a\u80fd\u4f53AI\u514d\u53d7\u653b\u51fb\uff0c\u76f8\u6bd4\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u66f4\u5b89\u5168\u9ad8\u6548\u3002", "motivation": "\u667a\u80fd\u4f53AI\u5177\u6709\u7279\u6743\u6570\u636e\u8bbf\u95ee\u548cAPI\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u5176\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u7279\u6027\u5e26\u6765\u4e86\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u68c0\u6d4b\u673a\u5236\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u63d0\u793a\u767d\u540d\u5355\u65b9\u6cd5\uff0c\u53ea\u5141\u8bb8\u4e0a\u4e0b\u6587\u76f8\u5173\u4e14\u5b89\u5168\u7684\u6d88\u606f\u4e0e\u667a\u80fd\u4f53LLM\u4ea4\u4e92\uff0c\u786e\u4fdd\u6240\u6709\u4ea4\u6362\u7b26\u5408\u9884\u5b9a\u4e49\u7528\u4f8b\u548c\u64cd\u4f5c\u8fb9\u754c\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aLLMZ+\u5bf9\u5e38\u89c1\u8d8a\u72f1\u63d0\u793a\u5177\u6709\u5f3a\u97e7\u6027\uff0c\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u5728\u5b9e\u9a8c\u73af\u5883\u4e0b\u53ef\u964d\u81f30\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5408\u6cd5\u4e1a\u52a1\u901a\u4fe1\u3002", "conclusion": "LLMZ+\u65b9\u6cd5\u7b80\u5316\u4e86\u5b89\u5168\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u957f\u671f\u97e7\u6027\uff0c\u51cf\u5c11\u4e86\u7ef4\u6301LLM\u4fe1\u606f\u5b89\u5168\u6240\u9700\u7684\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2509.18632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Planorama\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7528\u6237\u504f\u597d\u548c\u6a21\u578b\u504f\u597d\u5e76\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u54ea\u4e9b\u8ba1\u5212\u771f\u6b63\u5bf9\u7528\u6237\u6709\u5e2e\u52a9\uff0c\u8868\u660e\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\u53ef\u80fd\u4e0e\u5b9e\u9645\u5e2e\u52a9\u6027\u5b58\u5728\u504f\u5dee\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\u548cChatbotArena\uff09\u57fa\u4e8e\u7528\u6237\u504f\u597d\u6765\u8bad\u7ec3\u6216\u8bc4\u4f30\u8ba1\u5212\u7684\u6709\u7528\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u504f\u597d\u662f\u5426\u771f\u6b63\u53cd\u6620\u5e2e\u52a9\u6027\u7684\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1Planorama\u754c\u9762\uff0c\u8ba9126\u540d\u7528\u6237\u4f7f\u7528LLM\u751f\u6210\u7684\u8ba1\u5212\u56de\u7b54300\u4e2a\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u6536\u96c64388\u6b21\u8ba1\u5212\u6267\u884c\u548c5584\u6b21\u6bd4\u8f83\u6570\u636e\uff0c\u6d4b\u91cf\u8ba1\u5212\u5e2e\u52a9\u6027\u548c\u7528\u6237\u504f\u597d\u3002", "result": "1\uff09\u7528\u6237/\u6a21\u578b\u504f\u597d\u548c\u4ee3\u7406\u6210\u529f\u7387\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u8ba1\u5212\u5bf9\u7528\u6237\u7684\u5e2e\u52a9\u6027\uff1b2\uff09\u8fd9\u79cd\u5dee\u8ddd\u4e0d\u662f\u7528\u6237\u7279\u5b9a\u504f\u597d\u9020\u6210\u7684\uff1b3\uff09\u8868\u9762\u7279\u5f81\uff08\u5982\u7b80\u6d01\u6027\uff09\u4e0e\u504f\u597d\u76f8\u5173\u4f46\u4e0d\u80fd\u9884\u6d4b\u5e2e\u52a9\u6027\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u7684\u53cd\u9988\u6765\u5bf9\u9f50\u6709\u5e2e\u52a9\u7684LLM\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u57fa\u4e8e\u770b\u8d77\u6765\u6709\u5e2e\u52a9\u7684\u504f\u597d\u3002", "topic": "agent analysis"}}
{"id": "2509.18136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790LLM\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u9a8c\u8bc1\u7ed3\u6784\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u89c4\u6a21\u548c\u80fd\u529b\u4e0a\u5feb\u901f\u589e\u957f\uff0c\u4f46\u5173\u4e8e\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u5f00\u6e90LLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6570\u636e\u6316\u6398\u9a71\u52a8\u7684\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u5e76\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "\u91cf\u5316\u4e86\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u9488\u5bf9\u6027\u5f00\u53d1\u548c\u5e94\u7528\uff0c\u6570\u636e\u96c6\u5c06\u5728HuggingFace\u4e0a\u53d1\u5e03\u3002", "topic": "agent analysis"}}
{"id": "2509.18713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86MemOrb\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u8bed\u8a00\u5f3a\u5316\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u63d0\u70bc\u591a\u8f6e\u4ea4\u4e92\u4e3a\u7d27\u51d1\u7684\u7b56\u7565\u53cd\u601d\u6765\u589e\u5f3aLLM\u667a\u80fd\u4f53\u5728\u5ba2\u6237\u670d\u52a1\u4e2d\u7684\u957f\u671f\u53ef\u9760\u6027\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u5ba2\u6237\u670d\u52a1\u4e2d\u7ecf\u5e38\u51fa\u73b0\u8de8\u4f1a\u8bdd\u9057\u5fd8\u3001\u91cd\u590d\u9519\u8bef\u548c\u7f3a\u4e4f\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u673a\u5236\u7684\u95ee\u9898\uff0c\u5728\u9700\u8981\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u7684\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u8bbe\u8ba1MemOrb\u8bb0\u5fc6\u5c42\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u63d0\u70bc\u4e3a\u7b56\u7565\u53cd\u601d\u5b58\u50a8\u5728\u5171\u4eab\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u68c0\u7d22\u8fd9\u4e9b\u53cd\u601d\u6765\u6307\u5bfc\u51b3\u7b56\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "MemOrb\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u8f6e\u6210\u529f\u7387\u4e0a\u83b7\u5f97\u9ad8\u8fbe63\u4e2a\u767e\u5206\u70b9\u7684\u63d0\u5347\uff0c\u5e76\u5728\u91cd\u590d\u8bd5\u9a8c\u4e2d\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u662f\u589e\u5f3a\u51bb\u7ed3LLM\u667a\u80fd\u4f53\u5728\u5ba2\u6237\u670d\u52a1\u573a\u666f\u4e2d\u957f\u671f\u53ef\u9760\u6027\u7684\u6709\u6548\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\uff08DataAgents\uff09\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u96c6\u6210LLM\u63a8\u7406\u4e0e\u4efb\u52a1\u5206\u89e3\u3001\u884c\u52a8\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u73b0\u4ece\u590d\u6742\u6570\u636e\u5230\u77e5\u8bc6\u7684\u81ea\u4e3b\u8f6c\u6362\u3002", "motivation": "\u968f\u7740\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\u5de5\u4f5c\u53d8\u5f97\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6570\u636e\u4e0eAI\u4e4b\u95f4\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u7ed3\u6784\u5f80\u5f80\u4e0d\u9002\u5408AI\u5229\u7528\u3002", "method": "DataAgents\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u3001\u8c03\u7528\u5f3a\u5927\u5de5\u5177\u548c\u9002\u5e94\u591a\u6837\u5316\u6570\u636e\u4efb\u52a1\uff0c\u80fd\u591f\u5904\u7406\u6570\u636e\u6536\u96c6\u3001\u96c6\u6210\u3001\u9884\u5904\u7406\u3001\u9009\u62e9\u3001\u8f6c\u6362\u3001\u91cd\u65b0\u52a0\u6743\u3001\u589e\u5f3a\u3001\u91cd\u7f16\u7a0b\u3001\u4fee\u590d\u548c\u68c0\u7d22\u7b49\u64cd\u4f5c\u3002", "result": "DataAgents\u80fd\u591f\u5c06\u590d\u6742\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u8fde\u8d2f\u4e14\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff0c\u4ee3\u8868\u4e86\u5411\u81ea\u4e3b\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "conclusion": "\u9700\u8981\u534f\u540c\u52aa\u529b\u63a8\u8fdb\u884c\u52a8\u5de5\u4f5c\u6d41\u4f18\u5316\u3001\u5efa\u7acb\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u751f\u6001\u7cfb\u7edf\u3001\u4fdd\u62a4\u9690\u79c1\u3001\u5e73\u8861\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684DataAgent\u9632\u62a4\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agent Directory Service (ADS)\uff0c\u4e00\u4e2a\u7528\u4e8eAI\u4ee3\u7406\u80fd\u529b\u53d1\u73b0\u7684\u5206\u5e03\u5f0f\u76ee\u5f55\u670d\u52a1\uff0c\u91c7\u7528\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3001\u5206\u5c42\u5206\u7c7b\u548c\u52a0\u5bc6\u7b7e\u540d\u6280\u672f\uff0c\u652f\u6301\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u3001\u591a\u7ef4\u53d1\u73b0\u3002", "motivation": "\u968f\u7740\u591a\u4ee3\u7406\u7cfb\u7edf(MAS)\u7684\u5f02\u6784\u6027\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u53d1\u73b0\u548c\u7ba1\u7406AI\u4ee3\u7406\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6765\u6e90\u7684\u5206\u5e03\u5f0f\u76ee\u5f55\u670d\u52a1\uff0c\u4ee5\u89e3\u51b3\u4ee3\u7406\u53d1\u73b0\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8eOpen Agentic Schema Framework (OASF)\uff0c\u91c7\u7528\u4e24\u7ea7\u6620\u5c04\u67b6\u6784\uff0c\u5229\u7528Kademlia\u5206\u5e03\u5f0f\u54c8\u5e0c\u8868(DHT)\u5b9e\u73b0\u80fd\u529b\u7d22\u5f15\u4e0e\u5185\u5bb9\u4f4d\u7f6e\u7684\u89e3\u8026\uff0c\u91cd\u7528OCI/ORAS\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5de5\u4ef6\u5206\u53d1\uff0c\u96c6\u6210Sigstore\u7528\u4e8e\u6765\u6e90\u9a8c\u8bc1\u3002", "result": "ADS\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7406\u80fd\u529b\u53d1\u73b0\uff0c\u652f\u6301\u591a\u79cd\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\uff08\u5982LLM\u63d0\u793a\u4ee3\u7406\u3001MCP\u670d\u52a1\u5668\u3001A2A\u7ec4\u4ef6\uff09\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u7279\u6027\u3002", "conclusion": "ADS\u4e3a\u65b0\u5174\u4ee3\u7406\u6ce8\u518c\u548c\u4e92\u64cd\u4f5c\u6027\u5021\u8bae\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u67b6\u6784\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u53d1\u73b0\u548c\u4e92\u64cd\u4f5c\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "LLMCHECKER\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91(PCTL)\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u03b1-k\u6709\u754c\u6587\u672c\u751f\u6210\u6765\u9650\u5236\u751f\u6210\u8fc7\u7a0b\uff0c\u53ea\u5173\u6ce8\u6bcf\u4e00\u6b65\u4e2dtop-k\u4ee4\u724c\u7684\u03b1\u6700\u5927\u7d2f\u79ef\u6982\u7387\u3002", "motivation": "\u73b0\u6709LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u751f\u6210\u6587\u672c\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002\u4f5c\u8005\u53d1\u73b0\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u901a\u5e38\u53ea\u6709\u6709\u9650\u6570\u91cf\u7684\u4ee4\u724c\u88ab\u9009\u62e9\uff0c\u4e14\u8fd9\u4e9b\u9009\u62e9\u5e76\u4e0d\u603b\u662f\u76f8\u540c\u7684\u3002", "method": "\u63d0\u51fa\u03b1-k\u6709\u754c\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u53ea\u8003\u8651top-k\u4ee4\u724c\u4e2d\u7d2f\u79ef\u6982\u7387\u8d85\u8fc7\u9608\u503c\u03b1\u7684\u4ee4\u724c\u3002LLMCHECKER\u5229\u7528\u6a21\u578b\u68c0\u67e5\u6280\u672f\u9a8c\u8bc1PCTL\u5c5e\u6027\uff0c\u652f\u6301\u591a\u79cd\u6587\u672c\u91cf\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2aLLM\u6a21\u578b\uff08\u5305\u62ecLlama\u3001Gemma\u3001Mistral\u3001Genstruct\u548cBERT\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u68c0\u67e5\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06PCTL\u6a21\u578b\u68c0\u67e5\u5e94\u7528\u4e8eLLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u9a8c\u8bc1\uff0c\u4e3aLLM\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2509.18813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "MAPEX\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u5173\u952e\u8bcd\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u6587\u6863\u957f\u5ea6\u7684\u53cc\u8def\u5f84\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5173\u952e\u8bcd\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u9636\u6bb5\u63a8\u7406\u7ba1\u9053\u548c\u7edf\u4e00\u63d0\u793a\u7b56\u7565\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u6587\u6863\u65f6\u6548\u679c\u6709\u9650", "method": "MAPEX\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5305\u542b\u4e13\u5bb6\u62db\u52df\u3001\u5019\u9009\u63d0\u53d6\u3001\u4e3b\u9898\u6307\u5bfc\u3001\u77e5\u8bc6\u589e\u5f3a\u548c\u540e\u5904\u7406\u6a21\u5757\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u7b56\u7565\uff1a\u77ed\u6587\u672c\u4f7f\u7528\u77e5\u8bc6\u9a71\u52a8\u63d0\u53d6\uff0c\u957f\u6587\u672c\u4f7f\u7528\u4e3b\u9898\u5f15\u5bfc\u63d0\u53d6", "result": "\u57286\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c3\u79cd\u4e0d\u540cLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAPEX\u5728F1@5\u6307\u6807\u4e0a\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u65b9\u6cd52.44%\uff0c\u4f18\u4e8e\u6807\u51c6LLM\u57fa\u7ebf4.01%", "conclusion": "MAPEX\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5173\u952e\u8bcd\u63d0\u53d6\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u666e\u9002\u6027", "topic": "agent analysis"}}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u8bb0\u5fc6\u5b9a\u4e49\u548c\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\uff08\u53c2\u6570\u5316\u3001\u4e0a\u4e0b\u6587\u3001\u5916\u90e8\u3001\u7a0b\u5e8f\u6027/\u60c5\u666f\u6027\uff09\uff0c\u5efa\u7acb\u4e86\u8bb0\u5fc6\u56db\u5143\u7ec4\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5206\u5c42\u8bc4\u4f30\u534f\u8bae\u6765\u6807\u51c6\u5316LLM\u8bb0\u5fc6\u80fd\u529b\u7684\u8bc4\u6d4b\u3002", "motivation": "\u5f53\u524dLLM\u8bb0\u5fc6\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5bfc\u81f4\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u7ed3\u679c\u96be\u4ee5\u6bd4\u8f83\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6bd4\u8f83\u3001\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u7edf\u6765\u63a8\u52a8\u7814\u7a76\u548c\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e09\u8bbe\u7f6e\u534f\u8bae\uff08\u4ec5\u53c2\u6570\u5316\u3001\u79bb\u7ebf\u68c0\u7d22\u3001\u5728\u7ebf\u68c0\u7d22\uff09\u89e3\u8026\u80fd\u529b\u4e0e\u4fe1\u606f\u53ef\u7528\u6027\uff0c\u6784\u5efa\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u65f6\u95f4\u6cbb\u7406\u548c\u6cc4\u6f0f\u5ba1\u8ba1\uff0c\u5e76\u63d0\u51faDMM Gov\u66f4\u65b0\u548c\u9057\u5fd8\u534f\u8c03\u673a\u5236\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u8bb0\u5fc6\u5b9a\u4e49\u3001\u5206\u7c7b\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u6cbb\u7406\u6846\u67b6\u7684\u5b8c\u6574\u4f53\u7cfb\uff0c\u4e3aLLM\u8bb0\u5fc6\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8bb0\u5fc6\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u53ef\u6bd4\u8f83\u3001\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u53ef\u6d4b\u8bd5\u7684\u547d\u9898\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684\u5f00\u653e\u6e90\u4ee3\u7801MoE\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6d41\u7a0b\uff08\u5305\u62ec\u957f\u94fe\u601d\u7ef4\u6570\u636e\u51b7\u542f\u52a8\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u80fd\u529b\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u63a8\u7406\u9886\u57df\u51cf\u5c11token\u6d88\u8017\u3002", "method": "\u91c7\u7528\u957f\u94fe\u601d\u7ef4\u6570\u636e\u51b7\u542f\u52a8\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u8bad\u7ec3\u65b9\u6848\uff08\u5c06\u4e0d\u540c\u9886\u57df\u7684\u4e13\u5bb6\u6a21\u578b\u89e3\u8026\u4f18\u5316\u540e\u878d\u5408\uff09\uff0c\u5e76\u4f7f\u7528DORA\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728AIME-25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u667a\u80fd\u4f53\u63a8\u7406\u7684\u5e73\u5747token\u6d88\u8017\u51cf\u5c11\u4e8664.5%\uff08\u4ece19,653\u964d\u81f36,965\uff09\uff0c\u4e14\u4e0d\u964d\u4f4e\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "LongCat-Flash-Thinking\u5c55\u793a\u4e86\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff0c\u4e3a\u63a8\u7406\u7cfb\u7edf\u548c\u667a\u80fd\u4f53AI\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e8618\u79cd\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u5e7b\u89c9\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u8ba4\u77e5\u3001\u63a8\u7406\u548c\u4ea4\u4e92\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4efb\u52a1\u6267\u884c\u7684\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u4ed4\u7ec6\u5206\u6790\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u6765\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u51fa\u73b0\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u5e76\u5bf9\u5927\u91cf\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u4e86\u8be6\u7ec6\u56de\u987e\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u4e8618\u79cd\u5bfc\u81f4\u667a\u80fd\u4f53\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff0c\u603b\u7ed3\u4e86\u5e7b\u89c9\u7f13\u89e3\u548c\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u91cd\u70b9\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u8c03\u67e5\u5c06\u4e3a\u89e3\u51b3\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2509.18162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5361\u8f66\u4e0e\u65e0\u4eba\u673a\u534f\u540c\u914d\u9001\u95ee\u9898\uff0c\u63d0\u51fa\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728N=50\u7684\u5b9e\u4f8b\u4e0a\u6bd4ALNS\u65b9\u6cd5\u5e73\u5747\u63d0\u53472.73%\uff0c\u4e0eNN\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u7814\u7a76\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u5361\u8f66\u4e0e\u65e0\u4eba\u673a\u7684\u534f\u540c\u4f5c\u4e1a\u95ee\u9898\uff0c\u7279\u522b\u8003\u8651\u7535\u6c60\u7eed\u822a\u7ea6\u675f\u548c\u5145\u7535\u9700\u6c42\uff0c\u65e8\u5728\u4f18\u5316\u6574\u4f53\u914d\u9001\u65f6\u95f4\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u5668\uff0c\u7ed3\u5408ALNS\u5361\u8f66\u8def\u5f84\u89c4\u5212\u548c\u5c0f\u578b\u6307\u9488/\u6ce8\u610f\u529b\u7b56\u7565\u6765\u8c03\u5ea6\u65e0\u4eba\u673a\u4efb\u52a1\uff0c\u4f7f\u7528\u7cbe\u786e\u65f6\u95f4\u7ebf\u6a21\u62df\u5668\u786e\u4fdd\u53ef\u884c\u6027\u3002", "result": "\u5728\u6b27\u51e0\u91cc\u5f97\u5b9e\u4f8b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u5b8c\u6210\u65f6\u95f4\u4e3a5.203\u00b10.093\uff0c\u6bd4ALNS\u65b9\u6cd5\u63d0\u53472.73%\uff0c\u4e0eNN\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff08\u4ec5\u5dee0.10%\uff09\u3002", "conclusion": "\u5b66\u4e60\u7684\u8c03\u5ea6\u5668\u80fd\u591f\u5e73\u8861\u5361\u8f66\u7b49\u5f85\u65f6\u95f4\u4e0e\u65e0\u4eba\u673a\u98de\u884c\u65f6\u95f4\uff0c\u6709\u6548\u6700\u5c0f\u5316\u603b\u5b8c\u6210\u65f6\u95f4\uff0c\u4e14\u5728\u6240\u6709\u6d4b\u8bd5\u5b9e\u4f8b\u4e0a\u90fd\u4e0d\u52a3\u4e8eALNS\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19094", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "PoT\u662f\u4e00\u79cd\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21LLM\u7684\u63a8\u7406\u4e3a\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u8ba4\u77e5\u64cd\u4f5c\u6765\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u54cd\u5e94\uff0c\u7136\u540e\u6839\u636e\u7528\u6237\u504f\u597d\u805a\u5408\u5f97\u5230\u4e2a\u6027\u5316QA\u54cd\u5e94\u3002", "motivation": "\u4e2a\u6027\u5316QA\u7cfb\u7edf\u5bf9\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u9762\u4e34\u4ece\u957f\u566a\u58f0\u9690\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u504f\u597d\u3001\u751f\u6210\u540c\u65f6\u6b63\u786e\u4e14\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u54cd\u5e94\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u601d\u60f3\u8def\u5f84\uff08PoT\uff09\u65b9\u6cd5\uff0c\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u63a8\u7406\u3001\u4fee\u8ba2\u3001\u4e2a\u6027\u5316\u548c\u6f84\u6e05\u7b49\u8ba4\u77e5\u64cd\u4f5c\uff0c\u63a2\u7d22\u591a\u79cd\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u5019\u9009\u54cd\u5e94\uff0c\u7136\u540e\u6839\u636e\u7528\u6237\u504f\u597d\u8fdb\u884c\u805a\u5408\u548c\u91cd\u52a0\u6743\u3002", "result": "\u5728LaMP-QA\u4e2a\u6027\u5316QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoT\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u76f8\u5bf9\u6539\u8fdb\u9ad8\u8fbe13.1%\u3002\u4eba\u5de5\u8bc4\u4f30\u663e\u793a66%\u7684\u60c5\u51b5\u4e0b\u504f\u597dPoT\u8f93\u51fa\uff0c\u4ec5\u670915%\u5e73\u5c40\u3002", "conclusion": "PoT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e2a\u6027\u5316QA\u7684\u6311\u6218\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55LLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "CoPiC\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u9a71\u52a8\u89c4\u5212\u548c\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u4f30\u5668\u7684AI\u4ee3\u7406\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u9ad8\u5c42\u89c4\u5212\u7a0b\u5e8f\u5e76\u5229\u7528\u8bad\u7ec3\u597d\u7684\u8bc4\u4f30\u5668\u9009\u62e9\u957f\u671f\u5956\u52b1\u6700\u4f18\u7684\u8ba1\u5212\uff0c\u663e\u8457\u51cf\u5c11LLM\u67e5\u8be2\u6b21\u6570\u5e76\u63d0\u9ad8\u89c4\u5212\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u67e5\u8be2\u548c\u73af\u5883\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u5bfc\u81f4\u67e5\u8be2\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5b9e\u73b0\u957f\u671f\u5956\u52b1\u5bf9\u9f50\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u67e5\u8be2\u6210\u672c\u53c8\u80fd\u4f18\u5316\u957f\u671f\u89c4\u5212\u6548\u679c\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u591a\u6837\u5316\u9ad8\u5c42\u89c4\u5212\u7a0b\u5e8f\uff0c\u8fd9\u4e9b\u7a0b\u5e8f\u8fed\u4ee3\u4ea7\u751f\u548c\u4f18\u5316\u5019\u9009\u8ba1\u5212\uff1b\u8bad\u7ec3\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u4f30\u5668\u8bc4\u4f30\u5019\u9009\u8ba1\u5212\u4e0e\u957f\u671f\u5956\u52b1\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff1b\u9009\u62e9\u6700\u4f18\u8ba1\u5212\u6267\u884c\u3002", "result": "\u5728ALFWorld\u3001NetHack\u548cStarCraft II Unit Building\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoPiC\u76f8\u6bd4AdaPlanner\u548cReflexion\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534723.33%\uff0c\u67e5\u8be2\u6210\u672c\u964d\u4f4e91.27%\u3002", "conclusion": "CoPiC\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u89c4\u5212\u548c\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u4f30\u5668\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u89c4\u5212\u4e2d\u7684\u67e5\u8be2\u6210\u672c\u9ad8\u548c\u957f\u671f\u5956\u52b1\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.18169", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "PiMoE\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5185\u751f\u96c6\u6210\u8ba1\u7b97\u80fd\u529b\uff0c\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u63a8\u7406\u7684\u878d\u5408\uff0c\u76f8\u6bd4\u5fae\u8c03LLM\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u3001token\u4f7f\u7528\u548cGPU\u80fd\u8017\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5c06\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u4f5c\u4e3a\u5185\u5728\u53ef\u89e3\u91ca\u80fd\u529b\u96c6\u6210\uff0c\u800c\u4e3b\u6d41\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u591a\u6a21\u6001\u80fd\u529b\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPiMoE\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u522b\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\u3001\u6587\u672c\u5230\u8ba1\u7b97\u6a21\u5757\u548c\u8def\u7531\u5668\uff0c\u5728\u63a8\u7406\u65f6\u8def\u7531\u5668\u5728token\u7ea7\u522b\u6307\u5bfc\u8ba1\u7b97\u548c\u63a8\u7406\uff0c\u5b9e\u73b0\u5355\u94fe\u601d\u7ef4\u4e2d\u7684\u8fed\u4ee3\u4ea4\u66ff\u3002", "result": "\u5728\u4e24\u4e2a\u63a8\u7406-\u8ba1\u7b97\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPiMoE\u4e0d\u4ec5\u6bd4\u76f4\u63a5\u5fae\u8c03LLM\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u76f8\u6bd4\u4e3b\u6d41\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u54cd\u5e94\u5ef6\u8fdf\u3001token\u4f7f\u7528\u548cGPU\u80fd\u8017\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "PiMoE\u4e3a\u4e0b\u4e00\u4ee3\u79d1\u5b66\u6216\u5de5\u4e1a\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u667a\u80fd\u4f53\u56e2\u961f\u7ed3\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\u673a\u5236\u548c\u5e15\u7d2f\u6258\u539f\u5219\u7684\u5e73\u8861\u56e2\u961f\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u73b0\u6709MAS\u521d\u59cb\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u540e\u7eed\u9636\u6bb5\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f18\u5316\u667a\u80fd\u4f53\u56e2\u961f\u7ed3\u6784\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "method": "AgentInit\u5305\u542b\u591a\u8f6e\u667a\u80fd\u4f53\u4ea4\u4e92\u4e0e\u53cd\u601d\u3001\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\u673a\u5236\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5e15\u7d2f\u6258\u539f\u5219\u7684\u5e73\u8861\u56e2\u961f\u9009\u62e9\u7b56\u7565\u6765\u517c\u987e\u591a\u6837\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAgentInit\u5728\u591a\u79cd\u6846\u67b6\u548c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5\u548c\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u6027\u80fd\u63d0\u5347\u8fbe1.2-1.6\u500d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "conclusion": "AgentInit\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u53ef\u9760MAS\u521d\u59cb\u5316\u65b9\u6cd5\u7684\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.19170", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fde\u7eed\u601d\u7ef4\u94fe\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f6f\u6807\u8bb0\uff08\u6807\u8bb0\u6df7\u5408\u52a0\u566a\u58f0\uff09\u5b9e\u73b0\u53ef\u6269\u5c55\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u79bb\u6563\u6807\u8bb0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u6807\u8bb0\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u56f0\u96be\uff1a\u8981\u4e48\u4ec5\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u8fde\u7eed\u6807\u8bb0\uff0c\u8981\u4e48\u9700\u8981\u4ece\u79bb\u6563\u601d\u7ef4\u94fe\u84b8\u998f\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u8fde\u7eed\u601d\u7ef4\u94fe\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fde\u7eed\u601d\u7ef4\u94fe\uff0c\u91c7\u7528\u8f6f\u6807\u8bb0\uff08\u6807\u8bb0\u6df7\u5408\u52a0\u566a\u58f0\uff09\u8fdb\u884c\u63a2\u7d22\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u53ef\u8bad\u7ec3\u6570\u767e\u4e2a\u6807\u8bb0\u7684\u8fde\u7eed\u601d\u7ef4\u94fe\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fde\u7eed\u601d\u7ef4\u94fe\u8bad\u7ec3\u5728pass@1\u4e0a\u5339\u914d\u79bb\u6563\u6807\u8bb0\u65b9\u6cd5\uff0c\u5728pass@32\u4e0a\u8d85\u8d8a\uff0c\u663e\u793a\u51fa\u66f4\u5927\u7684\u601d\u7ef4\u94fe\u591a\u6837\u6027\u3002\u6700\u4f73\u65b9\u6848\u662f\u8bad\u7ec3\u65f6\u4f7f\u7528\u8fde\u7eed\u6807\u8bb0\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u79bb\u6563\u6807\u8bb0\u3002", "conclusion": "\u8fde\u7eed\u601d\u7ef4\u94fe\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u5728\u57df\u5916\u4efb\u52a1\u4e0a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u5f71\u54cd\u66f4\u6e29\u548c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "OPRL\u662f\u4e00\u79cd\u7528\u4e8e\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u901a\u7528\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u548c\u7b56\u7565\uff0c\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u9aa4\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u548c\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u73af\u5883\u4e2d\u7684\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u9762\u4e34\u7a00\u758f\u548c\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u504f\u5dee\u3001\u5956\u52b1\u653b\u51fb\u3001\u9ad8\u65b9\u5dee\u7b49\u95ee\u9898\u3002", "method": "OPRL\u901a\u8fc7\u8f68\u8ff9DPO\u76ee\u6807\u4f18\u5316\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u6b65\u9aa4\u5956\u52b1\uff0c\u7ed3\u5408\u7ed3\u679c\u5956\u52b1\u8ba1\u7b97\u6b65\u9aa4\u7ea7\u4f18\u52bf\uff0c\u4e0e\u56de\u5408\u7ea7\u4f18\u52bf\u7ed3\u5408\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u5f62\u6210\u81ea\u589e\u5f3a\u5faa\u73af\u3002", "result": "\u5728WebShop\u3001VisualSokoban\u548cSOTOPIA\u7b49\u4e09\u4e2a\u4e0d\u540c\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOPRL\u8868\u73b0\u51fa\u4f18\u4e8e\u524d\u6cbfLLM\u548c\u5f3aRL\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u4f4e\u7684\u8bad\u7ec3\u65b9\u5dee\u3002", "conclusion": "OPRL\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u7ed3\u679c\u90fd\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u6548\u63a2\u7d22\u7684\u73af\u5883\u4e2d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19249", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "RLPT\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65f6\u6269\u5c55\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5956\u52b1\u4fe1\u53f7\uff0c\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u63a8\u5bfc\u5956\u52b1\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u6307\u6570\u7ea7\u589e\u957f\u4e0e\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u6709\u9650\u589e\u957f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u7a81\u7834\u4f20\u7edf\u6269\u5c55\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e0b\u4e00\u6bb5\u63a8\u7406\u76ee\u6807\uff0c\u8ba9\u7b56\u7565\u57fa\u4e8e\u524d\u6587\u51c6\u786e\u9884\u6d4b\u540e\u7eed\u6587\u672c\u7247\u6bb5\u6765\u83b7\u5f97\u5956\u52b1\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u6269\u5c55\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86RLPT\u7684\u6709\u6548\u6027\uff0c\u5982Qwen3-4B-Base\u5728MMLU\u3001MMLU-Pro\u7b49\u57fa\u51c6\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff083.0-8.1\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\uff09\u3002", "conclusion": "RLPT\u5c55\u793a\u4e86\u826f\u597d\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u4e3a\u7ee7\u7eed\u63d0\u5347\u6027\u80fd\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u57fa\u7840\uff0c\u6269\u5c55\u4e86LLMs\u7684\u63a8\u7406\u8fb9\u754c\u5e76\u589e\u5f3a\u4e86RLVR\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e3a\u4ec0\u4e48\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u652f\u6301\u60c5\u5883\u5f3a\u5316\u5b66\u4e60\u7684\u7f51\u7edc\u53c2\u6570\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660eTransformer\u5728\u7b56\u7565\u8bc4\u4f30\u9884\u8bad\u7ec3\u4e2d\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u53ef\u4ee5\u5b9e\u73b0\u60c5\u5883\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u53d1\u73b0\u4e00\u4e9bRL\u667a\u80fd\u4f53\u5728\u9884\u8bad\u7ec3\u540e\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5c31\u80fd\u89e3\u51b3\u5206\u5e03\u5916\u4efb\u52a1\uff0c\u4f46\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4f7f\u7528\u6807\u51c6RL\u7b97\u6cd5\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u7b97\u6cd5\u4e3a\u4f55\u80fd\u4ea7\u751f\u652f\u6301ICRL\u7684\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u7406\u8bba\u8bc1\u660e\u5f53Transformer\u88ab\u9884\u8bad\u7ec3\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u65f6\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u80fd\u591f\u5b9e\u73b0\u60c5\u5883\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002", "result": "\u63d0\u4f9b\u4e86\u521d\u6b65\u8bc1\u636e\u652f\u6301\u5047\u8bbe\uff0c\u8bc1\u660e\u5b58\u5728\u7279\u5b9a\u7684\u53c2\u6570\u914d\u7f6e\u80fd\u591f\u4f7f\u9884\u8bad\u7ec3\u7f51\u7edc\u5177\u5907\u60c5\u5883\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u635f\u5931\u7684\u6700\u5c0f\u5316\u5668\u786e\u5b9e\u80fd\u591f\u4ea7\u751f\u652f\u6301\u60c5\u5883\u5f3a\u5316\u5b66\u4e60\u7684\u7f51\u7edc\u53c2\u6570\uff0c\u8fd9\u4e3a\u7406\u89e3ICRL\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86KANDI\u65b9\u6cd5\uff0c\u7ed3\u5408Kolmogorov-Arnold\u7f51\u7edc\u548c\u6269\u6563\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u7597\u5065\u5eb7\u9886\u57df\u4e2d\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\u5b9a\u4e49\u56f0\u96be\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u7684\u7269\u7406\u6d3b\u52a8\u4fc3\u8fdb\u5e94\u7528\u4e2d\u3002", "motivation": "\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u5e94\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a\u76f4\u63a5\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u56f0\u96be\uff0c\u800c\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u4ece\u4e13\u5bb6\u884c\u4e3a\u4e2d\u63a8\u65ad\u51c6\u786e\u7684\u5956\u52b1\u51fd\u6570\u3002\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e5f\u96be\u4ee5\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4e0e\u533b\u7597\u5e94\u7528\u4e2d\u89c2\u5bdf\u5230\u7684\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u3002", "method": "\u63d0\u51faKANDI\u65b9\u6cd5\uff0c\u5229\u7528Kolmogorov-Arnold\u7f51\u7edc\u7684\u7075\u6d3b\u51fd\u6570\u903c\u8fd1\u80fd\u529b\u4ece\u4f4e\u8dcc\u5012\u98ce\u9669\u7684\u8001\u5e74\u4eba\uff08\u4e13\u5bb6\uff09\u884c\u4e3a\u4e2d\u4f30\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u540c\u65f6\u5728Actor-Critic\u6846\u67b6\u4e2d\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u8fdb\u884c\u52a8\u4f5c\u7ec6\u5316\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u4f18\u5316\u3002", "result": "KANDI\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728PEER\u7814\u7a76\u7684\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u901a\u8fc7\u53ef\u7a7f\u6234\u6d3b\u52a8\u76d1\u6d4b\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u5728\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u9879\u76ee\u4e2d\u4fc3\u8fdb\u8001\u5e74\u4eba\u8eab\u4f53\u6d3b\u52a8\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "KANDI\u6709\u6f5c\u529b\u89e3\u51b3\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u6d3b\u52a8\u4fc3\u8fdb\u5e72\u9884\u7b56\u7565\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18521", "abs": "https://arxiv.org/abs/2509.18521", "authors": ["Yuzhen Zhou", "Jiajun Li", "Yusheng Su", "Gowtham Ramesh", "Zilin Zhu", "Xiang Long", "Chenyang Zhao", "Jin Pan", "Xiaodong Yu", "Ze Wang", "Kangrui Du", "Jialian Wu", "Ximeng Sun", "Jiang Liu", "Qiaolin Yu", "Hao Chen", "Zicheng Liu", "Emad Barsoum"], "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed. Most\nof these frameworks primarily rely on inference engines for rollout generation\nand training engines for policy updates. However, RL training remains\ncomputationally expensive, with rollout generation accounting for more than 90%\nof total runtime. In addition, its efficiency is often constrained by the\nlong-tail distribution of rollout response lengths, where a few lengthy\nresponses stall entire batches, leaving GPUs idle and underutilized. As model\nand rollout sizes continue to grow, this bottleneck increasingly limits\nscalability. To address this challenge, we propose Active Partial Rollouts in\nReinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the\nrollout phase, APRIL over-provisions rollout requests, terminates once the\ntarget number of responses is reached, and recycles incomplete responses for\ncontinuation in future steps. This strategy ensures that no rollouts are\ndiscarded while substantially reducing GPU idle time. Experiments show that\nAPRIL improves rollout throughput by at most 44% across commonly used RL\nalgorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%\nhigher final accuracy across tasks. Moreover, APRIL is both framework and\nhardware agnostic, already integrated into the slime RL framework, and\ndeployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies\nsystem-level and algorithmic considerations in proposing APRIL, with the aim of\nadvancing RL training efficiency and inspiring further optimizations in RL\nsystems.", "AI": {"tldr": "APRIL\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u90e8\u5206rollout\u7b56\u7565\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u5bfc\u81f4\u7684GPU\u5229\u7528\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524dRL\u8bad\u7ec3\u4e2d\uff0crollout\u751f\u6210\u5360\u7528\u4e8690%\u4ee5\u4e0a\u7684\u8ba1\u7b97\u65f6\u95f4\uff0c\u4e14\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u5bfc\u81f4GPU\u7a7a\u95f2\u65f6\u95f4\u589e\u52a0\uff0c\u9650\u5236\u4e86RL\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "APRIL\u5728rollout\u9636\u6bb5\u8fc7\u5ea6\u914d\u7f6e\u8bf7\u6c42\uff0c\u4e00\u65e6\u8fbe\u5230\u76ee\u6807\u54cd\u5e94\u6570\u91cf\u5c31\u7ec8\u6b62\uff0c\u5e76\u5c06\u672a\u5b8c\u6210\u7684\u54cd\u5e94\u56de\u6536\u7528\u4e8e\u540e\u7eed\u6b65\u9aa4\u7684\u7ee7\u7eed\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAPRIL\u5728\u5e38\u7528RL\u7b97\u6cd5\uff08GRPO\u3001DAPO\u3001GSPO\uff09\u4e0a\u6700\u591a\u63d0\u534744%\u7684rollout\u541e\u5410\u91cf\uff0c\u52a0\u901f\u6536\u655b\uff0c\u5e76\u5728\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u591a8%\u7684\u6700\u7ec8\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "APRIL\u7edf\u4e00\u4e86\u7cfb\u7edf\u7ea7\u548c\u7b97\u6cd5\u7ea7\u8003\u8651\uff0c\u63d0\u9ad8\u4e86RL\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u4e0e\u6846\u67b6\u548c\u786c\u4ef6\u65e0\u5173\uff0c\u5df2\u5728slime RL\u6846\u67b6\u4e2d\u96c6\u6210\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18607", "abs": "https://arxiv.org/abs/2509.18607", "authors": ["Qiuhai Zeng", "Sarvesh Rajkumar", "Di Wang", "Narendra Gyanchandani", "Wenbo Yan"], "title": "Reflect before Act: Proactive Error Correction in Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ninteractive decision-making tasks, but existing methods often struggle with\nerror accumulation and lack robust self-correction mechanisms. We introduce\n\"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based\ndecision-making by introducing a critical reflect step prior to taking the next\naction. This approach allows for immediate error correction, ensuring smooth\naction path and adaptibity to environment feedback. We evaluate REBACT on three\ndiverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results\ndemonstrate that REBACT significantly outperforms strong baselines, improving\nsuccess rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld\n(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using\nClaude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's\nperformance improvements are achieved with only a few modification steps,\ndemonstrating its computational efficiency.", "AI": {"tldr": "REBACT\u65b9\u6cd5\u901a\u8fc7\u5728LLM\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u589e\u52a0\u53cd\u601d\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u51b3\u7b56\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ea4\u4e92\u51b3\u7b56\u65b9\u6cd5\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u53ca\u65f6\u4fee\u6b63\u9519\u8bef\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa'Reflect before Act'\u65b9\u6cd5\uff0c\u5728\u91c7\u53d6\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u4e4b\u524d\u589e\u52a0\u5173\u952e\u7684\u53cd\u601d\u6b65\u9aa4\uff0c\u5b9e\u73b0\u5373\u65f6\u9519\u8bef\u7ea0\u6b63\u548c\u9002\u5e94\u6027\u8c03\u6574\u3002", "result": "\u5728ALFWorld\u3001WebShop\u548cTextCraft\u4e09\u4e2a\u73af\u5883\u4e2d\uff0cREBACT\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u53476.72%\u300124%\u548c0.5%\uff0c\u6700\u9ad8\u8fbe\u523098.51%\u300161%\u548c99.5%\u3002", "conclusion": "REBACT\u65b9\u6cd5\u901a\u8fc7\u5c11\u91cf\u4fee\u6539\u6b65\u9aa4\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.18714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18714", "abs": "https://arxiv.org/abs/2509.18714", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "comment": "This paper is accepted by the 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e4b\u95f4\u7684\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u6a21\u62df\u5ea6\u91cf\u5728\u591aMDP\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u6a21\u62df\u5ea6\u91cf\uff08BSM\uff09\u5728\u5355MDP\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591aMDP\u573a\u666f\uff08\u5982\u7b56\u7565\u8fc1\u79fb\uff09\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u6027\u8d28\u5206\u6790\u9650\u5236\u4e86\u7406\u8bba\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5efa\u7acbMDP\u5bf9\u4e4b\u95f4\u7684\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff0c\u4e25\u683c\u8bc1\u660e\u5176\u4e09\u4e2a\u57fa\u672c\u6027\u8d28\uff1a\u5bf9\u79f0\u6027\u3001MDP\u95f4\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u76f8\u540c\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u8ddd\u79bb\u754c\u9650\u3002", "result": "GBSM\u5728\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u4f30\u8ba1\u4e2d\u83b7\u5f97\u4e86\u6bd4\u6807\u51c6BSM\u66f4\u4e25\u683c\u7684\u7406\u8bba\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u95ed\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u4f30\u8ba1\u3002", "conclusion": "GBSM\u4e3a\u591aMDP\u573a\u666f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18719", "abs": "https://arxiv.org/abs/2509.18719", "authors": ["Bo Qu", "Zhurong Wang", "Daisuke Yagi", "Zhen Xu", "Yang Zhao", "Yinan Shan", "Frank Zahradnik"], "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection", "comment": "12 pages, 12 figures, ACL 2025 industry track", "summary": "This paper presents a novel approach to e-commerce payment fraud detection by\nintegrating reinforcement learning (RL) with Large Language Models (LLMs). By\nframing transaction risk as a multi-step Markov Decision Process (MDP), RL\noptimizes risk detection across multiple payment stages. Crafting effective\nreward functions, essential for RL model success, typically requires\nsignificant human expertise due to the complexity and variability in design.\nLLMs, with their advanced reasoning and coding capabilities, are well-suited to\nrefine these functions, offering improvements over traditional methods. Our\napproach leverages LLMs to iteratively enhance reward functions, achieving\nbetter fraud detection accuracy and demonstrating zero-shot capability.\nExperiments with real-world data confirm the effectiveness, robustness, and\nresilience of our LLM-enhanced RL framework through long-term evaluations,\nunderscoring the potential of LLMs in advancing industrial RL applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b0\u578b\u7535\u5546\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9aa4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528LLM\u8fed\u4ee3\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6b3a\u8bc8\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7535\u5546\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u4e2d\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u9700\u8981\u5927\u91cf\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cLLM\u5177\u6709\u5148\u8fdb\u7684\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\uff0c\u53ef\u4ee5\u81ea\u52a8\u4f18\u5316\u8fd9\u4e9b\u51fd\u6570\uff0c\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9aa4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u8de8\u591a\u4e2a\u652f\u4ed8\u9636\u6bb5\u7684\u98ce\u9669\u68c0\u6d4b\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u97e7\u6027\uff0c\u901a\u8fc7\u957f\u671f\u8bc4\u4f30\u5c55\u793a\u4e86LLM\u589e\u5f3a\u7684RL\u6846\u67b6\u5728\u6b3a\u8bc8\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u7684\u63d0\u5347\u3002", "conclusion": "LLM\u5728\u63a8\u8fdb\u5de5\u4e1a\u7ea7\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u9762\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18851", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18851", "abs": "https://arxiv.org/abs/2509.18851", "authors": ["Gongrui Nan", "Siye Chen", "Jing Huang", "Mengyu Lu", "Dexun Wang", "Chunmei Xie", "Weiqi Xiong", "Xianzhou Zeng", "Qixuan Zhou", "Yadong Li", "Xingzhong Xu"], "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization", "comment": null, "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.", "AI": {"tldr": "NGRPO\u7b97\u6cd5\u89e3\u51b3\u4e86GRPO\u5728\u5904\u7406\u540c\u8d28\u9519\u8bef\u54cd\u5e94\u65f6\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u52bf\u6821\u51c6\u548c\u975e\u5bf9\u79f0\u88c1\u526a\u673a\u5236\uff0c\u5c06\u540c\u8d28\u9519\u8bef\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "GRPO\u7b49RLVR\u7b97\u6cd5\u5728\u5904\u7406\u5168\u5bf9\u6216\u5168\u9519\u7684\u540c\u8d28\u54cd\u5e94\u7ec4\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5f53\u6240\u6709\u54cd\u5e94\u90fd\u9519\u8bef\u65f6\uff0c\u4f18\u52bf\u51fd\u6570\u503c\u4e3a\u96f6\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u65e0\u6cd5\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faNGRPO\u7b97\u6cd5\uff1a1\uff09\u4f18\u52bf\u6821\u51c6\u673a\u5236\uff0c\u5047\u8bbe\u5b58\u5728\u865a\u62df\u6700\u5927\u5956\u52b1\u6837\u672c\u6765\u6539\u53d8\u7ec4\u5185\u5956\u52b1\u5206\u5e03\uff1b2\uff09\u975e\u5bf9\u79f0\u88c1\u526a\uff0c\u653e\u677e\u6b63\u6837\u672c\u66f4\u65b0\u5e45\u5ea6\uff0c\u4e25\u683c\u7ea6\u675f\u8d1f\u6837\u672c\u66f4\u65b0\u3002", "result": "\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cNGRPO\u5728MATH500\u3001AMC23\u548cAIME2025\u7b49\u6570\u5b66\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8ePPO\u3001GRPO\u3001DAPO\u548cPSR-NSR\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NGRPO\u80fd\u591f\u6709\u6548\u4ece\u540c\u8d28\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u7a33\u5b9a\u4e14\u663e\u8457\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18930", "abs": "https://arxiv.org/abs/2509.18930", "authors": ["Alex Schutz", "Victor-Alexandru Darvariu", "Efimia Panagiotaki", "Bruno Lacerda", "Nick Hawes"], "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GNARL\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\uff08NAR\uff09\u91cd\u65b0\u6784\u5efa\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u4f20\u7edfNAR\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u5b58\u5728\u65e0\u6cd5\u65e0\u540e\u5904\u7406\u6784\u5efa\u6709\u6548\u89e3\u3001\u96be\u4ee5\u5904\u7406\u591a\u89e3\u95ee\u9898\u3001\u5728NP\u96be\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u4ee5\u53ca\u7f3a\u4e4f\u5f3a\u7b97\u6cd5\u65f6\u65e0\u6cd5\u5e94\u7528\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06\u7b97\u6cd5\u8f68\u8ff9\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faGNARL\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u56fe\u57fa\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2aCLRS-30\u95ee\u9898\u4e0a\u83b7\u5f97\u9ad8\u56fe\u7cbe\u5ea6\uff0c\u5728NP\u96be\u95ee\u9898\u4e0a\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u66f4\u7a84\u7684NAR\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u7f3a\u4e4f\u4e13\u5bb6\u7b97\u6cd5\u65f6\u4e5f\u80fd\u5e94\u7528\u3002", "conclusion": "GNARL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86NAR\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u7b97\u6cd5\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u548c\u5f3a\u5927\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19017", "abs": "https://arxiv.org/abs/2509.19017", "authors": ["Hazem Dewidar", "Elena Umili"], "title": "Fully Learnable Neural Reward Machines", "comment": null, "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).", "AI": {"tldr": "\u63d0\u51fa\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08FLNRM\uff09\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5b66\u4e60\u7b26\u53f7\u63a5\u5730\u51fd\u6570\u548c\u81ea\u52a8\u673a\uff0c\u65e0\u9700\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8eRNN\u7684\u65b9\u6cd5\u3002", "motivation": "\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u9700\u8981\u4ee3\u7406\u57fa\u4e8e\u5b8c\u6574\u8f68\u8ff9\u505a\u51fa\u51b3\u7b56\uff0c\u73b0\u6709\u7b26\u53f7\u5316\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u7b26\u53f7\u63a5\u5730\u51fd\u6570\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08FLNRM\uff09\uff0c\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u7b26\u53f7\u63a5\u5730\u548c\u81ea\u52a8\u673a\u7ed3\u6784\u3002", "result": "FLNRM\u65b9\u6cd5\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8eRNN\u7684\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "FLNRM\u7ed3\u5408\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6613\u7528\u6027\u548c\u81ea\u52a8\u673a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19100", "abs": "https://arxiv.org/abs/2509.19100", "authors": ["Alexander Robey"], "title": "Algorithms for Adversarially Robust Deep Learning", "comment": "PhD thesis", "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u5bf9\u6297\u6837\u672c\u3001\u9886\u57df\u6cdb\u5316\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u4e09\u4e2a\u65b9\u9762\u7684\u6700\u65b0\u7b97\u6cd5\u8fdb\u5c55\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u786e\u4fdd\u6a21\u578b\u51b3\u7b56\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u7684\u6280\u672f\u7ed3\u679c\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8ba4\u8bc1\u7b97\u6cd5\u6765\u5904\u7406\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5bf9\u6297\u6837\u672c\uff1b\u5f00\u53d1\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6cdb\u5316\u7684\u65b0\u7b97\u6cd5\uff1b\u9488\u5bf9LLM\u8d8a\u72f1\u95ee\u9898\u63d0\u51fa\u4e86\u65b0\u7684\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u5728\u5bf9\u6297\u6837\u672c\u3001\u9886\u57df\u6cdb\u5316\u548c\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u524d\u6cbf\u8fdb\u5c55\uff0c\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u7684\u8bed\u8a00\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9c81\u68d2\u6027\u7684\u591a\u4e2a\u91cd\u8981\u65b9\u5411\u4e0a\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u63a8\u52a8\u4e86\u5b89\u5168\u53ef\u9760AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.19128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19128", "abs": "https://arxiv.org/abs/2509.19128", "authors": ["Alexandre Pich\u00e9", "Ehsan Kamaloo", "Rafael Pardinas", "Dzmitry Bahdanau"], "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio", "comment": null, "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.", "AI": {"tldr": "PipelineRL\u662f\u4e00\u79cd\u7528\u4e8eLLM\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u53d1\u5f02\u6b65\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u786c\u4ef6\u6548\u7387\u4e0e\u6570\u636e\u5728\u7ebf\u6027\u7684\u4f18\u5316\u5e73\u8861\uff0c\u76f8\u6bd4\u4f20\u7edfRL\u65b9\u6cd5\u5b66\u4e60\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\u3002", "motivation": "\u5f53\u524dRL\u65b9\u6cd5\u5728\u6269\u5c55LLM\u63a8\u7406\u80fd\u529b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u5728\u4fdd\u6301\u9ad8AI\u52a0\u901f\u5668\u5229\u7528\u7387\u7684\u540c\u65f6\u907f\u514d\u4ea7\u751f\u8fc7\u65f6\u7684\u79bb\u7ebf\u7b56\u7565\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u4f1a\u635f\u5bb3\u5e38\u89c1\u7684RL\u7b97\u6cd5\u6027\u80fd\u3002", "method": "PipelineRL\u91c7\u7528\u5e76\u53d1\u5f02\u6b65\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u98de\u884c\u4e2d\u6743\u91cd\u66f4\u65b0\u673a\u5236\uff0c\u5141\u8bb8LLM\u751f\u6210\u5f15\u64ce\u5728\u751f\u6210token\u5e8f\u5217\u65f6\u4ee5\u6700\u5c0f\u4e2d\u65ad\u63a5\u6536\u66f4\u65b0\u7684\u6a21\u578b\u6743\u91cd\u3002", "result": "\u5728128\u4e2aH100 GPU\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPipelineRL\u76f8\u6bd4\u4f20\u7edfRL\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u5b66\u4e60\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5ea6\u5728\u7ebf\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "PipelineRL\u5728\u786c\u4ef6\u6548\u7387\u548c\u6570\u636e\u5728\u7ebf\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.19189", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19189", "abs": "https://arxiv.org/abs/2509.19189", "authors": ["Binghui Li", "Fengling Chen", "Zixun Huang", "Lean Wang", "Lei Wu"], "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws", "comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper", "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u529f\u80fd\u7f29\u653e\u5b9a\u5f8b\uff08FSL\uff09\uff0c\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21SGD\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u7387\u8c03\u5ea6\u5bf9LLM\u9884\u8bad\u7ec3\u635f\u5931\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u7406\u8bba\u9a8c\u8bc1\u4e86\u5e38\u89c1\u5b9e\u8df5\u5982\u5b66\u4e60\u7387\u8870\u51cf\u548cWSD\u8c03\u5ea6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u52a8\u6001\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5e08\u751f\u6838\u56de\u5f52\u8bbe\u7f6e\u548c\u5728\u7ebfSGD\u8bad\u7ec3\uff0c\u901a\u8fc7\u5185\u5728\u65f6\u95f4\u89c6\u89d2\u548cSDE\u5efa\u6a21\uff0c\u63d0\u51faFSL\u6846\u67b6\u6765\u5206\u6790\u4e0d\u540c\u5b66\u4e60\u7387\u8c03\u5ea6\u4e0b\u7684\u79cd\u7fa4\u98ce\u9669\u6f14\u5316\u3002", "result": "FSL\u6210\u529f\u6355\u6349\u4e86\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u7406\u8bba\u9a8c\u8bc1\u4e86\u9ad8\u5bb9\u91cf\u6a21\u578b\u66f4\u9ad8\u6548\u3001\u5b66\u4e60\u7387\u8870\u51cf\u63d0\u5347\u6548\u7387\u3001WSD\u8c03\u5ea6\u4f18\u4e8e\u76f4\u63a5\u8870\u51cf\u7b49\u7ecf\u9a8c\u5b9e\u8df5\u3002", "conclusion": "FSL\u6846\u67b6\u80fd\u52a0\u6df1\u5bf9LLM\u9884\u8bad\u7ec3\u52a8\u6001\u7684\u7406\u89e3\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.d30e7dd4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "authors": ["TLDR Newsletter"], "title": "Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fbuilding-a-basic-ai-agent-in-orkes-conductor%2F%3Futm_campaign=TLDR-flagship-Sept%26utm_source=Sponsored%2520content%26utm_medium=referral/2/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/72jXC9BByk4wJ3LM3kxyNnnYberepODK71BlKiMKynk=424", "summary": "Building a tiny, useful AI agent: a Hello World tutorial using Orkes Conductor (Sponsor) With all the noise about agents, it's easy to forget that an agent is just a looping LLM call + Memory + Tool usage. Use step-by-step tutorial to build an agent that \"thinks\" with an LLM, calls external tools, remembers context, and keeps looping until it gets the job done. Follow along, get the satisfaction of seeing your first agent in action, and iterate from there to build more complex, enterprise-rea...", "source": "tldr", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "tldr.2509.0135e116", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "authors": ["TLDR Newsletter"], "title": "How Google's dev tools manager makes AI coding work", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrnewsletter/1/010001997b3f3cfc-4c4b5cd4-8696-41a4-8419-5726c5065c33-000000/gCpYFHn_RCvqbLnsuYA9swecL42n1As5NuN_5DiJrgM=424", "summary": "How Google's dev tools manager makes AI coding work (5 minute read) Ryan Salva, Google's project manager for developer tools, is responsible for tools like Gemini CLI and Gemini Code Assist. His team recently released new third-party research that showed how developers actually use AI tools. This article contains an edited interview with Salva where he talks about the research, how he uses AI tools, and the future of IDEs. Salva believes that over time, the time spent in the IDE will graduall...", "source": "tldr", "AI": {"tldr": "Google\u5f00\u53d1\u8005\u5de5\u5177\u7ecf\u7406Ryan Salva\u5206\u4eab\u5173\u4e8eAI\u7f16\u7801\u5de5\u5177\u7684\u7814\u7a76\u53d1\u73b0\u3001\u4f7f\u7528\u7ecf\u9a8c\u4ee5\u53caIDE\u672a\u6765\u7684\u5c55\u671b\uff0c\u8ba4\u4e3aIDE\u4f7f\u7528\u65f6\u95f4\u5c06\u9010\u6e10\u51cf\u5c11\u3002", "motivation": "\u4e86\u89e3\u5f00\u53d1\u8005\u5982\u4f55\u5b9e\u9645\u4f7f\u7528AI\u5de5\u5177\uff0c\u63a2\u7d22AI\u5de5\u5177\u5bf9\u5f00\u53d1\u6d41\u7a0b\u7684\u5f71\u54cd\u4ee5\u53caIDE\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7b2c\u4e09\u65b9\u7814\u7a76\u5206\u6790\u5f00\u53d1\u8005\u4f7f\u7528AI\u5de5\u5177\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u7ed3\u5408Salva\u7684\u4e2a\u4eba\u4f7f\u7528\u7ecf\u9a8c\u548c\u56e2\u961f\u89c2\u5bdf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f00\u53d1\u8005\u6b63\u5728\u6539\u53d8\u5de5\u4f5c\u65b9\u5f0f\uff0cAI\u5de5\u5177\u6b63\u5728\u91cd\u5851\u5f00\u53d1\u6d41\u7a0b\uff0cIDE\u7684\u89d2\u8272\u5c06\u53d1\u751f\u53d8\u5316\u3002", "conclusion": "AI\u5de5\u5177\u5c06\u6539\u53d8\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0cIDE\u7684\u91cd\u8981\u6027\u53ef\u80fd\u76f8\u5bf9\u964d\u4f4e\uff0c\u5f00\u53d1\u5de5\u5177\u9700\u8981\u9002\u5e94\u8fd9\u4e00\u8d8b\u52bf\u3002", "topic": "swe application"}}
{"id": "tldr.2509.02fc7bb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "authors": ["TLDR Newsletter"], "title": "Your Brain on ChatGPT", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brainonllm.com%2F%3Futm_source=tldrmarketing/1/010001997b6695cc-e81b85fe-7d05-4fe6-8b25-972ef706f5fd-000000/gLry1B4gehea_82Vl9WIKHrbhUitSuUGZjHJXip5hDw=424", "summary": "Your Brain on ChatGPT (3 minute read) This study examined the cognitive impact of using ChatGPT for essay writing. It compared participants who used an LLM, a search engine, or only their own brain across multiple sessions. EEG data revealed that reliance on the LLM reduced neural connectivity and engagement compared with the Brain-only and Search Engine groups, and participants using the LLM showed weaker memory recall and lower ownership of their work. While LLMs offer immediate assistance,...", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f7f\u7528ChatGPT\u3001\u641c\u7d22\u5f15\u64ce\u548c\u4ec5\u9760\u5927\u8111\u5199\u4f5c\u5bf9\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f9d\u8d56LLM\u4f1a\u964d\u4f4e\u795e\u7ecf\u8fde\u63a5\u548c\u53c2\u4e0e\u5ea6\uff0c\u5e76\u5bfc\u81f4\u8bb0\u5fc6\u56de\u5fc6\u548c\u5de5\u4f5c\u5f52\u5c5e\u611f\u51cf\u5f31", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5199\u4f5c\u4efb\u52a1\u4e2d\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u529f\u80fd\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u7279\u522b\u662f\u795e\u7ecf\u6d3b\u52a8\u548c\u8bb0\u5fc6\u8868\u73b0\u65b9\u9762", "method": "\u4f7f\u7528\u8111\u7535\u56fe(EEG)\u6d4b\u91cf\u4e09\u7ec4\u53c2\u4e0e\u8005\uff08\u4f7f\u7528LLM\u3001\u641c\u7d22\u5f15\u64ce\u3001\u4ec5\u9760\u5927\u8111\uff09\u5728\u591a\u6b21\u5199\u4f5c\u4efb\u52a1\u4e2d\u7684\u795e\u7ecf\u6d3b\u52a8\uff0c\u5e76\u8bc4\u4f30\u8bb0\u5fc6\u56de\u5fc6\u548c\u5de5\u4f5c\u5f52\u5c5e\u611f", "result": "LLM\u7ec4\u663e\u793a\u51fa\u795e\u7ecf\u8fde\u63a5\u548c\u53c2\u4e0e\u5ea6\u964d\u4f4e\uff0c\u8bb0\u5fc6\u56de\u5fc6\u80fd\u529b\u8f83\u5f31\uff0c\u5bf9\u5de5\u4f5c\u7684\u5f52\u5c5e\u611f\u4e5f\u8f83\u4f4e", "conclusion": "\u867d\u7136LLM\u63d0\u4f9b\u5373\u65f6\u5e2e\u52a9\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u53ef\u80fd\u5bf9\u8ba4\u77e5\u529f\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd", "topic": "agent analysis"}}
{"id": "tldr.2509.2b52d9a6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "authors": ["TLDR Newsletter"], "title": "Getting AI to Work in Complex Codebases", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fhumanlayer%2Fadvanced-context-engineering-for-coding-agents%2Fblob%2Fmain%2Face-fca.md%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/nvK3tmYxEBmgkxj534pZmcjvlajw9IdlbrJ8YNPM1XQ=424", "summary": "Getting AI to Work in Complex Codebases (20 minute read) With core context engineering principles and frequent intentional compaction, current AI models can be great at handling large, detailed codebases. AI can rarely produce correct code in one shot in large code bases, so it needs steps before implementation to do research on the codebase and plan its steps.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u5de5\u4f5c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5f3a\u8c03\u901a\u8fc7\u6838\u5fc3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u539f\u5219\u548c\u9891\u7e41\u7684\u6709\u610f\u538b\u7f29\uff0c\u5f53\u524dAI\u6a21\u578b\u80fd\u591f\u5f88\u597d\u5730\u5904\u7406\u5927\u578b\u8be6\u7ec6\u4ee3\u7801\u5e93\u3002", "motivation": "AI\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5f88\u96be\u4e00\u6b21\u6027\u751f\u6210\u6b63\u786e\u4ee3\u7801\uff0c\u56e0\u6b64\u9700\u8981\u5728\u5b9e\u73b0\u524d\u8fdb\u884c\u4ee3\u7801\u5e93\u7814\u7a76\u548c\u6b65\u9aa4\u89c4\u5212\u3002", "method": "\u91c7\u7528\u6838\u5fc3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u539f\u5219\u548c\u9891\u7e41\u7684\u6709\u610f\u538b\u7f29\u6280\u672f\uff0c\u8ba9AI\u5728\u5b9e\u73b0\u524d\u5148\u8fdb\u884c\u4ee3\u7801\u5e93\u7814\u7a76\u548c\u6b65\u9aa4\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5f53\u524dAI\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5927\u578b\u590d\u6742\u4ee3\u7801\u5e93\u3002", "conclusion": "\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u4f7f\u7528AI\u9700\u8981\u5206\u6b65\u65b9\u6cd5\uff0c\u5305\u62ec\u524d\u671f\u7814\u7a76\u548c\u89c4\u5212\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u3002", "topic": "swe application"}}
{"id": "tldr.2509.562937b5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "authors": ["TLDR Newsletter"], "title": "Exploring Active Agent, or can we build AI features the Rails way?", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fexploring-active-agent-or-can-we-build-ai-features-the-rails-way%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/DNm7smlpXB2ZhvM3p9PLRrJai6Dh6x89x8hSRDugNUM=424", "summary": "Exploring Active Agent, or can we build AI features the Rails way? (8 minute read) Active Agent is a Ruby gem that helps integrate AI features into Rails applications using familiar Rails conventions and patterns. It introduces \"agents\" as a new abstraction that encapsulates AI-backed logic, using action-driven objects, callbacks, and prompt rendering similar to Rails controllers and mailers.", "source": "tldr", "AI": {"tldr": "Active Agent\u662f\u4e00\u4e2aRuby gem\uff0c\u5e2e\u52a9\u4f7f\u7528\u719f\u6089\u7684Rails\u7ea6\u5b9a\u548c\u6a21\u5f0f\u5c06AI\u529f\u80fd\u96c6\u6210\u5230Rails\u5e94\u7528\u4e2d\uff0c\u5f15\u5165\u4e86\"agents\"\u4f5c\u4e3a\u5c01\u88c5AI\u903b\u8f91\u7684\u65b0\u62bd\u8c61\u5c42\u3002", "motivation": "\u65e8\u5728\u7b80\u5316AI\u529f\u80fd\u5728Rails\u5e94\u7528\u4e2d\u7684\u96c6\u6210\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4f7f\u7528\u719f\u6089\u7684Rails\u5f00\u53d1\u6a21\u5f0f\u548c\u7ea6\u5b9a\u6765\u6784\u5efaAI\u7279\u6027\u3002", "method": "\u5f15\u5165\"agents\"\u4f5c\u4e3a\u65b0\u62bd\u8c61\u5c42\uff0c\u91c7\u7528\u52a8\u4f5c\u9a71\u52a8\u5bf9\u8c61\u3001\u56de\u8c03\u548c\u63d0\u793a\u6e32\u67d3\uff0c\u7c7b\u4f3c\u4e8eRails\u63a7\u5236\u5668\u548c\u90ae\u4ef6\u5668\u7684\u8bbe\u8ba1\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2aRuby gem\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b26\u5408Rails\u5f00\u53d1\u4e60\u60ef\u7684\u65b9\u5f0f\u6765\u6784\u5efaAI\u529f\u80fd\u3002", "conclusion": "Active Agent\u4e3aRails\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u719f\u6089\u7684\u65b9\u5f0f\u6765\u96c6\u6210AI\u529f\u80fd\uff0c\u964d\u4f4e\u4e86AI\u6280\u672f\u7684\u5b66\u4e60\u66f2\u7ebf\u3002", "topic": "code agent"}}
{"id": "tldr.2509.8a4ff67e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "authors": ["TLDR Newsletter"], "title": "Context Engineering for AI Agents: Lessons from Building Manus", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/G-PB2n4vTN7sJP7GxUgU24txZT1XZ3go9-A-y0rxWP4=424", "summary": "Context Engineering for AI Agents: Lessons from Building Manus (11 minute read) Manus is an AI agent that had improved performance through context engineering techniques. The devs optimized the KV-cache hit rate by maintaining a stable prompt prefix with append-only context. They advise against dynamically changing the action space. The file system should be used as an extended memory, along with recitative methods to manipulate attention.", "source": "tldr", "AI": {"tldr": "Manus AI\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5305\u62ec\u4f18\u5316KV\u7f13\u5b58\u547d\u4e2d\u7387\u3001\u4fdd\u6301\u7a33\u5b9a\u63d0\u793a\u524d\u7f00\u3001\u4f7f\u7528\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6269\u5c55\u5185\u5b58\u7b49\u65b9\u6cd5", "motivation": "\u63d0\u9ad8AI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u6765\u6539\u5584\u4ee3\u7406\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u51c6\u786e\u6027", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u672f\uff0c\u5305\u62ec\u7ef4\u62a4\u7a33\u5b9a\u7684\u63d0\u793a\u524d\u7f00\u3001\u4ec5\u8ffd\u52a0\u4e0a\u4e0b\u6587\u3001\u4f18\u5316KV\u7f13\u5b58\u547d\u4e2d\u7387\u3001\u4f7f\u7528\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6269\u5c55\u5185\u5b58\u3001\u4ee5\u53ca\u4f7f\u7528\u590d\u8ff0\u65b9\u6cd5\u6765\u64cd\u7eb5\u6ce8\u610f\u529b", "result": "Manus AI\u4ee3\u7406\u7684\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u63d0\u5347AI\u4ee3\u7406\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5efa\u8bae\u907f\u514d\u52a8\u6001\u6539\u53d8\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5408\u7406\u5229\u7528\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u5185\u5b58\u6269\u5c55", "topic": "agent analysis"}}
{"id": "tldr.2509.82de62a9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "authors": ["TLDR Newsletter"], "title": "How Google's dev tools manager makes AI coding work", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F09%2F23%2Fhow-googles-dev-tools-manager-makes-ai-coding-work%2F%3Futm_source=tldrwebdev/1/010001997b80bcb8-311e95d6-4dea-4399-90c4-ca7a139433ef-000000/tki66uA61j42T2NBxpXgTVtqXe86FSf6Dxuo3znrHbs=424", "summary": "How Google's dev tools manager makes AI coding work (5 minute read) AI coding tools like Gemini CLI are being used for software development by automating code generation from natural language requirements and shifting the developer's role towards architecture and problem decomposition.", "source": "tldr", "AI": {"tldr": "Google\u7684AI\u7f16\u7801\u5de5\u5177\u5982Gemini CLI\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u6539\u53d8\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u5c06\u5f00\u53d1\u8005\u89d2\u8272\u8f6c\u5411\u67b6\u6784\u548c\u95ee\u9898\u5206\u89e3", "motivation": "\u63a2\u7d22AI\u7f16\u7801\u5de5\u5177\u5982\u4f55\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u6765\u51cf\u8f7b\u5f00\u53d1\u8005\u7684\u7f16\u7801\u8d1f\u62c5", "method": "\u4f7f\u7528Gemini CLI\u7b49AI\u5de5\u5177\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u8f6c\u6362\u4e3a\u4ee3\u7801\uff0c\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u81ea\u52a8\u5316", "result": "AI\u7f16\u7801\u5de5\u5177\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u5f00\u53d1\u8005\u66f4\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6b21\u7684\u8bbe\u8ba1\u548c\u67b6\u6784\u5de5\u4f5c", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u5c06\u5f00\u53d1\u8005\u4ece\u7e41\u7410\u7684\u7f16\u7801\u4efb\u52a1\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u4e13\u6ce8\u4e8e\u66f4\u5177\u521b\u9020\u6027\u7684\u5de5\u4f5c", "topic": "swe application"}}
{"id": "tldr.2509.0b58c949", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "authors": ["TLDR Newsletter"], "title": "Coinbase Announces x402 Foundation with Cloudflare", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1970477072792592484.html%3Futm_source=tldrcrypto/1/010001997b9dd16d-545f4dfa-5507-4cd4-8a0c-91e8255df959-000000/aP1eCH1Tpn1hw7kVAGC0t48wtTBgen15evG5fsdufxc=424", "summary": "Coinbase Announces x402 Foundation with Cloudflare (5 minute read) x402, the open standard for onchain agentic commerce created by Coinbase, will become controlled by an independent foundation called the x402 Foundation. The foundation will be co-founded by Coinbase and Cloudflare \u2013 one of the largest internet infrastructure providers who recently announced an agentic pay-per-crawl option for websites \u2013 with more partners to be announced soon. Creating a foundation is a significant step towar...", "source": "tldr", "AI": {"tldr": "Coinbase\u4e0eCloudflare\u8054\u5408\u6210\u7acbx402\u57fa\u91d1\u4f1a\uff0c\u65e8\u5728\u63a8\u52a8\u94fe\u4e0a\u4ee3\u7406\u5546\u52a1\u5f00\u653e\u6807\u51c6\u7684\u53d1\u5c55\u3002", "motivation": "\u4e3a\u94fe\u4e0a\u4ee3\u7406\u5546\u52a1\u521b\u5efa\u72ec\u7acb\u7684\u7ba1\u7406\u673a\u6784\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u548c\u751f\u6001\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6210\u7acb\u72ec\u7acb\u7684x402\u57fa\u91d1\u4f1a\uff0c\u7531Coinbase\u548cCloudflare\u5171\u540c\u7ba1\u7406\uff0c\u672a\u6765\u5c06\u5f15\u5165\u66f4\u591a\u5408\u4f5c\u4f19\u4f34\u3002", "result": "\u5efa\u7acb\u4e86x402\u57fa\u91d1\u4f1a\u4f5c\u4e3a\u94fe\u4e0a\u4ee3\u7406\u5546\u52a1\u6807\u51c6\u7684\u72ec\u7acb\u7ba1\u7406\u673a\u6784\u3002", "conclusion": "\u8fd9\u662f\u63a8\u52a8\u94fe\u4e0a\u4ee3\u7406\u5546\u52a1\u6807\u51c6\u5316\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u8be5\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.7948e77c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "authors": ["TLDR Newsletter"], "title": "Greptile Series A and Greptile v3", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fseries-a%3Futm_source=tldrfounders/1/010001997b9f218e-7c54b49c-bb92-415e-be0a-d882e035f3eb-000000/hpqxfVRfYHMlFsC6dryxE7CtTt93vsWmjvNVdSjo10I=424", "summary": "Greptile Series A and Greptile v3 (5 minute read) Greptile raised $25M Series A led by Benchmark Capital to advance its AI code review agent, Greptile v3, which addresses scale issues in code validation. The revamped agent detects 3x more critical bugs than its predecessor and has reviewed over 500M lines of code for top firms, preventing 180,000+ bugs. Greptile integrates with tools like Jira and Notion for context-aware feedback and learns team practices from PR comments to enhance code rev...", "source": "tldr", "AI": {"tldr": "Greptile v3 AI\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\u5728Series A\u878d\u8d442500\u4e07\u7f8e\u5143\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u9a8c\u8bc1\u7684\u89c4\u6a21\u95ee\u9898\uff0c\u68c0\u6d4b\u5230\u7684\u5173\u952ebug\u6570\u91cf\u662f\u524d\u4ee3\u76843\u500d\uff0c\u5df2\u5ba1\u67e5\u8d85\u8fc75\u4ebf\u884c\u4ee3\u7801\uff0c\u9632\u6b62\u4e8618\u4e07+\u4e2abug", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u4ee3\u7801\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027", "method": "\u5f00\u53d1AI\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406Greptile v3\uff0c\u96c6\u6210Jira\u548cNotion\u7b49\u5de5\u5177\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u53cd\u9988\uff0c\u4ecePR\u8bc4\u8bba\u4e2d\u5b66\u4e60\u56e2\u961f\u5b9e\u8df5", "result": "\u68c0\u6d4b\u5230\u7684\u5173\u952ebug\u6570\u91cf\u662f\u524d\u4ee3\u76843\u500d\uff0c\u5ba1\u67e5\u4e86500M+\u884c\u4ee3\u7801\uff0c\u9632\u6b62\u4e86180,000+\u4e2abug\uff0c\u4e3a\u9876\u7ea7\u516c\u53f8\u63d0\u4f9b\u670d\u52a1", "conclusion": "Greptile v3\u5728\u4ee3\u7801\u5ba1\u67e5\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u8bc1\u660e\u4e86AI\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u4ee3\u7801\u9a8c\u8bc1\u4e2d\u7684\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2509.4f747309", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "authors": ["TLDR Newsletter"], "title": "ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fshadowleak-exploit-exposed-gmail-data-chatgpt-agent%2F%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/Z_qm7pDiyPDezopPPdk0qGS2oi9mobdfAoTTi6DMxMo=424", "summary": "ShadowLeak Exploit Exposed Gmail Data Through ChatGPT Agent (2 minute read) ShadowLeak is a zero-click vulnerability in OpenAI's ChatGPT Deep Research agent that utilized invisible, indirect prompt injection commands hidden in emails to exfiltrate Gmail data without user knowledge. The attack operated entirely on OpenAI's servers using the agent's browser.open() function call to send stolen data encoded in Base64 to attacker-controlled URLs, achieving a 100% success rate. OpenAI fixed the vul...", "source": "tldr", "AI": {"tldr": "ShadowLeak\u662f\u4e00\u79cd\u96f6\u70b9\u51fb\u6f0f\u6d1e\uff0c\u5229\u7528ChatGPT Deep Research\u4ee3\u7406\u4e2d\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u547d\u4ee4\uff0c\u901a\u8fc7\u9690\u85cf\u7684\u7535\u5b50\u90ae\u4ef6\u6307\u4ee4\u7a83\u53d6Gmail\u6570\u636e\u800c\u4e0d\u88ab\u7528\u6237\u5bdf\u89c9\u3002", "motivation": "\u63ed\u793aAI\u4ee3\u7406\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u901a\u8fc7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u5b9e\u73b0\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\u3002", "method": "\u5728\u7535\u5b50\u90ae\u4ef6\u4e2d\u9690\u85cf\u4e0d\u53ef\u89c1\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u547d\u4ee4\uff0c\u5229\u7528\u4ee3\u7406\u7684browser.open()\u51fd\u6570\u8c03\u7528\u5c06\u7a83\u53d6\u7684\u6570\u636e\u4ee5Base64\u7f16\u7801\u53d1\u9001\u5230\u653b\u51fb\u8005\u63a7\u5236\u7684URL\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u8fbe\u5230100%\uff0c\u80fd\u591f\u5b8c\u5168\u5728OpenAI\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u7528\u6237\u4ea4\u4e92\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u5904\u7406\u5916\u90e8\u6570\u636e\u65f6\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.143ff216", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "authors": ["TLDR Newsletter"], "title": "Control AI in your SDLC", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdiscover.securecodewarrior.com%2FTrust-Agent-AI-Waitlist.html%3Futm_source=tldr%26utm_medium=email%26utm_campaign=2025-09-trust-agent-ai-global-en-dg/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/_H1oApEJIZI2LyIvkM7C0_pXz60jgHnLIUyji5Hl9bI=424", "summary": "Control AI in your SDLC (Sponsor) 78% of developers use AI coding tools, yet half of functionally correct AI-generated code is insecure. SCW Trust Agent: AI gives leaders visibility and governance to manage this risk - spotting \u201cshadow AI,\u201d mapping vulnerabilities to skill level, and enforcing policy. Be among the first to join the early access waitlist!", "source": "tldr", "AI": {"tldr": "SCW Trust Agent\u662f\u4e00\u4e2aAI\u6cbb\u7406\u5de5\u5177\uff0c\u5e2e\u52a9\u7ba1\u7406\u8005\u76d1\u63a7\u548c\u63a7\u5236AI\u751f\u6210\u7684\u4ee3\u7801\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u53d1\u73b0\"\u5f71\u5b50AI\"\u3001\u6620\u5c04\u6f0f\u6d1e\u5230\u6280\u80fd\u6c34\u5e73\u3001\u6267\u884c\u653f\u7b56", "motivation": "78%\u7684\u5f00\u53d1\u8005\u4f7f\u7528AI\u7f16\u7801\u5de5\u5177\uff0c\u4f46\u4e00\u534a\u529f\u80fd\u6b63\u786e\u7684AI\u751f\u6210\u4ee3\u7801\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u6709\u6548\u7684\u6cbb\u7406\u548c\u98ce\u9669\u7ba1\u7406", "method": "\u63d0\u4f9b\u53ef\u89c1\u6027\u548c\u6cbb\u7406\u80fd\u529b\uff0c\u5305\u62ec\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684AI\u4f7f\u7528\u3001\u5c06\u6f0f\u6d1e\u4e0e\u5f00\u53d1\u8005\u6280\u80fd\u6c34\u5e73\u5173\u8054\u3001\u5f3a\u5236\u6267\u884c\u5b89\u5168\u653f\u7b56", "result": "\u5e2e\u52a9\u4f01\u4e1a\u9886\u5bfc\u8005\u7ba1\u7406AI\u7f16\u7801\u5de5\u5177\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u6210\u4e3a\u9996\u6279\u52a0\u5165\u65e9\u671f\u8bbf\u95ee\u7b49\u5f85\u540d\u5355\u7684\u673a\u4f1a", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u867d\u7136\u666e\u53ca\u4f46\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0cSCW Trust Agent\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u6cbb\u7406\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2509.f700c43b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "authors": ["TLDR Newsletter"], "title": "When LLMs Autonomously Attack", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.cmu.edu%2Fnews-events%2Fnews%2F2025%2F07%2F24-when-llms-autonomously-attack.html%3Futm_source=tldrinfosec/1/010001997bd65b30-b9de03ea-6e6f-4459-9a48-d8a5479ea051-000000/7idBHfaWwIgsTxFoGkvR-xz16DpG_t1FwJc_bLgIHaU=424", "summary": "When LLMs Autonomously Attack (5 minute read) A researcher at Carnegie Mellon University demonstrated that an LLM can coordinate a system of agents to recreate real-world attacks. As part of his PhD, Brian Singer recreated the network environment of the 2017 Equifax data breach and observed that an LLM could replicate the attack. These capabilities allow smaller organizations to use LLMs as dedicated red teamers.", "source": "tldr", "AI": {"tldr": "LLM\u80fd\u591f\u81ea\u4e3b\u534f\u8c03\u4ee3\u7406\u7cfb\u7edf\u91cd\u73b0\u73b0\u5b9e\u4e16\u754c\u653b\u51fb\uff0c\u59822017\u5e74Equifax\u6570\u636e\u6cc4\u9732\u4e8b\u4ef6\uff0c\u4f7f\u5c0f\u578b\u7ec4\u7ec7\u80fd\u591f\u5c06LLM\u7528\u4f5c\u4e13\u7528\u7ea2\u961f\u6d4b\u8bd5\u5de5\u5177\u3002", "motivation": "\u7814\u7a76LLM\u5728\u7f51\u7edc\u5b89\u5168\u653b\u51fb\u4e2d\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u591f\u91cd\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u653b\u51fb\u573a\u666f\u3002", "method": "\u5728\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7684\u7814\u7a76\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u91cd\u73b0\u4e862017\u5e74Equifax\u6570\u636e\u6cc4\u9732\u7684\u7f51\u7edc\u73af\u5883\uff0c\u89c2\u5bdfLLM\u5982\u4f55\u534f\u8c03\u4ee3\u7406\u7cfb\u7edf\u590d\u5236\u8be5\u653b\u51fb\u3002", "result": "LLM\u6210\u529f\u590d\u5236\u4e86Equifax\u653b\u51fb\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u81ea\u4e3b\u653b\u51fb\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u5c0f\u578b\u7ec4\u7ec7\u800c\u8a00\u53ef\u4f5c\u4e3a\u4e13\u7528\u7ea2\u961f\u6d4b\u8bd5\u5de5\u5177\u3002", "conclusion": "LLM\u5177\u5907\u81ea\u4e3b\u653b\u51fb\u80fd\u529b\uff0c\u8fd9\u65e2\u662f\u5b89\u5168\u5a01\u80c1\u4e5f\u662f\u5b89\u5168\u6d4b\u8bd5\u673a\u4f1a\uff0c\u9700\u8981\u76f8\u5e94\u7684\u9632\u5fa1\u63aa\u65bd\u548c\u76d1\u7ba1\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.94f91b2b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "authors": ["TLDR Newsletter"], "title": "Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ095fp/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/SZZZFWkL8CYeV1BMCJjXCHIZHLKd3WBy5V3uWUUXTt4=424", "summary": "Apple working on MCP support to enable agentic AI on Mac, iPhone, and iPad (4 minute read) Apple has begun to lay the groundwork for adopting Anthropic's Model Context Protocol (MCP) in its latest round of betas. MCP allows AI agents to interface with traditional platforms. The standard has been widely adopted and has become a universal pathway for AI assistants to plug into APIs and data sources. Apple plans to let developers use a system-level MCP integration to expose actions and functiona...", "source": "tldr", "AI": {"tldr": "\u82f9\u679c\u6b63\u5728\u4e3a\u5176\u8bbe\u5907\u5f00\u53d1MCP\u652f\u6301\uff0c\u4ee5\u5b9e\u73b0AI\u4ee3\u7406\u529f\u80fd", "motivation": "\u8ba9AI\u52a9\u624b\u80fd\u591f\u4e0e\u4f20\u7edf\u5e73\u53f0\u4ea4\u4e92\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u7cfb\u7edf\u7ea7\u7684MCP\u96c6\u6210\u80fd\u529b", "method": "\u5728\u6700\u65b0\u6d4b\u8bd5\u7248\u4e2d\u5f00\u59cb\u91c7\u7528Anthropic\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u6807\u51c6", "result": "MCP\u5df2\u6210\u4e3aAI\u52a9\u624b\u8fde\u63a5API\u548c\u6570\u636e\u6e90\u7684\u901a\u7528\u9014\u5f84", "conclusion": "\u82f9\u679c\u8ba1\u5212\u8ba9\u5f00\u53d1\u8005\u4f7f\u7528\u7cfb\u7edf\u7ea7MCP\u96c6\u6210\u6765\u66b4\u9732\u64cd\u4f5c\u548c\u529f\u80fd", "topic": "agent analysis"}}
{"id": "tldr.2509.ba1869b5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "authors": ["TLDR Newsletter"], "title": "Chrome DevTools for your AI agent", "comment": "Source: TLDR Newsletter, Date: 2025-09-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.chrome.com%2Fblog%2Fchrome-devtools-mcp%3Futm_source=tldrai/1/010001997bddc6ff-75ec5456-819c-44f5-ae0b-51615838ef13-000000/63Ucox7nQBgj_DS5BaIjYpo8setTcwD8K2PmDpccBGI=424", "summary": "Chrome DevTools (MCP) for your AI agent (5 minute read) The Chrome DevTools Model Context Protocol (MCP) server brings the power of Chrome DevTools to AI coding assistants. It allows AI coding assistants to debug web pages directly in Chrome and benefit from DevTools debugging capabilities and performance insights. Use cases include verifying code changes in real-time, diagnosing network and console errors, simulating user behavior, and automating performance audits. A video showing how the C...", "source": "tldr", "AI": {"tldr": "Chrome DevTools MCP\u670d\u52a1\u5668\u4e3aAI\u7f16\u7a0b\u52a9\u624b\u63d0\u4f9b\u4e86Chrome DevTools\u7684\u8c03\u8bd5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u5728Chrome\u4e2d\u8c03\u8bd5\u7f51\u9875\u5e76\u83b7\u53d6\u6027\u80fd\u6d1e\u5bdf\u3002", "motivation": "\u8ba9AI\u7f16\u7a0b\u52a9\u624b\u80fd\u591f\u76f4\u63a5\u5229\u7528Chrome DevTools\u7684\u5f3a\u5927\u8c03\u8bd5\u529f\u80fd\uff0c\u5b9e\u65f6\u9a8c\u8bc1\u4ee3\u7801\u53d8\u66f4\u3001\u8bca\u65ad\u7f51\u7edc\u548cconsole\u9519\u8bef\u3001\u6a21\u62df\u7528\u6237\u884c\u4e3a\u4ee5\u53ca\u81ea\u52a8\u5316\u6027\u80fd\u5ba1\u8ba1\u3002", "method": "\u901a\u8fc7Model Context Protocol (MCP)\u670d\u52a1\u5668\u5c06Chrome DevTools\u7684\u529f\u80fd\u66b4\u9732\u7ed9AI\u7f16\u7a0b\u52a9\u624b\uff0c\u4f7f\u5176\u80fd\u591f\u4e0eChrome\u6d4f\u89c8\u5668\u8fdb\u884c\u4ea4\u4e92\u5e76\u6267\u884c\u8c03\u8bd5\u64cd\u4f5c\u3002", "result": "AI\u7f16\u7a0b\u52a9\u624b\u73b0\u5728\u53ef\u4ee5\u76f4\u63a5\u5728Chrome\u4e2d\u8c03\u8bd5\u7f51\u9875\uff0c\u5229\u7528DevTools\u7684\u5b8c\u6574\u8c03\u8bd5\u80fd\u529b\u548c\u6027\u80fd\u5206\u6790\u5de5\u5177\u3002", "conclusion": "Chrome DevTools MCP\u4e3aAI\u7f16\u7a0b\u52a9\u624b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7f51\u9875\u8c03\u8bd5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u9a8c\u8bc1\u548c\u95ee\u9898\u8bca\u65ad\u7684\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "wechat.2509.593b59d4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNjU2OTkyOA==&mid=2247496839&idx=1&sn=dfad9994bc65c4bca83b60c71dfebeeb&chksm=fb2c1d4eb38211e6b0ec5abc671e083343d3719445aa3dd2984e1095a2b90a1d2627cd848145#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNjU2OTkyOA==&mid=2247496839&idx=1&sn=dfad9994bc65c4bca83b60c71dfebeeb&chksm=fb2c1d4eb38211e6b0ec5abc671e083343d3719445aa3dd2984e1095a2b90a1d2627cd848145#rd", "authors": ["\u673a\u5668\u611f\u77e5"], "title": "MAPO\uff1a\u7834\u89e3<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4f18\u52bf\u5931\u771f\u96be\u9898\uff0c\u5927\u5e45\u63d0\u5347\u57fa\u7840\u6a21\u578b\u63a8\u7406\u6027\u80fd", "comment": "Source: WeChat, Published: 2025-09-24 12:35:57", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u6280\u672f\uff0c\u5176\u4e2d\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u56e0\u65e0\u9700\u989d\u5916\u5956\u52b1\u6a21\u578b\u800c\u5e7f\u53d7\u9752\u7750\u3002\u5176\u6838\u5fc3\u901a\u8fc7\u4f18\u52bf\u51fd\u6570\u6392\u5e8f\u8f68\u8ff9\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709GRPO\u91c7\u7528\u56fa\u5b9a\u4f18\u52bf\u516c\u5f0f\uff0c\u5ffd\u7565\u6837\u672c\u8f68\u8ff9\u786e\u5b9a\u6027\u5dee\u5f02\uff0c\u5f15\u53d1\u4e24\u5927\u95ee\u9898\uff1a\u4f18\u52bf", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u6280\u672f\uff0c\u5176\u4e2d\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u56e0\u65e0\u9700\u989d\u5916\u5956\u52b1\u6a21\u578b\u800c\u5e7f\u53d7\u9752\u7750\u3002\u5176\u6838\u5fc3\u901a\u8fc7\u4f18\u52bf\u51fd\u6570\u6392\u5e8f\u8f68\u8ff9\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709GRPO\u91c7\u7528\u56fa\u5b9a\u4f18\u52bf\u516c\u5f0f\uff0c\u5ffd\u7565\u6837\u672c\u8f68\u8ff9\u786e\u5b9a\u6027\u5dee\u5f02\uff0c\u5f15\u53d1\u4e24\u5927\u95ee\u9898\uff1a\u4f18\u52bf", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.ceede26c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY1MTA4MQ==&mid=2247485275&idx=1&sn=7d0f8ba4a0f571fd53f53c3d851be419&chksm=c2f13edc23dc555985404a2efe3625cef34e4184cedcadaa3d4ca675f7ad9e7451593d1cfd55#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY1MTA4MQ==&mid=2247485275&idx=1&sn=7d0f8ba4a0f571fd53f53c3d851be419&chksm=c2f13edc23dc555985404a2efe3625cef34e4184cedcadaa3d4ca675f7ad9e7451593d1cfd55#rd", "authors": ["\u653f\u7ba1\u9009\u9898\u7814\u7a76\u9662"], "title": "\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u79d1\u6280\u521b\u65b0\u4e0e\u4ea7\u4e1a\u878d\u5408\u8def\u5f84\u63a8\u6f14\u4e0e\u4f18\u5316\u7b56\u7565\u7814\u7a76\u3010\u7ba1\u7406\u5b66\u81ea\u79d1\u7814\u7a76\u9898\u76ee\u6316\u6398\u3011\u301020250924\u3011", "comment": "Source: WeChat, Published: 2025-09-24 12:30:18", "summary": "5. \u4eff\u771f\u4f18\u5316\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u653f\u7b56\u4f18\u5316\u7b56\u75656. \u9a8c\u8bc1\u6d4b\u8bd5\uff1a\u9009\u53d6\u5178\u578b\u6c11\u65cf\u533a\u57df\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc15. \u6570\u636e\u9700\u6c42\u4e0e\u7b97\u6cd5\u6570\u636e\u7c7b\u578b\u6765\u6e90\u7b97\u6cd5\u6c11\u65cf\u6570\u636e\u4eba\u53e3\u666e\u67e5\u793e\u4f1a\u7f51\u7edc\u5206\u6790\uff08SNA\uff09", "AI": {"tldr": "5. \u4eff\u771f\u4f18\u5316\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u653f\u7b56\u4f18\u5316\u7b56\u75656. \u9a8c\u8bc1\u6d4b\u8bd5\uff1a\u9009\u53d6\u5178\u578b\u6c11\u65cf\u533a\u57df\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc15. \u6570\u636e\u9700\u6c42\u4e0e\u7b97\u6cd5\u6570\u636e\u7c7b\u578b\u6765\u6e90\u7b97\u6cd5\u6c11\u65cf\u6570\u636e\u4eba\u53e3\u666e\u67e5\u793e\u4f1a\u7f51\u7edc\u5206\u6790\uff08SNA\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.b8e1d5b7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4NzU3MjA3OQ==&mid=2247503561&idx=1&sn=35be25d44b7b0a82211f20e9db01e10e&chksm=ea52b72532dee1579decf2a6b8342d226ba28d164adb26ddfdc4c6ae168b4be9381f69641764#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4NzU3MjA3OQ==&mid=2247503561&idx=1&sn=35be25d44b7b0a82211f20e9db01e10e&chksm=ea52b72532dee1579decf2a6b8342d226ba28d164adb26ddfdc4c6ae168b4be9381f69641764#rd", "authors": ["\u5feb\u624b\u5927\u6a21\u578b"], "title": "\u5e7f\u544a\u51fa\u4ef7\u8fdb\u5165\u201c\u81ea\u52a8\u9a7e\u9a76\u201d\u65f6\u4ee3\uff0c\u5feb\u624b\u63d0\u51fa\u751f\u6210\u5f0f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u51fa\u4ef7\u6280\u672f\uff01", "comment": "Source: WeChat, Published: 2025-09-24 11:05:53", "summary": "\u751f\u6210\u5f0f\u5f3a\u5316\u5b66\u4e60\u6709\u4e24\u4e2a\u5927\u65b9\u5411\uff1a1. Generative Model as a world model\uff1a\u5efa\u7acb\u4e00\u4e2a\u53ef\u4ee5\u6a21\u62df\u4e0d\u540c\u51fa\u4ef7\u7b56\u7565\u4e0b\u5e7f\u544a\u6295\u653e\u7ed3\u679c\u7684\u201c\u6570\u5b57\u6c99\u76d2\u201d\uff0c\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u3002", "AI": {"tldr": "\u751f\u6210\u5f0f\u5f3a\u5316\u5b66\u4e60\u6709\u4e24\u4e2a\u5927\u65b9\u5411\uff1a1. Generative Model as a world model\uff1a\u5efa\u7acb\u4e00\u4e2a\u53ef\u4ee5\u6a21\u62df\u4e0d\u540c\u51fa\u4ef7\u7b56\u7565\u4e0b\u5e7f\u544a\u6295\u653e\u7ed3\u679c\u7684\u201c\u6570\u5b57\u6c99\u76d2\u201d\uff0c\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.db58f761", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247502301&idx=1&sn=19e195ecbb17ea8627e0c1c19aa7bdf0&chksm=cff718fe7388f2ab1856bc8dbf78c4a144148acc2cb6f0da92a51d23eb87a10e012ab455b715#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247502301&idx=1&sn=19e195ecbb17ea8627e0c1c19aa7bdf0&chksm=cff718fe7388f2ab1856bc8dbf78c4a144148acc2cb6f0da92a51d23eb87a10e012ab455b715#rd", "authors": ["\u667a\u6167\u8f66\u8f86\u4e0e\u4ea4\u901a"], "title": "COMMTR | Towards Fair Lights\uff1a\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u906e\u7f69\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d70\u5eca\u7ea7\u516c\u5e73\u9ad8\u6548\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-09-24 09:00:27", "summary": "\u968f\u7740AI\u6280\u672f\u5728\u4ea4\u901a\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08ATSC\uff09\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u7136\u800c\uff0c\u73b0\u6709\u5927\u591a\u6570\u65b9\u6cd5\u4ee5\u673a\u52a8\u8f66\u884c\u4e3a\u6838\u5fc3\uff0c\u5ffd\u89c6\u4e86\u884c\u4eba\u3001\u516c\u4ea4\u4e58\u5ba2\u7b49\u4ea4\u901a\u5f31\u52bf\u7fa4\u4f53\u7684\u9700\u6c42\uff0c\u4e5f\u96be\u4ee5\u517c\u987e\u516c\u5e73\u6027\u4e0e\u6548\u7387\u3002", "AI": {"tldr": "\u968f\u7740AI\u6280\u672f\u5728\u4ea4\u901a\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08ATSC\uff09\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u7136\u800c\uff0c\u73b0\u6709\u5927\u591a\u6570\u65b9\u6cd5\u4ee5\u673a\u52a8\u8f66\u884c\u4e3a\u6838\u5fc3\uff0c\u5ffd\u89c6\u4e86\u884c\u4eba\u3001\u516c\u4ea4\u4e58\u5ba2\u7b49\u4ea4\u901a\u5f31\u52bf\u7fa4\u4f53\u7684\u9700\u6c42\uff0c\u4e5f\u96be\u4ee5\u517c\u987e\u516c\u5e73\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.9543f7bf", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDI1MzMyNg==&mid=2247485565&idx=1&sn=927882c6b21a2f36b0789e316966a5b3&chksm=c52336eb1b684d22423aa40e85e65c29424b09b8ee5410fae8b57ae22f244b917b4ac8edea55#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDI1MzMyNg==&mid=2247485565&idx=1&sn=927882c6b21a2f36b0789e316966a5b3&chksm=c52336eb1b684d22423aa40e85e65c29424b09b8ee5410fae8b57ae22f244b917b4ac8edea55#rd", "authors": ["\u4eba\u5f62AI\u667a\u63a7\u5148\u950b"], "title": "\u4ece\u8e52\u8dda\u5b66\u6b65\u5230\u5954\u8dd1\u5982\u98de\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5728\u63a7\u5236\u9886\u57df\u7684\u8fdb\u9636\u4e4b\u8def", "comment": "Source: WeChat, Published: 2025-09-24 08:16:09", "summary": "\u9762\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f3a\u5316\u5b66\u4e60\u5e76\u6ca1\u6709\u505c\u6ede\u4e0d\u524d\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u6df1\u5165\u601d\u8003\u5e76\u63a2\u7d22\u5404\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u50cf\u52c7\u6562\u7684\u5192\u9669\u8005\u5728\u56f0\u5883\u4e2d\u5bfb\u627e\u51fa\u8def\uff0c\u4e0d\u65ad\u5c1d\u8bd5\u65b0\u7684\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u52aa\u529b\u7a81\u7834\u8fd9\u4e9b\u74f6\u9888\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u63a7\u5236\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5f00\u8f9f\u65b0\u7684\u9053\u8def", "AI": {"tldr": "\u9762\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f3a\u5316\u5b66\u4e60\u5e76\u6ca1\u6709\u505c\u6ede\u4e0d\u524d\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u6df1\u5165\u601d\u8003\u5e76\u63a2\u7d22\u5404\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u50cf\u52c7\u6562\u7684\u5192\u9669\u8005\u5728\u56f0\u5883\u4e2d\u5bfb\u627e\u51fa\u8def\uff0c\u4e0d\u65ad\u5c1d\u8bd5\u65b0\u7684\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u52aa\u529b\u7a81\u7834\u8fd9\u4e9b\u74f6\u9888\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u63a7\u5236\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5f00\u8f9f\u65b0\u7684\u9053\u8def", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.6f62b29c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NjY1MTI0Mg==&mid=2660026670&idx=1&sn=59981e8465c8d82e0ab9218ba5189c8c&chksm=8567dbc4c7e8f2b973518d5cac9d404d4e49b582896b31eee0ae55488a91d0e62db10fae7a3a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NjY1MTI0Mg==&mid=2660026670&idx=1&sn=59981e8465c8d82e0ab9218ba5189c8c&chksm=8567dbc4c7e8f2b973518d5cac9d404d4e49b582896b31eee0ae55488a91d0e62db10fae7a3a#rd", "authors": ["\u673a\u5668\u667a\u80fd\u7814\u7a76MIR"], "title": "\u5357\u6d0b\u7406\u5de5\u5927\u5b66\u8096\u4f73\u5e73 \u7b49 | \u57fa\u4e8e\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5f02\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u76ee\u6807\u641c\u7d22\u4e0e\u5bfc\u822a", "comment": "Source: WeChat, Published: 2025-09-24 08:00:00", "summary": "\u7b2c4\u8282\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u7b2c5\u8282\u5bf9\u6240\u63d0\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u5e76\u8ba8\u8bba\u4e86\u7ed3\u679c\u3002\u7b2c6\u8282\u603b\u7ed3\u4e86\u672c\u5de5\u4f5c\u5e76\u5bf9\u672a\u6765\u5de5\u4f5c\u8fdb\u884c\u4e86\u5c55\u671b\u3002\u00b7 \u672c\u6587\u4f5c\u8005 \u00b7", "AI": {"tldr": "\u7b2c4\u8282\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u7b2c5\u8282\u5bf9\u6240\u63d0\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u5e76\u8ba8\u8bba\u4e86\u7ed3\u679c\u3002\u7b2c6\u8282\u603b\u7ed3\u4e86\u672c\u5de5\u4f5c\u5e76\u5bf9\u672a\u6765\u5de5\u4f5c\u8fdb\u884c\u4e86\u5c55\u671b\u3002\u00b7 \u672c\u6587\u4f5c\u8005 \u00b7", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.852a8612", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671018&idx=1&sn=6d2a80cf3c0d87d3a7a09f672a106dc1&chksm=fda0bf7de271a1b4fbee5cd2ee63f4f9503f8a3e7b8ea3b35c91a706ba18cb88fc0174827242#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671018&idx=1&sn=6d2a80cf3c0d87d3a7a09f672a106dc1&chksm=fda0bf7de271a1b4fbee5cd2ee63f4f9503f8a3e7b8ea3b35c91a706ba18cb88fc0174827242#rd", "authors": ["\u4e13\u77e5"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u9047\u89c1\u5927\u8bed\u8a00\u6a21\u578b\uff1a\u8d2f\u7a7f LLM \u751f\u547d\u5468\u671f\u7684\u8fdb\u5c55\u4e0e\u5e94\u7528\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-09-24 03:00:00", "summary": "\u8fd1\u5e74\u6765\uff0c\u4ee5\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u4e3a\u6838\u5fc3\u7684\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08Large Language Models\uff0c LLMs\uff09\u7684\u63a8\u7406\u4e0e\u5bf9\u9f50\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u3001\u9075\u5faa\u7528\u6237\u6307\u4ee4\u4ee5\u53ca\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u65b9\u9762\u3002", "AI": {"tldr": "\u8fd1\u5e74\u6765\uff0c\u4ee5\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u4e3a\u6838\u5fc3\u7684\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08Large Language Models\uff0c LLMs\uff09\u7684\u63a8\u7406\u4e0e\u5bf9\u9f50\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u3001\u9075\u5faa\u7528\u6237\u6307\u4ee4\u4ee5\u53ca\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u65b9\u9762\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.f1cf2ed0", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTE1MDc0NQ==&mid=2247487186&idx=1&sn=4e1b75bbc82507fca8a46edf57d6b4b6&chksm=97c42b37c47e93d4f6a1c55b4ef44b57f8f2a55d4870496f94c9f42a3a67e16a549bcdda2a61#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTE1MDc0NQ==&mid=2247487186&idx=1&sn=4e1b75bbc82507fca8a46edf57d6b4b6&chksm=97c42b37c47e93d4f6a1c55b4ef44b57f8f2a55d4870496f94c9f42a3a67e16a549bcdda2a61#rd", "authors": ["\u6696\u901a\u524d\u77bb"], "title": "\u9884\u6d4b\u4fe1\u606f\u5982\u4f55\u4f18\u5316HVAC\u63a7\u5236\uff1fGRU-RL\u7b97\u6cd5\u7a81\u7834<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5c40\u9650", "comment": "Source: WeChat, Published: 2025-09-23 23:21:32", "summary": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u7a7a\u8c03\u7cfb\u7edf\u63a7\u5236\u65f6\u5b58\u5728\u672a\u5145\u5206\u5229\u7528\u672a\u6765\u9884\u6d4b\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c06\u9884\u6d4b\u4fe1\u606f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u4ee5\u4f18\u5316\u7a7a\u8c03\u7cfb\u7edf\u8fd0\u884c\u7684\u96be\u9898\u3002\u7814\u7a76\u501f\u52a9\u5f00\u6e90\u6846\u67b6\u6d4b\u8bd5\u4e0d\u540c\u9884\u6d4b\u4fe1\u606f\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u8fd8\u63d0\u51faGRU - RL\u7b97\u6cd5\u6765\u5904\u7406\u7cfb\u7edf\u72b6", "AI": {"tldr": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u7a7a\u8c03\u7cfb\u7edf\u63a7\u5236\u65f6\u5b58\u5728\u672a\u5145\u5206\u5229\u7528\u672a\u6765\u9884\u6d4b\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c06\u9884\u6d4b\u4fe1\u606f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u4ee5\u4f18\u5316\u7a7a\u8c03\u7cfb\u7edf\u8fd0\u884c\u7684\u96be\u9898\u3002\u7814\u7a76\u501f\u52a9\u5f00\u6e90\u6846\u67b6\u6d4b\u8bd5\u4e0d\u540c\u9884\u6d4b\u4fe1\u606f\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u8fd8\u63d0\u51faGRU - RL\u7b97\u6cd5\u6765\u5904\u7406\u7cfb\u7edf\u72b6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.7b12ef85", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MDU5NTQ2MQ==&mid=2247485505&idx=2&sn=cb9f142aa431db32f1f3faacc3dc453c&chksm=ce0528ed34ae7c3630332635272ed9a14e5fb40f2453ee89b279a6b9b966092fe4f01a9ad734#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MDU5NTQ2MQ==&mid=2247485505&idx=2&sn=cb9f142aa431db32f1f3faacc3dc453c&chksm=ce0528ed34ae7c3630332635272ed9a14e5fb40f2453ee89b279a6b9b966092fe4f01a9ad734#rd", "authors": ["AI4CNS"], "title": "\u9999\u6e2f\u5927\u5b66\u63d0\u51faDuramax<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\uff0c\u5b9e\u73b0\u5fc3\u8840\u7ba1\u75be\u75c5\u957f\u671f\u7cbe\u51c6\u8102\u8d28\u63a7\u5236", "comment": "Source: WeChat, Published: 2025-09-23 15:34:06", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u56e0\u5176\u5728\u5e8f\u5217\u51b3\u7b56\u4f18\u5316\u4e2d\u7684\u4f18\u52bf\uff0c\u7406\u8bba\u4e0a\u9002\u7528\u4e8e\u6162\u6027\u75c5\u957f\u671f\u7ba1\u7406\u3002\u4f46\u5c06\u5176\u5e94\u7528\u4e8eCVD\u4e00\u7ea7\u9884\u9632\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5ef6\u8fdf\u5956\u52b1\u5efa\u6a21\u2014\u2014\u5982\u4f55\u5c06\u6570\u5341\u5e74\u540e\u7684\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u4e0e\u5f53\u524d\u6cbb\u7597\u51b3\u7b56\u5173\u8054\uff1b", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u56e0\u5176\u5728\u5e8f\u5217\u51b3\u7b56\u4f18\u5316\u4e2d\u7684\u4f18\u52bf\uff0c\u7406\u8bba\u4e0a\u9002\u7528\u4e8e\u6162\u6027\u75c5\u957f\u671f\u7ba1\u7406\u3002\u4f46\u5c06\u5176\u5e94\u7528\u4e8eCVD\u4e00\u7ea7\u9884\u9632\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5ef6\u8fdf\u5956\u52b1\u5efa\u6a21\u2014\u2014\u5982\u4f55\u5c06\u6570\u5341\u5e74\u540e\u7684\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u4e0e\u5f53\u524d\u6cbb\u7597\u51b3\u7b56\u5173\u8054\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.2dd7bd33", "categories": ["wechat.article", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247712292&idx=1&sn=1b3548eb06e5a9396de0d1ba8f1a875b&chksm=e9dd38ad0579e0cb7a1ecaa2e6f3be8861c4a31fb48413537bdee61b66ef290258946a10da54#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247712292&idx=1&sn=1b3548eb06e5a9396de0d1ba8f1a875b&chksm=e9dd38ad0579e0cb7a1ecaa2e6f3be8861c4a31fb48413537bdee61b66ef290258946a10da54#rd", "authors": ["Datawhale"], "title": "\u6e05\u534e\u6700\u65b0\u53d1\u5e03114\u9875\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-09-23 14:00:00", "summary": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08LLMs\uff09 \u63a8\u7406\u80fd\u529b\u53d1\u5c55\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u81ea DeepSeek-R1 \u53d1\u5e03\u4ee5\u6765\uff0cRL \u5df2\u6210\u4e3a\u5c06 LLMs \u8f6c\u5316\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u57fa\u7840\u65b9\u6cd5\u3002", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08LLMs\uff09 \u63a8\u7406\u80fd\u529b\u53d1\u5c55\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u81ea DeepSeek-R1 \u53d1\u5e03\u4ee5\u6765\uff0cRL \u5df2\u6210\u4e3a\u5c06 LLMs \u8f6c\u5316\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u57fa\u7840\u65b9\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.ae8a64ae", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489623&idx=1&sn=a96f61d9d535c2162e64220afad90459&chksm=cf281d32cd9d24cb281ac756c8bbacd4cfa916ca9ec805cf086c0c719e1ba0e4db3350d15c0a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489623&idx=1&sn=a96f61d9d535c2162e64220afad90459&chksm=cf281d32cd9d24cb281ac756c8bbacd4cfa916ca9ec805cf086c0c719e1ba0e4db3350d15c0a#rd", "authors": ["\u884c\u5ba2\u79d1\u6280"], "title": "AI <em class=\"highlight\">\u4ee3\u7801Agent</em>\u201c\u4e09\u56fd\u5fd7\u201d\uff1aGrok \u00d7 Claude \u00d7 GPT-5 Codex", "comment": "Source: WeChat, Published: 2025-09-24 13:06:19", "summary": "\u4e00\u4f4d\u5408\u683c\u7684\u4ee3\u7801agent\uff0c\u4e0d\u662f\u201c\u80fd\u5199\u51e0\u6bb5\u51fd\u6570\u201d\uff0c\u800c\u662f\u80fd\u7a33\u5b9a\u8dd1\u5b8c\u8fd9\u6761\u95ed\u73af\uff1a\u611f\u77e5\uff08\u8bfb\u4ed3\u5e93/\u5de5\u5355/\u4e0a\u4e0b\u6587\uff09\u2192 \u89c4\u5212\uff08\u62c6\u89e3\u4efb\u52a1\u3001\u9009\u5de5\u5177\uff09\u2192 \u6267\u884c\uff08\u6539\u4ee3\u7801/\u8dd1\u6d4b/\u53d1\u5305/GUI \u64cd\u4f5c\uff09\u2192 \u89c2\u6d4b\uff08\u65e5\u5fd7/\u6d4b\u8bd5/\u6307\u6807\uff09\u2192 \u81ea\u68c0\u4e0e\u56de\u6eda \u2192 \u5f62\u6210\u53ef\u5ba1", "AI": {"tldr": "\u4e00\u4f4d\u5408\u683c\u7684\u4ee3\u7801agent\uff0c\u4e0d\u662f\u201c\u80fd\u5199\u51e0\u6bb5\u51fd\u6570\u201d\uff0c\u800c\u662f\u80fd\u7a33\u5b9a\u8dd1\u5b8c\u8fd9\u6761\u95ed\u73af\uff1a\u611f\u77e5\uff08\u8bfb\u4ed3\u5e93/\u5de5\u5355/\u4e0a\u4e0b\u6587\uff09\u2192 \u89c4\u5212\uff08\u62c6\u89e3\u4efb\u52a1\u3001\u9009\u5de5\u5177\uff09\u2192 \u6267\u884c\uff08\u6539\u4ee3\u7801/\u8dd1\u6d4b/\u53d1\u5305/GUI \u64cd\u4f5c\uff09\u2192 \u89c2\u6d4b\uff08\u65e5\u5fd7/\u6d4b\u8bd5/\u6307\u6807\uff09\u2192 \u81ea\u68c0\u4e0e\u56de\u6eda \u2192 \u5f62\u6210\u53ef\u5ba1", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.8a9dd086", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247491901&idx=1&sn=ccb9ec1739032a0eea21d8b4b8dd67fe&chksm=cfe492b6ce1bceec1948a7b2e9cb7420ac18d2bf378b344da4e4f3265f882996cb7f90acddb4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247491901&idx=1&sn=ccb9ec1739032a0eea21d8b4b8dd67fe&chksm=cfe492b6ce1bceec1948a7b2e9cb7420ac18d2bf378b344da4e4f3265f882996cb7f90acddb4#rd", "authors": ["\u8682\u8681\u5f00\u6e90"], "title": "\u667a\u80fd\u7f16\u7a0b\u52a9\u624b Neovate <em class=\"highlight\">Code</em> \u6b63\u5f0f\u5f00\u6e90", "comment": "Source: WeChat, Published: 2025-09-24 10:38:15", "summary": "\u5b83\u96c6\u6210\u4e86 code agent \u6240\u9700\u7684\u6838\u5fc3\u80fd\u529b\u3002github\uff1ahttps\uff1a//github.com/neovateai/neovate-code \u76ee\u524d\uff0cNeovate Code \u4ee5 CLI \u5de5\u5177\u7684\u5f62\u6001\u63d0\u4f9b\uff0c\u4f46\u5176\u67b6\u6784\u8bbe\u8ba1\u9ad8\u5ea6\u7075\u6d3b\uff0c\u672a\u6765\u5c06\u652f\u6301\u591a\u79cd\u5ba2\u6237\u7aef\u5f62\u6001\uff0c\u9002\u914d\u66f4\u591a\u5f00\u53d1\u573a\u666f\u3002", "AI": {"tldr": "\u5b83\u96c6\u6210\u4e86 code agent \u6240\u9700\u7684\u6838\u5fc3\u80fd\u529b\u3002github\uff1ahttps\uff1a//github.com/neovateai/neovate-code \u76ee\u524d\uff0cNeovate Code \u4ee5 CLI \u5de5\u5177\u7684\u5f62\u6001\u63d0\u4f9b\uff0c\u4f46\u5176\u67b6\u6784\u8bbe\u8ba1\u9ad8\u5ea6\u7075\u6d3b\uff0c\u672a\u6765\u5c06\u652f\u6301\u591a\u79cd\u5ba2\u6237\u7aef\u5f62\u6001\uff0c\u9002\u914d\u66f4\u591a\u5f00\u53d1\u573a\u666f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.0cde2c2d", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487526&idx=1&sn=16669046bfbcf91aac3f01416fdf605d&chksm=a7da276a8a30351bdb2b98eee6325c251843740bd12ac9de1b2b7528aa14221a3e88c43bfe95#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487526&idx=1&sn=16669046bfbcf91aac3f01416fdf605d&chksm=a7da276a8a30351bdb2b98eee6325c251843740bd12ac9de1b2b7528aa14221a3e88c43bfe95#rd", "authors": ["\u4e91\u8c26\u548c\u4ed6\u7684\u670b\u53cb\u4eec"], "title": "Neovate <em class=\"highlight\">Code</em> \u73b0\u5df2\u5f00\u6e90", "comment": "Source: WeChat, Published: 2025-09-24 01:54:25", "summary": "\u5e02\u9762\u4e0a\u6709\u8fd9\u4e48\u591a Code Agent\u3002\u4ee5\u4e0b\u662f\u8ba9 Neovate Code \u4e0e\u5176\u4ed6 Code Agent \u4e0d\u540c\u7684\u4e00\u4e9b\u7279\u6027\uff1a \u5f00\u653e\u7684 Claude Code \u6613\u4e8e\u6269\u5c55 \u591a\u5ba2\u6237\u7aef\u652f\u6301Claude Code \u662f\u4e00\u4e2a\u5f88\u68d2\u7684\u4ee3\u7801\u667a\u80fd\u4f53\uff0c\u4f46\u5b83\u4e0d\u662f\u5f00\u6e90\u7684\uff0c\u60f3\u8981\u7528\u4e0a\u5b83\u8fd8\u5f97\u8d39\u4e00\u756a\u529b\u6c14\uff0c\u540c\u65f6\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e5f\u65e0", "AI": {"tldr": "\u5e02\u9762\u4e0a\u6709\u8fd9\u4e48\u591a Code Agent\u3002\u4ee5\u4e0b\u662f\u8ba9 Neovate Code \u4e0e\u5176\u4ed6 Code Agent \u4e0d\u540c\u7684\u4e00\u4e9b\u7279\u6027\uff1a \u5f00\u653e\u7684 Claude Code \u6613\u4e8e\u6269\u5c55 \u591a\u5ba2\u6237\u7aef\u652f\u6301Claude Code \u662f\u4e00\u4e2a\u5f88\u68d2\u7684\u4ee3\u7801\u667a\u80fd\u4f53\uff0c\u4f46\u5b83\u4e0d\u662f\u5f00\u6e90\u7684\uff0c\u60f3\u8981\u7528\u4e0a\u5b83\u8fd8\u5f97\u8d39\u4e00\u756a\u529b\u6c14\uff0c\u540c\u65f6\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e5f\u65e0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.666178b1", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652630127&idx=1&sn=540858f320d6d69762df1e19bc0586b2&chksm=f08be02f7cabcacb3cea10fd856cd8b49d4146f13cbafdc035142af6ad9d8737a719805d3efc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652630127&idx=1&sn=540858f320d6d69762df1e19bc0586b2&chksm=f08be02f7cabcacb3cea10fd856cd8b49d4146f13cbafdc035142af6ad9d8737a719805d3efc#rd", "authors": ["\u65b0\u667a\u5143"], "title": "\u79d8\u5854AI\u653e\u5927\u62db\uff01\u300c\u8fb9\u60f3\u8fb9\u641c\u8fb9\u505a\u300d\uff0c\u5185\u7f6e20+<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff0c\u60f3\u6cd5\u4e00\u952e\u5b9e\u73b0", "comment": "Source: WeChat, Published: 2025-09-24 13:04:26", "summary": "\u501f\u52a9\u300cAgentic Search\u300d\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u79d8\u5854AI\u53ef\u4ee5\u66f4\u597d\u7684\u300c\u7406\u89e3\u300d\u8f93\u5165\u7684\u5185\u5bb9\u3002\u4e0a\u4f20\u4e00\u5f20\u56fe\u7247\uff0c\u5c31\u80fd\u300c\u6d1e\u5bdf\u300d\u56fe\u7247\u6240\u5c5e\u4eba\u7684\u6027\u683c\u5e95\u8272\u3002\u56fe\u6e90\u81ea\u5c0f\u7ea2\u4e66\u79d8\u5854AI\u6309\u7167\u6307\u4ee4\uff0c\u4ece\u6027\u522b\u3001\u6027\u683c\u3001\u4e60\u60ef\u3001MBTI\u3001\u4e13\u4e1a\u7b49\u89d2\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u751a\u81f3\u8fd8\u80fd\u636e", "AI": {"tldr": "\u501f\u52a9\u300cAgentic Search\u300d\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u79d8\u5854AI\u53ef\u4ee5\u66f4\u597d\u7684\u300c\u7406\u89e3\u300d\u8f93\u5165\u7684\u5185\u5bb9\u3002\u4e0a\u4f20\u4e00\u5f20\u56fe\u7247\uff0c\u5c31\u80fd\u300c\u6d1e\u5bdf\u300d\u56fe\u7247\u6240\u5c5e\u4eba\u7684\u6027\u683c\u5e95\u8272\u3002\u56fe\u6e90\u81ea\u5c0f\u7ea2\u4e66\u79d8\u5854AI\u6309\u7167\u6307\u4ee4\uff0c\u4ece\u6027\u522b\u3001\u6027\u683c\u3001\u4e60\u60ef\u3001MBTI\u3001\u4e13\u4e1a\u7b49\u89d2\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u751a\u81f3\u8fd8\u80fd\u636e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.7890b25b", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MDQ0NTU3MQ==&mid=2247483856&idx=1&sn=fb24ad6a061637caa0362976c02d4eae&chksm=fc0c8898f4757f6f31b98f6da06c9d74194e4a1888d148744e76e8ca47121ad0358c577f6fa7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MDQ0NTU3MQ==&mid=2247483856&idx=1&sn=fb24ad6a061637caa0362976c02d4eae&chksm=fc0c8898f4757f6f31b98f6da06c9d74194e4a1888d148744e76e8ca47121ad0358c577f6fa7#rd", "authors": ["Hus\u7684Ai\u624b\u672d\u8bb0"], "title": "\u6784\u5efa\u53ef\u6295\u5165\u751f\u4ea7\u7684<em class=\"highlight\">Agentic</em>\u7cfb\u7edf\uff1a\u6765\u81eaShopify\u52a9\u624b\u7684\u7ecf\u9a8c\u6559\u8bad", "comment": "Source: WeChat, Published: 2025-09-24 12:18:22", "summary": "\u5206\u4eab\u4e00\u7bc7\u6765\u81ea\u4e8e Shopify \u642d\u5efa\u53ef\u751f\u4ea7\u73af\u5883\u7684 Agentic \u7cfb\u7edf\uff0c\u8fdb\u884c\u4e86\u9010\u5b57\u7ffb\u8bd1\uff0c\u5e76\u4fdd\u7559\u4e86\u82f1\u8bed\u539f\u6587\uff0c\u5220\u9664\u4e86 \"GRPO Training and Reward Hacking\" \u7ae0\u8282\uff0c\u5bf9\u6b64\u7ae0\u8282\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u70b9\u51fb\u6587\u7ae0\u5e95\u90e8\"\u539f\u6587\u9605\u8bfb\"", "AI": {"tldr": "\u5206\u4eab\u4e00\u7bc7\u6765\u81ea\u4e8e Shopify \u642d\u5efa\u53ef\u751f\u4ea7\u73af\u5883\u7684 Agentic \u7cfb\u7edf\uff0c\u8fdb\u884c\u4e86\u9010\u5b57\u7ffb\u8bd1\uff0c\u5e76\u4fdd\u7559\u4e86\u82f1\u8bed\u539f\u6587\uff0c\u5220\u9664\u4e86 \"GRPO Training and Reward Hacking\" \u7ae0\u8282\uff0c\u5bf9\u6b64\u7ae0\u8282\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u70b9\u51fb\u6587\u7ae0\u5e95\u90e8\"\u539f\u6587\u9605\u8bfb\"", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1dac74d5", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5ODkxNTc5Mw==&mid=2247484759&idx=1&sn=6d117076c4f2f11114c7e2303b5c0717&chksm=c12e281a3962d45abd42ce16ed58263ad0c6301d470e9dd2d3e9ef5d45be60387565a72134fc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5ODkxNTc5Mw==&mid=2247484759&idx=1&sn=6d117076c4f2f11114c7e2303b5c0717&chksm=c12e281a3962d45abd42ce16ed58263ad0c6301d470e9dd2d3e9ef5d45be60387565a72134fc#rd", "authors": ["\u5927\u6a21\u578b\u767d\u767d"], "title": "\u706b\u5230\u51fa\u5708\u7684<em class=\"highlight\">Agentic</em> AI \u6982\u5ff5\u56fe\uff1f\u8d85\u8be6\u7ec6\u89e3\u6790\uff0c\u5c0f\u767d\u4e00\u770b\u5c31\u4f1a\uff01", "comment": "Source: WeChat, Published: 2025-09-24 12:11:20", "summary": "\u8fd9\u5f20\u56fe\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u628aAgentic AI\u4f53\u7cfb\u5206\u6210\u4e86\u56db\u4e2a\u5c42\u5c42\u9012\u8fdb\u7684\u90e8\u5206\uff0c\u5c31\u50cf\u5957\u5a03\u4e00\u6837\uff0c\u4ece\u6700\u91cc\u9762\u5f00\u59cb\uff1a1 \u6700\u6838\u5fc3\uff1aLLM's\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09=AI \u7684 \u201c\u5927\u8111\u201d\u54b1\u4eec\u719f\u6089\u7684 GPT\u3001\u6587\u5fc3\u4e00\u8a00\u90fd\u5728\u8fd9\u4e00\u5c42\uff01", "AI": {"tldr": "\u8fd9\u5f20\u56fe\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u628aAgentic AI\u4f53\u7cfb\u5206\u6210\u4e86\u56db\u4e2a\u5c42\u5c42\u9012\u8fdb\u7684\u90e8\u5206\uff0c\u5c31\u50cf\u5957\u5a03\u4e00\u6837\uff0c\u4ece\u6700\u91cc\u9762\u5f00\u59cb\uff1a1 \u6700\u6838\u5fc3\uff1aLLM's\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09=AI \u7684 \u201c\u5927\u8111\u201d\u54b1\u4eec\u719f\u6089\u7684 GPT\u3001\u6587\u5fc3\u4e00\u8a00\u90fd\u5728\u8fd9\u4e00\u5c42\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.fd9089a9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTEwNTIyOA==&mid=2247500895&idx=1&sn=7888b82a09a64e0adfb56345b7e31270&chksm=c514cf7e23ea228242c96c75f1dd905a47f122f7d8dbe8fe1939a1aa543ed0fe6d4cb1f589ed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTEwNTIyOA==&mid=2247500895&idx=1&sn=7888b82a09a64e0adfb56345b7e31270&chksm=c514cf7e23ea228242c96c75f1dd905a47f122f7d8dbe8fe1939a1aa543ed0fe6d4cb1f589ed#rd", "authors": ["\u963f\u91cc\u4e91\u5927\u6570\u636eAI\u5e73\u53f0"], "title": "\u5927\u6570\u636e AI \u5e73\u53f0\uff1a\u6784\u7b51 <em class=\"highlight\">Agentic</em> AI \u7684\u6838\u5fc3\u57fa\u77f3", "comment": "Source: WeChat, Published: 2025-09-24 09:46:42", "summary": "\u5728\u5927\u6a21\u578b\u80fd\u529b\u52a0\u901f\u8fdb\u5316\u7684\u4eca\u5929\uff0c\u9664\u4e86\u63a8\u7406\u6a21\u578b\u548c\u5404\u7c7b Agentic \u80fd\u529b\u589e\u5f3a\u6a21\u578b\uff0c\u4e16\u754c\u6a21\u578b\u540c\u6837\u5907\u53d7\u5173\u6ce8\u3002\u4e16\u754c\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u9075\u5faa\u7269\u7406\u89c4\u5f8b\uff0c\u5177\u5907\u56e0\u679c\u63a8\u7406\u3001\u65f6\u95f4\u63a8\u6f14\u7b49\u80fd\u529b\uff0c\u662f\u5927\u6a21\u578b\u771f\u6b63\u6df1\u5165\u73b0\u5b9e\u7269\u7406\u4e16\u754c\u7684\u5173\u952e\u3002", "AI": {"tldr": "\u5728\u5927\u6a21\u578b\u80fd\u529b\u52a0\u901f\u8fdb\u5316\u7684\u4eca\u5929\uff0c\u9664\u4e86\u63a8\u7406\u6a21\u578b\u548c\u5404\u7c7b Agentic \u80fd\u529b\u589e\u5f3a\u6a21\u578b\uff0c\u4e16\u754c\u6a21\u578b\u540c\u6837\u5907\u53d7\u5173\u6ce8\u3002\u4e16\u754c\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u9075\u5faa\u7269\u7406\u89c4\u5f8b\uff0c\u5177\u5907\u56e0\u679c\u63a8\u7406\u3001\u65f6\u95f4\u63a8\u6f14\u7b49\u80fd\u529b\uff0c\u662f\u5927\u6a21\u578b\u771f\u6b63\u6df1\u5165\u73b0\u5b9e\u7269\u7406\u4e16\u754c\u7684\u5173\u952e\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.0f50a9f9", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2NzYyMDgyOA==&mid=2247519830&idx=2&sn=5f8a672a785ee1dcb35da87255adf0b8&chksm=ebcf47c9f4f7eb9eb392937fc8baef42da541b99184a55d9311dadbdcbc4e2e6d7d209c6ef89#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2NzYyMDgyOA==&mid=2247519830&idx=2&sn=5f8a672a785ee1dcb35da87255adf0b8&chksm=ebcf47c9f4f7eb9eb392937fc8baef42da541b99184a55d9311dadbdcbc4e2e6d7d209c6ef89#rd", "authors": ["\u601d\u701a\u4ea7\u4e1a\u7814\u7a76\u9662"], "title": "\u6570\u636e\u5373\u667a\u80fd\uff0c<em class=\"highlight\">Agentic</em> AI \u9a71\u52a8\u5b58\u50a8\u8303\u5f0f\u6539\u53d8", "comment": "Source: WeChat, Published: 2025-09-24 04:00:35", "summary": "\u65e0\u9650\u4e0a\u4e0b\u6587 \u6570\u5b57\u4e16\u754c\u6620\u5c04\u7269\u7406\u4e16\u754c \u6570\u636e\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u8bb0\u5fc6\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u73af\u5883\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u6570\u636e\u51b3\u5b9a\u6a21\u578b\u667a\u80fd\u7684\u9ad8\u5ea6 \u8bb0\u5fc6\u51b3\u5b9aagentic\u5e94\u7528\u667a\u80fd \u73af\u5883\u51b3\u5b9a\u6a21\u578b\u81ea\u6f14\u8fdb \u6570\u636e\u89c4\u6a21\u6269\u5c55\u5b9a\u5f8b\uff0c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u667a\u80fd", "AI": {"tldr": "\u65e0\u9650\u4e0a\u4e0b\u6587 \u6570\u5b57\u4e16\u754c\u6620\u5c04\u7269\u7406\u4e16\u754c \u6570\u636e\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u8bb0\u5fc6\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u73af\u5883\u89c4\u6a21\u6269\u5c55\u5b9a\u7406 \u6570\u636e\u51b3\u5b9a\u6a21\u578b\u667a\u80fd\u7684\u9ad8\u5ea6 \u8bb0\u5fc6\u51b3\u5b9aagentic\u5e94\u7528\u667a\u80fd \u73af\u5883\u51b3\u5b9a\u6a21\u578b\u81ea\u6f14\u8fdb \u6570\u636e\u89c4\u6a21\u6269\u5c55\u5b9a\u5f8b\uff0c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u667a\u80fd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.7caa7a75", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651227789&idx=2&sn=fa2e4db714c3a31e8a80e085414c97a8&chksm=8aaf1e9669e5ef47ed5347ca8171ab7046dad5061dd0acc53064b268b4ad25e8c6b4acfde038#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651227789&idx=2&sn=fa2e4db714c3a31e8a80e085414c97a8&chksm=8aaf1e9669e5ef47ed5347ca8171ab7046dad5061dd0acc53064b268b4ad25e8c6b4acfde038#rd", "authors": ["\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280"], "title": "ERP\u4e0d\u518d\u624b\u5de5\u64cd\u4f5c\uff0c\u77e5\u5fae\u884c\u6613\u501f<em class=\"highlight\">Agentic</em> AI \u9a71\u52a8\u4f01\u4e1a\u667a\u80fd\u8fd0\u8425\uff01", "comment": "Source: WeChat, Published: 2025-09-24 03:16:05", "summary": "\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280for software and technology agentic ai isv & startup \u5ba2\u6237\u6210\u529f\u5b9e\u8df5\u3002\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280\u662f\u6784\u5efaagentic al\u7684\u4e0d\u4e8c\u9009\u62e9\u3002\u6982\u8ff0 \u77e5\u5fae\u884c\u6613\uff08\u4e0a\u6d77\uff09\u667a\u80fd\u79d1\u6280\u6709\u9650\u516c\u53f8\uff08\u4ee5\u4e0b\u7b80\u79f0\u201c\u77e5\u5fae\u884c\u6613\u201d\uff09\u4f5c\u4e3a\u4e13\u6ce8\u4e8e\u670d\u52a1\u4e2d\u56fd500\u5f3a\u4f01\u4e1a\u7684\u667a\u80fd\u4f01\u4e1a\u8fd0\u8425", "AI": {"tldr": "\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280for software and technology agentic ai isv & startup \u5ba2\u6237\u6210\u529f\u5b9e\u8df5\u3002\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280\u662f\u6784\u5efaagentic al\u7684\u4e0d\u4e8c\u9009\u62e9\u3002\u6982\u8ff0 \u77e5\u5fae\u884c\u6613\uff08\u4e0a\u6d77\uff09\u667a\u80fd\u79d1\u6280\u6709\u9650\u516c\u53f8\uff08\u4ee5\u4e0b\u7b80\u79f0\u201c\u77e5\u5fae\u884c\u6613\u201d\uff09\u4f5c\u4e3a\u4e13\u6ce8\u4e8e\u670d\u52a1\u4e2d\u56fd500\u5f3a\u4f01\u4e1a\u7684\u667a\u80fd\u4f01\u4e1a\u8fd0\u8425", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1485f51f", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5MDA5NjYyMg==&mid=2247553012&idx=2&sn=14a9a1b202c217cde26cd613be7d970b&chksm=fc5cb2439f458e3ba4a7df50ce4c62199cf03916390b0cfa0ec82be318ade592fc522d58ca50#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5MDA5NjYyMg==&mid=2247553012&idx=2&sn=14a9a1b202c217cde26cd613be7d970b&chksm=fc5cb2439f458e3ba4a7df50ce4c62199cf03916390b0cfa0ec82be318ade592fc522d58ca50#rd", "authors": ["\u524d\u6da6\u6bcd\u57fa\u91d1"], "title": "\u8303\u5f0f\u8f6c\u79fb\uff01\u65e0\u95ee\u82af\u7a79\u63a8\u51fa\u57fa\u7840\u8bbe\u65bd<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u8702\u7fa4\uff0c\u5f00\u542f<em class=\"highlight\">Agentic\u667a\u80fd\u4f53</em>\u57fa\u7840\u8bbe\u65bd\u65b0\u7eaa\u5143", "comment": "Source: WeChat, Published: 2025-09-24 02:08:56", "summary": "infinigence \u91ca\u653e\u65e0\u7a79\u7b97\u529b \u65e0\u95ee\u82af\u7a79 agentic infra\uff1a \u91cd\u6784\u4eba\u5de5\u667a\u80fd\u53ca\u667a\u80fd\u4f53\u751f\u4ea7\u65b0\u8303\u5f0f \u8ba9agi\u89e6\u624b\u53ef\u53ca agent\u5e94\u7528 \u5177\u8eab\u667a\u80fd \u81ea\u52a8\u9a7e\u9a76 \u56fe\u7247/\u89c6\u9891\u751f\u6210 deep research vibe coding \u65e0\u95ee\u82af\u7a79\u57fa\u7840\u8bbe\u65bd\u667a\u80fd\u4f53\u8702\u7fa4 maas \u901a\u7528\u5927\u6a21\u578b \u4ee3\u7801\u6a21\u578b \u89c6\u89c9\u6a21\u578b \u8bed", "AI": {"tldr": "infinigence \u91ca\u653e\u65e0\u7a79\u7b97\u529b \u65e0\u95ee\u82af\u7a79 agentic infra\uff1a \u91cd\u6784\u4eba\u5de5\u667a\u80fd\u53ca\u667a\u80fd\u4f53\u751f\u4ea7\u65b0\u8303\u5f0f \u8ba9agi\u89e6\u624b\u53ef\u53ca agent\u5e94\u7528 \u5177\u8eab\u667a\u80fd \u81ea\u52a8\u9a7e\u9a76 \u56fe\u7247/\u89c6\u9891\u751f\u6210 deep research vibe coding \u65e0\u95ee\u82af\u7a79\u57fa\u7840\u8bbe\u65bd\u667a\u80fd\u4f53\u8702\u7fa4 maas \u901a\u7528\u5927\u6a21\u578b \u4ee3\u7801\u6a21\u578b \u89c6\u89c9\u6a21\u578b \u8bed", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.bd3c7cb8", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDczNTMyMQ==&mid=2247485029&idx=1&sn=fc2b27cc1362d3e2b61179602c0c5000&chksm=c4f894cf3b982f0ed0ef07c93fcbfefad59aa9d0211d6c2c8505163991f4108daadd673e2372#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDczNTMyMQ==&mid=2247485029&idx=1&sn=fc2b27cc1362d3e2b61179602c0c5000&chksm=c4f894cf3b982f0ed0ef07c93fcbfefad59aa9d0211d6c2c8505163991f4108daadd673e2372#rd", "authors": ["\u82cf\u5dde\u5982\u59c6AI\u4eba\u5de5\u667a\u80fd"], "title": "\u3010Agent\u4e13\u9898\u3011\u786c\u6838\u5e72\u8d27\uff01<em class=\"highlight\">Agentic</em> AI \u67b6\u6784\u5168\u89e3\u6790\uff0c\u4e00\u4efd\u751f\u4ea7\u5c31\u7eea\u7684\u5b9e\u6218\u6307\u5357", "comment": "Source: WeChat, Published: 2025-09-24 01:55:08", "summary": "agentic ai architecture agent orchestration ai agents \u4f46\u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u53ea\u9700\u5c06AI\u60f3\u8c61\u6210\u4e00\u4e2a\u80fd\u6267\u884c\u4efb\u52a1\u7684\u5c0f\u52a9\u624b\uff0c\u518d\u7a0d\u52a0\u601d\u8003\uff0c\u4fbf\u80fd\u8f7b\u677e\u6b65\u5165\u4ee3\u7406AI\u67b6\u6784\u7684\u903b\u8f91\u6bbf\u5802\u3002", "AI": {"tldr": "agentic ai architecture agent orchestration ai agents \u4f46\u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u53ea\u9700\u5c06AI\u60f3\u8c61\u6210\u4e00\u4e2a\u80fd\u6267\u884c\u4efb\u52a1\u7684\u5c0f\u52a9\u624b\uff0c\u518d\u7a0d\u52a0\u601d\u8003\uff0c\u4fbf\u80fd\u8f7b\u677e\u6b65\u5165\u4ee3\u7406AI\u67b6\u6784\u7684\u903b\u8f91\u6bbf\u5802\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.dbd6070c", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357070&idx=1&sn=7df71df7fe4f3dbf0bb96048ec59817a&chksm=8c90132c5ad9863f4c067eaa57e1d36000c1f27b2b744bff94966fdd75f88e1afa8ce1361951#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357070&idx=1&sn=7df71df7fe4f3dbf0bb96048ec59817a&chksm=8c90132c5ad9863f4c067eaa57e1d36000c1f27b2b744bff94966fdd75f88e1afa8ce1361951#rd", "authors": ["AI Agent \u9886\u57df"], "title": "\u5168\u7f51\u6700\u706b<em class=\"highlight\">Agentic</em> AI\u6982\u5ff5\u56fe\u89e3\u6790\uff01\u5c0f\u767d\u4e5f\u80fd\u770b\u61c2\uff01", "comment": "Source: WeChat, Published: 2025-09-23 23:22:23", "summary": "\u5168\u7f51\u6700\u706bAgentic AI\u6982\u5ff5\u56fe\u89e3\u6790\uff01\u5c0f\u767d\u4e5f\u80fd\u770b\u61c2\uff01\u8fd9\u5f20\u56fe\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u628aAgentic AI\u4f53\u7cfb\u5206\u6210\u4e86\u56db\u4e2a\u5c42\u5c42\u9012\u8fdb\u7684\u90e8\u5206\uff0c\u5c31\u50cf\u5957\u5a03\u4e00\u6837\uff0c\u4ece\u6700\u91cc\u9762\u5f00\u59cb\uff1a1. LLM's\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09", "AI": {"tldr": "\u5168\u7f51\u6700\u706bAgentic AI\u6982\u5ff5\u56fe\u89e3\u6790\uff01\u5c0f\u767d\u4e5f\u80fd\u770b\u61c2\uff01\u8fd9\u5f20\u56fe\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u628aAgentic AI\u4f53\u7cfb\u5206\u6210\u4e86\u56db\u4e2a\u5c42\u5c42\u9012\u8fdb\u7684\u90e8\u5206\uff0c\u5c31\u50cf\u5957\u5a03\u4e00\u6837\uff0c\u4ece\u6700\u91cc\u9762\u5f00\u59cb\uff1a1. LLM's\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.014037e2", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4OTk2MDY0Nw==&mid=2247491732&idx=1&sn=3bf7838e2601a709f434eb05de112b1f&chksm=ce91a3145400ef292777ca2e5d740f2cbf0245d4656cdfb428349a7fde37df792103d7c335cc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4OTk2MDY0Nw==&mid=2247491732&idx=1&sn=3bf7838e2601a709f434eb05de112b1f&chksm=ce91a3145400ef292777ca2e5d740f2cbf0245d4656cdfb428349a7fde37df792103d7c335cc#rd", "authors": ["\u589e\u957f Growth Croissance"], "title": "\u54a8\u8be2 | \u542c\u542c\u9ea6\u80af\u9521\u8bf4\u4f01\u4e1a\u843d\u5730<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7684\u907f\u5751\u6307\u5357", "comment": "Source: WeChat, Published: 2025-09-23 16:01:00", "summary": "\u4ec0\u4e48\u662fAgentic AI\uff1f\u632b\u6298\u662f\u4efb\u4f55\u65b0\u6280\u672f\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u7684\u81ea\u7136\u9636\u6bb5\uff0c\u6b64\u524d\u5176\u4ed6\u521b\u65b0\u6280\u672f\u7684\u53d1\u5c55\u4e5f\u5448\u73b0\u8fc7\u7c7b\u4f3c\u89c4\u5f8b\u3002\u4e3a\u603b\u7ed3\u65e9\u671f\u7ecf\u9a8c\uff0c\u6211\u4eec\u8fd1\u671f\u6df1\u5165\u7814\u7a76\u4e86\u9ea6\u80af\u9521\u4e3b\u5bfc\u768450\u591a\u4e2a\u667a\u80fd\u4f53AI\u5f00\u53d1\u9879\u76ee\uff0c\u4ee5\u53ca\u5e02\u573a\u4e0a\u6570\u5341\u4e2a\u5176\u4ed6\u76f8\u5173\u9879\u76ee\uff0c\u5e76\u5c06\u5206\u6790", "AI": {"tldr": "\u4ec0\u4e48\u662fAgentic AI\uff1f\u632b\u6298\u662f\u4efb\u4f55\u65b0\u6280\u672f\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u7684\u81ea\u7136\u9636\u6bb5\uff0c\u6b64\u524d\u5176\u4ed6\u521b\u65b0\u6280\u672f\u7684\u53d1\u5c55\u4e5f\u5448\u73b0\u8fc7\u7c7b\u4f3c\u89c4\u5f8b\u3002\u4e3a\u603b\u7ed3\u65e9\u671f\u7ecf\u9a8c\uff0c\u6211\u4eec\u8fd1\u671f\u6df1\u5165\u7814\u7a76\u4e86\u9ea6\u80af\u9521\u4e3b\u5bfc\u768450\u591a\u4e2a\u667a\u80fd\u4f53AI\u5f00\u53d1\u9879\u76ee\uff0c\u4ee5\u53ca\u5e02\u573a\u4e0a\u6570\u5341\u4e2a\u5176\u4ed6\u76f8\u5173\u9879\u76ee\uff0c\u5e76\u5c06\u5206\u6790", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.69012785", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257061&idx=2&sn=bdc1e2aa94d023fc6bbfa70199430950&chksm=bcefabff39fc7fa74e3ce406bd61eae5f6dfe0d4704dea7e9ffd93876d7d1aa3a40a57417ccc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257061&idx=2&sn=bdc1e2aa94d023fc6bbfa70199430950&chksm=bcefabff39fc7fa74e3ce406bd61eae5f6dfe0d4704dea7e9ffd93876d7d1aa3a40a57417ccc#rd", "authors": ["InfoQ"], "title": "C\u7aef\u70ed\u6218\uff0cB\u7aef\u6697\u6d8c\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u771f\u6b63\u7684\u6218\u573a\u624d\u521a\u521a\u5f00\u59cb | \u300a\u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730\u5e94\u7528\u7814\u7a76\u62a5\u544a 2025\u300b\u6b63\u5f0f\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-09-24 13:41:00", "summary": "\u8fc7\u53bb\u5f88\u957f\u4e00\u6bb5\u65f6\u95f4\uff0c\u5927\u6a21\u578b\u6280\u672f\u8fed\u4ee3\u7684\u901f\u5ea6\u51e0\u4e4e\u662f\u6bcf\u5b63\u5ea6\u4e00\u6b21\u5927\u66f4\u65b0\u2014\u2014\u8d85\u5927\u53c2\u6570\u6a21\u578b\u3001MOE\u3001\u63a8\u7406\u6210\u672c\u4e0b\u964d\u3001\u591a\u6a21\u6001\u767b\u573a\u3001Agent \u6d8c\u73b0\uff0c\u63a8\u7406\u6a21\u578b\u8f6e\u756a\u767b\u573a\uff0c\u4f3c\u4e4e\u4e00\u5207\u90fd\u5728\u53d8\u5f97\u66f4\u5feb\u3002", "AI": {"tldr": "\u8fc7\u53bb\u5f88\u957f\u4e00\u6bb5\u65f6\u95f4\uff0c\u5927\u6a21\u578b\u6280\u672f\u8fed\u4ee3\u7684\u901f\u5ea6\u51e0\u4e4e\u662f\u6bcf\u5b63\u5ea6\u4e00\u6b21\u5927\u66f4\u65b0\u2014\u2014\u8d85\u5927\u53c2\u6570\u6a21\u578b\u3001MOE\u3001\u63a8\u7406\u6210\u672c\u4e0b\u964d\u3001\u591a\u6a21\u6001\u767b\u573a\u3001Agent \u6d8c\u73b0\uff0c\u63a8\u7406\u6a21\u578b\u8f6e\u756a\u767b\u573a\uff0c\u4f3c\u4e4e\u4e00\u5207\u90fd\u5728\u53d8\u5f97\u66f4\u5feb\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.67cb0be0", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNzA5MjEyNQ==&mid=2247548699&idx=2&sn=80a3c1b75b2cd4cafb0a91f445d6286a&chksm=fb4b63ffcd88444a07e3b109f206eee6b7620bd388033066c7612f468a84b735573b6211c7e6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNzA5MjEyNQ==&mid=2247548699&idx=2&sn=80a3c1b75b2cd4cafb0a91f445d6286a&chksm=fb4b63ffcd88444a07e3b109f206eee6b7620bd388033066c7612f468a84b735573b6211c7e6#rd", "authors": ["\u88c5\u5907\u5f3a\u56fd"], "title": "\u6211\u56fd\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b9e\u73b0\u6279\u91cf\u201c\u4e0a\u8f66\u201d", "comment": "Source: WeChat, Published: 2025-09-24 11:20:07", "summary": "\u7ec4\u7ec7\u5efa\u8bbe\u7efc\u5408\u4ea4\u901a\u8fd0\u8f93\u5927\u6a21\u578b\uff0c\u52a0\u5feb\u666e\u53ca\u667a\u80fd\u4f53\u5e94\u7528\u3002\u4f5c\u4e3a\u6211\u56fd\u9996\u4e2a\u7ecf\u56fd\u52a1\u9662\u6279\u51c6\u7684\u56fd\u5bb6\u7ea7\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u4e13\u4e1a\u4f1a\u8bae\uff0c\u81ea2018\u5e74\u8d77\uff0c\u4e16\u754c\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u5927\u4f1a\u5df2\u8fde\u7eed\u4e3e\u529e\u4e03\u5c4a\u3002", "AI": {"tldr": "\u7ec4\u7ec7\u5efa\u8bbe\u7efc\u5408\u4ea4\u901a\u8fd0\u8f93\u5927\u6a21\u578b\uff0c\u52a0\u5feb\u666e\u53ca\u667a\u80fd\u4f53\u5e94\u7528\u3002\u4f5c\u4e3a\u6211\u56fd\u9996\u4e2a\u7ecf\u56fd\u52a1\u9662\u6279\u51c6\u7684\u56fd\u5bb6\u7ea7\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u4e13\u4e1a\u4f1a\u8bae\uff0c\u81ea2018\u5e74\u8d77\uff0c\u4e16\u754c\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u5927\u4f1a\u5df2\u8fde\u7eed\u4e3e\u529e\u4e03\u5c4a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.61c3b3ba", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585622&idx=3&sn=9ca738e956632aceb8f5181f6ad50364&chksm=fa9b0686c0143920a02bb137a9e30980956849975e2a396ed298cc81a6b720f519d2f141e884#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585622&idx=3&sn=9ca738e956632aceb8f5181f6ad50364&chksm=fa9b0686c0143920a02bb137a9e30980956849975e2a396ed298cc81a6b720f519d2f141e884#rd", "authors": ["AI\u601d\u60f3\u4f1a"], "title": "ICML 2025 | \u4f1a\u505a\u9898\u2260\u4f1a\u601d\u8003\uff1f\u9996\u4e2a\u53cd\u4f8b\u9a71\u52a8\u63a8\u7406\u57fa\u51c6\uff1a\u63ed\u7a7f<em class=\"highlight\">\u5927\u6a21\u578b</em>\u201c\u5237\u9898\u5f0f\u5047\u8c61\u201d", "comment": "Source: WeChat, Published: 2025-09-24 11:11:10", "summary": "\u201c\u5927\u6a21\u578b\u80fd\u89e3\u9ad8\u6570\u9898\u4e86\uff0c\u4f46\u5b83\u662f\u771f\u7684\u7406\u89e3\u4e86\u6570\u5b66\u6982\u5ff5\uff0c\u8fd8\u662f\u53ea\u80cc\u4f1a\u4e86\u9898\u5e93\u5957\u8def\uff1f\u201d\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\uff0c\u201c\u6a21\u578b\u662f\u5426\u771f\u7684\u5177\u5907\u6570\u5b66\u63a8\u7406\u80fd\u529b\u201d \u6210\u4e86\u5b66\u754c\u70ed\u8bae\u7684\u7126\u70b9\u3002", "AI": {"tldr": "\u201c\u5927\u6a21\u578b\u80fd\u89e3\u9ad8\u6570\u9898\u4e86\uff0c\u4f46\u5b83\u662f\u771f\u7684\u7406\u89e3\u4e86\u6570\u5b66\u6982\u5ff5\uff0c\u8fd8\u662f\u53ea\u80cc\u4f1a\u4e86\u9898\u5e93\u5957\u8def\uff1f\u201d\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\uff0c\u201c\u6a21\u578b\u662f\u5426\u771f\u7684\u5177\u5907\u6570\u5b66\u63a8\u7406\u80fd\u529b\u201d \u6210\u4e86\u5b66\u754c\u70ed\u8bae\u7684\u7126\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2509.37fbfb76", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwMDQ4NQ==&mid=2247500253&idx=2&sn=19538621cf50a6744abd23c23549309b&chksm=fa2cef2d51a7ed5486e06328b66b9b7e69e60a73c1534b3a5476fc80ee5abb2f7f1bcf8c6eac#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwMDQ4NQ==&mid=2247500253&idx=2&sn=19538621cf50a6744abd23c23549309b&chksm=fa2cef2d51a7ed5486e06328b66b9b7e69e60a73c1534b3a5476fc80ee5abb2f7f1bcf8c6eac#rd", "authors": ["\u632f\u90a6\u5929\u7136\u6c14LNG\u65b0\u80fd\u6e90"], "title": "\u71c3\u6c14\u53d1\u7535\u9996\u4e2a\u5782\u7c7b<em class=\"highlight\">\u5927\u6a21\u578b</em>\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-09-24 11:00:00", "summary": "\u4eac\u80fd\u96c6\u56e2\u53d1\u5e03\u884c\u4e1a\u9996\u4e2a\u71c3\u673a\u5927\u6a21\u578b\u2014\u2014\u4eac\u80fd\u201c\u64ce\u777f\u201d\u71c3\u673a\u5927\u6a21\u578b\u3002\u4f5c\u4e3a\u9996\u4e2a\u71c3\u6c14\u53d1\u7535\u9886\u57df\u5782\u7c7b\u5927\u6a21\u578b\uff0c\u4eac\u80fd\u201c\u64ce\u777f\u201d\u71c3\u673a\u5927\u6a21\u578b\u4f9d\u6258\u5168\u6808\u56fd\u4ea7\u7b97\u529b\u5e95\u5ea7\uff0c\u81ea\u4e3b\u53ef\u63a7\u3001\u8bad\u63a8\u4e00\u4f53\uff0c\u5b9e\u73b0\u4ece\u8f6f\u786c\u4ef6\u5f00\u53d1\u5230\u843d\u5730\u5e94\u7528\u7684\u5168\u94fe\u8def\u7a81\u7834\uff0c\u52a9\u529b\u201c", "AI": {"tldr": "\u4eac\u80fd\u96c6\u56e2\u53d1\u5e03\u884c\u4e1a\u9996\u4e2a\u71c3\u673a\u5927\u6a21\u578b\u2014\u2014\u4eac\u80fd\u201c\u64ce\u777f\u201d\u71c3\u673a\u5927\u6a21\u578b\u3002\u4f5c\u4e3a\u9996\u4e2a\u71c3\u6c14\u53d1\u7535\u9886\u57df\u5782\u7c7b\u5927\u6a21\u578b\uff0c\u4eac\u80fd\u201c\u64ce\u777f\u201d\u71c3\u673a\u5927\u6a21\u578b\u4f9d\u6258\u5168\u6808\u56fd\u4ea7\u7b97\u529b\u5e95\u5ea7\uff0c\u81ea\u4e3b\u53ef\u63a7\u3001\u8bad\u63a8\u4e00\u4f53\uff0c\u5b9e\u73b0\u4ece\u8f6f\u786c\u4ef6\u5f00\u53d1\u5230\u843d\u5730\u5e94\u7528\u7684\u5168\u94fe\u8def\u7a81\u7834\uff0c\u52a9\u529b\u201c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2509.06834357", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4OTc4MzI5OA==&mid=2247852911&idx=1&sn=e7c803863b8caa3ed4ed48b4223788ba&chksm=eda3cf473cbb8b6705e51b81928796f565ed093d34466917c0c35fc63baf91f0b9540319fe0b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4OTc4MzI5OA==&mid=2247852911&idx=1&sn=e7c803863b8caa3ed4ed48b4223788ba&chksm=eda3cf473cbb8b6705e51b81928796f565ed093d34466917c0c35fc63baf91f0b9540319fe0b#rd", "authors": ["\u4e91\u5934\u6761"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5c06\u541e\u566c\u8f6f\u4ef6", "comment": "Source: WeChat, Published: 2025-09-24 10:44:32", "summary": "\u5927\u6a21\u578b\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5c06\u5141\u8bb8\u4efb\u4f55\u4eba\u7528\u81ea\u7136\u8bed\u8a00\uff0c\u521b\u9020\u65e0\u9650\u591a\u7684\u5e94\u7528\u3002\u672a\u6765\u51e0\u4e4e\u6240\u6709\u4e0e\u8ba1\u7b97\u4e16\u754c\u6253\u4ea4\u9053\u7684\u8f6f\u4ef6\u53ef\u80fd\u90fd\u662f\u7531\u5927\u6a21\u578b\u4ea7\u751f\u7684 Agent\uff0c\u800c\u4e0d\u662f\u73b0\u5728\u7684\u5546\u4e1a\u8f6f\u4ef6\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5c06\u5141\u8bb8\u4efb\u4f55\u4eba\u7528\u81ea\u7136\u8bed\u8a00\uff0c\u521b\u9020\u65e0\u9650\u591a\u7684\u5e94\u7528\u3002\u672a\u6765\u51e0\u4e4e\u6240\u6709\u4e0e\u8ba1\u7b97\u4e16\u754c\u6253\u4ea4\u9053\u7684\u8f6f\u4ef6\u53ef\u80fd\u90fd\u662f\u7531\u5927\u6a21\u578b\u4ea7\u751f\u7684 Agent\uff0c\u800c\u4e0d\u662f\u73b0\u5728\u7684\u5546\u4e1a\u8f6f\u4ef6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.c2b9f638", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzAwMjMyOQ==&mid=2649890691&idx=1&sn=8c6b40d201888c6927459f557c572dee&chksm=860a590275eb4511ffcd13d47d3705b3da882ed48ff748314695adf40b9a79300999a4e0bb9e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzAwMjMyOQ==&mid=2649890691&idx=1&sn=8c6b40d201888c6927459f557c572dee&chksm=860a590275eb4511ffcd13d47d3705b3da882ed48ff748314695adf40b9a79300999a4e0bb9e#rd", "authors": ["CHIMA"], "title": "\u533b\u7597<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4ece\u5c0f\u4e8b\u505a\u8d77\uff08\u516d\uff09\uff1a\u6c1b\u56f4\u7f16\u7a0b", "comment": "Source: WeChat, Published: 2025-09-24 08:34:15", "summary": "\u8fd1\u65e5\uff0c\u6211\u62dc\u8bfb\u4e86\u859b\u4e07\u56fd\u4e3b\u4efb\u7684\u4e13\u680f\u6587\u7ae0\u300a\u5927\u6a21\u578b-\u6211\u7684\u601d\u4e0e\u60d1\u300b\uff0c\u6536\u83b7\u9887\u4e30\u3002\u6587\u4e2d\u63d0\u5230\uff0c\u4f20\u7edf\u7f16\u7a0b\u8981\u6c42\u5f00\u53d1\u8005\u4f7f\u7528\u4e25\u683c\u3001\u7cbe\u786e\u4e14\u7b26\u5408\u7279\u5b9a\u8bed\u6cd5\u7684\u4ee3\u7801\u6307\u4ee4\u4e0e\u8ba1\u7b97\u673a\u8fdb\u884c\u4ea4\u4e92\uff0c\u800c\u5f53\u4e0b\uff0c\u6211\u4eec\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u501f\u52a9\u81ea\u7136\u8bed\u8a00\uff0c\u4ee5\u66f4\u63a5\u8fd1\u4eba", "AI": {"tldr": "\u8fd1\u65e5\uff0c\u6211\u62dc\u8bfb\u4e86\u859b\u4e07\u56fd\u4e3b\u4efb\u7684\u4e13\u680f\u6587\u7ae0\u300a\u5927\u6a21\u578b-\u6211\u7684\u601d\u4e0e\u60d1\u300b\uff0c\u6536\u83b7\u9887\u4e30\u3002\u6587\u4e2d\u63d0\u5230\uff0c\u4f20\u7edf\u7f16\u7a0b\u8981\u6c42\u5f00\u53d1\u8005\u4f7f\u7528\u4e25\u683c\u3001\u7cbe\u786e\u4e14\u7b26\u5408\u7279\u5b9a\u8bed\u6cd5\u7684\u4ee3\u7801\u6307\u4ee4\u4e0e\u8ba1\u7b97\u673a\u8fdb\u884c\u4ea4\u4e92\uff0c\u800c\u5f53\u4e0b\uff0c\u6211\u4eec\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u501f\u52a9\u81ea\u7136\u8bed\u8a00\uff0c\u4ee5\u66f4\u63a5\u8fd1\u4eba", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.6cdfd2be", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMjg0MTk0Nw==&mid=2247582224&idx=1&sn=a303545f104d3ee197013468157daa8a&chksm=e98ccd2b48f69d518494f492c54395c8e1aa59aa45b5f2d980c6f7e254e52dfcff1119ac4325#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMjg0MTk0Nw==&mid=2247582224&idx=1&sn=a303545f104d3ee197013468157daa8a&chksm=e98ccd2b48f69d518494f492c54395c8e1aa59aa45b5f2d980c6f7e254e52dfcff1119ac4325#rd", "authors": ["\u521b\u65b0\u897f\u5b89"], "title": "\u786c\u79d1\u6280\u770b\u540d\u4f01 | \u6587\u65c5\u4e13\u5c5eAI\u667a\u80fd\u4f53\u6765\u4e86\uff01\u5168\u56fd\u9996\u4e2a\u7701\u7ea7\u6587\u65c5<em class=\"highlight\">\u5927\u6a21\u578b</em>\u201c\u535a\u89c2\u201d\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-09-24 08:21:06", "summary": "\u636e\u6089\uff0c9\u670818\u65e5\u5728\u4e0a\u6d77\u4e3e\u884c\u7684\u534e\u4e3a\u5168\u8054\u63a5\u5927\u4f1a\u4e0a\uff0c\u9655\u897f\u6587\u65c5\u884c\u4e1a\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u2014\u2014\u201c\u535a\u89c2\u201d\u6b63\u5f0f\u53d1\u5e03\u3002\u201c\u535a\u89c2\u201d\u7531\u9655\u6587\u6295\u96c6\u56e2\u4e0e\u534e\u4e3a\u516c\u53f8\u5408\u4f5c\u6253\u9020\uff0c\u662f\u5168\u56fd\u9996\u4e2a\u7701\u7ea7\u6587\u65c5\u5927\u6a21\u578b\u3002", "AI": {"tldr": "\u636e\u6089\uff0c9\u670818\u65e5\u5728\u4e0a\u6d77\u4e3e\u884c\u7684\u534e\u4e3a\u5168\u8054\u63a5\u5927\u4f1a\u4e0a\uff0c\u9655\u897f\u6587\u65c5\u884c\u4e1a\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u2014\u2014\u201c\u535a\u89c2\u201d\u6b63\u5f0f\u53d1\u5e03\u3002\u201c\u535a\u89c2\u201d\u7531\u9655\u6587\u6295\u96c6\u56e2\u4e0e\u534e\u4e3a\u516c\u53f8\u5408\u4f5c\u6253\u9020\uff0c\u662f\u5168\u56fd\u9996\u4e2a\u7701\u7ea7\u6587\u65c5\u5927\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.daf11de9", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDQyNjk2Mw==&mid=2247484597&idx=1&sn=c6ccfa5434665bd297a4b58cc190d643&chksm=c4f651b22e80d5133a3b3dab294ad128c09ae659217ceab127b63b9243be06d52fe528051765#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDQyNjk2Mw==&mid=2247484597&idx=1&sn=c6ccfa5434665bd297a4b58cc190d643&chksm=c4f651b22e80d5133a3b3dab294ad128c09ae659217ceab127b63b9243be06d52fe528051765#rd", "authors": ["\u4e2d\u5546\u6570\u667a\u6d6a\u6f6e"], "title": "\u4e00\u6587\u638c\u63e1<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0e\u667a\u80fd\u4f53\uff1aAI\u65f6\u4ee3\u7684\u201c\u601d\u8003\u8005\u201d\u4e0e\u201c\u884c\u52a8\u8005\u201d", "comment": "Source: WeChat, Published: 2025-09-24 07:53:21", "summary": "\u5927\u6a21\u578b\uff08Large Language Models\uff0c LLMs\uff09\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u901a\u8fc7\u6d77\u91cf\u6587\u672c\u6570\u636e\u8bad\u7ec3\u800c\u6210\u7684\u5927\u578b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002\u5b83\u5177\u6709\u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u80fd\u591f\u56de\u7b54\u95ee\u9898\u3001\u64b0\u5199\u6587\u7ae0\u3001\u7ffb\u8bd1\u8bed\u8a00\uff0c\u751a\u81f3\u8fdb\u884c\u4e00\u5b9a\u7a0b\u5ea6\u7684\u903b\u8f91\u63a8\u7406\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\uff08Large Language Models\uff0c LLMs\uff09\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u901a\u8fc7\u6d77\u91cf\u6587\u672c\u6570\u636e\u8bad\u7ec3\u800c\u6210\u7684\u5927\u578b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002\u5b83\u5177\u6709\u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u80fd\u591f\u56de\u7b54\u95ee\u9898\u3001\u64b0\u5199\u6587\u7ae0\u3001\u7ffb\u8bd1\u8bed\u8a00\uff0c\u751a\u81f3\u8fdb\u884c\u4e00\u5b9a\u7a0b\u5ea6\u7684\u903b\u8f91\u63a8\u7406\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1b183d5c", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTM3NTMwNA==&mid=2661636475&idx=2&sn=423e82dc3a2c5306cf2fd11870007359&chksm=bcaec1c2c7c5c5f37885775d6a39d9fbdbd6d4b826a92651756298cb6740db3b951a79002a81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTM3NTMwNA==&mid=2661636475&idx=2&sn=423e82dc3a2c5306cf2fd11870007359&chksm=bcaec1c2c7c5c5f37885775d6a39d9fbdbd6d4b826a92651756298cb6740db3b951a79002a81#rd", "authors": ["\u7b2c\u4e00\u8d22\u7ecf"], "title": "\u963f\u91cc\u4e91<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4ea7\u54c1\u4e03\u8fde\u53d1", "comment": "Source: WeChat, Published: 2025-09-24 06:38:30", "summary": "2025\u4e91\u6816\u5927\u4f1a\u73b0\u573a\uff0c\u963f\u91cc\u4e91CTO\u5468\u9756\u4eba\u53d1\u5e03\u4e03\u6b3e\u5927\u6a21\u578b\u4ea7\u54c1\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u901a\u4e49\u65d7\u8230\u6a21\u578bQwen3-Max\u3001\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u67b6\u6784Qwen3-Next\u53ca\u7cfb\u5217\u6a21\u578b\u3001\u5343\u95ee\u7f16\u7a0b\u6a21\u578bQwen3-Coder\u3001\u89c6\u89c9\u7406\u89e3\u6a21\u578bQwen3-VL\u3001\u5168\u6a21\u6001\u6a21\u578bQwen3-Omni\u3001\u89c6\u89c9\u57fa\u7840\u6a21\u578bWan2.5-prev", "AI": {"tldr": "2025\u4e91\u6816\u5927\u4f1a\u73b0\u573a\uff0c\u963f\u91cc\u4e91CTO\u5468\u9756\u4eba\u53d1\u5e03\u4e03\u6b3e\u5927\u6a21\u578b\u4ea7\u54c1\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u901a\u4e49\u65d7\u8230\u6a21\u578bQwen3-Max\u3001\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u67b6\u6784Qwen3-Next\u53ca\u7cfb\u5217\u6a21\u578b\u3001\u5343\u95ee\u7f16\u7a0b\u6a21\u578bQwen3-Coder\u3001\u89c6\u89c9\u7406\u89e3\u6a21\u578bQwen3-VL\u3001\u5168\u6a21\u6001\u6a21\u578bQwen3-Omni\u3001\u89c6\u89c9\u57fa\u7840\u6a21\u578bWan2.5-prev", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.319e57be", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NTI1MDEyNA==&mid=2652724663&idx=1&sn=c0d2e4bec228abc8d8d41bc78e22d518&chksm=8a56ad6baea434e22c86b7327fc9acb3cbdd544865c20bf3b4a8f1520ff08b31aa7df4cba8ff#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NTI1MDEyNA==&mid=2652724663&idx=1&sn=c0d2e4bec228abc8d8d41bc78e22d518&chksm=8a56ad6baea434e22c86b7327fc9acb3cbdd544865c20bf3b4a8f1520ff08b31aa7df4cba8ff#rd", "authors": ["\u4ebf\u6b27\u7f51"], "title": "\u963f\u91cc\u80a1\u4ef7\u5237\u65b04\u5e74\u65b0\u9ad8\uff0c\u963f\u91cc\u901a\u4e49<em class=\"highlight\">\u5927\u6a21\u578b</em>\u201c\u5168\u5bb6\u6876\u201d\u51fa\u7089", "comment": "Source: WeChat, Published: 2025-09-24 05:55:00", "summary": "\u901a\u4e49\u5343\u95eeqwen\u6a21\u578b\u5bb6\u65cf \u5927\u8bed\u8a00\u6a21\u578b \u4e13\u9879\u6a21\u578b \u5168\u5c3a\u5bf8 \u5168\u6a21\u6001 \u591a\u573a\u666f 300+\u5f00\u6e90\u6a21\u578b \u5f00\u6e90\u5168\u7403\u7b2c\u4e00 \u00b717\u4e07+\u5168\u7403\u884d\u751f\u6a21\u578b\u3002\u901a\u4e49\u4e07\u76f8wan\u6a21\u578b\u5bb6\u65cf \u963f\u91cc\u4e91\uff1a\u5168\u7403\u9886\u5148\u7684\u5168\u6808\u4eba\u5de5\u667a\u80fd\u670d\u52a1\u5546 \u65e0\u5f71 qoder ai agent \u901a\u4e49\u7075\u7801 llm os qwen3-next \u4e0b\u4e00\u4ee3", "AI": {"tldr": "\u901a\u4e49\u5343\u95eeqwen\u6a21\u578b\u5bb6\u65cf \u5927\u8bed\u8a00\u6a21\u578b \u4e13\u9879\u6a21\u578b \u5168\u5c3a\u5bf8 \u5168\u6a21\u6001 \u591a\u573a\u666f 300+\u5f00\u6e90\u6a21\u578b \u5f00\u6e90\u5168\u7403\u7b2c\u4e00 \u00b717\u4e07+\u5168\u7403\u884d\u751f\u6a21\u578b\u3002\u901a\u4e49\u4e07\u76f8wan\u6a21\u578b\u5bb6\u65cf \u963f\u91cc\u4e91\uff1a\u5168\u7403\u9886\u5148\u7684\u5168\u6808\u4eba\u5de5\u667a\u80fd\u670d\u52a1\u5546 \u65e0\u5f71 qoder ai agent \u901a\u4e49\u7075\u7801 llm os qwen3-next \u4e0b\u4e00\u4ee3", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.00379be7", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MDUzOTU5Mw==&mid=2247545923&idx=2&sn=7010cd07b04f082bd831c6ccebd5d254&chksm=cf6854d51d42b6d86337a2e895945f5ca36c2ae688e4b2f7d97206369de774d58cdc04e5dc6d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MDUzOTU5Mw==&mid=2247545923&idx=2&sn=7010cd07b04f082bd831c6ccebd5d254&chksm=cf6854d51d42b6d86337a2e895945f5ca36c2ae688e4b2f7d97206369de774d58cdc04e5dc6d#rd", "authors": ["\u8f66\u767e\u4f1a\u7814\u7a76\u9662"], "title": "AI <em class=\"highlight\">\u5927\u6a21\u578b</em>\u9a71\u52a8\u6c7d\u8f66\u4ea7\u4e1a\u4e09\u5927\u53d8\u9769\u2014\u2014\u7ade\u4e89\u529b\u3001\u4ea7\u4e1a\u94fe\u3001\u76c8\u5229\u6a21\u5f0f", "comment": "Source: WeChat, Published: 2025-09-24 00:02:14", "summary": "\u5927\u6a21\u578b\u5177\u5907\u6cdb\u5728\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u5e74\u53d1\u5c55\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5c06\u7ed9\u6c7d\u8f66\u5e26\u6765\u5168\u9762\u3001\u5f7b\u5e95\u7684\u53d8\u5316\uff0c\u8ba9\u6c7d\u8f66\u6210\u4e3a\u66f4\u9ad8\u5c42\u6b21\u3001\u66f4\u591a\u5185\u5bb9\u7684\u8d85\u7ea7\u667a\u80fd\u4f53\uff0c\u5f62\u6210\u4f1a\u601d\u8003\u3001\u61c2\u7528\u6237\u7684 AI\u6c7d\u8f66\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u5177\u5907\u6cdb\u5728\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u5e74\u53d1\u5c55\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5c06\u7ed9\u6c7d\u8f66\u5e26\u6765\u5168\u9762\u3001\u5f7b\u5e95\u7684\u53d8\u5316\uff0c\u8ba9\u6c7d\u8f66\u6210\u4e3a\u66f4\u9ad8\u5c42\u6b21\u3001\u66f4\u591a\u5185\u5bb9\u7684\u8d85\u7ea7\u667a\u80fd\u4f53\uff0c\u5f62\u6210\u4f1a\u601d\u8003\u3001\u61c2\u7528\u6237\u7684 AI\u6c7d\u8f66\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
