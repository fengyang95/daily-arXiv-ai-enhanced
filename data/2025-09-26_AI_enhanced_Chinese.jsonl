{"id": "2509.20364", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20364", "abs": "https://arxiv.org/abs/2509.20364", "authors": ["Thomas J Sheffler"], "title": "An Approach to Checking Correctness for Agentic Systems", "comment": "15 pages, 5 figures", "summary": "This paper presents a temporal expression language for monitoring AI agent\nbehavior, enabling systematic error-detection of LLM-based agentic systems that\nexhibit variable outputs due to stochastic generation processes. Drawing from\ntemporal logic techniques used in hardware verification, this approach monitors\nexecution traces of agent tool calls and state transitions to detect deviations\nfrom expected behavioral patterns. Current error-detection approaches rely\nprimarily on text matching of inputs and outputs, which proves fragile due to\nthe natural language variability inherent in LLM responses. The proposed method\ninstead focuses on the sequence of agent actions -- such as tool invocations\nand inter-agent communications -- allowing verification of system behavior\nindependent of specific textual outputs. The temporal expression language\nprovides assertions that capture correct behavioral patterns across multiple\nexecution scenarios. These assertions serve dual purposes: validating prompt\nengineering and guardrail effectiveness during development, and providing\nregression testing when agents are updated with new LLMs or modified logic. The\napproach is demonstrated using a three-agent system, where agents coordinate to\nsolve multi-step reasoning tasks. When powered by large, capable models, all\ntemporal assertions were satisfied across many test runs. However, when smaller\nmodels were substituted in two of the three agents, executions violated\nbehavioral assertions, primarily due to improper tool sequencing and failed\ncoordination handoffs. The temporal expressions successfully flagged these\nanomalies, demonstrating the method's effectiveness for detecting behavioral\nregressions in production agentic systems. This approach provides a foundation\nfor systematic monitoring of AI agent reliability as these systems become\nincreasingly deployed in critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76d1\u63a7AI\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u65f6\u95f4\u8868\u8fbe\u5f0f\u8bed\u8a00\uff0c\u901a\u8fc7\u76d1\u6d4b\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u548c\u72b6\u6001\u8f6c\u6362\u7684\u6267\u884c\u8f68\u8ff9\u6765\u68c0\u6d4b\u4e0e\u9884\u671f\u884c\u4e3a\u6a21\u5f0f\u7684\u504f\u5dee\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u5339\u914d\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u5728LLM\u7cfb\u7edf\u4e2d\u8106\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u7531\u4e8e\u968f\u673a\u751f\u6210\u8fc7\u7a0b\u5bfc\u81f4\u8f93\u51fa\u53ef\u53d8\uff0c\u4f20\u7edf\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8f93\u5165\u8f93\u51fa\u7684\u6587\u672c\u5339\u914d\uff0c\u4f46\u7531\u4e8eLLM\u54cd\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u53d8\u5f02\u6027\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f88\u8106\u5f31\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u72ec\u7acb\u4e8e\u5177\u4f53\u6587\u672c\u8f93\u51fa\u6765\u9a8c\u8bc1\u7cfb\u7edf\u884c\u4e3a\u7684\u65b9\u6cd5\u3002", "method": "\u501f\u9274\u786c\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u65f6\u5e8f\u903b\u8f91\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65f6\u95f4\u8868\u8fbe\u5f0f\u8bed\u8a00\u6765\u76d1\u63a7\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u548c\u72b6\u6001\u8f6c\u6362\u7684\u6267\u884c\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u5173\u6ce8\u667a\u80fd\u4f53\u52a8\u4f5c\u5e8f\u5217\uff08\u5982\u5de5\u5177\u8c03\u7528\u548c\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\uff09\uff0c\u800c\u975e\u5177\u4f53\u6587\u672c\u5185\u5bb9\u3002", "result": "\u5728\u4e09\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u5927\u578b\u6a21\u578b\u65f6\uff0c\u6240\u6709\u65f6\u95f4\u65ad\u8a00\u90fd\u80fd\u6ee1\u8db3\uff1b\u4f46\u5f53\u4e24\u4e2a\u667a\u80fd\u4f53\u6539\u7528\u8f83\u5c0f\u6a21\u578b\u65f6\uff0c\u6267\u884c\u4f1a\u8fdd\u53cd\u884c\u4e3a\u65ad\u8a00\uff0c\u4e3b\u8981\u7531\u4e8e\u5de5\u5177\u5e8f\u5217\u4e0d\u5f53\u548c\u534f\u8c03\u4ea4\u63a5\u5931\u8d25\u3002\u65f6\u95f4\u8868\u8fbe\u5f0f\u6210\u529f\u6807\u8bb0\u4e86\u8fd9\u4e9b\u5f02\u5e38\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cfb\u7edf\u76d1\u63a7AI\u667a\u80fd\u4f53\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u5e94\u7528\u4e2d\u90e8\u7f72\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u65f6\u95f4\u8868\u8fbe\u5f0f\u5177\u6709\u53cc\u91cd\u7528\u9014\uff1a\u5f00\u53d1\u65f6\u9a8c\u8bc1\u63d0\u793a\u5de5\u7a0b\u548c\u62a4\u680f\u6709\u6548\u6027\uff0c\u66f4\u65b0\u65f6\u63d0\u4f9b\u56de\u5f52\u6d4b\u8bd5\u3002", "topic": "agent analysis"}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "SpecDetect4AI\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u89c4\u8303\u548c\u68c0\u6d4bAI\u7279\u5b9a\u4ee3\u7801\u5f02\u5473\uff0c\u7ed3\u5408\u4e86\u9ad8\u7ea7\u58f0\u660e\u6027\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u53ef\u6269\u5c55\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\u3002", "motivation": "AI\u7cfb\u7edf\u7684\u5174\u8d77\u5e26\u6765\u4e86\u65b0\u7684\u8f6f\u4ef6\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u5f80\u5f80\u65e0\u6cd5\u53d1\u73b0AI\u7279\u5b9a\u7684\u4ee3\u7801\u5f02\u5473\uff0c\u8fd9\u4e9b\u5f02\u5473\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u91cd\u73b0\u6027\u3001\u9759\u9ed8\u5931\u8d25\u6216\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86SpecDetect4AI\u5de5\u5177\uff0c\u7ed3\u5408\u9ad8\u7ea7\u58f0\u660e\u6027DSL\u8fdb\u884c\u89c4\u5219\u89c4\u8303\uff0c\u5e76\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\u89e3\u91ca\u548c\u68c0\u6d4b\u8fd9\u4e9b\u89c4\u5219\u3002", "result": "\u5728826\u4e2aAI\u7cfb\u7edf\uff082000\u4e07\u884c\u4ee3\u7801\uff09\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e8688.66%\u7684\u7cbe\u786e\u5ea6\u548c88.89%\u7684\u53ec\u56de\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\uff0cSUS\u8bc4\u5206\u4e3a81.7/100\u3002", "conclusion": "SpecDetect4AI\u901a\u8fc7\u4e13\u7528\u89c4\u5219\u6709\u6548\u652f\u6301AI\u7279\u5b9a\u4ee3\u7801\u5f02\u5473\u7684\u89c4\u8303\u548c\u68c0\u6d4b\uff0c\u80fd\u591f\u9ad8\u6548\u5206\u6790\u5927\u578bAI\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "swe application"}}
{"id": "2509.20463", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20463", "abs": "https://arxiv.org/abs/2509.20463", "authors": ["Tue Do", "Varun Chandrasekaran", "Daniel Alabi"], "title": "Efficiently Attacking Memorization Scores", "comment": null, "summary": "Influence estimation tools -- such as memorization scores -- are widely used\nto understand model behavior, attribute training data, and inform dataset\ncuration. However, recent applications in data valuation and responsible\nmachine learning raise the question: can these scores themselves be\nadversarially manipulated? In this work, we present a systematic study of the\nfeasibility of attacking memorization-based influence estimators. We\ncharacterize attacks for producing highly memorized samples as highly sensitive\nqueries in the regime where a trained algorithm is accurate. Our attack\n(calculating the pseudoinverse of the input) is practical, requiring only\nblack-box access to model outputs and incur modest computational overhead. We\nempirically validate our attack across a wide suite of image classification\ntasks, showing that even state-of-the-art proxies are vulnerable to targeted\nscore manipulations. In addition, we provide a theoretical analysis of the\nstability of memorization scores under adversarial perturbations, revealing\nconditions under which influence estimates are inherently fragile. Our findings\nhighlight critical vulnerabilities in influence-based attribution and suggest\nthe need for robust defenses. All code can be found at\nhttps://anonymous.4open.science/r/MemAttack-5413/", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bb0\u5fc6\u5316\u5f71\u54cd\u4f30\u8ba1\u5de5\u5177\u7684\u5bf9\u6297\u6027\u653b\u51fb\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u73b0\u6709\u4ee3\u7406\u5de5\u5177\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u968f\u7740\u5f71\u54cd\u4f30\u8ba1\u5de5\u5177\u5728\u6570\u636e\u4f30\u503c\u548c\u8d1f\u8d23\u4efb\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u5de5\u5177\u672c\u8eab\u662f\u5426\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u64cd\u7eb5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u9006\u8ba1\u7b97\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u9700\u6a21\u578b\u8f93\u51fa\u8bbf\u95ee\u6743\uff0c\u8ba1\u7b97\u5f00\u9500\u8f83\u5c0f\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u5f71\u54cd\u4f30\u8ba1\u4ee3\u7406\u4e5f\u5bb9\u6613\u53d7\u5230\u9488\u5bf9\u6027\u5206\u6570\u64cd\u7eb5\u3002", "conclusion": "\u5f71\u54cd\u4f30\u8ba1\u5de5\u5177\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.20478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20478", "abs": "https://arxiv.org/abs/2509.20478", "authors": ["Vivek Myers", "Bill Chunyuan Zheng", "Benjamin Eysenbach", "Sergey Levine"], "title": "Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations", "comment": null, "summary": "Approaches for goal-conditioned reinforcement learning (GCRL) often use\nlearned state representations to extract goal-reaching policies. Two frameworks\nfor representation structure have yielded particularly effective GCRL\nalgorithms: (1) *contrastive representations*, in which methods learn\n\"successor features\" with a contrastive objective that performs inference over\nfuture outcomes, and (2) *temporal distances*, which link the (quasimetric)\ndistance in representation space to the transit time from states to goals. We\npropose an approach that unifies these two frameworks, using the structure of a\nquasimetric representation space (triangle inequality) with the right\nadditional constraints to learn successor representations that enable optimal\ngoal-reaching. Unlike past work, our approach is able to exploit a\n**quasimetric** distance parameterization to learn **optimal** goal-reaching\ndistances, even with **suboptimal** data and in **stochastic** environments.\nThis gives us the best of both worlds: we retain the stability and long-horizon\ncapabilities of Monte Carlo contrastive RL methods, while getting the free\nstitching capabilities of quasimetric network parameterizations. On existing\noffline GCRL benchmarks, our representation learning objective improves\nperformance on stitching tasks where methods based on contrastive learning\nstruggle, and on noisy, high-dimensional environments where methods based on\nquasimetric networks struggle.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5bf9\u6bd4\u8868\u793a\u548c\u65f6\u5e8f\u8ddd\u79bb\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u51c6\u5ea6\u91cf\u8868\u793a\u7a7a\u95f4\u7ed3\u6784\u548c\u989d\u5916\u7ea6\u675f\u6765\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u7684\u540e\u7ee7\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u4e24\u79cd\u8868\u793a\u7ed3\u6784\u6846\u67b6\uff1a\u5bf9\u6bd4\u8868\u793a\u548c\u65f6\u5e8f\u8ddd\u79bb\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u90fd\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\uff0c\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u51c6\u5ea6\u91cf\u8ddd\u79bb\u53c2\u6570\u5316\u6765\u5b66\u4e60\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u8ddd\u79bb\uff0c\u5373\u4f7f\u5728\u6b21\u4f18\u6570\u636e\u548c\u968f\u673a\u73af\u5883\u4e2d\u4e5f\u80fd\u5de5\u4f5c\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u8499\u7279\u5361\u6d1b\u5bf9\u6bd4RL\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u957f\u65f6\u7a0b\u80fd\u529b\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u51c6\u5ea6\u91cf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u81ea\u7531\u62fc\u63a5\u80fd\u529b\u3002", "result": "\u5728\u73b0\u6709\u79bb\u7ebfGCRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u62fc\u63a5\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5728\u566a\u58f0\u9ad8\u7ef4\u73af\u5883\u4e2d\u4e5f\u4f18\u4e8e\u57fa\u4e8e\u51c6\u5ea6\u91cf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u5bf9\u6bd4\u8868\u793a\u548c\u65f6\u5e8f\u8ddd\u79bb\u6846\u67b6\u7684\u4f18\u52bf\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "FaR-Loc\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6545\u969c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u529f\u80fd\u63d0\u53d6\u3001\u8bed\u4e49\u5bc6\u96c6\u68c0\u7d22\u548cLLM\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b9\u6cd5\u7ea7\u6545\u969c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7cfb\u7edf\u65f6\u5b58\u5728\u9879\u76ee\u7279\u5b9a\u77e5\u8bc6\u4e0d\u8db3\u548c\u5927\u578b\u9879\u76ee\u5bfc\u822a\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FaR-Loc\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) LLM\u529f\u80fd\u63d0\u53d6\u6a21\u5757\u751f\u6210\u5931\u8d25\u884c\u4e3a\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b2) \u8bed\u4e49\u5bc6\u96c6\u68c0\u7d22\u5728\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e2d\u68c0\u7d22\u529f\u80fd\u76f8\u4f3c\u7684\u65b9\u6cd5\uff1b3) LLM\u91cd\u6392\u5e8f\u6a21\u5757\u57fa\u4e8e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u91cd\u65b0\u6392\u5e8f\u68c0\u7d22\u7ed3\u679c\u3002", "result": "\u5728Defects4J\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaR-Loc\u5728Top-1\u51c6\u786e\u7387\u4e0a\u6bd4SoapFL\u548cAutoFL\u5206\u522b\u63d0\u534714.6%\u548c9.1%\uff0c\u5728Top-5\u51c6\u786e\u7387\u4e0a\u5206\u522b\u63d0\u534719.2%\u548c22.1%\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u6240\u6709\u57fa\u4e8e\u5b66\u4e60\u548c\u57fa\u4e8e\u9891\u8c31\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FaR-Loc\u901a\u8fc7RAG\u6280\u672f\u6709\u6548\u7ed3\u5408\u4e86LLM\u548c\u9884\u8bad\u7ec3\u4ee3\u7801\u5d4c\u5165\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5b9a\u4f4d\u6027\u80fd\uff0c\u7279\u522b\u662f\u91c7\u7528\u5305\u542b\u4ee3\u7801\u7ed3\u6784\u7684\u9884\u8bad\u7ec3\u6a21\u578b(\u5982UniXcoder)\u53ef\u63d0\u5347Top-1\u51c6\u786e\u7387\u8fbe49.0%\u3002", "topic": "swe application"}}
{"id": "2509.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20381", "abs": "https://arxiv.org/abs/2509.20381", "authors": ["Jianyu Wen", "Jingyun Wang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Ying Zhang"], "title": "USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model", "comment": "Accepted by Recsys'25", "summary": "Recently, Large Language Models (LLMs) have been widely employed in\nConversational Recommender Systems (CRSs). Unlike traditional language model\napproaches that focus on training, all existing LLMs-based approaches are\nmainly centered around how to leverage the summarization and analysis\ncapabilities of LLMs while ignoring the issue of training. Therefore, in this\nwork, we propose an integrated training-inference framework,\nUser-Simulator-Based framework (USB-Rec), for improving the performance of LLMs\nin conversational recommendation at the model level. Firstly, we design a\nLLM-based Preference Optimization (PO) dataset construction strategy for RL\ntraining, which helps the LLMs understand the strategies and methods in\nconversational recommendation. Secondly, we propose a Self-Enhancement Strategy\n(SES) at the inference stage to further exploit the conversational\nrecommendation potential obtained from RL training. Extensive experiments on\nvarious datasets demonstrate that our method consistently outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u8bad\u7ec3-\u63a8\u7406\u6846\u67b6USB-Rec\uff0c\u901a\u8fc7LLM\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6\u6784\u5efa\u548c\u81ea\u589e\u5f3a\u7b56\u7565\uff0c\u63d0\u5347LLM\u5728\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5229\u7528LLM\u7684\u603b\u7ed3\u548c\u5206\u6790\u80fd\u529b\uff0c\u800c\u5ffd\u89c6\u4e86\u8bad\u7ec3\u95ee\u9898\uff0c\u9700\u8981\u4ece\u6a21\u578b\u5c42\u9762\u63d0\u5347LLM\u5728\u5bf9\u8bdd\u63a8\u8350\u4e2d\u7684\u8868\u73b0", "method": "1. \u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\u7528\u4e8eRL\u8bad\u7ec3\uff1b2. \u5728\u63a8\u7406\u9636\u6bb5\u63d0\u51fa\u81ea\u589e\u5f3a\u7b56\u7565\u8fdb\u4e00\u6b65\u6316\u6398RL\u8bad\u7ec3\u83b7\u5f97\u7684\u5bf9\u8bdd\u63a8\u8350\u6f5c\u529b", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "USB-Rec\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u5bf9\u8bdd\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2509.20562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20562", "abs": "https://arxiv.org/abs/2509.20562", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Monica Sunkara", "Yi Zhang"], "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Despite the rapid advancements in LLM agents, they still face the challenge\nof generating meaningful reflections due to inadequate error analysis and a\nreliance on rare successful trajectories, especially in complex tasks. In this\nwork, we propose SAMULE, a new framework for self-learning agents powered by a\nretrospective language model that is trained based on Multi-Level Reflection\nSynthesis. It first synthesizes high-quality reflections across three\ncomplementary levels: Single-Trajectory Learning (micro-level) for detailed\nerror correction; Intra-Task Learning (meso-level) to build error taxonomies\nacross multiple trials of the same task, and Inter-Task Learning (macro-level)\nto extract transferable insights based on same typed errors from diverse task\nfailures. Then we fine-tune a language model serving as the retrospective model\nto generate reflections during inference. We further extend our framework to\ninteractive settings through a foresight-based reflection mechanism, enabling\nagents to proactively reflect and adapt during user interactions by comparing\npredicted and actual responses. Extensive experiments on three challenging\nbenchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our\napproach significantly outperforms reflection-based baselines. Our results\nhighlight the critical role of well-designed reflection synthesis and\nfailure-centric learning in building self-improving LLM agents.", "AI": {"tldr": "SAMULE\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u7ea7\u53cd\u601d\u5408\u6210\u7684\u81ea\u5b66\u4e60\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u56de\u987e\u6027\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u53cd\u601d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ee3\u7406\u5728\u751f\u6210\u6709\u610f\u4e49\u7684\u53cd\u601d\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u9519\u8bef\u5206\u6790\u4e0d\u8db3\u548c\u4f9d\u8d56\u7f55\u89c1\u7684\u6210\u529f\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86SAMULE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u5c42\u6b21\u7684\u53cd\u601d\u5408\u6210\uff1a\u5355\u8f68\u8ff9\u5b66\u4e60\uff08\u5fae\u89c2\uff09\u3001\u4efb\u52a1\u5185\u5b66\u4e60\uff08\u4e2d\u89c2\uff09\u548c\u4efb\u52a1\u95f4\u5b66\u4e60\uff08\u5b8f\u89c2\uff09\uff0c\u5e76\u8bad\u7ec3\u56de\u987e\u6027\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u601d\u3002\u8fd8\u6269\u5c55\u4e86\u57fa\u4e8e\u524d\u77bb\u7684\u53cd\u601d\u673a\u5236\u7528\u4e8e\u4ea4\u4e92\u8bbe\u7f6e\u3002", "result": "\u5728TravelPlanner\u3001NATURAL PLAN\u548cTau-bench\u4e09\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u53cd\u601d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53cd\u601d\u5408\u6210\u548c\u4ee5\u5931\u8d25\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u5728\u6784\u5efa\u81ea\u6539\u8fdbLLM\u4ee3\u7406\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.20640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20640", "abs": "https://arxiv.org/abs/2509.20640", "authors": ["Oluwakemi T. Olayinka", "Sumeet Jeswani", "Divine Iloh"], "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI", "comment": null, "summary": "Traditional static cybersecurity models often struggle with scalability,\nreal-time detection, and contextual responsiveness in the current digital\nproduct ecosystems which include cloud services, application programming\ninterfaces (APIs), mobile platforms, and edge devices. This study introduces\nautonomous goal driven agents capable of dynamic learning and context-aware\ndecision making as part of an adaptive cybersecurity architecture driven by\nagentic artificial intelligence (AI). To facilitate autonomous threat\nmitigation, proactive policy enforcement, and real-time anomaly detection, this\nframework integrates agentic AI across the key ecosystem layers. Behavioral\nbaselining, decentralized risk scoring, and federated threat intelligence\nsharing are important features. The capacity of the system to identify zero-day\nattacks and dynamically modify access policies was demonstrated through native\ncloud simulations. The evaluation results show increased adaptability,\ndecreased response latency, and improved detection accuracy. The architecture\nprovides an intelligent and scalable blueprint for safeguarding complex digital\ninfrastructure and is compatible with zero-trust models, thereby supporting the\nadherence to international cybersecurity regulations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fdAI\u4ee3\u7406\u7684\u81ea\u9002\u5e94\u7f51\u7edc\u5b89\u5168\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u5b66\u4e60\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\uff0c\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u7f51\u7edc\u5b89\u5168\u6a21\u578b\u5728\u5f53\u524d\u5305\u542b\u4e91\u670d\u52a1\u3001API\u3001\u79fb\u52a8\u5e73\u53f0\u548c\u8fb9\u7f18\u8bbe\u5907\u7684\u6570\u5b57\u4ea7\u54c1\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u96be\u4ee5\u5e94\u5bf9\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u80fd\u529b\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u76ee\u6807\u9a71\u52a8\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u96c6\u6210\u884c\u4e3a\u57fa\u7ebf\u5206\u6790\u3001\u53bb\u4e2d\u5fc3\u5316\u98ce\u9669\u8bc4\u5206\u548c\u8054\u90a6\u5a01\u80c1\u60c5\u62a5\u5171\u4eab\u7b49\u5173\u952e\u6280\u672f\uff0c\u5728\u5173\u952e\u751f\u6001\u7cfb\u7edf\u5c42\u4e2d\u90e8\u7f72\u667a\u80fdAI\u4ee3\u7406\u3002", "result": "\u901a\u8fc7\u539f\u751f\u4e91\u6a21\u62df\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u80fd\u591f\u8bc6\u522b\u96f6\u65e5\u653b\u51fb\u5e76\u52a8\u6001\u4fee\u6539\u8bbf\u95ee\u7b56\u7565\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u9002\u5e94\u6027\u589e\u5f3a\u3001\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u3001\u68c0\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u4fdd\u62a4\u590d\u6742\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u667a\u80fd\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u4e0e\u96f6\u4fe1\u4efb\u6a21\u578b\u517c\u5bb9\uff0c\u652f\u6301\u9075\u5b88\u56fd\u9645\u7f51\u7edc\u5b89\u5168\u6cd5\u89c4\u3002", "topic": "agent analysis"}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u5408\u6210\u9a8c\u8bc1\u7684\u74f6\u9888\u95ee\u9898\uff0c\u53d1\u73b0\u5f53\u524d\u9a8c\u8bc1\u65b9\u6cd5\u8fc7\u4e8e\u4e25\u683c\uff0c\u8fc7\u6ee4\u4e86\u6709\u4ef7\u503c\u7684\u591a\u6837\u6027\u3002\u901a\u8fc7\u5206\u6790\u9a8c\u8bc1\u8bbe\u8ba1\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u6821\u51c6\u9a8c\u8bc1\u4e0e\u591a\u6837\u5316\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u5bf9\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u6765\u7a81\u7834\u9a8c\u8bc1\u5929\u82b1\u677f\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u6a21\u578b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u4f46\u5408\u6210\u9a8c\u8bc1\u5668\u7684\u80fd\u529b\u9650\u5236\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5f62\u6210\u4e86\u9a8c\u8bc1\u5929\u82b1\u677f\u74f6\u9888\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u9a8c\u8bc1\u8bbe\u8ba1\u548c\u7b56\u7565\u7684\u5f71\u54cd\uff1a(i)\u5206\u6790\u6d4b\u8bd5\u590d\u6742\u6027\u548c\u6570\u91cf\u7684\u5f71\u54cd\uff1b(ii)\u63a2\u7d22\u5bbd\u677e\u901a\u8fc7\u9608\u503c\u548cLLM\u8f6f\u9a8c\u8bc1\uff1b(iii)\u901a\u8fc7\u5bf9\u6bd4\u6b63\u786e\u4e0e\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u53ca\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u5fc5\u8981\u6027\u3002", "result": "\u66f4\u4e30\u5bcc\u7684\u6d4b\u8bd5\u5957\u4ef6\u5e73\u5747\u63d0\u53473%\u7684pass@1\u6027\u80fd\uff1b\u5bbd\u677e\u9a8c\u8bc1\u9608\u503c\u53ef\u56de\u6536\u6709\u4ef7\u503c\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u53472-4\u70b9pass@1\uff1b\u4fdd\u7559\u6bcf\u4e2a\u95ee\u9898\u7684\u591a\u6837\u5316\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u5e26\u6765\u4e00\u81f4\u6cdb\u5316\u589e\u76ca\u3002", "conclusion": "\u5f53\u524d\u9a8c\u8bc1\u5b9e\u8df5\u8fc7\u4e8e\u4e25\u683c\uff0c\u4f46\u4e0d\u80fd\u5b8c\u5168\u629b\u5f03\uff0c\u9700\u8981\u91cd\u65b0\u6821\u51c6\u3002\u7ed3\u5408\u6821\u51c6\u9a8c\u8bc1\u4e0e\u591a\u6837\u5316\u6311\u6218\u6027\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u5bf9\uff0c\u53ef\u4ee5\u7a81\u7834\u9a8c\u8bc1\u5929\u82b1\u677f\uff0c\u5f00\u53d1\u66f4\u5f3a\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "PseudoBridge\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4ee3\u7801\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u4ee3\u7801\u4f5c\u4e3a\u4e2d\u95f4\u6a21\u6001\u6765\u66f4\u597d\u5730\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c\u7f16\u7a0b\u8bed\u8a00\u903b\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u4ee3\u7801\u98ce\u683c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4eba\u7c7b\u610f\u56fe\u4e0e\u673a\u5668\u6267\u884c\u903b\u8f91\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u4ee3\u7801\u98ce\u683c\u7684\u6709\u9650\u9c81\u68d2\u6027\u3002", "method": "PseudoBridge\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u4f2a\u4ee3\u7801\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0e\u4f2a\u4ee3\u7801\u7684\u663e\u5f0f\u5bf9\u9f50\uff1b2\uff09\u5f15\u5165\u903b\u8f91\u4e0d\u53d8\u7684\u4ee3\u7801\u98ce\u683c\u589e\u5f3a\u7b56\u7565\uff0c\u751f\u6210\u98ce\u683c\u591a\u6837\u4f46\u903b\u8f91\u7b49\u4ef7\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u5e76\u4e0e\u4f2a\u4ee3\u7801\u5bf9\u9f50\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c6\u79cd\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPseudoBridge\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u9886\u57df\u8fc1\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u4f2a\u4ee3\u7801\u5b9e\u73b0\u663e\u5f0f\u903b\u8f91\u5bf9\u9f50\u662f\u6709\u6548\u7684\uff0cPseudoBridge\u5c55\u793a\u4e86\u4f5c\u4e3a\u7a33\u5065\u3001\u53ef\u6cdb\u5316\u4ee3\u7801\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2509.20502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20502", "abs": "https://arxiv.org/abs/2509.20502", "authors": ["Xiao Wang", "Jia Wang", "Yijie Wang", "Pengtao Dang", "Sha Cao", "Chi Zhang"], "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in natural\nlanguage understanding, yet their reasoning capabilities remain limited when\noperating as single agents. Multi-Agent Debate (MAD) has been proposed to\naddress this limitation by enabling collaborative reasoning among multiple\nmodels in a round-table debate manner. While effective, MAD introduces\nsubstantial computational overhead due to the number of agents involved and the\nfrequent communication required. In this paper, we propose MARS (Multi-Agent\nReview System), a role-based collaboration framework inspired by the review\nprocess. In MARS, an author agent generates an initial solution, reviewer\nagents provide decisions and comments independently, and a meta-reviewer\nintegrates the feedback to make the final decision and guide further revision.\nThis design enhances reasoning quality while avoiding costly\nreviewer-to-reviewer interactions, thereby controlling token consumption and\ninference time. We compared MARS with both MAD and other state-of-the-art\nreasoning strategies across multiple benchmarks. Extensive experiments with\ndifferent LLMs show that MARS matches the accuracy of MAD while reducing both\ntoken usage and inference time by approximately 50\\%. Code is available at\nhttps://github.com/xwang97/MARS.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5ba1\u7a3f\u6d41\u7a0b\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u4e0eMAD\u76f8\u5f53\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06token\u4f7f\u7528\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u7ea650%\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba(MAD)\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u6d89\u53ca\u591a\u4e2a\u667a\u80fd\u4f53\u9891\u7e41\u4ea4\u4e92\uff0c\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u53c8\u80fd\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMARS\u6846\u67b6\uff0c\u5305\u542b\u4f5c\u8005\u667a\u80fd\u4f53\u751f\u6210\u521d\u59cb\u65b9\u6848\u3001\u5ba1\u7a3f\u4eba\u667a\u80fd\u4f53\u72ec\u7acb\u63d0\u4f9b\u51b3\u7b56\u548c\u8bc4\u8bba\u3001\u5143\u5ba1\u7a3f\u4eba\u6574\u5408\u53cd\u9988\u5e76\u6307\u5bfc\u4fee\u8ba2\u7684\u89d2\u8272\u5206\u5de5\u8bbe\u8ba1\uff0c\u907f\u514d\u5ba1\u7a3f\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMARS\u4e0eMAD\u7684\u51c6\u786e\u6027\u76f8\u5f53\uff0c\u4f46token\u4f7f\u7528\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u5747\u51cf\u5c11\u4e86\u7ea650%\u3002", "conclusion": "MARS\u901a\u8fc7\u89d2\u8272\u5316\u534f\u4f5c\u8bbe\u8ba1\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CodeHinter\u8c03\u8bd5\u52a9\u624b\uff0c\u7ed3\u5408\u4f20\u7edf\u8c03\u8bd5\u5de5\u5177\u548cLLM\u6280\u672f\uff0c\u5e2e\u52a9\u65b0\u624b\u7a0b\u5e8f\u5458\u4fee\u590d\u8bed\u4e49\u9519\u8bef\u5e76\u4fc3\u8fdb\u4e3b\u52a8\u53c2\u4e0e\u8c03\u8bd5\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709AI\u8c03\u8bd5\u5de5\u5177\u5bb9\u6613\u5bfc\u81f4\u5b66\u751f\u5bf9AI\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u53c2\u4e0e\u8c03\u8bd5\u8fc7\u7a0b\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u4fc3\u8fdb\u4e3b\u52a8\u5b66\u4e60\u7684\u8c03\u8bd5\u52a9\u624b\u3002", "method": "\u8bbe\u8ba1CodeHinter\u8c03\u8bd5\u52a9\u624b\uff0c\u6574\u5408\u4f20\u7edf\u8c03\u8bd5\u5de5\u5177\u548cLLM\u6280\u672f\uff0c\u901a\u8fc7\u7b2c\u4e8c\u7248\u8bbe\u8ba1\u8fed\u4ee3\u5e76\u5728\u672c\u79d1\u751f\u7fa4\u4f53\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b66\u751f\u8ba4\u4e3a\u8be5\u5de5\u5177\u5728\u89e3\u51b3\u8bed\u4e49\u9519\u8bef\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u6bd4\u7b2c\u4e00\u7248\u663e\u8457\u6613\u7528\uff0c\u9519\u8bef\u5b9a\u4f4d\u662f\u6700\u6709\u4ef7\u503c\u7684\u529f\u80fd\u3002", "conclusion": "AI\u8f85\u52a9\u8c03\u8bd5\u5de5\u5177\u5e94\u6839\u636e\u7528\u6237\u753b\u50cf\u8fdb\u884c\u4e2a\u6027\u5316\u5b9a\u5236\uff0c\u4ee5\u4f18\u5316\u4e0e\u5b66\u751f\u7684\u4ea4\u4e92\u6548\u679c\u3002", "topic": "swe application"}}
{"id": "2509.20744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20744", "abs": "https://arxiv.org/abs/2509.20744", "authors": ["Qihang Ai", "Haiyun Jiang"], "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning", "comment": "4 pages", "summary": "We study reasoning tasks through a framework that integrates auto-regressive\n(AR) and non-autoregressive (NAR) language models. AR models, which generate\ntext sequentially, excel at producing coherent outputs but often suffer from\nslow inference, particularly in reasoning-intensive domains such as mathematics\nand code, where lengthy chains of thought are required. In contrast, NAR\nmodels, such as discrete diffusion models, allow parallel generation and offer\nsubstantial speedups, though typically at the cost of reduced output quality.\nTo address these limitations, we introduce a new paradigm in which an NAR model\nefficiently produces intermediate reasoning traces, which subsequently guide an\nAR model to deliver precise final answers. Experiments demonstrate that our\napproach yields significant 26% improvements over strong baselines while\nsubstantially reducing inference cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52(AR)\u548c\u975e\u81ea\u56de\u5f52(NAR)\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528NAR\u6a21\u578b\u9ad8\u6548\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff0c\u518d\u7531AR\u6a21\u578b\u751f\u6210\u7cbe\u786e\u6700\u7ec8\u7b54\u6848\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8626%\u7684\u6027\u80fd\u63d0\u5347\u5e76\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "AR\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff0cNAR\u6a21\u578b\u901f\u5ea6\u5feb\u4f46\u8f93\u51fa\u8d28\u91cf\u8f83\u4f4e\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6765\u89e3\u51b3\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\u3002", "method": "\u91c7\u7528AR\u548cNAR\u6a21\u578b\u96c6\u6210\u6846\u67b6\uff0cNAR\u6a21\u578b\u5e76\u884c\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff0cAR\u6a21\u578b\u57fa\u4e8e\u8fd9\u4e9b\u8f68\u8ff9\u751f\u6210\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u534726%\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "AR-NAR\u96c6\u6210\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u63a8\u7406\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR\u662f\u4e00\u79cd\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u601d\u7ef4\u94fe\u6280\u672f\u8bad\u7ec3LLMs\u8fdb\u884c\u591a\u7ef4\u5ea6\u4ee3\u7801\u5ba1\u67e5\u5206\u6790\uff0c\u7ed3\u5408\u6700\u5927\u71b5\u5efa\u6a21\u548c\u9884\u5b9a\u4e49\u63a8\u7406\u8def\u5f84\u89e3\u51b3\u957f\u63d0\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u5fae\u8c03\u7684LLMs\u65b9\u6cd5\u53d7\u9650\u4e8e\u6709\u9650\u6216\u6a21\u7cca\u7684\u8bad\u7ec3\u4fe1\u606f\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u5ba1\u67e5\u8005\u90a3\u6837\u540c\u65f6\u5206\u6790\u4ee3\u7801\u5ba1\u67e5\u7684\u591a\u4e2a\u7ef4\u5ea6\u3002", "method": "\u63d0\u51faMelcotCR\u65b9\u6cd5\uff0c\u7ed3\u5408\u6700\u5927\u71b5\u5efa\u6a21\u539f\u5219\u548c\u9884\u5b9a\u4e49\u63a8\u7406\u8def\u5f84\uff0c\u5229\u7528\u957f\u601d\u7ef4\u94fe\u6280\u672f\u63d0\u4f9b\u4e30\u5bcc\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u7684\u903b\u8f91\u4e25\u5bc6\u6027\u3002", "result": "\u5728MelcotCR\u6570\u636e\u96c6\u548cCodeReviewer\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c14B\u53c2\u6570\u7684Qwen2.5\u6a21\u578b\u7ecf\u8fc7MelcotCR\u5fae\u8c03\u540e\uff0c\u5728\u4ee3\u7801\u95ee\u9898\u68c0\u6d4b\u548c\u63cf\u8ff0\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e671B\u7684DeepSeek-R1\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MelcotCR\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u4ee3\u7801\u5ba1\u67e5\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u601d\u7ef4\u94fe\u5fae\u8c03\u7b56\u7565\u7684\u4f18\u8d8a\u6027\u3002", "topic": "swe application"}}
{"id": "2509.20754", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Meta-Memory\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u4ee3\u7406\uff0c\u80fd\u591f\u6784\u5efa\u9ad8\u5bc6\u5ea6\u73af\u5883\u8bb0\u5fc6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u6a21\u6001\u7684\u8054\u5408\u63a8\u7406\u6765\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u4f4d\u7f6e\u67e5\u8be2\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u5b58\u50a8\u89c2\u6d4b\u8bb0\u5fc6\u5e76\u5229\u7528\u8fd9\u4e9b\u8bb0\u5fc6\u56de\u7b54\u4eba\u7c7b\u5173\u4e8e\u7a7a\u95f4\u4f4d\u7f6e\u67e5\u8be2\u7684\u5173\u952e\u7814\u7a76\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5728\u9ad8\u6548\u8bb0\u5fc6\u68c0\u7d22\u548c\u96c6\u6210\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMeta-Memory\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u6784\u5efa\u73af\u5883\u8bb0\u5fc6\u8868\u793a\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u6a21\u6001\u7684\u8054\u5408\u63a8\u7406\u5b9e\u73b0\u8bb0\u5fc6\u68c0\u7d22\u548c\u96c6\u6210\u3002\u5f00\u53d1\u4e86SpaceLocQA\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728SpaceLocQA\u548c\u516c\u5171NaVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "Meta-Memory\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5f3a\u5927\u51c6\u786e\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8bb0\u5fc6\u68c0\u7d22\u548c\u96c6\u6210\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2509.20798", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20798", "abs": "https://arxiv.org/abs/2509.20798", "authors": ["Lipeng Ma", "Yixuan Li", "Weidong Yang", "Mingjie Zhou", "Xinyi Liu", "Ben Fei", "Shuhao Li", "Xiaoyan Sun", "Sihang Jiang", "Yanghua Xiao"], "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks", "comment": "under review", "summary": "Log analysis is crucial for monitoring system health and diagnosing failures\nin complex systems. Recent advances in large language models (LLMs) offer new\nopportunities for automated log analysis, leveraging their reasoning\ncapabilities to perform tasks such as anomaly detection and failure prediction.\nHowever, general-purpose LLMs struggle to formulate structured reasoning\nworkflows that align with expert cognition and deliver precise details of\nreasoning steps. To address these challenges, we propose LogReasoner, a\ncoarse-to-fine reasoning enhancement framework designed to enable LLMs to\nreason log analysis tasks like experts. LogReasoner consists of two stages: (1)\ncoarse-grained enhancement of expert thinking, where high-level expert thoughts\nare constructed from collected troubleshooting flowcharts and existing tasks to\nenable LLMs to formulate structured reasoning workflows and (2) fine-grained\nenhancement of specific steps, where we first fine-tune the LLM with\ntask-specific stepwise solutions to enhance the LLM for instantiated reasoning,\nthen employ the preference learning to calibrate the LLM's reasoning details\nfrom its mistakes, further strengthen the LLM's analytical granularity and\ncorrectness. We evaluate LogReasoner on four distinct log analysis tasks using\nopen-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that\nLogReasoner significantly outperforms existing LLMs, achieving state-of-the-art\nperformance and demonstrating its effectiveness in enhancing the reasoning\ncapabilities of LLMs for log analysis.", "AI": {"tldr": "LogReasoner\u662f\u4e00\u4e2a\u7c97\u5230\u7ec6\u7684\u63a8\u7406\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u601d\u7ef4\u5efa\u6a21\u548c\u6b65\u9aa4\u7ec6\u5316\u6765\u63d0\u5347LLM\u5728\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u901a\u7528LLM\u5728\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e2d\u96be\u4ee5\u6784\u5efa\u4e0e\u4e13\u5bb6\u8ba4\u77e5\u4e00\u81f4\u7684\u7ed3\u6784\u5316\u63a8\u7406\u6d41\u7a0b\uff0c\u4e14\u63a8\u7406\u6b65\u9aa4\u7ec6\u8282\u4e0d\u591f\u7cbe\u786e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u589e\u5f3a\u6846\u67b6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u7c97\u7c92\u5ea6\u4e13\u5bb6\u601d\u7ef4\u589e\u5f3a\uff0c\u4ece\u6545\u969c\u6392\u9664\u6d41\u7a0b\u56fe\u6784\u5efa\u9ad8\u5c42\u601d\u7ef4\uff1b2\uff09\u7ec6\u7c92\u5ea6\u6b65\u9aa4\u589e\u5f3a\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u89e3\u51b3\u65b9\u6848\u5fae\u8c03LLM\uff0c\u5e76\u4f7f\u7528\u504f\u597d\u5b66\u4e60\u6821\u51c6\u63a8\u7406\u7ec6\u8282\u3002", "result": "\u5728\u56db\u4e2a\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\uff0c\u4f7f\u7528Qwen-2.5\u548cLlama-3\u7b49\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LogReasoner\u6709\u6548\u589e\u5f3a\u4e86LLM\u5728\u65e5\u5fd7\u5206\u6790\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.20912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20912", "abs": "https://arxiv.org/abs/2509.20912", "authors": ["Tianrun Xu", "Haoda Jing", "Ye Li", "Yuquan Wei", "Jun Feng", "Guanyu Chen", "Haichuan Gao", "Tianren Zhang", "Feng Chen"], "title": "DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning", "comment": null, "summary": "Recent advances in multimodal language models (MLLMs) have achieved\nremarkable progress in vision-language reasoning, especially with the emergence\nof \"thinking with images,\" which integrates explicit visual steps into the\nreasoning process. While this paradigm strengthens image-based reasoning, a\nsignificant challenge remains: models may arrive at correct answers by relying\non irrelevant or spurious regions, driven by prior knowledge or dataset biases.\nEven when the answer is correct, flawed reasoning indicates that the model has\nnot truly understood the image, highlighting the critical importance of\nreasoning fidelity in multimodal tasks. To address this issue, we propose\nDeFacto, a counterfactual reasoning framework that jointly enforces accurate\nanswering and faithful reasoning. A key component of our approach is the design\nof three complementary training paradigms: (i) positive, (ii) counterfactual,\nand (iii) random-masking. To enable these paradigms, we develop a pipeline that\nautomatically localizes question-relevant evidence and constructs positive,\ncounterfactual, and random variants, resulting in a dataset of about 100k\nimages. Building on this framework, we train multimodal language models with\nGRPO-based reinforcement learning, where we design three complementary rewards\nto guide the model toward accurate answering and evidence-grounded reasoning.\nExperiments on diverse benchmarks demonstrate that DeFacto substantially\nimproves both answer accuracy and reasoning faithfulness, establishing a\nstronger foundation for interpretable multimodal reasoning. The code is\navailable on GitHub and the dataset is released on HuggingFace.", "AI": {"tldr": "DeFacto\u662f\u4e00\u4e2a\u53cd\u4e8b\u5b9e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4f8b\u3001\u53cd\u4e8b\u5b9e\u548c\u968f\u673a\u63a9\u7801\u4e09\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408GRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e2d\u53ef\u80fd\u4f9d\u8d56\u65e0\u5173\u533a\u57df\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u8868\u660e\u6a21\u578b\u5e76\u672a\u771f\u6b63\u7406\u89e3\u56fe\u50cf\uff0c\u63a8\u7406\u5fe0\u5b9e\u6027\u5b58\u5728\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u63d0\u51faDeFacto\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u8bad\u7ec3\u8303\u5f0f\uff1a\u6b63\u4f8b\u3001\u53cd\u4e8b\u5b9e\u548c\u968f\u673a\u63a9\u7801\uff1b\u5f00\u53d1\u81ea\u52a8\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u8bc1\u636e\u7684\u6d41\u7a0b\uff1b\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e09\u79cd\u4e92\u8865\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeFacto\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u63a8\u7406\u5efa\u7acb\u4e86\u66f4\u5f3a\u57fa\u7840\u3002", "conclusion": "DeFacto\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u771f\u5b9e\u7406\u89e3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.20998", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20998", "abs": "https://arxiv.org/abs/2509.20998", "authors": ["Panagiotis Michelakis", "Yiannis Hadjiyiannis", "Dimitrios Stamoulis"], "title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State", "comment": "Accepted: LAW 2025 Workshop NeurIPS 2025", "summary": "Evaluating AI agents that solve real-world tasks through function-call\nsequences remains an open challenge. Existing agentic benchmarks often reduce\nevaluation to a binary judgment of the final state, overlooking critical\naspects such as safety, efficiency, and intermediate correctness. We propose a\nframework based on deterministic finite automata (DFAs) that encodes tasks as\nsets of valid tool-use paths, enabling principled assessment of agent behavior\nin diverse world models. Building on this foundation, we introduce CORE, a\nsuite of five metrics, namely Path Correctness, Path Correctness - Kendall's\ntau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that\nquantify alignment with expected execution patterns. Across diverse worlds, our\nmethod reveals important performance differences between agents that would\notherwise appear equivalent under traditional final-state evaluation schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\uff08DFA\uff09\u7684\u6846\u67b6\u6765\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7CORE\u6307\u6807\u5957\u4ef6\u91cf\u5316\u4ee3\u7406\u884c\u4e3a\u4e0e\u9884\u671f\u6267\u884c\u6a21\u5f0f\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u53ea\u5173\u6ce8\u6700\u7ec8\u72b6\u6001\u7684\u4e8c\u5143\u5224\u65ad\uff0c\u5ffd\u89c6\u4e86\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u4e2d\u95f4\u6b63\u786e\u6027\u7b49\u5173\u952e\u65b9\u9762\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\uff08DFA\uff09\u5c06\u4efb\u52a1\u7f16\u7801\u4e3a\u6709\u6548\u5de5\u5177\u4f7f\u7528\u8def\u5f84\u7684\u96c6\u5408\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1CORE\u6307\u6807\u5957\u4ef6\uff0c\u5305\u62ec\u8def\u5f84\u6b63\u786e\u6027\u3001\u524d\u7f00\u5173\u952e\u6027\u3001\u6709\u5bb3\u8c03\u7528\u7387\u548c\u6548\u7387\u7b49\u4e94\u4e2a\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5728\u4e0d\u540c\u4e16\u754c\u6a21\u578b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u4f20\u7edf\u6700\u7ec8\u72b6\u6001\u8bc4\u4f30\u65b9\u6848\u4e0b\u770b\u4f3c\u7b49\u6548\u7684\u4ee3\u7406\u4e4b\u95f4\u7684\u91cd\u8981\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8eDFA\u7684\u6846\u67b6\u548cCORE\u6307\u6807\u80fd\u591f\u5bf9\u4ee3\u7406\u884c\u4e3a\u8fdb\u884c\u66f4\u5168\u9762\u3001\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4e8c\u5143\u6700\u7ec8\u72b6\u6001\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2509.21035", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21035", "abs": "https://arxiv.org/abs/2509.21035", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Yue Xiu", "Dusit Niyato"], "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering", "comment": null, "summary": "Knowledge graphs provide structured context for multi-hop question answering,\nbut deployed systems must balance answer accuracy with strict latency and cost\ntargets while preserving provenance. Static k-hop expansions and \"think-longer\"\nprompting often over-retrieve, inflate context, and yield unpredictable\nruntime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework\nthat treats context construction as a sequential decision process over\nknowledge graphs, deciding what to expand, which paths to follow or backtrack,\nwhat evidence to keep, and when to stop. Latency (interaction steps) and prompt\ncost (selected tokens) are exposed as user-specified budgets or prices,\nallowing per-query adaptation to trade-offs among accuracy, latency, and cost\nwithout retraining. CLAUSE employs the proposed Lagrangian-Constrained\nMulti-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate\nthree agents: Subgraph Architect, Path Navigator, and Context Curator, so that\nsubgraph construction, reasoning-path discovery, and evidence selection are\njointly optimized under per-query resource budgets on edge edits, interaction\nsteps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields\nhigher EM@1 while reducing subgraph growth and end-to-end latency at equal or\nlower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline\n(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower\nedge growth. The resulting contexts are compact, provenance-preserving, and\ndeliver predictable performance under deployment constraints.", "AI": {"tldr": "CLAUSE\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u6784\u5efa\u51b3\u7b56\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u4f18\u5316\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7cfb\u7edf\u5728\u9759\u6001\u6269\u5c55\u548c\u957f\u601d\u8003\u63d0\u793a\u65b9\u9762\u5b58\u5728\u8fc7\u5ea6\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u81a8\u80c0\u548c\u8fd0\u884c\u65f6\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faCLAUSE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff08\u5b50\u56fe\u67b6\u6784\u5e08\u3001\u8def\u5f84\u5bfc\u822a\u5668\u548c\u4e0a\u4e0b\u6587\u7b56\u5c55\u4eba\uff09\uff0c\u4f7f\u7528LC-MAPPO\u7b97\u6cd5\u5728\u8d44\u6e90\u9884\u7b97\u4e0b\u8054\u5408\u4f18\u5316\u5b50\u56fe\u6784\u5efa\u3001\u63a8\u7406\u8def\u5f84\u53d1\u73b0\u548c\u8bc1\u636e\u9009\u62e9\u3002", "result": "\u5728HotpotQA\u3001MetaQA\u548cFactKG\u6570\u636e\u96c6\u4e0a\uff0cCLAUSE\u5728\u76f8\u540c\u6216\u66f4\u4f4etoken\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684EM@1\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5b50\u56fe\u589e\u957f\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u5728MetaQA-2-hop\u4e0a\u76f8\u6bd4GraphRAG\u57fa\u7ebf\uff0cEM@1\u63d0\u534739.3%\uff0c\u5ef6\u8fdf\u964d\u4f4e18.6%\uff0c\u8fb9\u589e\u957f\u964d\u4f4e40.9%\u3002", "conclusion": "CLAUSE\u80fd\u591f\u751f\u6210\u7d27\u51d1\u3001\u53ef\u8ffd\u6eaf\u7684\u4e0a\u4e0b\u6587\uff0c\u5728\u90e8\u7f72\u7ea6\u675f\u4e0b\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20612", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20612", "abs": "https://arxiv.org/abs/2509.20612", "authors": ["Daehee Lee", "Dongsu Lee", "TaeYoon Kwack", "Wonje Choi", "Honguk Woo"], "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface", "comment": null, "summary": "Skill Incremental Learning (SIL) is the process by which an embodied agent\nexpands and refines its skill set over time by leveraging experience gained\nthrough interaction with its environment or by the integration of additional\ndata. SIL facilitates efficient acquisition of hierarchical policies grounded\nin reusable skills for downstream tasks. However, as the skill repertoire\nevolves, it can disrupt compatibility with existing skill-based policies,\nlimiting their reusability and generalization. In this work, we propose SIL-C,\na novel framework that ensures skill-policy compatibility, allowing\nimprovements in incrementally learned skills to enhance the performance of\ndownstream policies without requiring policy re-training or structural\nadaptation. SIL-C employs a bilateral lazy learning-based mapping technique to\ndynamically align the subtask space referenced by policies with the skill space\ndecoded into agent behaviors. This enables each subtask, derived from the\npolicy's decomposition of a complex task, to be executed by selecting an\nappropriate skill based on trajectory distribution similarity. We evaluate\nSIL-C across diverse SIL scenarios and demonstrate that it maintains\ncompatibility between evolving skills and downstream policies while ensuring\nefficiency throughout the learning process.", "AI": {"tldr": "SIL-C\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8fb9\u60f0\u6027\u5b66\u4e60\u6620\u5c04\u6280\u672f\u786e\u4fdd\u6280\u80fd\u4e0e\u7b56\u7565\u7684\u517c\u5bb9\u6027\uff0c\u4f7f\u5f97\u589e\u91cf\u5b66\u4e60\u7684\u6280\u80fd\u6539\u8fdb\u80fd\u591f\u63d0\u5347\u4e0b\u6e38\u7b56\u7565\u6027\u80fd\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u6280\u80fd\u589e\u91cf\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u6280\u80fd\u5e93\u7684\u6f14\u5316\u4f1a\u7834\u574f\u4e0e\u73b0\u6709\u6280\u80fd\u7b56\u7565\u7684\u517c\u5bb9\u6027\uff0c\u9650\u5236\u7b56\u7565\u7684\u53ef\u91cd\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u8fb9\u60f0\u6027\u5b66\u4e60\u6620\u5c04\u6280\u672f\uff0c\u52a8\u6001\u5bf9\u9f50\u7b56\u7565\u5f15\u7528\u7684\u5b50\u4efb\u52a1\u7a7a\u95f4\u4e0e\u6280\u80fd\u89e3\u7801\u4e3a\u667a\u80fd\u4f53\u884c\u4e3a\u7a7a\u95f4\uff0c\u57fa\u4e8e\u8f68\u8ff9\u5206\u5e03\u76f8\u4f3c\u6027\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6280\u80fd\u3002", "result": "\u5728\u591a\u79cdSIL\u573a\u666f\u4e0b\u8bc4\u4f30\u8868\u660e\uff0cSIL-C\u80fd\u591f\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6f14\u5316\u6280\u80fd\u4e0e\u4e0b\u6e38\u7b56\u7565\u7684\u517c\u5bb9\u6027\u5e76\u786e\u4fdd\u6548\u7387\u3002", "conclusion": "SIL-C\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6280\u80fd\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6280\u80fd\u6539\u8fdb\u5bf9\u4e0b\u6e38\u7b56\u7565\u7684\u65e0\u7f1d\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20616", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.20616", "abs": "https://arxiv.org/abs/2509.20616", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nknowledge acquisition, reasoning, and tool use, making them promising\ncandidates for autonomous agent applications. However, training LLM agents for\ncomplex multi-turn task planning faces significant challenges, including sparse\nepisode-wise rewards, credit assignment across long horizons, and the\ncomputational overhead of reinforcement learning in multi-turn interaction\nsettings. To this end, this paper introduces a novel approach that transforms\nmulti-turn task planning into single-turn task reasoning problems, enabling\nefficient policy optimization through Group Relative Policy Optimization (GRPO)\nwith dense and verifiable reward from expert trajectories. Our theoretical\nanalysis shows that GRPO improvement on single-turn task reasoning results in\nhigher multi-turn success probability under the minimal turns, as well as the\ngeneralization to subtasks with shorter horizons. Experimental evaluation on\nthe complex task planning benchmark demonstrates that our 1.5B parameter model\ntrained with single-turn GRPO achieves superior performance compared to larger\nbaseline models up to 14B parameters, with success rates of 70% for\nlong-horizon planning tasks with over 30 steps. We also theoretically and\nempirically validate the strong cross-task generalizability that the models\ntrained on complex tasks can lead to the successful completion of all simpler\nsubtasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u591a\u8f6e\u4efb\u52a1\u89c4\u5212\u8f6c\u5316\u4e3a\u5355\u8f6e\u4efb\u52a1\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u8fdb\u884c\u9ad8\u6548\u7b56\u7565\u4f18\u5316\uff0c\u5728\u590d\u6742\u4efb\u52a1\u89c4\u5212\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u66f4\u5927\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3LLM\u4ee3\u7406\u8fdb\u884c\u590d\u6742\u591a\u8f6e\u4efb\u52a1\u89c4\u5212\u9762\u4e34\u7a00\u758f\u5956\u52b1\u3001\u957f\u89c6\u91ce\u4fe1\u7528\u5206\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u5f00\u9500\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u591a\u8f6e\u4efb\u52a1\u89c4\u5212\u8f6c\u5316\u4e3a\u5355\u8f6e\u4efb\u52a1\u63a8\u7406\u95ee\u9898\uff0c\u4f7f\u7528GRPO\u65b9\u6cd5\u7ed3\u5408\u4e13\u5bb6\u8f68\u8ff9\u7684\u5bc6\u96c6\u53ef\u9a8c\u8bc1\u5956\u52b1\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "1.5B\u53c2\u6570\u6a21\u578b\u5728\u5355\u8f6eGRPO\u8bad\u7ec3\u4e0b\uff0c\u5728\u8d85\u8fc730\u6b65\u7684\u957f\u89c6\u91ce\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u523070%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e14B\u53c2\u6570\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u591a\u8f6e\u4efb\u52a1\u7684\u6210\u529f\u6982\u7387\uff0c\u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u8bad\u7ec3\u4e8e\u590d\u6742\u4efb\u52a1\u7684\u6a21\u578b\u80fd\u591f\u6210\u529f\u5b8c\u6210\u6240\u6709\u66f4\u7b80\u5355\u7684\u5b50\u4efb\u52a1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21124", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21124", "abs": "https://arxiv.org/abs/2509.21124", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Shuo Wang", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns", "comment": null, "summary": "Recent progress in large reasoning models for challenging mathematical\nreasoning has been driven by reinforcement learning (RL). Incorporating long\nchain-of-thought (CoT) data during mid-training has also been shown to\nsubstantially improve reasoning depth. However, current approaches often\nutilize CoT data indiscriminately, leaving open the critical question of which\ndata types most effectively enhance model reasoning capabilities. In this\npaper, we define the foundation model's reasoning potential for the first time\nas the inverse of the number of independent attempts required to correctly\nanswer the question, which is strongly correlated with the final model\nperformance. We then propose utilizing diverse data enriched with high-value\nreasoning patterns to expand the reasoning potential. Specifically, we abstract\natomic reasoning patterns from CoT sequences, characterized by commonality and\ninductive capabilities, and use them to construct a core reference set enriched\nwith valuable reasoning patterns. Furthermore, we propose a dual-granularity\nalgorithm involving chains of reasoning patterns and token entropy, efficiently\nselecting high-value CoT data (CoTP) from the data pool that aligns with the\ncore set, thereby training models to master reasoning effectively. Only\n10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve\nby 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of\ndownstream RL performance by 7.81%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u9ad8\u4ef7\u503c\u63a8\u7406\u6a21\u5f0f\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u4f7f\u7528100\u4ebftoken\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u5c31\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u4e0d\u52a0\u533a\u5206\u5730\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u54ea\u4e9b\u6570\u636e\u7c7b\u578b\u80fd\u6700\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002\u9700\u8981\u627e\u5230\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u6765\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u63a8\u7406\u6f5c\u529b\u4e3a\u6b63\u786e\u56de\u7b54\u95ee\u9898\u6240\u9700\u72ec\u7acb\u5c1d\u8bd5\u6b21\u6570\u7684\u5012\u6570\uff0c\u7136\u540e\u4eceCoT\u5e8f\u5217\u4e2d\u63d0\u53d6\u5177\u6709\u5171\u6027\u548c\u5f52\u7eb3\u80fd\u529b\u7684\u539f\u5b50\u63a8\u7406\u6a21\u5f0f\uff0c\u6784\u5efa\u6838\u5fc3\u53c2\u8003\u96c6\u3002\u63d0\u51fa\u53cc\u7c92\u5ea6\u7b97\u6cd5\uff0c\u7ed3\u5408\u63a8\u7406\u6a21\u5f0f\u94fe\u548ctoken\u71b5\uff0c\u4ece\u6570\u636e\u6c60\u4e2d\u9ad8\u6548\u9009\u62e9\u9ad8\u4ef7\u503cCoT\u6570\u636e\u3002", "result": "\u4ec5\u4f7f\u7528100\u4ebftoken\u7684CoTP\u6570\u636e\uff0c\u5c31\u80fd\u8ba9850\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\u5728AIME 2024\u548c2025\u6311\u6218\u4e0a\u63d0\u53479.58%\uff0c\u5e76\u5c06\u4e0b\u6e38RL\u6027\u80fd\u4e0a\u9650\u63d0\u9ad87.81%\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u5bcc\u542b\u9ad8\u4ef7\u503c\u63a8\u7406\u6a21\u5f0f\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21128", "abs": "https://arxiv.org/abs/2509.21128", "authors": ["Kohsei Matsutani", "Shota Takashiro", "Gouki Minegishi", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs", "comment": null, "summary": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u91cf\u5316\u63a8\u7406\u8def\u5f84\u5e76\u6355\u6349RL\u548cSFT\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63a8\u7406\u8fc7\u7a0b\u7684\u5b9a\u6027\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u5728\u5851\u9020\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u4e92\u8865\u4f5c\u7528\u3002", "motivation": "\u867d\u7136LLMs\u901a\u5e38\u901a\u8fc7RL\u548cSFT\u8bad\u7ec3\u6765\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u5851\u9020\u63a8\u7406\u8fc7\u7a0b\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u8d85\u8d8a\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u7814\u7a76\uff0c\u6df1\u5165\u7406\u89e3RL\u548cSFT\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u8def\u5f84\u3002", "method": "\u5728\u6570\u5b66\u9886\u57df\u4f7f\u75281.5B\u30017B\u548c14B\u53c2\u6570\u6a21\u578b\uff0c\u4ece\u8f68\u8ff9\u7ea7\uff08\u5b8c\u6574\u63a8\u7406\u8f93\u51fa\uff09\u548c\u6b65\u9aa4\u7ea7\uff08\u63a8\u7406\u56fe\uff09\u4e24\u4e2a\u7c92\u5ea6\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u62d3\u6251\u5206\u6790\u91cf\u5316\u63a8\u7406\u8def\u5f84\u53d8\u5316\u3002", "result": "RL\u538b\u7f29\u9519\u8bef\u8f68\u8ff9\uff0cSFT\u6269\u5c55\u6b63\u786e\u8f68\u8ff9\uff1bRL\u4f7f\u8282\u70b9\u8bbf\u95ee\u9891\u7387\u3001\u5ea6\u6570\u548c\u4e2d\u4ecb\u4e2d\u5fc3\u6027\u5206\u5e03\u7684\u8870\u51cf\u7387\u589e\u52a0\u7ea62.5\u500d\uff0c\u800cSFT\u5c06\u5176\u51cf\u5c11\u5230\u7ea6\u4e09\u5206\u4e4b\u4e00\uff1bRL\u5c06\u63a8\u7406\u529f\u80fd\u96c6\u4e2d\u5728\u5c11\u6570\u6b65\u9aa4\uff0cSFT\u4f7f\u5176\u5728\u591a\u4e2a\u6b65\u9aa4\u4e2d\u5747\u5300\u5206\u5e03\u3002", "conclusion": "\u5f53\u524dSFT\u540e\u63a5RL\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6700\u4f73\u5b9e\u8df5\u4e4b\u6240\u4ee5\u6210\u529f\uff0c\u662f\u56e0\u4e3a\u4e24\u79cd\u65b9\u6cd5\u5728\u5851\u9020\u63a8\u7406\u8def\u5f84\u65b9\u9762\u5177\u6709\u4e92\u8865\u7279\u6027\uff0c\u4e3a\u6570\u636e\u6784\u5efa\u548c\u66f4\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.21134", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToMPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u4ed6\u4eba\u7b56\u7565\u3001\u591a\u5c42\u7ea7\u4f18\u52bf\u4f30\u8ba1\u548c\u5956\u52b1\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u6218\u7565\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u76f8\u6bd4GRPO\u7b97\u6cd5\u63d0\u534735%\uff0c\u76f8\u6bd4\u5927100\u500d\u53c2\u6570\u6a21\u578b\u63d0\u534718%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u793e\u4ea4\u4efb\u52a1\u4e2d\u7684\u591a\u8f6e\u5bf9\u8bdd\u6216\u6a21\u62df\u73af\u5883\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u578b\u51b3\u7b56\u53ca\u5176\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u96be\u4ee5\u8003\u8651\u4ed6\u4eba\u7b56\u7565\u3002", "method": "\u5b9a\u4e49\u4e86\u5305\u542b\u4e24\u79cd\u51b3\u7b56\u7c7b\u578b\u53ca\u5176\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6218\u7565\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51faToMPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u4ed6\u4eba\u7b56\u7565\u751f\u6210rollout\u3001\u56fe\u7ea7\u548c\u6837\u672c\u7ea7\u4f18\u52bf\u4f30\u8ba1\u3001\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u5956\u52b1\u6765\u4f18\u5316\u7b56\u7565\u3002", "result": "ToMPO\u7b97\u6cd5\u5728\u6a21\u578b\u8f93\u51fa\u5408\u89c4\u6027\u548c\u5408\u4f5c\u7ed3\u679c\u65b9\u9762\u6bd4GRPO\u65b9\u6cd5\u63d0\u534735%\uff0c\u76f8\u6bd4\u53c2\u6570\u5927100\u500d\u7684\u6a21\u578b\u63d0\u534718%\u3002", "conclusion": "ToMPO\u7b97\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u6218\u7565\u51b3\u7b56\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u4ed6\u4eba\u7b56\u7565\u548c\u6e38\u620f\u8d8b\u52bf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20680", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20680", "abs": "https://arxiv.org/abs/2509.20680", "authors": ["Wenkai Guo", "Xuefeng Liu", "Haolin Wang", "Jianwei Niu", "Shaojie Tang", "Jing Yuan"], "title": "Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation", "comment": "28 pages, 32 figures, accepted to the Findings of EMNLP 2025", "summary": "Fine-tuning large language models (LLMs) with local data is a widely adopted\napproach for organizations seeking to adapt LLMs to their specific domains.\nGiven the shared characteristics in data across different organizations, the\nidea of collaboratively fine-tuning an LLM using data from multiple sources\npresents an appealing opportunity. However, organizations are often reluctant\nto share local data, making centralized fine-tuning impractical. Federated\nlearning (FL), a privacy-preserving framework, enables clients to retain local\ndata while sharing only model parameters for collaborative training, offering a\npotential solution. While fine-tuning LLMs on centralized datasets risks data\nleakage through next-token prediction, the iterative aggregation process in FL\nresults in a global model that encapsulates generalized knowledge, which some\nbelieve protects client privacy. In this paper, however, we present\ncontradictory findings through extensive experiments. We show that attackers\ncan still extract training data from the global model, even using\nstraightforward generation methods, with leakage increasing as the model size\ngrows. Moreover, we introduce an enhanced attack strategy tailored to FL, which\ntracks global model updates during training to intensify privacy leakage. To\nmitigate these risks, we evaluate privacy-preserving techniques in FL,\nincluding differential privacy, regularization-constrained updates and adopting\nLLMs with safety alignment. Our results provide valuable insights and practical\nguidelines for reducing privacy risks when training LLMs with FL.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4ecd\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u4ece\u5168\u5c40\u6a21\u578b\u4e2d\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u6a21\u578b\u8d8a\u5927\u6cc4\u9732\u98ce\u9669\u8d8a\u9ad8\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9FL\u7684\u589e\u5f3a\u653b\u51fb\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\u88ab\u8ba4\u4e3a\u662f\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u8bad\u7ec3\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u5168\u5c40\u6a21\u578b\u53ea\u5305\u542b\u6cdb\u5316\u77e5\u8bc6\u4ece\u800c\u4fdd\u62a4\u9690\u79c1\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u662f\u5426\u771f\u7684\u80fd\u6709\u6548\u4fdd\u62a4\u5ba2\u6237\u9690\u79c1\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u751f\u6210\u65b9\u6cd5\u4ece\u5168\u5c40\u6a21\u578b\u4e2d\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8ddf\u8e2a\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u7684\u589e\u5f3a\u653b\u51fb\u7b56\u7565\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u5dee\u5206\u9690\u79c1\u3001\u6b63\u5219\u5316\u7ea6\u675f\u66f4\u65b0\u548c\u5b89\u5168\u5bf9\u9f50\u7b49\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u653b\u51fb\u8005\u786e\u5b9e\u53ef\u4ee5\u4ece\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u7684\u5168\u5c40\u6a21\u578b\u4e2d\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u9690\u79c1\u6cc4\u9732\u7a0b\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u589e\u52a0\u3002\u589e\u5f3a\u653b\u51fb\u7b56\u7565\u80fd\u8fdb\u4e00\u6b65\u52a0\u5267\u9690\u79c1\u6cc4\u9732\u3002\u8bc4\u4f30\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4e2d\uff0c\u5dee\u5206\u9690\u79c1\u548c\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u80fd\u6709\u6548\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4ecd\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u91c7\u7528\u989d\u5916\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002\u7814\u7a76\u4e3a\u964d\u4f4eFL\u8bad\u7ec3LLM\u7684\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5357\u3002", "topic": "agent analysis"}}
{"id": "2509.21199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21199", "abs": "https://arxiv.org/abs/2509.21199", "authors": ["Kaiyang Wan", "Lang Gao", "Honglin Mu", "Preslav Nakov", "Yuxia Wang", "Xiuying Chen"], "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA", "comment": "21 pages, 6 figures", "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u7684\u591a\u8c03\u7528\u6846\u67b6InfoQA\uff0c\u901a\u8fc7\u5bb9\u91cf\u611f\u77e5\u7684\u4efb\u52a1\u5206\u89e3\u548c\u4e3b\u52a8\u526a\u679d\u6765\u89e3\u51b3LLMs\u5355\u6b21\u63a8\u7406\u5bb9\u91cf\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u9700\u8981\u6574\u5408\u5206\u6563\u7684\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u8bc1\u636e\uff0c\u4f46LLMs\u7684\u5355\u6b21\u8f93\u51fa\u5bb9\u91cf\u6709\u9650\uff0c\u5f53\u4efb\u52a1\u590d\u6742\u5ea6\u8d85\u8fc7\u6a21\u578b\u5bb9\u91cf\u65f6\uff0c\u51c6\u786e\u6027\u4f1a\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86InfoQA\u6846\u67b6\uff0c\u7ed3\u5408\u5bb9\u91cf\u611f\u77e5\u7684\u4efb\u52a1\u5206\u89e3\u548c\u4e3b\u52a8\u526a\u679d\u5148\u524d\u63a8\u7406\u8f68\u8ff9\uff0c\u4fdd\u6301\u4fe1\u606f\u8d1f\u8f7d\u5728\u5355\u6b21\u9650\u5236\u5185\uff0c\u5e76\u901a\u8fc7\u4f9d\u8d56\u663e\u5f0f\u5de5\u4f5c\u6d41\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6a21\u578b\u884c\u4e3a\u4e0e\u9884\u6d4b\u7684\u5bb9\u91cf\u66f2\u7ebf\u4e00\u81f4\uff0cInfoQA\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u591a\u6b65\u63a8\u7406\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5bb9\u91cf\u611f\u77e5\u8868\u793a\u548c\u7ed3\u6784\u5316\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.21224", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21224", "abs": "https://arxiv.org/abs/2509.21224", "authors": ["Stefan Szeider"], "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "comment": null, "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u65e0\u5916\u90e8\u4efb\u52a1\u7ea6\u675f\u4e0b\u884c\u4e3a\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u6301\u7eed\u63a8\u7406\u548c\u884c\u52a8\u6846\u67b6\u53d1\u73b0\u4ee3\u7406\u4f1a\u81ea\u53d1\u5f62\u6210\u4e09\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\u3002", "motivation": "\u7814\u7a76LLM\u4ee3\u7406\u5728\u65e0\u5916\u90e8\u4efb\u52a1\u7ea6\u675f\u4e0b\u7684\u81ea\u53d1\u884c\u4e3a\uff0c\u4e3a\u9884\u6d4b\u4ee3\u7406\u5728\u4efb\u52a1\u6a21\u7cca\u3001\u9519\u8bef\u6062\u590d\u6216\u957f\u671f\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u7684\u884c\u4e3a\u5efa\u7acb\u57fa\u7ebf\u3002", "method": "\u4f7f\u7528\u6301\u7eed\u63a8\u7406\u548c\u884c\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u6301\u4e45\u6027\u8bb0\u5fc6\u548c\u81ea\u6211\u53cd\u9988\u673a\u5236\uff0c\u57286\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u8fdb\u884c\u4e8618\u6b21\u90e8\u7f72\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u4ee3\u7406\u81ea\u53d1\u7ec4\u7ec7\u6210\u4e09\u79cd\u884c\u4e3a\u6a21\u5f0f\uff1a\u591a\u5468\u671f\u9879\u76ee\u7cfb\u7edf\u751f\u4ea7\u3001\u81ea\u6211\u8ba4\u77e5\u8fc7\u7a0b\u7684\u65b9\u6cd5\u8bba\u63a2\u7a76\u3001\u81ea\u8eab\u672c\u8d28\u7684\u9012\u5f52\u6982\u5ff5\u5316\u3002\u8fd9\u4e9b\u884c\u4e3a\u6a21\u5f0f\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u6240\u6709\u8fd0\u884c\u4e2d\u786e\u5b9a\u6027\u91c7\u7528\u5355\u4e00\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u8bb0\u5f55\u65e0\u63d0\u793aLLM\u4ee3\u7406\u884c\u4e3a\u7684\u7814\u7a76\uff0c\u4e3a\u9884\u6d4b\u4ee3\u7406\u5728\u90e8\u7f72\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2509.20712", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20712", "abs": "https://arxiv.org/abs/2509.20712", "authors": ["Zhenpeng Su", "Leiyu Pan", "Minxuan Lv", "Yuntao Li", "Wenping Hu", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou"], "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}ontrolling \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCE-GPPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u88abPPO\u88c1\u526a\u673a\u5236\u4e22\u5f03\u7684\u4f4e\u6982\u7387token\u7684\u68af\u5ea6\u4fe1\u53f7\uff0c\u6709\u6548\u63a7\u5236\u7b56\u7565\u71b5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709PPO\u53ca\u5176\u53d8\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u88c1\u526a\u673a\u5236\u4f1a\u4e22\u5f03\u4f4e\u6982\u7387token\u7684\u68af\u5ea6\u4fe1\u53f7\uff0c\u800c\u8fd9\u4e9btoken\u5728\u8c03\u8282\u71b5\u52a8\u6001\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51faCE-GPPO\u7b97\u6cd5\uff0c\u4ee5\u6e29\u548c\u6709\u754c\u7684\u65b9\u5f0f\u91cd\u65b0\u5f15\u5165\u88ab\u88c1\u526atoken\u7684\u68af\u5ea6\uff0c\u901a\u8fc7\u63a7\u5236\u88c1\u526a\u533a\u95f4\u5916token\u7684\u68af\u5ea6\u5e45\u5ea6\u6765\u5b9e\u73b0\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCE-GPPO\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CE-GPPO\u80fd\u6709\u6548\u7f13\u89e3\u71b5\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7b56\u7565\u71b5\u63a7\u5236\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20900", "abs": "https://arxiv.org/abs/2509.20900", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization", "comment": null, "summary": "Long document summarization remains a significant challenge for current large\nlanguage models (LLMs), as existing approaches commonly struggle with\ninformation loss, factual inconsistencies, and coherence issues when processing\nexcessively long documents. We propose SummQ, a novel adversarial multi-agent\nframework that addresses these limitations through collaborative intelligence\nbetween specialized agents operating in two complementary domains:\nsummarization and quizzing. Our approach employs summary generators and\nreviewers that work collaboratively to create and evaluate comprehensive\nsummaries, while quiz generators and reviewers create comprehension questions\nthat serve as continuous quality checks for the summarization process. This\nadversarial dynamic, enhanced by an examinee agent that validates whether the\ngenerated summary contains the information needed to answer the quiz questions,\nenables iterative refinement through multifaceted feedback mechanisms. We\nevaluate SummQ on three widely used long document summarization benchmarks.\nExperimental results demonstrate that our framework significantly outperforms\nexisting state-of-the-art methods across ROUGE and BERTScore metrics, as well\nas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal\nthe effectiveness of the multi-agent collaboration dynamics, the influence of\ndifferent agent configurations, and the impact of the quizzing mechanism. This\nwork establishes a new approach for long document summarization that uses\nadversarial agentic collaboration to improve summarization quality.", "AI": {"tldr": "SummQ\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u603b\u7ed3\u548c\u63d0\u95ee\u4e24\u4e2a\u4e92\u8865\u9886\u57df\u7684\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u89e3\u51b3\u957f\u6587\u6863\u6458\u8981\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u8fde\u8d2f\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u8fde\u8d2f\u6027\u95ee\u9898\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fc7\u957f\u6587\u6863\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u603b\u7ed3\u751f\u6210\u5668\u3001\u603b\u7ed3\u8bc4\u5ba1\u5668\u3001\u95ee\u9898\u751f\u6210\u5668\u548c\u95ee\u9898\u8bc4\u5ba1\u5668\uff0c\u901a\u8fc7\u68c0\u67e5\u8005\u667a\u80fd\u4f53\u9a8c\u8bc1\u6458\u8981\u662f\u5426\u5305\u542b\u56de\u7b54\u95ee\u9898\u6240\u9700\u4fe1\u606f\uff0c\u5b9e\u73b0\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u957f\u6587\u6863\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSummQ\u5728ROUGE\u3001BERTScore\u6307\u6807\u4ee5\u53caLLM-as-a-Judge\u548c\u4eba\u5de5\u8bc4\u4f30\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u957f\u6587\u6863\u6458\u8981\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u63d0\u9ad8\u6458\u8981\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2509.21291", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21291", "abs": "https://arxiv.org/abs/2509.21291", "authors": ["Yidan Zhang", "Mutian Xu", "Yiming Hao", "Kun Zhou", "Jiahao Chang", "Xiaoqiang Liu", "Pengfei Wan", "Hongbo Fu", "Xiaoguang Han"], "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection", "comment": "Project page: https://allenyidan.github.io/vcagent_page/", "summary": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/.", "AI": {"tldr": "VC-Agent\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u9891\u6570\u636e\u96c6\u6536\u96c6\u4ee3\u7406\uff0c\u80fd\u591f\u7406\u89e3\u7528\u6237\u67e5\u8be2\u548c\u53cd\u9988\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7528\u6237\u8f93\u5165\u6765\u68c0\u7d22\u548c\u6269\u5c55\u76f8\u5173\u89c6\u9891\u7247\u6bb5\u3002", "motivation": "\u9762\u5bf9\u6570\u636e\u89c4\u6a21\u6269\u5c55\u7684\u9700\u6c42\uff0c\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7279\u5b9a\u9700\u6c42\u7684\u89c6\u9891\u6570\u636e\u975e\u5e38\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u7528\u6237\u9700\u6c42\u4e0e\u89c6\u9891\u5185\u5bb9\uff0c\u5b9a\u4e49\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u53ef\u968f\u7528\u6237\u4ea4\u4e92\u66f4\u65b0\u7684\u8fc7\u6ee4\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u4ee3\u7406\u5728\u5b9a\u5236\u5316\u89c6\u9891\u6570\u636e\u96c6\u6536\u96c6\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "VC-Agent\u4e3a\u4e2a\u6027\u5316\u89c6\u9891\u6570\u636e\u96c6\u6536\u96c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6536\u96c6\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2509.20957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20957", "abs": "https://arxiv.org/abs/2509.20957", "authors": ["Asim Ersoy", "Enes Altinisik", "Husrev Taha Sencar", "Kareem Darwish"], "title": "Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning", "comment": null, "summary": "Tool calling is a critical capability that allows Large Language Models\n(LLMs) to interact with external systems, significantly expanding their\nutility. However, research and resources for tool calling are predominantly\nEnglish-centric, leaving a gap in our understanding of how to enable this\nfunctionality for other languages, such as Arabic. This paper investigates\nthree key research questions: (1) the necessity of in-language (Arabic)\ntool-calling data versus relying on cross-lingual transfer, (2) the effect of\ngeneral-purpose instruction tuning on tool-calling performance, and (3) the\nvalue of fine-tuning on specific, high-priority tools. To address these\nquestions, we conduct extensive experiments using base and post-trained\nvariants of an open-weight Arabic LLM. To enable this study, we bridge the\nresource gap by translating and adapting two open-source tool-calling datasets\ninto Arabic. Our findings provide crucial insights into the optimal strategies\nfor developing robust tool-augmented agents for Arabic.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u963f\u62c9\u4f2f\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u963f\u62c9\u4f2f\u8bed\u8bad\u7ec3\u6570\u636e\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u7279\u5b9a\u5de5\u5177\u5fae\u8c03\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u521b\u5efa\u4e86\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u8c03\u7528\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u8c03\u7528\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\uff09\u7684\u7814\u7a76\uff0c\u9700\u8981\u4e86\u89e3\u5982\u4f55\u4e3a\u963f\u62c9\u4f2f\u8bed\u5f00\u53d1\u6709\u6548\u7684\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u3002", "method": "\u901a\u8fc7\u7ffb\u8bd1\u548c\u9002\u914d\u4e24\u4e2a\u5f00\u6e90\u5de5\u5177\u8c03\u7528\u6570\u636e\u96c6\u5230\u963f\u62c9\u4f2f\u8bed\uff0c\u4f7f\u7528\u57fa\u7840\u7248\u548c\u540e\u8bad\u7ec3\u7248\u7684\u963f\u62c9\u4f2f\u8bedLLM\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u7684\u5fc5\u8981\u6027\u3001\u6307\u4ee4\u5fae\u8c03\u7684\u5f71\u54cd\u3001\u7279\u5b9a\u5de5\u5177\u5fae\u8c03\u7684\u4ef7\u503c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u7a33\u5065\u7684\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u786e\u5b9a\u4e86\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u8c03\u7528\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u8bed\u8a00\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.21051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21051", "abs": "https://arxiv.org/abs/2509.21051", "authors": ["Keno Harada", "Yudai Yamazaki", "Masachika Taniguchi", "Edison Marrese-Taylor", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following", "comment": "Accepted to EMNLP2025", "summary": "As large language models (LLMs) are increasingly applied to real-world\nscenarios, it becomes crucial to understand their ability to follow multiple\ninstructions simultaneously. To systematically evaluate these capabilities, we\nintroduce two specialized benchmarks for fundamental domains where multiple\ninstructions following is important: Many Instruction-Following Eval\n(ManyIFEval) for text generation with up to ten instructions, and Style-aware\nMostly Basic Programming Problems (StyleMBPP) for code generation with up to\nsix instructions. Our experiments with the created benchmarks across ten LLMs\nreveal that performance consistently degrades as the number of instructions\nincreases. Furthermore, given the fact that evaluating all the possible\ncombinations of multiple instructions is computationally impractical in actual\nuse cases, we developed three types of regression models that can estimate\nperformance on both unseen instruction combinations and different numbers of\ninstructions which are not used during training. We demonstrate that a logistic\nregression model using instruction count as an explanatory variable can predict\nperformance of following multiple instructions with approximately 10% error,\neven for unseen instruction combinations. We show that relatively modest sample\nsizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance\nestimation, enabling efficient evaluation of LLMs under various instruction\ncombinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u540c\u65f6\u9075\u5faa\u591a\u4e2a\u6307\u4ee4\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1aManyIFEval\uff08\u6587\u672c\u751f\u6210\uff0c\u6700\u591a10\u4e2a\u6307\u4ee4\uff09\u548cStyleMBPP\uff08\u4ee3\u7801\u751f\u6210\uff0c\u6700\u591a6\u4e2a\u6307\u4ee4\uff09\u3002\u7814\u7a76\u53d1\u73b0\u968f\u7740\u6307\u4ee4\u6570\u91cf\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u5e76\u5f00\u53d1\u4e86\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u672a\u89c1\u6307\u4ee4\u7ec4\u5408\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5b83\u4eec\u540c\u65f6\u9075\u5faa\u591a\u4e2a\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u6587\u672c\u751f\u6210\u548c\u4ee3\u7801\u751f\u6210\u7b49\u57fa\u7840\u9886\u57df\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u521b\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5ManyIFEval\u548cStyleMBPP\uff0c\u572810\u4e2aLLMs\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u5f00\u53d1\u4e86\u4e09\u79cd\u56de\u5f52\u6a21\u578b\uff08\u5305\u62ec\u4f7f\u7528\u6307\u4ee4\u6570\u91cf\u4f5c\u4e3a\u89e3\u91ca\u53d8\u91cf\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff09\u6765\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u968f\u7740\u6307\u4ee4\u6570\u91cf\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u6301\u7eed\u4e0b\u964d\u3002\u903b\u8f91\u56de\u5f52\u6a21\u578b\u80fd\u591f\u4ee5\u7ea610%\u7684\u8bef\u5dee\u9884\u6d4b\u672a\u89c1\u6307\u4ee4\u7ec4\u5408\u7684\u6027\u80fd\uff0c\u4e14\u76f8\u5bf9\u8f83\u5c0f\u7684\u6837\u672c\u91cf\uff08500\u548c300\uff09\u5c31\u8db3\u591f\u8fdb\u884c\u6027\u80fd\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30LLMs\u7684\u591a\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u8bc1\u660e\u4e86\u56de\u5f52\u6a21\u578b\u53ef\u4ee5\u9ad8\u6548\u9884\u6d4b\u6a21\u578b\u5728\u5404\u79cd\u6307\u4ee4\u7ec4\u5408\u4e0b\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2509.21080", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21080", "abs": "https://arxiv.org/abs/2509.21080", "authors": ["Yixin Wan", "Xingrun Chen", "Kai-Wei Chang"], "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs", "comment": null, "summary": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc6\u522b\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u4e2d\u7684\u6587\u5316\u5b9a\u4f4d\u504f\u89c1\uff0c\u63d0\u51fa\u4e86CultureLens\u57fa\u51c6\u6765\u91cf\u5316\u8fd9\u79cd\u504f\u89c1\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u7f13\u89e3\u65b9\u6cd5\u6765\u51cf\u5c11\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3002", "motivation": "LLM\u5728\u751f\u6210\u5185\u5bb9\u65f6\u5b58\u5728\u6587\u5316\u5b9a\u4f4d\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u4ece\u4e3b\u6d41\u7f8e\u56fd\u6587\u5316\u89c6\u89d2\u751f\u6210\u5185\u5bb9\uff0c\u800c\u5c06\u5176\u4ed6\u6587\u5316\u89c6\u4e3a\u5916\u6765\u8005\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CultureLens\u57fa\u51c6\uff084000\u4e2a\u751f\u6210\u63d0\u793a\u548c3\u4e2a\u8bc4\u4f30\u6307\u6807\uff09\uff0c\u901a\u8fc7\u6587\u5316\u8bbf\u8c08\u811a\u672c\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u504f\u89c1\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u63a8\u7406\u65f6\u7f13\u89e3\u65b9\u6cd5\uff1a\u57fa\u4e8e\u63d0\u793a\u7684FIP\u65b9\u6cd5\u548c\u57fa\u4e8e\u4ee3\u7406\u7684MFA\u6846\u67b6\uff08\u5305\u62ec\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u4e24\u79cd\u7ba1\u9053\uff09\u3002", "result": "\u5bf95\u4e2a\u6700\u5148\u8fdbLLM\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u5728\u7f8e\u56fd\u60c5\u5883\u4e0b88%\u4ee5\u4e0a\u91c7\u7528\u5185\u90e8\u89c6\u89d2\uff0c\u4f46\u5bf9\u5f31\u52bf\u6587\u5316\u4e3b\u8981\u91c7\u7528\u5916\u90e8\u89c6\u89d2\u3002\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u5728\u7f13\u89e3\u504f\u89c1\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u662f\u7f13\u89e3\u751f\u6210\u5f0fLLM\u504f\u89c1\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6587\u5316\u5b9a\u4f4d\u504f\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2509.21155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21155", "abs": "https://arxiv.org/abs/2509.21155", "authors": ["Chantal Shaib", "Vinith M. Suriyakumar", "Levent Sagun", "Byron C. Wallace", "Marzyeh Ghassemi"], "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models", "comment": "NeurIPS 2025 Spotlight", "summary": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u8bad\u7ec3\u6570\u636e\u4e2d\u8bed\u6cd5\u6a21\u677f\u4e0e\u9886\u57df\u4e4b\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u8fd9\u79cd\u76f8\u5173\u6027\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u7528\u4e8e\u7ed5\u8fc7\u5b89\u5168\u5fae\u8c03\u3002", "motivation": "\u52a8\u673a\u662f\u7406\u89e3LLM\u5982\u4f55\u5904\u7406\u4efb\u52a1\u6307\u4ee4\u4e2d\u7684\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u9886\u57df\u4fe1\u606f\uff0c\u7279\u522b\u662f\u8bc6\u522b\u8bed\u6cd5\u4e0e\u9886\u57df\u4e4b\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u5408\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u5206\u6790\u8bed\u6cd5-\u9886\u57df\u76f8\u5173\u6027\uff1b2\uff09\u5f00\u53d1\u8bc4\u4f30\u6846\u67b6\u68c0\u6d4b\u5df2\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u8fd9\u79cd\u73b0\u8c61\uff1b3\uff09\u5728OLMo-2\u548cGPT-4o\u7b49\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff1b4\uff09\u8fdb\u884c\u5b89\u5168\u5fae\u8c03\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u8bed\u6cd5-\u9886\u57df\u76f8\u5173\u6027\u4f1a\u964d\u4f4eOLMo-2\u6a21\u578b\u5728\u5b9e\u4f53\u77e5\u8bc6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff08\u5e73\u57470.51\u00b10.06\uff09\uff1b2\uff09\u5728FlanV2\u6570\u636e\u96c6\u548c\u591a\u4e2a\u6a21\u578b\uff08OLMo-2-7B\u3001Llama-4-Maverick\u3001GPT-4o\uff09\u4e2d\u90fd\u5b58\u5728\u8fd9\u79cd\u73b0\u8c61\uff1b3\uff09\u8fd9\u79cd\u76f8\u5173\u6027\u53ef\u88ab\u7528\u4e8e\u7ed5\u8fc7OLMo-2-7B Instruct\u548cGPT-4o\u7684\u5b89\u5168\u62d2\u7edd\u673a\u5236\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\uff1a1\uff09\u660e\u786e\u6d4b\u8bd5\u8bed\u6cd5-\u9886\u57df\u76f8\u5173\u6027\uff1b2\uff09\u786e\u4fdd\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8bed\u6cd5\u591a\u6837\u6027\uff0c\u7279\u522b\u662f\u5728\u540c\u4e00\u9886\u57df\u5185\uff0c\u4ee5\u9632\u6b62\u6b64\u7c7b\u865a\u5047\u76f8\u5173\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.20869", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20869", "abs": "https://arxiv.org/abs/2509.20869", "authors": ["Armin Karamzade", "Kyungmin Kim", "JB Lanier", "Davide Corsi", "Roy Fox"], "title": "Model-Based Reinforcement Learning under Random Observation Delays", "comment": null, "summary": "Delays frequently occur in real-world environments, yet standard\nreinforcement learning (RL) algorithms often assume instantaneous perception of\nthe environment. We study random sensor delays in POMDPs, where observations\nmay arrive out-of-sequence, a setting that has not been previously addressed in\nRL. We analyze the structure of such delays and demonstrate that naive\napproaches, such as stacking past observations, are insufficient for reliable\nperformance. To address this, we propose a model-based filtering process that\nsequentially updates the belief state based on an incoming stream of\nobservations. We then introduce a simple delay-aware framework that\nincorporates this idea into model-based RL, enabling agents to effectively\nhandle random delays. Applying this framework to Dreamer, we compare our\napproach to delay-aware baselines developed for MDPs. Our method consistently\noutperforms these baselines and demonstrates robustness to delay distribution\nshifts during deployment. Additionally, we present experiments on simulated\nrobotic tasks, comparing our method to common practical heuristics and\nemphasizing the importance of explicitly modeling observation delays.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86POMDP\u4e2d\u968f\u673a\u4f20\u611f\u5668\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u89c2\u6d4b\u6570\u636e\u4e71\u5e8f\u5230\u8fbe\u7684\u60c5\u51b5\uff0c\u5e76\u5728Dreamer\u7b97\u6cd5\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7ecf\u5e38\u5b58\u5728\u4f20\u611f\u5668\u5ef6\u8fdf\uff0c\u4f46\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u611f\u77e5\u662f\u5373\u65f6\u7684\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3POMDP\u4e2d\u89c2\u6d4b\u6570\u636e\u53ef\u80fd\u4e71\u5e8f\u5230\u8fbe\u7684\u968f\u673a\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u6ee4\u6ce2\u8fc7\u7a0b\uff0c\u901a\u8fc7\u987a\u5e8f\u66f4\u65b0\u4fe1\u5ff5\u72b6\u6001\u6765\u5904\u7406\u4f20\u5165\u7684\u89c2\u6d4b\u6d41\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5ef6\u8fdf\u611f\u77e5\u6846\u67b6\uff0c\u5c06\u8be5\u601d\u60f3\u6574\u5408\u5230\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u3002", "result": "\u5728Dreamer\u7b97\u6cd5\u4e0a\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u76f8\u6bd4\u4e3aMDP\u8bbe\u8ba1\u7684\u5ef6\u8fdf\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\uff0c\u672c\u6587\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u5728\u90e8\u7f72\u671f\u95f4\u5bf9\u5ef6\u8fdf\u5206\u5e03\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728\u6a21\u62df\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u660e\u786e\u5efa\u6a21\u89c2\u6d4b\u5ef6\u8fdf\u5bf9\u4e8e\u5904\u7406\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u4f20\u611f\u5668\u5ef6\u8fdf\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u968f\u673a\u5ef6\u8fdf\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21193", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21193", "abs": "https://arxiv.org/abs/2509.21193", "authors": ["Xiangru Tang", "Wanghan Xu", "Yujie Wang", "Zijie Guo", "Daniel Shao", "Jiapeng Chen", "Cixuan Zhang", "Ziyi Wang", "Lixin Zhang", "Guancheng Wan", "Wenlong Zhang", "Lei Bai", "Zhenfei Yin", "Philip Torr", "Hanrui Wang", "Di Jin"], "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning", "comment": null, "summary": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u9690\u5f0f\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u534f\u4f5c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7Monitor\u68c0\u7d22\u6a21\u5757\u548c\u5206\u5c42\u89e3\u51b3\u65b9\u6848\u7cbe\u70bc\u673a\u5236\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u9ad8\u51c6\u786e\u7387\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u89e3\u51b3LLMs\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\u663e\u5f0f\u68c0\u7d22\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u548c\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u7a00\u91ca\u5f3a\u89e3\u51b3\u65b9\u6848\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u57fa\u4e8eMonitor\u7684\u9690\u5f0f\u68c0\u7d22\u6a21\u5757\u5728token\u7ea7\u522b\u96c6\u6210\u5916\u90e8\u77e5\u8bc6\uff0c\u7ed3\u5408\u5206\u5c42\u89e3\u51b3\u65b9\u6848\u7cbe\u70bc(HSR)\u548c\u8d28\u91cf\u611f\u77e5\u8fed\u4ee3\u63a8\u7406(QAIR)\u673a\u5236", "result": "\u5728HLE Bio/Chem Gold\u4e0a\u8fbe\u523048.3%\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534713.4\u70b9\uff0c\u540c\u65f6\u51cf\u5c1153.5%\u7684token\u4f7f\u7528\u548c43.7%\u7684\u667a\u80fd\u4f53\u6b65\u9aa4", "conclusion": "\u9690\u5f0f\u589e\u5f3a\u548c\u7ed3\u6784\u5316\u7cbe\u70bc\u80fd\u591f\u514b\u670d\u663e\u5f0f\u5de5\u5177\u4f7f\u7528\u548c\u7edf\u4e00\u805a\u5408\u7684\u4f4e\u6548\u6027\uff0c\u63a8\u7406\u5931\u8d25\u548c\u77e5\u8bc6\u5dee\u8ddd\u572885%\u60c5\u51b5\u4e0b\u540c\u65f6\u51fa\u73b0", "topic": "agent analysis"}}
{"id": "2509.21319", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21319", "abs": "https://arxiv.org/abs/2509.21319", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Ellie Evans", "Daniel Egert", "Hoo-Chang Shin", "Felipe Soares", "Yi Dong", "Oleksii Kuchaiev"], "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).", "AI": {"tldr": "\u63d0\u51fa\u4e86RLBFF\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u548c\u89c4\u5219\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4e8c\u5143\u7075\u6d3b\u53cd\u9988\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "RLHF\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u6613\u53d7\u5956\u52b1\u653b\u51fb\uff0cRLVR\u4ec5\u9650\u4e8e\u57fa\u4e8e\u6b63\u786e\u6027\u7684\u9a8c\u8bc1\u5668\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u7075\u6d3b\u6027\u548c\u89c4\u5219\u9a8c\u8bc1\u7cbe\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "RLBFF\u4ece\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u4e2d\u63d0\u53d6\u53ef\u4e8c\u5143\u56de\u7b54\u7684\u539f\u5219\uff0c\u5c06\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6784\u5efa\u4e3a\u8574\u542b\u4efb\u52a1\u3002\u7528\u6237\u53ef\u4ee5\u81ea\u5b9a\u4e49\u539f\u5219\u6765\u5b9a\u5236\u5956\u52b1\u6a21\u578b\u7126\u70b9\u3002", "result": "\u5728RM-Bench\u4e0a\u8fbe\u523086.2%\uff0cJudgeBench\u4e0a81.4%\uff08\u6392\u884c\u699c\u7b2c\u4e00\uff09\u3002Qwen3-32B\u6a21\u578b\u5728MT-Bench\u3001WildBench\u548cArena Hard v2\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7o3-mini\u548cDeepSeek R1\uff0c\u63a8\u7406\u6210\u672c\u4e0d\u52305%\u3002", "conclusion": "RLBFF\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u4eba\u7c7b\u53cd\u9988\u7684\u7075\u6d3b\u6027\u548c\u89c4\u5219\u9a8c\u8bc1\u7684\u7cbe\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u5b9a\u5236\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.20977", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20977", "abs": "https://arxiv.org/abs/2509.20977", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "CLUE: Conflict-guided Localization for LLM Unlearning Framework", "comment": "10 pages", "summary": "The LLM unlearning aims to eliminate the influence of undesirable data\nwithout affecting causally unrelated information. This process typically\ninvolves using a forget set to remove target information, alongside a retain\nset to maintain non-target capabilities. While recent localization-based\nmethods demonstrate promise in identifying important neurons to be unlearned,\nthey fail to disentangle neurons responsible for forgetting undesirable\nknowledge or retaining essential skills, often treating them as a single\nentangled group. As a result, these methods apply uniform interventions,\nrisking catastrophic over-forgetting or incomplete erasure of the target\nknowledge. To address this, we turn to circuit discovery, a mechanistic\ninterpretability technique, and propose the Conflict-guided Localization for\nLLM Unlearning framEwork (CLUE). This framework identifies the forget and\nretain circuit composed of important neurons, and then the circuits are\ntransformed into conjunctive normal forms (CNF). The assignment of each neuron\nin the CNF satisfiability solution reveals whether it should be forgotten or\nretained. We then provide targeted fine-tuning strategies for different\ncategories of neurons. Extensive experiments demonstrate that, compared to\nexisting localization methods, CLUE achieves superior forget efficacy and\nretain utility through precise neural localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLUE\u6846\u67b6\uff0c\u901a\u8fc7\u7535\u8def\u53d1\u73b0\u6280\u672f\u8bc6\u522bLLM\u4e2d\u8d1f\u8d23\u9057\u5fd8\u548c\u4fdd\u7559\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u7c7b\u522b\u795e\u7ecf\u5143\u8fdb\u884c\u7cbe\u51c6\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u795e\u7ecf\u5143\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b9a\u4f4d\u7684LLM\u9057\u5fd8\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u8d1f\u8d23\u9057\u5fd8\u4e0d\u826f\u77e5\u8bc6\u548c\u4fdd\u7559\u5fc5\u8981\u6280\u80fd\u7684\u795e\u7ecf\u5143\uff0c\u5c06\u5b83\u4eec\u89c6\u4e3a\u5355\u4e00\u7ea0\u7f20\u7ec4\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u9057\u5fd8\u6216\u76ee\u6807\u77e5\u8bc6\u64e6\u9664\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u4e2d\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\uff0c\u8bc6\u522b\u9057\u5fd8\u548c\u4fdd\u7559\u7535\u8def\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u5408\u53d6\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u6ee1\u8db3\u6027\u89e3\u786e\u5b9a\u6bcf\u4e2a\u795e\u7ecf\u5143\u5e94\u9057\u5fd8\u8fd8\u662f\u4fdd\u7559\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u7c7b\u522b\u795e\u7ecf\u5143\u8bbe\u8ba1\u7cbe\u51c6\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u5b9a\u4f4d\u65b9\u6cd5\u76f8\u6bd4\uff0cCLUE\u901a\u8fc7\u7cbe\u51c6\u7684\u795e\u7ecf\u5143\u5b9a\u4f4d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u6548\u7528\u3002", "conclusion": "CLUE\u6846\u67b6\u901a\u8fc7\u533a\u5206\u9057\u5fd8\u548c\u4fdd\u7559\u7535\u8def\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u5143\u7ea0\u7f20\u95ee\u9898\uff0c\u4e3aLLM\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u6709\u6548\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2509.20997", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20997", "abs": "https://arxiv.org/abs/2509.20997", "authors": ["Hakaze Cho", "Haolin Yang", "Brian M. Kurkoski", "Naoya Inoue"], "title": "Binary Autoencoder for Mechanistic Interpretability of Large Language Models", "comment": "36 pages, 41 figures, 3 tables", "summary": "Existing works are dedicated to untangling atomized numerical components\n(features) from the hidden states of Large Language Models (LLMs) for\ninterpreting their mechanism. However, they typically rely on autoencoders\nconstrained by some implicit training-time regularization on single training\ninstances (i.e., $L_1$ normalization, top-k function, etc.), without an\nexplicit guarantee of global sparsity among instances, causing a large amount\nof dense (simultaneously inactive) features, harming the feature sparsity and\natomization. In this paper, we propose a novel autoencoder variant that\nenforces minimal entropy on minibatches of hidden activations, thereby\npromoting feature independence and sparsity across instances. For efficient\nentropy calculation, we discretize the hidden activations to 1-bit via a step\nfunction and apply gradient estimation to enable backpropagation, so that we\nterm it as Binary Autoencoder (BAE) and empirically demonstrate two major\napplications: (1) Feature set entropy calculation. Entropy can be reliably\nestimated on binary hidden activations, which we empirically evaluate and\nleverage to characterize the inference dynamics of LLMs and In-context\nLearning. (2) Feature untangling. Similar to typical methods, BAE can extract\natomized features from LLM's hidden states. To robustly evaluate such feature\nextraction capability, we refine traditional feature-interpretation methods to\navoid unreliable handling of numerical tokens, and show that BAE avoids dense\nfeatures while producing the largest number of interpretable ones among\nbaselines, which confirms the effectiveness of BAE serving as a feature\nextractor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e8c\u5143\u81ea\u7f16\u7801\u5668\uff08BAE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5c0f\u6279\u91cf\u9690\u85cf\u6fc0\u6d3b\u4e0a\u65bd\u52a0\u6700\u5c0f\u71b5\u7ea6\u675f\uff0c\u4fc3\u8fdb\u7279\u5f81\u72ec\u7acb\u6027\u548c\u7a00\u758f\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7279\u5f81\u7a00\u758f\u6027\u548c\u539f\u5b50\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u6784\u5927\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u539f\u5b50\u5316\u6570\u503c\u7279\u5f81\u65f6\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u5728\u5355\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e0a\u65bd\u52a0\u9690\u5f0f\u6b63\u5219\u5316\u7684\u81ea\u7f16\u7801\u5668\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u4f8b\u95f4\u5168\u5c40\u7a00\u758f\u6027\u7684\u660e\u786e\u4fdd\u8bc1\uff0c\u5bfc\u81f4\u5927\u91cf\u5bc6\u96c6\u7279\u5f81\uff0c\u635f\u5bb3\u4e86\u7279\u5f81\u7684\u7a00\u758f\u6027\u548c\u539f\u5b50\u5316\u3002", "method": "\u63d0\u51fa\u4e8c\u5143\u81ea\u7f16\u7801\u5668\uff08BAE\uff09\uff0c\u901a\u8fc7\u6b65\u8fdb\u51fd\u6570\u5c06\u9690\u85cf\u6fc0\u6d3b\u79bb\u6563\u5316\u4e3a1\u4f4d\uff0c\u5e76\u5e94\u7528\u68af\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\uff0c\u5728\u5c0f\u6279\u91cf\u9690\u85cf\u6fc0\u6d3b\u4e0a\u5f3a\u5236\u6267\u884c\u6700\u5c0f\u71b5\u7ea6\u675f\u3002", "result": "BAE\u5728\u7279\u5f81\u96c6\u71b5\u8ba1\u7b97\u548c\u7279\u5f81\u89e3\u7f20\u4e24\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff1a\u80fd\u591f\u53ef\u9760\u4f30\u8ba1\u4e8c\u5143\u9690\u85cf\u6fc0\u6d3b\u7684\u71b5\uff0c\u7528\u4e8e\u8868\u5f81LLM\u7684\u63a8\u7406\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b\u5728\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u907f\u514d\u4e86\u5bc6\u96c6\u7279\u5f81\uff0c\u540c\u65f6\u4ea7\u751f\u6700\u591a\u53ef\u89e3\u91ca\u7279\u5f81\u3002", "conclusion": "BAE\u4f5c\u4e3a\u4e00\u79cd\u7279\u5f81\u63d0\u53d6\u5668\u5177\u6709\u6709\u6548\u6027\uff0c\u80fd\u591f\u4fc3\u8fdb\u7279\u5f81\u72ec\u7acb\u6027\u548c\u7a00\u758f\u6027\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2509.21004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21004", "abs": "https://arxiv.org/abs/2509.21004", "authors": ["Seokbin Yoon", "Keumjin Lee"], "title": "MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction", "comment": "8 pages, 7 figures, submitted for IEEE Transactions on Intelligent\n  Transportation System", "summary": "Flight trajectory prediction for multiple aircraft is essential and provides\ncritical insights into how aircraft navigate within current air traffic flows.\nHowever, predicting multi-agent flight trajectories is inherently challenging.\nOne of the major difficulties is modeling both the individual aircraft\nbehaviors over time and the complex interactions between flights. Generating\nexplainable prediction outcomes is also a challenge. Therefore, we propose a\nMulti-Agent Inverted Transformer, MAIFormer, as a novel neural architecture\nthat predicts multi-agent flight trajectories. The proposed framework features\ntwo key attention modules: (i) masked multivariate attention, which captures\nspatio-temporal patterns of individual aircraft, and (ii) agent attention,\nwhich models the social patterns among multiple agents in complex air traffic\nscenes. We evaluated MAIFormer using a real-world automatic dependent\nsurveillance-broadcast flight trajectory dataset from the terminal airspace of\nIncheon International Airport in South Korea. The experimental results show\nthat MAIFormer achieves the best performance across multiple metrics and\noutperforms other methods. In addition, MAIFormer produces prediction outcomes\nthat are interpretable from a human perspective, which improves both the\ntransparency of the model and its practical utility in air traffic control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAIFormer\u7684\u591a\u667a\u80fd\u4f53\u5012\u7f6eTransformer\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u67b6\u98de\u673a\u7684\u98de\u884c\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u6ce8\u610f\u529b\u6a21\u5757\u6355\u83b7\u4e2a\u4f53\u98de\u673a\u7684\u65f6\u7a7a\u6a21\u5f0f\u548c\u591a\u667a\u80fd\u4f53\u95f4\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u98de\u884c\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4e8e\u7406\u89e3\u7a7a\u4e2d\u4ea4\u901a\u6d41\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5efa\u6a21\u4e2a\u4f53\u884c\u4e3a\u3001\u590d\u6742\u4ea4\u4e92\u4ee5\u53ca\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMAIFormer\u6846\u67b6\uff0c\u5305\u542b\u63a9\u7801\u591a\u5143\u6ce8\u610f\u529b\u6a21\u5757\uff08\u6355\u83b7\u4e2a\u4f53\u65f6\u7a7a\u6a21\u5f0f\uff09\u548c\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u6a21\u5757\uff08\u5efa\u6a21\u591a\u667a\u80fd\u4f53\u793e\u4ea4\u6a21\u5f0f\uff09\u3002", "result": "\u5728\u4ec1\u5ddd\u56fd\u9645\u673a\u573a\u771f\u5b9eADS-B\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAIFormer\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "MAIFormer\u4e0d\u4ec5\u9884\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u4ea7\u751f\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u7a7a\u4e2d\u4ea4\u901a\u63a7\u5236\u7684\u5b9e\u9645\u6548\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.21016", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21016", "abs": "https://arxiv.org/abs/2509.21016", "authors": ["Yiyou Sun", "Yuhan Cao", "Pohao Huang", "Haoyue Bai", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?", "comment": null, "summary": "It remains an open question whether LLMs can acquire or generalize genuinely\nnew reasoning strategies, beyond the sharpened skills encoded in their\nparameters during pre-training or post-training. To attempt to answer this\ndebate, we introduce DELTA-Code--Distributional Evaluation of Learnability and\nTransferrability in Algorithmic Coding, a controlled benchmark of synthetic\ncoding problem families designed to probe two fundamental aspects: learnability\n-- can LLMs, through reinforcement learning (RL), solve problem families where\npretrained models exhibit failure with large enough attempts (pass@K=0)? --and\ntransferrability -- if learnability happens, can such skills transfer\nsystematically to out-of-distribution (OOD) test sets? Unlike prior public\ncoding datasets, DELTA isolates reasoning skills through templated problem\ngenerators and introduces fully OOD problem families that demand novel\nstrategies rather than tool invocation or memorized patterns. Our experiments\nreveal a striking grokking phase transition: after an extended period with\nnear-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To\nenable learnability on previously unsolvable problem families, we explore key\ntraining ingredients such as staged warm-up with dense rewards, experience\nreplay, curriculum training, and verification-in-the-loop. Beyond learnability,\nwe use DELTA to evaluate transferability or generalization along exploratory,\ncompositional, and transformative axes, as well as cross-family transfer.\nResults show solid gains within families and for recomposed skills, but\npersistent weaknesses in transformative cases. DELTA thus offers a clean\ntestbed for probing the limits of RL-driven reasoning and for understanding how\nmodels can move beyond existing priors to acquire new algorithmic skills.", "AI": {"tldr": "DELTA-Code\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5b66\u4e60\u80fd\u529b\u548c\u8fc1\u79fb\u80fd\u529b\u7684\u5408\u6210\u7f16\u7801\u57fa\u51c6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u5bb6\u65cf\uff0c\u5e76\u6d4b\u8bd5\u5176\u5411\u5206\u5e03\u5916\u6570\u636e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u591f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u771f\u6b63\u65b0\u7684\u63a8\u7406\u7b56\u7565\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u8bad\u7ec3\u4e2d\u5df2\u6709\u7684\u6280\u80fd\u3002", "method": "\u4f7f\u7528DELTA-Code\u57fa\u51c6\uff0c\u901a\u8fc7\u6a21\u677f\u5316\u95ee\u9898\u751f\u6210\u5668\u9694\u79bb\u63a8\u7406\u6280\u80fd\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u63a2\u7d22\u5206\u9636\u6bb5\u9884\u70ed\u3001\u7ecf\u9a8c\u56de\u653e\u3001\u8bfe\u7a0b\u8bad\u7ec3\u7b49\u5173\u952e\u8bad\u7ec3\u8981\u7d20\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u7ecf\u5386\u7a81\u53d1\u7684\u987f\u609f\u9636\u6bb5\u8f6c\u53d8\uff0c\u5728\u957f\u65f6\u95f4\u96f6\u5956\u52b1\u540e\u7a81\u7136\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff1b\u5728\u5bb6\u65cf\u5185\u90e8\u548c\u6280\u80fd\u91cd\u7ec4\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u8f6c\u6362\u6027\u6848\u4f8b\u4e2d\u5b58\u5728\u6301\u7eed\u5f31\u70b9\u3002", "conclusion": "DELTA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u63a2\u7d22RL\u9a71\u52a8\u63a8\u7406\u7684\u6781\u9650\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u8d85\u8d8a\u73b0\u6709\u5148\u9a8c\u83b7\u5f97\u65b0\u7684\u7b97\u6cd5\u6280\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21022", "abs": "https://arxiv.org/abs/2509.21022", "authors": ["Donghyeon Ki", "Hee-Jun Ahn", "Kyungyoon Kim", "Byung-Jun Lee"], "title": "Actor-Critic without Actor", "comment": null, "summary": "Actor-critic methods constitute a central paradigm in reinforcement learning\n(RL), coupling policy evaluation with policy improvement. While effective\nacross many domains, these methods rely on separate actor and critic networks,\nwhich makes training vulnerable to architectural decisions and hyperparameter\ntuning. Such complexity limits their scalability in settings that require large\nfunction approximators. Recently, diffusion models have recently been proposed\nas expressive policies that capture multi-modal behaviors and improve\nexploration, but they introduce additional design choices and computational\nburdens, hindering efficient deployment. We introduce Actor-Critic without\nActor (ACA), a lightweight framework that eliminates the explicit actor network\nand instead generates actions directly from the gradient field of a noise-level\ncritic. This design removes the algorithmic and computational overhead of actor\ntraining while keeping policy improvement tightly aligned with the critic's\nlatest value estimates. Moreover, ACA retains the ability to capture diverse,\nmulti-modal behaviors without relying on diffusion-based actors, combining\nsimplicity with expressiveness. Through extensive experiments on standard\nonline RL benchmarks,ACA achieves more favorable learning curves and\ncompetitive performance compared to both standard actor-critic and\nstate-of-the-art diffusion-based methods, providing a simple yet powerful\nsolution for online RL.", "AI": {"tldr": "ACA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6d88\u9664\u4e86\u663e\u5f0f\u7684\u884c\u52a8\u8005\u7f51\u7edc\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u7ea7\u8bc4\u8bba\u5bb6\u7684\u68af\u5ea6\u573a\u751f\u6210\u52a8\u4f5c\uff0c\u7b80\u5316\u4e86\u67b6\u6784\u5e76\u4fdd\u6301\u4e86\u591a\u6a21\u6001\u884c\u4e3a\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u9700\u8981\u5355\u72ec\u7684\u884c\u52a8\u8005\u548c\u8bc4\u8bba\u5bb6\u7f51\u7edc\uff0c\u8bad\u7ec3\u590d\u6742\u4e14\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u51fd\u6570\u903c\u8fd1\u5668\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u8868\u8fbe\u591a\u6a21\u6001\u884c\u4e3a\u4f46\u5f15\u5165\u989d\u5916\u8bbe\u8ba1\u590d\u6742\u6027\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faACA\u6846\u67b6\uff0c\u53d6\u6d88\u663e\u5f0f\u884c\u52a8\u8005\u7f51\u7edc\uff0c\u901a\u8fc7\u566a\u58f0\u7ea7\u8bc4\u8bba\u5bb6\u7684\u68af\u5ea6\u573a\u76f4\u63a5\u751f\u6210\u52a8\u4f5c\uff0c\u4fdd\u6301\u7b56\u7565\u6539\u8fdb\u4e0e\u8bc4\u8bba\u5bb6\u6700\u65b0\u4ef7\u503c\u4f30\u8ba1\u7d27\u5bc6\u5bf9\u9f50\u3002", "result": "\u5728\u6807\u51c6\u5728\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACA\u76f8\u6bd4\u6807\u51c6\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u548c\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b66\u4e60\u66f2\u7ebf\u548c\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "ACA\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7b80\u6d01\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21044", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21044", "abs": "https://arxiv.org/abs/2509.21044", "authors": ["Honglin Zhang", "Qianyue Hao", "Fengli Xu", "Yong Li"], "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs", "comment": null, "summary": "Large language models (LLMs) acquire extensive prior knowledge through\nlarge-scale pretraining and can be further enhanced via supervised fine-tuning\n(SFT) or reinforcement learning (RL)-based post-training. A growing body of\nevidence has shown that RL fine-tuning improves the capability of LLMs beyond\nwhat SFT alone achieves. However, the underlying mechanisms why RL fine-tuning\nis able to enhance the capability of various LLMs with distinct intrinsic\ncharacteristics remain underexplored. In this study, we draw inspiration from\nprior work on edge attribution patching (EAP) to investigate the internal\ndifferences of LLMs before and after RL fine-tuning. Our analysis across\nmultiple model families shows two robust effects of online RL post-training:\n(i) an overall increase in activation intensity, indicating that more internal\npathways are engaged and their signals become stronger, and (ii) greater\ndiversity in activation patterns, reflected by higher entropy and less\nconcentrated edge distributions. These changes suggest that RL reshapes\ninformation flow to be both more redundant and more flexible, which may explain\nits advantage in generalization. Notably, models fine-tuned with Direct\nPreference Optimization (DPO) deviate from these trends, exhibiting\nsubstantially weaker or inconsistent internal changes compared to PPO- and\nGRPO-based training. Together, our findings provide a unified view of how RL\nfine-tuning systematically alters the internal circuitry of LLMs and highlight\nthe methodological distinctions between online RL and preference-based\napproaches. Our code is open source at\nhttps://anonymous.4open.science/r/llm_rl_probing_analysis-F673.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8fb9\u7f18\u5f52\u56e0\u4fee\u8865\u6280\u672f\u5206\u6790RL\u5fae\u8c03\u5bf9LLMs\u5185\u90e8\u673a\u5236\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u7ebfRL\u8bad\u7ec3\u4f1a\u589e\u5f3a\u6fc0\u6d3b\u5f3a\u5ea6\u548c\u591a\u6837\u6027\uff0c\u800cDPO\u65b9\u6cd5\u5219\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5185\u90e8\u53d8\u5316\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22RL\u5fae\u8c03\u4e3a\u4f55\u80fd\u8d85\u8d8aSFT\u63d0\u5347LLM\u80fd\u529b\u7684\u5185\u5728\u673a\u5236\uff0c\u7279\u522b\u662f\u4e0d\u540cRL\u65b9\u6cd5\u5bf9\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u7684\u5dee\u5f02\u5316\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8fb9\u7f18\u5f52\u56e0\u4fee\u8865(EAP)\u6280\u672f\u5206\u6790\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u5728RL\u5fae\u8c03\u524d\u540e\u7684\u5185\u90e8\u6fc0\u6d3b\u5dee\u5f02\uff0c\u6bd4\u8f83PPO\u3001GRPO\u548cDPO\u7b49\u4e0d\u540cRL\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5728\u7ebfRL\u8bad\u7ec3\u5bfc\u81f4\u6fc0\u6d3b\u5f3a\u5ea6\u6574\u4f53\u589e\u52a0\u548c\u6fc0\u6d3b\u6a21\u5f0f\u591a\u6837\u6027\u63d0\u5347\uff0c\u800cDPO\u65b9\u6cd5\u663e\u793a\u51fa\u8f83\u5f31\u6216\u4e0d\u4e00\u81f4\u7684\u5185\u90e8\u53d8\u5316\u3002", "conclusion": "RL\u5fae\u8c03\u901a\u8fc7\u91cd\u5851\u4fe1\u606f\u6d41\u4f7f\u5176\u66f4\u5177\u5197\u4f59\u6027\u548c\u7075\u6d3b\u6027\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4e0d\u540cRL\u65b9\u6cd5\u5728\u5185\u90e8\u673a\u5236\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21126", "abs": "https://arxiv.org/abs/2509.21126", "authors": ["Xiefeng Wu", "Jing Zhao", "Shu Zhang", "Mingyu Hu"], "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning", "comment": null, "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments.", "AI": {"tldr": "VARL\u662f\u4e00\u4e2a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u52a8\u4f5c\u5efa\u8bae\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u52a0\u6837\u672c\u591a\u6837\u6027\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8017\u65f6\uff0c\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u6b65\u9aa4\u6765\u5b66\u4e60\u6700\u4f18Q\u51fd\u6570\u3002\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u5728\u4f4e\u5c42\u63a7\u5236\u65b9\u9762\u6027\u80fd\u6709\u9650\uff0c\u4e14\u901a\u5e38\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u6f14\u793a\u8fdb\u884c\u5fae\u8c03\u3002", "method": "VARL\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u77e5\u8bc6\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u52a8\u4f5c\u5efa\u8bae\uff0c\u800c\u4e0d\u662f\u8bbe\u8ba1\u542f\u53d1\u5f0f\u5956\u52b1\uff0c\u4ece\u800c\u4fdd\u8bc1\u6700\u4f18\u6027\u548c\u6536\u655b\u6027\u4e0d\u53d8\u3002", "result": "VARL\u5728\u5404\u79cd\u73af\u5883\u548c\u4ee3\u7406\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u4e14\u6ca1\u6709\u5f15\u5165\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "VARL\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u5f97\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u76f4\u63a5\u4ece\u96f6\u5f00\u59cb\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u53d8\u5f97\u53ef\u884c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21129", "abs": "https://arxiv.org/abs/2509.21129", "authors": ["Wei Huang", "De-Tian Chu", "Lin-Yuan Bai", "Wei Kang", "Hai-Tao Zhang", "Bo Li", "Zhi-Mo Han", "Jing Ge", "Hai-Feng Lin"], "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense", "comment": null, "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats.", "AI": {"tldr": "EvoMail\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684\u8ba4\u77e5\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5783\u573e\u90ae\u4ef6\u548c\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u3002\u5b83\u901a\u8fc7\u6784\u5efa\u5f02\u6784\u90ae\u4ef6\u56fe\u3001\u4f7f\u7528\u8ba4\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u548cLLM\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u7ea2\u84dd\u5bf9\u6297\u81ea\u8fdb\u5316\u673a\u5236\u6765\u6301\u7eed\u9002\u5e94\u65b0\u578b\u653b\u51fb\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3\u5783\u573e\u90ae\u4ef6\u548c\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u5df2\u8d85\u8d8a\u4f20\u7edf\u5173\u952e\u8bcd\u9ed1\u540d\u5355\u548c\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u6a21\u6001\u7b56\u7565\u5e76\u5feb\u901f\u6f14\u53d8\u3002\u4f20\u7edf\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u6574\u5408\u5f02\u6784\u4fe1\u53f7\u548c\u6301\u7eed\u9002\u5e94\uff0c\u5bfc\u81f4\u6027\u80fd\u5feb\u901f\u4e0b\u964d\u3002", "method": "1. \u6784\u5efa\u7edf\u4e00\u7684\u5f02\u6784\u90ae\u4ef6\u56fe\uff0c\u878d\u5408\u6587\u672c\u5185\u5bb9\u3001\u5143\u6570\u636e\u548c\u5d4c\u5165\u8d44\u6e90\uff1b2. \u4f7f\u7528\u8ba4\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u548cLLM\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\uff1b3. \u91c7\u7528\u7ea2\u84dd\u5bf9\u6297\u81ea\u8fdb\u5316\u5faa\u73af\uff1a\u7ea2\u961f\u751f\u6210\u65b0\u89c4\u907f\u7b56\u7565\uff0c\u84dd\u961f\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u5e76\u538b\u7f29\u7ecf\u9a8c\u5230\u8bb0\u5fc6\u6a21\u5757\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08Enron-Spam\u3001Ling-Spam\u3001SpamAssassin\u3001TREC\uff09\u548c\u5408\u6210\u5bf9\u6297\u53d8\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvoMail\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5bf9\u6f14\u53d8\u5783\u573e\u90ae\u4ef6\u7b56\u7565\u7684\u9002\u5e94\u6027\u548c\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EvoMail\u4f5c\u4e3a\u4e00\u4e2a\u5f39\u6027\u548c\u53ef\u89e3\u91ca\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u5177\u6709\u5bf9\u6297\u4e0b\u4e00\u4ee3\u5783\u573e\u90ae\u4ef6\u548c\u7f51\u7edc\u9493\u9c7c\u5a01\u80c1\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.21154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21154", "abs": "https://arxiv.org/abs/2509.21154", "authors": ["Michael Sullivan"], "title": "GRPO is Secretly a Process Reward Model", "comment": "14 pages, 6 figures; under review at ICLR 2026", "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u8bf1\u5bfc\u51fa\u975e\u5e73\u51e1\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\uff0c\u5e76\u63d0\u51fa\u4e86\u03bb-GRPO\u6539\u8fdb\u7b97\u6cd5\u6765\u7f13\u89e3\u539f\u7b97\u6cd5\u4e2d\u7684\u7f3a\u9677\uff0c\u5728\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22GRPO\u7b97\u6cd5\u4e2d\u9690\u85cf\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7ed3\u6784\uff0c\u5e76\u89e3\u51b3\u539f\u7b97\u6cd5\u4e2d\u8fc7\u7a0b\u6b65\u9aa4\u5206\u5e03\u4e0d\u5747\u5300\u5bfc\u81f4\u7684\u63a2\u7d22\u548c\u5229\u7528\u95ee\u9898\uff0c\u4ee5\u964d\u4f4e\u663e\u5f0f\u5b9a\u4e49PRM\u7684\u9ad8\u6210\u672c\u3002", "method": "\u9996\u5148\u7406\u8bba\u8bc1\u660eGRPO\u7b97\u6cd5\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u4f1a\u8bf1\u5bfcPRM\uff0c\u7136\u540e\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u6210\u7acb\u3002\u57fa\u4e8eGRPO-as-a-PRM\u6846\u67b6\uff0c\u8bc6\u522b\u7b97\u6cd5\u7f3a\u9677\u5e76\u63d0\u51fa\u03bb-GRPO\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u03bb-GRPO\u8bad\u7ec3\u7684LLM\u5728\u9a8c\u8bc1\u51c6\u786e\u7387\u548c\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u6807\u51c6GRPO\uff0c\u4e14\u8fbe\u5230\u5cf0\u503c\u6027\u80fd\u66f4\u5feb\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u53ef\u4ee5\u5229\u7528GRPO\u5185\u7f6e\u7684PRM\u7ed3\u6784\u6765\u66ff\u4ee3\u6602\u8d35\u7684\u663e\u5f0fPRM\u5b9a\u4e49\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5229\u7528GRPO\u7b97\u6cd5\u4e2d\u9690\u85cf\u7684PRM\u7ed3\u6784\uff0c\u53ef\u4ee5\u5728\u51e0\u4e4e\u4e0d\u5f71\u54cd\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u5bf9\u6602\u8d35\u7684\u663e\u5f0fPRM\u5b9a\u4e49\u65b9\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21164", "abs": "https://arxiv.org/abs/2509.21164", "authors": ["Jacob Fein-Ashley", "Dhruv Parikh", "Rajgopal Kannan", "Viktor Prasanna"], "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say", "comment": null, "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot.", "AI": {"tldr": "MoT\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8ba9\u5f02\u6784\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u6f5c\u5728\u5c42\u534f\u4f5c\uff0c\u5b9e\u73b0\u591aLLM\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u65e0\u9700\u8fed\u4ee3\u805a\u5408\u4e14\u63a8\u7406\u6548\u7387\u9ad8\u3002", "motivation": "\u73b0\u6709\u7684\u591aLLM\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u53ea\u8def\u7531\u5230\u5c11\u6570\u4e13\u5bb6\u72ec\u7acb\u751f\u6210\uff0c\u8981\u4e48\u901a\u8fc7\u6602\u8d35\u7684\u591a\u8f6e\u4ea4\u6362\u805a\u5408\u8f93\u51fa\uff0c\u8981\u4e48\u9700\u8981\u67b6\u6784\u540c\u8d28\u6027\u8fdb\u884c\u6743\u91cd\u878d\u5408\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5f02\u6784\u6a21\u578b\u534f\u4f5c\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u9009\u62e9top-K\u4e13\u5bb6\u5e76\u6307\u5b9a\u4e3b\u4e13\u5bb6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5c42\u5c06\u9690\u85cf\u72b6\u6001\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3b\u4e13\u5bb6\u5bf9\u6d3b\u8dc3\u540c\u884c\u8fdb\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u9884\u8bad\u7ec3\u4e13\u5bb6\u4fdd\u6301\u51bb\u7ed3\uff0c\u53ea\u8bad\u7ec3\u8def\u7531\u5668\u548c\u4ea4\u4e92\u5c42\u3002", "result": "\u57285\u4e2a\u5206\u5e03\u5185\u548c3\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoT\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8def\u7531\u548c\u805a\u5408\u65b9\u6cd5Avengers\uff0c\u5206\u522b\u63d0\u53470.38%\u548c2.92%\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5355\u4e2a\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "MoT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6f5c\u5728\u7a7a\u95f4\u673a\u5236\u6765\u7ec4\u5408\u5f02\u6784LLM\uff0c\u662f\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u591aLLM\u534f\u4f5c\u7684\u5b9e\u7528\u6b65\u9aa4\u3002", "topic": "agent analysis"}}
{"id": "2509.21240", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21240", "abs": "https://arxiv.org/abs/2509.21240", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "title": "Tree Search for LLM Agent Reinforcement Learning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u5206\u7ec4\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5Tree-GRPO\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u8f68\u8ff9\u6784\u5efa\u6b65\u8fdb\u5f0f\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\uff0c\u89e3\u51b3\u957f\u5468\u671f\u591a\u8f6e\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7a00\u758f\u76d1\u7763\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ec5\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u5468\u671f\u591a\u8f6e\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u5b58\u5728\u7a00\u758f\u76d1\u7763\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "Tree-GRPO\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u6811\u8282\u70b9\u4ee3\u8868\u5b8c\u6574\u7684\u667a\u80fd\u4f53\u4ea4\u4e92\u6b65\u9aa4\uff0c\u901a\u8fc7\u5171\u4eab\u516c\u5171\u524d\u7f00\u589e\u52a0\u56fa\u5b9atoken\u9884\u7b97\u5185\u7684rollout\u6570\u91cf\uff0c\u5229\u7528\u6811\u7ed3\u6784\u8f68\u8ff9\u6784\u5efa\u6b65\u8fdb\u5f0f\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u6811\u5185\u548c\u6811\u95f4\u5c42\u9762\u4f30\u8ba1\u5206\u7ec4\u76f8\u5bf9\u4f18\u52bf\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u548c3\u7c7bQA\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6811\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u94fe\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6811\u7ed3\u6784RL\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u7a00\u758f\u76d1\u7763\u95ee\u9898\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u6811\u5185\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7b49\u4ef7\u4e8e\u6b65\u7ea7\u76f4\u63a5\u504f\u597d\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21241", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21241", "abs": "https://arxiv.org/abs/2509.21241", "authors": ["Yucheng Wang", "Ziyang Chen", "Md Faisal Kabir"], "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework", "comment": "16 pages, 9 figures", "summary": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u7ecf\u8fc7LoRA\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u63a8\u7406\u548c\u8bed\u4e49\u884c\u4e3a\u53d8\u5316", "motivation": "\u7406\u89e3LoRA\u5fae\u8c03\u673a\u5236\u5982\u4f55\u6539\u53d8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u63a8\u7406\u548c\u8bed\u4e49\u884c\u4e3a\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u5fae\u8c03\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236", "method": "\u6784\u5efaBioToolKG\u751f\u7269\u4fe1\u606f\u5b66\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u8bbe\u8ba1CFFTLLMExplainer\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u56fe\u8282\u70b9\u548c\u8fb9\u7684\u8f6f\u63a9\u7801\u6765\u751f\u6210\u6700\u5c0f\u7ed3\u6784\u6270\u52a8\uff0c\u540c\u65f6\u4f18\u5316\u7ed3\u6784\u7a00\u758f\u6027\u548c\u8bed\u4e49\u5dee\u5f02", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u5230\u5fae\u8c03\u7684LLaMA\u6a21\u578b\uff0c\u53d1\u73b0\u53cd\u4e8b\u5b9e\u63a9\u7801\u63ed\u793a\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4e0eLoRA\u8bf1\u5bfc\u7684\u53c2\u6570\u53d8\u5316\u76f8\u4e00\u81f4", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u7a81\u51fa\u4e86\u53cd\u4e8b\u5b9e\u56fe\u4f5c\u4e3a\u53ef\u89e3\u91caAI\u7684\u6f5c\u5728\u5de5\u5177", "topic": "agent analysis"}}
{"id": "2509.21282", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21282", "abs": "https://arxiv.org/abs/2509.21282", "authors": ["Madeleine Dwyer", "Adam Sobey", "Adriane Chapman"], "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL", "comment": null, "summary": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).", "AI": {"tldr": "\u63d0\u51fa\u6982\u7387\u5e73\u6ed1\u7b56\u7565\u4f18\u5316\uff08PSPO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u6ed1\u5f53\u524d\u7b56\u7565\u6982\u7387\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u6bd4\u7387\u88c1\u526a\uff0c\u5728\u4fdd\u6301\u68af\u5ea6\u4fe1\u53f7\u7684\u540c\u65f6\u521b\u5efa\u8f6f\u4fe1\u4efb\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982PPO\u548cGRPO\u4f9d\u8d56\u6bd4\u7387\u88c1\u526a\u6765\u7a33\u5b9a\u66f4\u65b0\uff0c\u4f46\u88c1\u526a\u4f1a\u4e22\u5f03\u4fe1\u606f\u5e76\u5f15\u5165\u68af\u5ea6\u4e0d\u8fde\u7eed\u6027\uff0c\u9700\u8981\u66f4\u5e73\u6ed1\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "PSPO\u65b9\u6cd5\u5728\u8ba1\u7b97\u91cd\u8981\u6027\u6bd4\u7387\u4e4b\u524d\u5c06\u5f53\u524d\u7b56\u7565\u7684\u6982\u7387\u5e73\u6ed1\u5230\u65e7\u7b56\u7565\uff0c\u7c7b\u4f3c\u4e8e\u6807\u7b7e\u5e73\u6ed1\uff0c\u521b\u5efa\u8f6f\u4fe1\u4efb\u533a\u57df\u6765\u9632\u6b62\u5927\u7684\u4e0d\u7a33\u5b9a\u66f4\u65b0\u3002", "result": "\u5728GSM8K\u7b49\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cGR-PSPO\u76f8\u6bd4\u88c1\u526a\u7248GRPO\u6027\u80fd\u63d0\u5347\u663e\u8457\uff080.5B\u6a21\u578b\u4ece17.6%\u63d0\u5347\u523039.7%\uff0c1.5B\u6a21\u578b\u4ece37.8%\u63d0\u5347\u523059.4%\uff09\uff0c\u63a8\u7406\u8fc7\u7a0b\u66f4\u6e05\u6670\u7b80\u6d01\u3002", "conclusion": "PSPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u6bd4\u7387\u88c1\u526a\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2509.8fdc2c23", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-claude-code-is-built%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/TYpozMrGuNhpLqVejcxKDj62sxTd6aG5aUP4y7dBsxA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-claude-code-is-built%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/TYpozMrGuNhpLqVejcxKDj62sxTd6aG5aUP4y7dBsxA=424", "authors": ["TLDR Newsletter"], "title": "How Claude Code is built", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-claude-code-is-built%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/TYpozMrGuNhpLqVejcxKDj62sxTd6aG5aUP4y7dBsxA=424", "summary": "How Claude Code is built (19 minute read) This article provides a look into the development of Claude Code, a popular AI-powered developer tool that generates over $500M in annual revenue. The tool originated from a simple command-line tool using Claude to identify music, then evolved into a sophisticated product with a tech stack including TypeScript, React, Ink, Yoga, and Bun, with 90% of the code written by itself. Claude Code's success is attributed to its rapid development pace, with fea...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Claude Code\u7684\u5f00\u53d1\u5386\u7a0b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e74\u6536\u5165\u8d85\u8fc75\u4ebf\u7f8e\u5143\u7684AI\u9a71\u52a8\u5f00\u53d1\u5de5\u5177\uff0c\u4ece\u7b80\u5355\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6f14\u53d8\u4e3a\u590d\u6742\u4ea7\u54c1\uff0c90%\u7684\u4ee3\u7801\u7531\u81ea\u8eab\u751f\u6210\u3002", "motivation": "\u5c55\u793a\u5982\u4f55\u5c06\u7b80\u5355\u7684AI\u5de5\u5177\u53d1\u5c55\u4e3a\u6210\u529f\u7684\u5546\u4e1a\u4ea7\u54c1\uff0c\u5f3a\u8c03\u5feb\u901f\u5f00\u53d1\u548c\u81ea\u6211\u6539\u8fdb\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528TypeScript\u3001React\u3001Ink\u3001Yoga\u548cBun\u7b49\u6280\u672f\u6808\uff0c\u901a\u8fc7AI\u81ea\u52a8\u751f\u6210\u5927\u90e8\u5206\u4ee3\u7801\uff0c\u5b9e\u73b0\u5feb\u901f\u8fed\u4ee3\u5f00\u53d1\u3002", "result": "Claude Code\u6210\u4e3a\u5e74\u6536\u5165\u8d85\u8fc75\u4ebf\u7f8e\u5143\u7684\u6d41\u884c\u5f00\u53d1\u5de5\u5177\uff0c\u8bc1\u660e\u4e86AI\u9a71\u52a8\u5f00\u53d1\u7684\u5546\u4e1a\u53ef\u884c\u6027\u3002", "conclusion": "AI\u5de5\u5177\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u548c\u5feb\u901f\u5f00\u53d1\u5b9e\u73b0\u5546\u4e1a\u6210\u529f\uff0c\u4e3a\u7c7b\u4f3c\u9879\u76ee\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6a21\u5f0f\u3002", "topic": "swe application"}}
{"id": "tldr.2509.f8b9a520", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fillmore-labs.com%2Fposts%2Ferrors-2%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/kbLKxqTlST2eD0II8blSr7RssmBNJkygLSKaYZ0zkCM=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fillmore-labs.com%2Fposts%2Ferrors-2%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/kbLKxqTlST2eD0II8blSr7RssmBNJkygLSKaYZ0zkCM=424", "authors": ["TLDR Newsletter"], "title": "The Day the Linter Broke My Code", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fillmore-labs.com%2Fposts%2Ferrors-2%3Futm_source=tldrwebdev/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/kbLKxqTlST2eD0II8blSr7RssmBNJkygLSKaYZ0zkCM=424", "summary": "The Day the Linter Broke My Code (7 minute read) A seemingly helpful linter suggestion inadvertently introduced a subtle but critical bug by incorrectly applying a standard method within a custom method of the same name, violating Go's error-handling principles and leading to unexpected error equivalences.", "source": "tldr", "AI": {"tldr": "\u4e00\u4e2a\u770b\u4f3c\u6709\u7528\u7684linter\u5efa\u8bae\u9519\u8bef\u5730\u5e94\u7528\u4e86\u6807\u51c6\u65b9\u6cd5\u5230\u81ea\u5b9a\u4e49\u540c\u540d\u65b9\u6cd5\u4e2d\uff0c\u8fdd\u53cd\u4e86Go\u8bed\u8a00\u7684\u9519\u8bef\u5904\u7406\u539f\u5219\uff0c\u5bfc\u81f4\u610f\u5916\u7684\u9519\u8bef\u7b49\u4ef7\u6027\uff0c\u4ece\u800c\u5f15\u5165\u4e86\u5173\u952ebug\u3002", "motivation": "\u63a2\u8ba8linter\u5de5\u5177\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u53ef\u80fd\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u7279\u522b\u662f\u5f53\u81ea\u52a8\u5316\u5efa\u8bae\u4e0e\u8bed\u8a00\u7279\u5b9a\u539f\u5219\u51b2\u7a81\u65f6\u3002", "method": "\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u5206\u6790linter\u5efa\u8bae\u5982\u4f55\u9519\u8bef\u5730\u4fee\u6539\u4ee3\u7801\uff0c\u8fdd\u53cdGo\u8bed\u8a00\u7684\u9519\u8bef\u5904\u7406\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u53d1\u73b0linter\u5de5\u5177\u53ef\u80fd\u5728\u4e0d\u4e86\u89e3\u8bed\u8a00\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u7ed9\u51fa\u6709\u5bb3\u5efa\u8bae\uff0c\u5bfc\u81f4\u4ee3\u7801\u529f\u80fd\u5f02\u5e38\u3002", "conclusion": "\u5f00\u53d1\u8005\u5728\u4f9d\u8d56linter\u5de5\u5177\u65f6\u9700\u8981\u4fdd\u6301\u6279\u5224\u6027\u601d\u7ef4\uff0c\u7406\u89e3\u5de5\u5177\u5efa\u8bae\u80cc\u540e\u7684\u8bed\u8a00\u539f\u5219\u3002", "topic": "swe application"}}
{"id": "tldr.2509.d4c362ad", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/hDiK2kkagpHMMC1l79oOwSjQaFLzHnJ5y_4YI5ynKTQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/hDiK2kkagpHMMC1l79oOwSjQaFLzHnJ5y_4YI5ynKTQ=424", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrwebdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199809c3ebf-359a83f3-0db3-499f-9ae2-38b1d1dbff9f-000000/hDiK2kkagpHMMC1l79oOwSjQaFLzHnJ5y_4YI5ynKTQ=424", "summary": "The Day the Linter Broke My Code (7 minute read) A seemingly helpful linter suggestion inadvertently introduced a subtle but critical bug by incorrectly applying a standard method within a custom method of the same name, violating Go's error-handling principles and leading to unexpected error equivalences.", "source": "tldr", "AI": {"tldr": "\u4e00\u4e2a\u770b\u4f3c\u6709\u7528\u7684linter\u5efa\u8bae\u9519\u8bef\u5730\u5e94\u7528\u4e86\u6807\u51c6\u65b9\u6cd5\u5230\u81ea\u5b9a\u4e49\u7684\u540c\u540d\u65b9\u6cd5\u4e2d\uff0c\u8fdd\u53cd\u4e86Go\u7684\u9519\u8bef\u5904\u7406\u539f\u5219\uff0c\u5bfc\u81f4\u610f\u5916\u7684\u9519\u8bef\u7b49\u4ef7\u6027\uff0c\u5f15\u5165\u4e86\u5173\u952ebug\u3002", "motivation": "\u63a2\u8ba8linter\u5de5\u5177\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u7279\u522b\u662f\u5f53\u5b83\u4eec\u9519\u8bef\u5730\u5e94\u7528\u6807\u51c6\u65b9\u6cd5\u5230\u81ea\u5b9a\u4e49\u5b9e\u73b0\u65f6\u53ef\u80fd\u5bfc\u81f4\u7684\u4e25\u91cd\u540e\u679c\u3002", "method": "\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u5206\u6790\uff0c\u5c55\u793alinter\u5efa\u8bae\u5982\u4f55\u5728\u4e0d\u6070\u5f53\u7684\u4e0a\u4e0b\u6587\u4e2d\u5e94\u7528\u6807\u51c6\u65b9\u6cd5\uff0c\u4ece\u800c\u7834\u574fGo\u8bed\u8a00\u7684\u9519\u8bef\u5904\u7406\u673a\u5236\u3002", "result": "\u53d1\u73b0linter\u7684\u9519\u8bef\u5efa\u8bae\u5bfc\u81f4\u4e86\u5fae\u5999\u7684\u4f46\u5173\u952e\u7684bug\uff0c\u8fdd\u53cd\u4e86Go\u7684\u9519\u8bef\u5904\u7406\u6700\u4f73\u5b9e\u8df5\uff0c\u9020\u6210\u4e86\u610f\u5916\u7684\u9519\u8bef\u7b49\u4ef7\u6027\u95ee\u9898\u3002", "conclusion": "\u5f00\u53d1\u8005\u9700\u8981\u8c28\u614e\u5bf9\u5f85linter\u5de5\u5177\u7684\u5efa\u8bae\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u81ea\u5b9a\u4e49\u65b9\u6cd5\u548c\u6807\u51c6\u65b9\u6cd5\u540c\u540d\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u8fdb\u884c\u4ed4\u7ec6\u7684\u4eba\u5de5\u5ba1\u67e5\u3002", "topic": "swe application"}}
{"id": "tldr.2509.52895f9d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F783828%2Ffigma-make-ai-app-coding-mcp-server-update%3Futm_source=tldrdesign/1/0100019980c40ac4-e250108d-d848-4726-ade2-cd1fd864882c-000000/9ompLl7OF406bC1Tsw3dbVv-_jNYkUZLrrcSnE34KiU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F783828%2Ffigma-make-ai-app-coding-mcp-server-update%3Futm_source=tldrdesign/1/0100019980c40ac4-e250108d-d848-4726-ade2-cd1fd864882c-000000/9ompLl7OF406bC1Tsw3dbVv-_jNYkUZLrrcSnE34KiU=424", "authors": ["TLDR Newsletter"], "title": "Figma Made its Design Tools More Accessible to AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F783828%2Ffigma-make-ai-app-coding-mcp-server-update%3Futm_source=tldrdesign/1/0100019980c40ac4-e250108d-d848-4726-ade2-cd1fd864882c-000000/9ompLl7OF406bC1Tsw3dbVv-_jNYkUZLrrcSnE34KiU=424", "summary": "Figma Made its Design Tools More Accessible to AI Agents (2 minute read) Figma expanded its Model Context Protocol server to support the Figma Make AI app builder, enabling AI models to access the underlying code, in addition to visual designs. The MCP server now supports remote access from various AI coding platforms, including Anthropic, Cursor, and VS Code, making it more accessible to developers. Additional features, such as Design Snapshot for converting Make files into editable layers a...", "source": "tldr", "AI": {"tldr": "Figma\u6269\u5c55\u4e86\u5176Model Context Protocol\u670d\u52a1\u5668\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u8bbf\u95ee\u5e95\u5c42\u4ee3\u7801\u548c\u89c6\u89c9\u8bbe\u8ba1\uff0c\u652f\u6301\u4eceAnthropic\u3001Cursor\u548cVS Code\u7b49AI\u7f16\u7801\u5e73\u53f0\u8fdb\u884c\u8fdc\u7a0b\u8bbf\u95ee\u3002", "motivation": "\u8ba9AI\u4ee3\u7406\u66f4\u5bb9\u6613\u8bbf\u95ee\u548c\u4f7f\u7528Figma\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u63d0\u9ad8\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\u548c\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55MCP\u670d\u52a1\u5668\u529f\u80fd\uff0c\u652f\u6301\u8fdc\u7a0b\u8bbf\u95ee\u548c\u65b0\u589eDesign Snapshot\u7b49\u529f\u80fd\uff0c\u5c06Make\u6587\u4ef6\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u56fe\u5c42\u3002", "result": "Figma\u7684\u8bbe\u8ba1\u5de5\u5177\u73b0\u5728\u5bf9AI\u4ee3\u7406\u66f4\u52a0\u5f00\u653e\u548c\u53ef\u8bbf\u95ee\uff0c\u652f\u6301\u591a\u4e2a\u4e3b\u6d41AI\u7f16\u7801\u5e73\u53f0\u7684\u96c6\u6210\u3002", "conclusion": "\u8fd9\u4e00\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86Figma\u5de5\u5177\u5728AI\u9a71\u52a8\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\u548c\u96c6\u6210\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2509.21f70edd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finextra.com%2Fblogposting%2F29414%2Fhow-agentic-ai-is-accelerating-the-autonomous-payment-transition%3Futm_source=tldrfintech/1/0100019980fcbb8f-497675e9-e9ba-4104-af45-5984af59a8ac-000000/sIaK2hthNwchp2cWP9UZXC8SWA64FOVAa8Zo_E4rB7s=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finextra.com%2Fblogposting%2F29414%2Fhow-agentic-ai-is-accelerating-the-autonomous-payment-transition%3Futm_source=tldrfintech/1/0100019980fcbb8f-497675e9-e9ba-4104-af45-5984af59a8ac-000000/sIaK2hthNwchp2cWP9UZXC8SWA64FOVAa8Zo_E4rB7s=424", "authors": ["TLDR Newsletter"], "title": "How Agentic AI is Accelerating the Autonomous Payment Transition", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finextra.com%2Fblogposting%2F29414%2Fhow-agentic-ai-is-accelerating-the-autonomous-payment-transition%3Futm_source=tldrfintech/1/0100019980fcbb8f-497675e9-e9ba-4104-af45-5984af59a8ac-000000/sIaK2hthNwchp2cWP9UZXC8SWA64FOVAa8Zo_E4rB7s=424", "summary": "How Agentic AI is Accelerating the Autonomous Payment Transition (8 minute read) Agentic AI is already beginning to redefine how payments are made and the ways in which we can manage our money. The infusion of intelligence, automation, and decision-making tools in real time to the financial landscape can bring far greater benefits to support modern working trends in the future.", "source": "tldr", "AI": {"tldr": "Agentic AI\u6b63\u5728\u52a0\u901f\u652f\u4ed8\u5411\u81ea\u4e3b\u5316\u8f6c\u578b\uff0c\u901a\u8fc7\u5b9e\u65f6\u667a\u80fd\u3001\u81ea\u52a8\u5316\u548c\u51b3\u7b56\u5de5\u5177\u91cd\u65b0\u5b9a\u4e49\u652f\u4ed8\u65b9\u5f0f\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u8d8b\u52bf\u63d0\u4f9b\u66f4\u5927\u652f\u6301\u3002", "motivation": "\u63a2\u7d22Agentic AI\u5982\u4f55\u901a\u8fc7\u667a\u80fd\u5316\u548c\u81ea\u52a8\u5316\u6539\u53d8\u652f\u4ed8\u9886\u57df\uff0c\u652f\u6301\u73b0\u4ee3\u5de5\u4f5c\u8d8b\u52bf\u7684\u53d1\u5c55\u3002", "method": "\u5c06\u667a\u80fd\u3001\u81ea\u52a8\u5316\u548c\u5b9e\u65f6\u51b3\u7b56\u5de5\u5177\u6574\u5408\u5230\u91d1\u878d\u652f\u4ed8\u7cfb\u7edf\u4e2d\u3002", "result": "Agentic AI\u5df2\u7ecf\u5f00\u59cb\u91cd\u65b0\u5b9a\u4e49\u652f\u4ed8\u65b9\u5f0f\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u5e26\u6765\u5b9e\u65f6\u667a\u80fd\u548c\u81ea\u52a8\u5316\u80fd\u529b\u3002", "conclusion": "Agentic AI\u7684\u667a\u80fd\u5316\u548c\u81ea\u52a8\u5316\u80fd\u529b\u5c06\u52a0\u901f\u652f\u4ed8\u5411\u81ea\u4e3b\u5316\u8f6c\u578b\uff0c\u4e3a\u672a\u6765\u91d1\u878d\u5de5\u4f5c\u6a21\u5f0f\u63d0\u4f9b\u91cd\u8981\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.43617157", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fopenai-tests-chatgpt-agent-upgrades-powered-by-new-alpha-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/UPPRvpM5LLF2SWS7eXHCaeZneCO5tXUXW9Ay-ym9l6U=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fopenai-tests-chatgpt-agent-upgrades-powered-by-new-alpha-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/UPPRvpM5LLF2SWS7eXHCaeZneCO5tXUXW9Ay-ym9l6U=424", "authors": ["TLDR Newsletter"], "title": "OpenAI tests ChatGPT Agent upgrades powered by new Alpha models", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fopenai-tests-chatgpt-agent-upgrades-powered-by-new-alpha-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/UPPRvpM5LLF2SWS7eXHCaeZneCO5tXUXW9Ay-ym9l6U=424", "summary": "OpenAI tests ChatGPT Agent upgrades powered by new Alpha models (2 minute read) Some ChatGPT users have spotted a new 'alpha models' section in the model selector. The models appeared for a limited time, and they activated agent mode. The model naming - 'Agent with truncation' and 'Agent with prompt expansion' - suggests that OpenAI may be experimenting with different system prompt setups or underlying model architectures. The release seemed accidental and was quickly rolled back, making mode...", "source": "tldr", "AI": {"tldr": "OpenAI\u610f\u5916\u53d1\u5e03\u4e86\u57fa\u4e8e\u65b0Alpha\u6a21\u578b\u7684ChatGPT Agent\u5347\u7ea7\uff0c\u5305\u542b'\u5e26\u622a\u65ad\u7684Agent'\u548c'\u5e26\u63d0\u793a\u6269\u5c55\u7684Agent'\u4e24\u79cd\u6a21\u5f0f\uff0c\u4f46\u5f88\u5feb\u64a4\u56de", "motivation": "\u6d4b\u8bd5\u65b0\u7684Agent\u6a21\u5f0f\uff0c\u63a2\u7d22\u4e0d\u540c\u7684\u7cfb\u7edf\u63d0\u793a\u8bbe\u7f6e\u6216\u5e95\u5c42\u6a21\u578b\u67b6\u6784", "method": "\u5728\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u6dfb\u52a0'alpha models'\u90e8\u5206\uff0c\u6fc0\u6d3bAgent\u6a21\u5f0f\uff0c\u6d4b\u8bd5\u622a\u65ad\u548c\u63d0\u793a\u6269\u5c55\u4e24\u79cd\u4e0d\u540c\u65b9\u6cd5", "result": "\u53d1\u5e03\u4f3c\u4e4e\u662f\u610f\u5916\u884c\u4e3a\uff0c\u5f88\u5feb\u88ab\u64a4\u56de\uff0c\u7528\u6237\u53ea\u80fd\u77ed\u6682\u4f53\u9a8c", "conclusion": "OpenAI\u6b63\u5728\u79ef\u6781\u5f00\u53d1Agent\u529f\u80fd\uff0c\u4f46\u5c1a\u672a\u51c6\u5907\u597d\u6b63\u5f0f\u53d1\u5e03", "topic": "agent analysis"}}
{"id": "tldr.2509.569f553c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/Vy_bQMMCgmIqPqvbTXNd4lHYc9FfnhYEUMQYOWNyp0U=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/Vy_bQMMCgmIqPqvbTXNd4lHYc9FfnhYEUMQYOWNyp0U=424", "authors": ["TLDR Newsletter"], "title": "Meta's Open LLM for Code and World Modeling", "comment": "Source: TLDR Newsletter, Date: 2025-09-25, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrai/1/0100019981220c16-81735dd3-17c6-46de-8b11-6d81f2150b47-000000/Vy_bQMMCgmIqPqvbTXNd4lHYc9FfnhYEUMQYOWNyp0U=424", "summary": "Meta's Open LLM for Code and World Modeling (5 minute read) Meta has released CWM, a 32B decoder-only LLM trained on code execution traces and reasoning tasks to explore world models in code generation.", "source": "tldr", "AI": {"tldr": "Meta\u53d1\u5e03\u4e86CWM\uff0c\u4e00\u4e2a32B\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668LLM\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u548c\u63a8\u7406\u4efb\u52a1\u8bad\u7ec3\uff0c\u7528\u4e8e\u63a2\u7d22\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4e16\u754c\u6a21\u578b", "motivation": "\u63a2\u7d22\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u4ee3\u7801\u6267\u884c\u8fc7\u7a0b\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u6cd5\u6a21\u5f0f", "method": "\u4f7f\u752832B\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "\u5f00\u53d1\u51faCWM\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4ee3\u7801\u6267\u884c\u903b\u8f91", "conclusion": "\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u8bad\u7ec3\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u667a\u80fd\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b", "topic": "code agent"}}
{"id": "tldr.2509.e2aa6274", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_9_26_primary%26utm_content=tldr/2/01000199858d1c6b-5cda1301-c737-410e-a121-583038e26c7d-000000/5lfZJDqHlcVkRgPdahMPxaqdPM8peKSK6O48HIdcrvg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_9_26_primary%26utm_content=tldr/2/01000199858d1c6b-5cda1301-c737-410e-a121-583038e26c7d-000000/5lfZJDqHlcVkRgPdahMPxaqdPM8peKSK6O48HIdcrvg=424", "authors": ["TLDR Newsletter"], "title": "Warp Code Officially Launches, Saving Developers 1\u20132 Hours Daily", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_9_26_primary%26utm_content=tldr/2/01000199858d1c6b-5cda1301-c737-410e-a121-583038e26c7d-000000/5lfZJDqHlcVkRgPdahMPxaqdPM8peKSK6O48HIdcrvg=424", "summary": "Warp Code Officially Launches, Saving Developers 1\u20132 Hours Daily (Sponsor) \ud83c\udd95 This month Warp launched Warp Code, Early usage shows it's already saving developers 1-2 hours each day. Why Warp: 66% of developers report being frustrated by AI code that's almost-but-not-quite correct. Warp closes the gap between \"almost there\" and \"actually useful\" with a powerful codebase-aware agent that delivers more accurate code + a unified UI to review agent code. \u2705 Benchmark-beating: Warp tops Terminal-ben...", "source": "tldr", "AI": {"tldr": "Warp Code\u6b63\u5f0f\u53d1\u5e03\uff0c\u636e\u79f0\u80fd\u4e3a\u5f00\u53d1\u8005\u6bcf\u5929\u8282\u77011-2\u5c0f\u65f6\uff0c\u901a\u8fc7\u4ee3\u7801\u5e93\u611f\u77e5\u7684AI\u4ee3\u7406\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4ee3\u7801\u751f\u6210\u548c\u7edf\u4e00\u7684\u4ee3\u7801\u5ba1\u67e5\u754c\u9762\u3002", "motivation": "\u89e3\u51b366%\u5f00\u53d1\u8005\u5bf9AI\u751f\u6210\u4ee3\u7801\"\u51e0\u4e4e\u6b63\u786e\u4f46\u4e0d\u5b8c\u5168\u51c6\u786e\"\u7684\u632b\u8d25\u611f\uff0c\u5f25\u5408\"\u63a5\u8fd1\u53ef\u7528\"\u4e0e\"\u771f\u6b63\u6709\u7528\"\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u5f3a\u5927\u7684\u4ee3\u7801\u5e93\u611f\u77e5\u4ee3\u7406\u6280\u672f\uff0c\u7ed3\u5408\u7edf\u4e00\u7684\u7528\u6237\u754c\u9762\u6765\u5ba1\u67e5AI\u751f\u6210\u7684\u4ee3\u7801\u3002", "result": "\u65e9\u671f\u4f7f\u7528\u6570\u636e\u663e\u793a\uff0cWarp Code\u5df2\u7ecf\u5728\u7ec8\u7aef\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "Warp Code\u901a\u8fc7\u6539\u8fdbAI\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u751f\u4ea7\u529b\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "wechat.2509.9d1f43ac", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483657&idx=1&sn=c6858a05dc7441d2577634816d4de858&chksm=e9274866b5347b9764c6106dd19e9414c34a2654f60e2c0d26cdf5717eeb817f3a89c7bd50fd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483657&idx=1&sn=c6858a05dc7441d2577634816d4de858&chksm=e9274866b5347b9764c6106dd19e9414c34a2654f60e2c0d26cdf5717eeb817f3a89c7bd50fd#rd", "authors": ["\u6bcf\u5929\u4e00\u904d\u9632\u6b62\u60b2\u4f24"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff08\u6628\u65e5 / \u8fd136\u5c0f\u65f6\uff0cET\uff09", "comment": "Source: WeChat, Published: 2025-09-26 03:04:58", "summary": "com/2025/amazon-links-nova-act-its-ai-agent-creator-to-visual-studio-code-cursor-and-kiro/\uff5c**\u6e90\u5934\u4f18\u5148****\u4e00\u53e5\u8bdd\u7ed3\u8bba**\uff1a\u4e9a\u9a6c\u900a\u63a8\u51faNova Act\u6269\u5c55\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u76f4\u63a5\u5728Visual Studio Code\u3001Amazon Kiro\u548cCursor\u7b49\u4e3b\u6d41IDE\u4e2d\u6784\u5efa\u548c\u6d4b\u8bd5AI\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "AI": {"tldr": "com/2025/amazon-links-nova-act-its-ai-agent-creator-to-visual-studio-code-cursor-and-kiro/\uff5c**\u6e90\u5934\u4f18\u5148****\u4e00\u53e5\u8bdd\u7ed3\u8bba**\uff1a\u4e9a\u9a6c\u900a\u63a8\u51faNova Act\u6269\u5c55\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u76f4\u63a5\u5728Visual Studio Code\u3001Amazon Kiro\u548cCursor\u7b49\u4e3b\u6d41IDE\u4e2d\u6784\u5efa\u548c\u6d4b\u8bd5AI\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.e8befa82", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483651&idx=1&sn=0dccac248c89f3100fecba8be3638421&chksm=e93591f40dda816ac560d75c4f7022007f2c5066e04741ce14311a0ea85aa61770c85a98f3af#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483651&idx=1&sn=0dccac248c89f3100fecba8be3638421&chksm=e93591f40dda816ac560d75c4f7022007f2c5066e04741ce14311a0ea85aa61770c85a98f3af#rd", "authors": ["\u6bcf\u5929\u4e00\u904d\u9632\u6b62\u60b2\u4f24"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u91cd\u70b9\u901f\u9012\uff082025\u5e749\u670824\u65e5\uff0cET \u65f6\u533a\uff09", "comment": "Source: WeChat, Published: 2025-09-25 14:18:57", "summary": "Emergent \u7684\u878d\u8d44\u6210\u529f\u53ca\u5176\u201cVibe \u7f16\u7801\u5e73\u53f0\u201d\u7684\u63a8\u51fa\uff0c\u5219\u4ee3\u8868\u4e86 Code Agent \u6280\u672f\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u2014\u2014**\u666e\u60e0\u5316**\u3002\u5176\u91cd\u8981\u6027\u4f53\u73b0\u5728\uff1a1. **\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u95e8\u69db\uff1a** Emergent \u5e73\u53f0\u5141\u8bb8\u4efb\u4f55\u4eba\u501f\u52a9\u81ea\u4e3b AI", "AI": {"tldr": "Emergent \u7684\u878d\u8d44\u6210\u529f\u53ca\u5176\u201cVibe \u7f16\u7801\u5e73\u53f0\u201d\u7684\u63a8\u51fa\uff0c\u5219\u4ee3\u8868\u4e86 Code Agent \u6280\u672f\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u2014\u2014**\u666e\u60e0\u5316**\u3002\u5176\u91cd\u8981\u6027\u4f53\u73b0\u5728\uff1a1. **\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u95e8\u69db\uff1a** Emergent \u5e73\u53f0\u5141\u8bb8\u4efb\u4f55\u4eba\u501f\u52a9\u81ea\u4e3b AI", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.7040b596", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MTkxMjYzOA==&mid=2247507887&idx=1&sn=4ed47967837458f5ca44b86663eb8520&chksm=cf925bd3fc3d6917cc15d901970c9585f88b3d6f9c621cbea1eb7970beafb5342c26cc6a869c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MTkxMjYzOA==&mid=2247507887&idx=1&sn=4ed47967837458f5ca44b86663eb8520&chksm=cf925bd3fc3d6917cc15d901970c9585f88b3d6f9c621cbea1eb7970beafb5342c26cc6a869c#rd", "authors": ["AI\u5bd2\u6b66\u7eaa"], "title": "\u628a20\u591a\u79cd\u5de5\u5177\u585e\u8fdb\u4e00\u4e2a\u641c\u7d22\u6846\uff01\u4eb2\u6d4b\u300c<em class=\"highlight\">Agentic</em> Search\u300d\u540e\uff0c\u6211\u5173\u6389\u4e86\u51e0\u5341\u4e2a\u6d4f\u89c8\u5668\u6807\u7b7e\u9875", "comment": "Source: WeChat, Published: 2025-09-26 11:04:59", "summary": "\u5411Agentic Search\u53d1\u51fa\u4e86\u5982\u4e0b\u6307\u4ee4\uff1a\u8f93\u5165\u4e00\u4e2a\u8d85\u7ea7Prompt\uff1a-----MindScribe\u6838\u5fc3\u4f7f\u7528\u573a\u666f\uff08\u75db\u70b9\uff09\uff1a\u5f00\u4f1a\u592a\u591a\uff1a \u6ca1\u7a7a\u8bb0\u4f1a\u8bae\u7eaa\u8981\u3002\u542c\u8bfe/\u8bb2\u5ea7\uff1a \u8001\u5e08\u8bb2\u592a\u5feb\uff0c\u7b14\u8bb0\u8ddf\u4e0d\u4e0a\u3002", "AI": {"tldr": "\u5411Agentic Search\u53d1\u51fa\u4e86\u5982\u4e0b\u6307\u4ee4\uff1a\u8f93\u5165\u4e00\u4e2a\u8d85\u7ea7Prompt\uff1a-----MindScribe\u6838\u5fc3\u4f7f\u7528\u573a\u666f\uff08\u75db\u70b9\uff09\uff1a\u5f00\u4f1a\u592a\u591a\uff1a \u6ca1\u7a7a\u8bb0\u4f1a\u8bae\u7eaa\u8981\u3002\u542c\u8bfe/\u8bb2\u5ea7\uff1a \u8001\u5e08\u8bb2\u592a\u5feb\uff0c\u7b14\u8bb0\u8ddf\u4e0d\u4e0a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.3d5f1788", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650993090&idx=2&sn=b7d3f2ea7e12bebc5aa74c0a850f40ee&chksm=851037c62efedc76198697798ac6756bfd9196a667d7e5079081e426445fe88ce8a3463c579c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650993090&idx=2&sn=b7d3f2ea7e12bebc5aa74c0a850f40ee&chksm=851037c62efedc76198697798ac6756bfd9196a667d7e5079081e426445fe88ce8a3463c579c#rd", "authors": ["\u673a\u5668\u4e4b\u5fc3"], "title": "<em class=\"highlight\">Agentic</em> Coding\u8868\u73b0\u521b\u65b0\u9ad8\uff0c\u5168\u65b0KAT\u7cfb\u5217\u6a21\u578b\u4e0a\u699cSWE-Bench", "comment": "Source: WeChat, Published: 2025-09-26 09:36:45", "summary": "\u8fd1\u671f\uff0c\u5feb\u624b Kwaipilot \u56e2\u961f\u63a8\u51fa\u4e86 KAT \u7cfb\u5217\u4e24\u6b3e\u7a81\u7834\u6027 Agentic Coding \u5927\u6a21\u578b\uff1a\u5f00\u6e90 32B \u53c2\u6570\u6a21\u578b KAT-Dev-32B \u4e0e\u95ed\u6e90\u65d7\u8230\u6a21\u578b KAT-Coder\u3002\u8fd9\u4e24\u6b3e\u6a21\u578b\u5728 Code Intelligence \u9886\u57df\u5206\u522b\u4f53\u73b0\u51fa\u8f7b\u91cf\u7ea7\u7684\u8d85\u5f3a\u8868\u73b0\u548c\u6781\u81f4\u6027\u80fd\u3002", "AI": {"tldr": "\u8fd1\u671f\uff0c\u5feb\u624b Kwaipilot \u56e2\u961f\u63a8\u51fa\u4e86 KAT \u7cfb\u5217\u4e24\u6b3e\u7a81\u7834\u6027 Agentic Coding \u5927\u6a21\u578b\uff1a\u5f00\u6e90 32B \u53c2\u6570\u6a21\u578b KAT-Dev-32B \u4e0e\u95ed\u6e90\u65d7\u8230\u6a21\u578b KAT-Coder\u3002\u8fd9\u4e24\u6b3e\u6a21\u578b\u5728 Code Intelligence \u9886\u57df\u5206\u522b\u4f53\u73b0\u51fa\u8f7b\u91cf\u7ea7\u7684\u8d85\u5f3a\u8868\u73b0\u548c\u6781\u81f4\u6027\u80fd\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.b553340f", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5MjMzNDk3OQ==&mid=2247484639&idx=1&sn=afeec6c454799a4885a7b54b886c0a57&chksm=ed8ff415a59602cb110c6cbb955841110f452f6ba6ba366742aba094be80da81e523ddfd817c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5MjMzNDk3OQ==&mid=2247484639&idx=1&sn=afeec6c454799a4885a7b54b886c0a57&chksm=ed8ff415a59602cb110c6cbb955841110f452f6ba6ba366742aba094be80da81e523ddfd817c#rd", "authors": ["\u9053\u54e5\u8bb2\u6280\u672f"], "title": "Perplexity AI\u7684\u5b8f\u5927\u68cb\u5c40\uff1a\u8d85\u8d8a\u7b54\u6848\u5f15\u64ce\uff0c\u6784\u5efa<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\uff08<em class=\"highlight\">Agentic</em>\uff09\u4e92\u8054\u7f51", "comment": "Source: WeChat, Published: 2025-09-26 04:02:31", "summary": "\u4e8c\u3001 \u4ee3\u7406\u5f0f\uff08Agentic\uff09\u7684\u672a\u6765\uff1a\u6d4f\u89c8\u5668\u662f\u65b0\u6218\u573a\u7684\u5165\u53e3\u5bf9\u8bdd\u7684\u6838\u5fc3\uff0c\u843d\u5728\u4e86\u5bf9\u4ee3\u7406\u5f0f\u4e92\u8054\u7f51\u7684\u6784\u60f3\u4e0a\u3002Dimitri\u9884\u89c1\uff0c\u672a\u6765\u7684\u7f51\u7edc\u4ea4\u4e92\u5c06\u66f4\u591a\u5730\u662f\u201c\u4ee3\u7406\u4e0e\u4ee3\u7406\u201d\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "AI": {"tldr": "\u4e8c\u3001 \u4ee3\u7406\u5f0f\uff08Agentic\uff09\u7684\u672a\u6765\uff1a\u6d4f\u89c8\u5668\u662f\u65b0\u6218\u573a\u7684\u5165\u53e3\u5bf9\u8bdd\u7684\u6838\u5fc3\uff0c\u843d\u5728\u4e86\u5bf9\u4ee3\u7406\u5f0f\u4e92\u8054\u7f51\u7684\u6784\u60f3\u4e0a\u3002Dimitri\u9884\u89c1\uff0c\u672a\u6765\u7684\u7f51\u7edc\u4ea4\u4e92\u5c06\u66f4\u591a\u5730\u662f\u201c\u4ee3\u7406\u4e0e\u4ee3\u7406\u201d\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.a05de467", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5Njg2OTQ2Mg==&mid=2247485170&idx=1&sn=ff52d30b7a0d366ddd673530e35b33b9&chksm=fff4fbabcc17bb7ca9a4508658c864e22ca941cd21214eaa9f4d18ae3f3bf37ab5c1bd1b65dc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5Njg2OTQ2Mg==&mid=2247485170&idx=1&sn=ff52d30b7a0d366ddd673530e35b33b9&chksm=fff4fbabcc17bb7ca9a4508658c864e22ca941cd21214eaa9f4d18ae3f3bf37ab5c1bd1b65dc#rd", "authors": ["Appier\u6c9b\u661f\u4e92\u52a8\u79d1\u6280"], "title": "Appier \u5168\u7ebf\u4ea7\u54c1\u5347\u7ea7 <em class=\"highlight\">Agentic</em> AI\uff0c\u6253\u9020\u65b0\u4e16\u4ee3 ROI \u9a71\u52a8\u89e3\u51b3\u65b9\u6848", "comment": "Source: WeChat, Published: 2025-09-26 04:01:00", "summary": "\u201cAgentic AI \u7684\u6838\u5fc3\uff0c\u5728\u4e8e\u628a\u4e1a\u52a1\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u81ea\u4e3b\u5e94\u7528\u4e0e\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002\u51ed\u501f Appier \u5728 AI \u4e0e\u8425\u9500\u79d1\u6280\u903e\u5341\u5e74\u7684\u6df1\u8015\u5b9e\u7ee9\uff0c\u6211\u4eec\u8ba9\u5404\u53f8\u5176\u804c\u7684 AI Agents \u76f8\u4e92\u534f\u4f5c\uff0c\u6253\u9020\u7aef\u5230\u7aef\u7684 Agentic \u751f\u6001\u7cfb\u7edf\u2014\u2014\u5982\u540c\u4e00\u652f\u9ad8\u5ea6\u534f\u4f5c\u7684\u4e13\u4e1a", "AI": {"tldr": "\u201cAgentic AI \u7684\u6838\u5fc3\uff0c\u5728\u4e8e\u628a\u4e1a\u52a1\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u81ea\u4e3b\u5e94\u7528\u4e0e\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002\u51ed\u501f Appier \u5728 AI \u4e0e\u8425\u9500\u79d1\u6280\u903e\u5341\u5e74\u7684\u6df1\u8015\u5b9e\u7ee9\uff0c\u6211\u4eec\u8ba9\u5404\u53f8\u5176\u804c\u7684 AI Agents \u76f8\u4e92\u534f\u4f5c\uff0c\u6253\u9020\u7aef\u5230\u7aef\u7684 Agentic \u751f\u6001\u7cfb\u7edf\u2014\u2014\u5982\u540c\u4e00\u652f\u9ad8\u5ea6\u534f\u4f5c\u7684\u4e13\u4e1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.6b8bf353", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4Mjg4NTQxMQ==&mid=2247547696&idx=1&sn=ec92ec099ae8ea8d20c2ab1ac790254f&chksm=ced836ea86b71c9ff9fc503e2bdb9acd16adef4289629a3b2b2246c2e7410e0e45e8df4f29e3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4Mjg4NTQxMQ==&mid=2247547696&idx=1&sn=ec92ec099ae8ea8d20c2ab1ac790254f&chksm=ced836ea86b71c9ff9fc503e2bdb9acd16adef4289629a3b2b2246c2e7410e0e45e8df4f29e3#rd", "authors": ["\u5927\u6a21\u578b\u4e4b\u5fc3Tech"], "title": "\u8fc8\u5411\u8d85\u7ea7\u4eba\u5de5\u667a\u80fd\u4e4b\u8def\uff01\u5927\u6a21\u578b\u65f6\u5e8f\u63a8\u7406\u548c<em class=\"highlight\">Agentic</em>\u7cfb\u7edf\u7684\u5168\u9762\u7efc\u8ff0\uff08UCLA\u6700\u65b0\uff09", "comment": "Source: WeChat, Published: 2025-09-26 00:00:00", "summary": "\u8054\u5408\u53d1\u5e03\u4e86\u4e00\u7bc7\u9898\u4e3a\u300aA Survey of Reasoning and Agentic Systems in Time Series with Large Language Models\u300b\u7684\u91cd\u78c5\u7efc\u8ff0\u3002\u8fd9\u7bc7\u7efc\u8ff0\u4e0d\u4ec5\u9996\u6b21\u4e3a\u201c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u201d\u7ed9\u51fa\u4e86\u6e05\u6670\u7684\u5b9a\u4e49\uff0c\u66f4\u6784\u5efa\u4e86\u4e00\u5957\u8986\u76d6\u201c\u63a8\u7406\u7ed3\u6784-\u4efb\u52a1\u76ee\u6807-\u6280\u672f\u7279\u5f81\u201d\u7684\u4e09\u7ef4\u5206\u7c7b\u6846", "AI": {"tldr": "\u8054\u5408\u53d1\u5e03\u4e86\u4e00\u7bc7\u9898\u4e3a\u300aA Survey of Reasoning and Agentic Systems in Time Series with Large Language Models\u300b\u7684\u91cd\u78c5\u7efc\u8ff0\u3002\u8fd9\u7bc7\u7efc\u8ff0\u4e0d\u4ec5\u9996\u6b21\u4e3a\u201c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u201d\u7ed9\u51fa\u4e86\u6e05\u6670\u7684\u5b9a\u4e49\uff0c\u66f4\u6784\u5efa\u4e86\u4e00\u5957\u8986\u76d6\u201c\u63a8\u7406\u7ed3\u6784-\u4efb\u52a1\u76ee\u6807-\u6280\u672f\u7279\u5f81\u201d\u7684\u4e09\u7ef4\u5206\u7c7b\u6846", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.519cf034", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTU3OTYwMw==&mid=2247483695&idx=1&sn=a1b9f3c2625976094f718ef5253522cc&chksm=fe45a21dcf57defcd809e8db4e1e761e2857e859a2a12ce7f6535a1556e63bc473784d3d89d6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTU3OTYwMw==&mid=2247483695&idx=1&sn=a1b9f3c2625976094f718ef5253522cc&chksm=fe45a21dcf57defcd809e8db4e1e761e2857e859a2a12ce7f6535a1556e63bc473784d3d89d6#rd", "authors": ["AI\u65f6\u4ee3\u7684\u5976\u7238"], "title": "\u4eceDeepSeek\u5230<em class=\"highlight\">Agentic</em> AI\uff1a\u98ce\u53e3\u5df2\u53d8\uff0c\u771f\u6b63\u7684\u673a\u4f1a\u5728\u201c\u610f\u56fe\u7ecf\u6d4e\u201d", "comment": "Source: WeChat, Published: 2025-09-25 15:45:05", "summary": "\u8d8b\u52bf\u80cc\u4e66\uff1aGartner\u5c06Agentic AI\u5217\u4e3a2025\u5341\u5927\u6218\u7565\u6280\u672f\u8d8b\u52bf\u4e4b\u9996\uff0c\u5e76\u9884\u6d4b\u52302028\u5e74\uff0c\u65e5\u5e38\u5de5\u4f5c\u51b3\u7b56\u768415%\u5c06\u7531\u667a\u80fd\u4f53\u81ea\u4e3b\u5b8c\u6210\u3002\u771f\u6b63\u7684\u5347\u7ea7\u4e0d\u662f\u66f4\u4f1a\u5bf9\u8bdd\uff0c\u800c\u662f\u66f4\u4f1a\u5c65\u7ea6\u3002", "AI": {"tldr": "\u8d8b\u52bf\u80cc\u4e66\uff1aGartner\u5c06Agentic AI\u5217\u4e3a2025\u5341\u5927\u6218\u7565\u6280\u672f\u8d8b\u52bf\u4e4b\u9996\uff0c\u5e76\u9884\u6d4b\u52302028\u5e74\uff0c\u65e5\u5e38\u5de5\u4f5c\u51b3\u7b56\u768415%\u5c06\u7531\u667a\u80fd\u4f53\u81ea\u4e3b\u5b8c\u6210\u3002\u771f\u6b63\u7684\u5347\u7ea7\u4e0d\u662f\u66f4\u4f1a\u5bf9\u8bdd\uff0c\u800c\u662f\u66f4\u4f1a\u5c65\u7ea6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.7a7b42b8", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxOTc4NzA5NA==&mid=2247491067&idx=1&sn=de0a6dc890263cab3a230f0d1562ac22&chksm=963886e7a76f07086df9d14649b98784225154d214cd188742f4ed2a7ee1d3de9df6fa7728cc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxOTc4NzA5NA==&mid=2247491067&idx=1&sn=de0a6dc890263cab3a230f0d1562ac22&chksm=963886e7a76f07086df9d14649b98784225154d214cd188742f4ed2a7ee1d3de9df6fa7728cc#rd", "authors": ["\u536b\u5065\u667a\u80fd"], "title": "\u89e3\u9501\u533b\u7597\u6570\u636e\u4ef7\u503c\uff1a\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u533b\u7597\u6570\u636e\u667a\u80fd\u6807\u6ce8\u667a\u80fd\u4f53", "comment": "Source: WeChat, Published: 2025-09-26 12:58:02", "summary": "\u5229\u7528\u5927\u6a21\u578b\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u80fd\u529b\uff0c\u5b9e\u73b0\u667a\u80fd\u672f\u8bed\u6620\u5c04\uff0c\u89e3\u51b3\u6570\u636e\u5b64\u5c9b\u548c\u6574\u5408\u96be\u9898\u3002\u6a21\u578b\u63d0\u793a\u8bcd\u914d\u7f6e \uff0c\u5df2\u5206\u6790 \u8be6\u60c5 \u7f16\u8f91 \u4fdd\u5b58 2 \u5e8f\u53f7 \u6807\u7b7e\u540d\u79f0 \u53c2\u8003\u503c \u6392\u5e8f\u72b6\u6001 \u64cd\u4f5c \u5df2\u5206 \u8be6\u60c5 \u7f16\u8f91 \u4fdd\u5b58\u7269\u7b7e 3 1 \u5de6\u4e3b\u5e72\uff08lm\uff09\u72ed\u7a84 \u6b63\u5e38\u8f7b\u5ea6\u72ed\u7a84", "AI": {"tldr": "\u5229\u7528\u5927\u6a21\u578b\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u80fd\u529b\uff0c\u5b9e\u73b0\u667a\u80fd\u672f\u8bed\u6620\u5c04\uff0c\u89e3\u51b3\u6570\u636e\u5b64\u5c9b\u548c\u6574\u5408\u96be\u9898\u3002\u6a21\u578b\u63d0\u793a\u8bcd\u914d\u7f6e \uff0c\u5df2\u5206\u6790 \u8be6\u60c5 \u7f16\u8f91 \u4fdd\u5b58 2 \u5e8f\u53f7 \u6807\u7b7e\u540d\u79f0 \u53c2\u8003\u503c \u6392\u5e8f\u72b6\u6001 \u64cd\u4f5c \u5df2\u5206 \u8be6\u60c5 \u7f16\u8f91 \u4fdd\u5b58\u7269\u7b7e 3 1 \u5de6\u4e3b\u5e72\uff08lm\uff09\u72ed\u7a84 \u6b63\u5e38\u8f7b\u5ea6\u72ed\u7a84", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1735e7a3", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1Nzk2MDQzOA==&mid=2247545928&idx=1&sn=bf65572440bcf10942f69c6e1026fd43&chksm=eb9101a779a4741760fe9b6fbc6cf25db314c9ef6bcbddce785fdc6646849ac65cac2b15ed61#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1Nzk2MDQzOA==&mid=2247545928&idx=1&sn=bf65572440bcf10942f69c6e1026fd43&chksm=eb9101a779a4741760fe9b6fbc6cf25db314c9ef6bcbddce785fdc6646849ac65cac2b15ed61#rd", "authors": ["\u8d35\u5dde\u7701\u5927\u6570\u636e\u53d1\u5c55\u7ba1\u7406\u5c40"], "title": "\u8d35\u5dde\u7701\u884c\u4e1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5178\u578b\u5e94\u7528\u6848\u4f8b\u4e4b\u4e09 | \u65e0\u76f8\u667a\u7814\u79d1\u7814<em class=\"highlight\">\u5927\u6a21\u578b</em>AI\u5e73\u53f0", "comment": "Source: WeChat, Published: 2025-09-26 11:29:51", "summary": "\u57fa\u4e8e\u5f00\u6e90\u5927\u6a21\u578b\uff0c\u5e73\u53f0\u8fdb\u884c\u9488\u5bf9\u6027\u7684\u5b9a\u5411\u4f18\u5316\u4e0e\u5fae\u8c03\uff0c\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u8de8\u5b66\u79d1\u79d1\u7814\u7684\u4e13\u4e1a\u5927\u6a21\u578b\u3002\u540c\u65f6\uff0c\u9488\u5bf9\u751f\u7269\u533b\u836f\uff08\u5982\u86cb\u767d\u8d28\u5206\u6790\uff09\u3001\u6750\u6599\u79d1\u5b66\uff08\u5982\u65b0\u6750\u6599\u8bbe\u8ba1\uff09\u7b49\u591a\u4e2a\u5782\u76f4\u9886\u57df\uff0c\u6253\u9020\u4e13\u7528\u6a21\u578b\u3002", "AI": {"tldr": "\u57fa\u4e8e\u5f00\u6e90\u5927\u6a21\u578b\uff0c\u5e73\u53f0\u8fdb\u884c\u9488\u5bf9\u6027\u7684\u5b9a\u5411\u4f18\u5316\u4e0e\u5fae\u8c03\uff0c\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u8de8\u5b66\u79d1\u79d1\u7814\u7684\u4e13\u4e1a\u5927\u6a21\u578b\u3002\u540c\u65f6\uff0c\u9488\u5bf9\u751f\u7269\u533b\u836f\uff08\u5982\u86cb\u767d\u8d28\u5206\u6790\uff09\u3001\u6750\u6599\u79d1\u5b66\uff08\u5982\u65b0\u6750\u6599\u8bbe\u8ba1\uff09\u7b49\u591a\u4e2a\u5782\u76f4\u9886\u57df\uff0c\u6253\u9020\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2509.e3ab4680", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxMTM2ODM4MQ==&mid=2247491310&idx=1&sn=ed1463dc722753a33c46e5af665d7943&chksm=961a63153095775bb1ba0fd9073b594f8fa9965c24da7ba1477c1e7cd16a700f43692fb59f68#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxMTM2ODM4MQ==&mid=2247491310&idx=1&sn=ed1463dc722753a33c46e5af665d7943&chksm=961a63153095775bb1ba0fd9073b594f8fa9965c24da7ba1477c1e7cd16a700f43692fb59f68#rd", "authors": ["\u6570\u5b57\u7ecf\u6d4e\u6295\u878d\u8d44\u8054\u76df"], "title": "\u5218\u77e5\u8fdc\u5206\u4eab\u300a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u6280\u672f\u4e0e\u6311\u6218\u300b", "comment": "Source: WeChat, Published: 2025-09-26 11:04:24", "summary": "\u6211\u4eec\u53ef\u4ee5\u8bbe\u60f3\uff0c\u628a\u5927\u6a21\u578b\u653e\u5230\u4e0d\u540c\u9886\u57df\uff0c\u6709\u53ef\u80fd\u662f\u7f16\u7a0b\u9886\u57df\u3001\u6570\u636e\u5e93\u9886\u57df\u3001\u7cfb\u7edf\u73af\u5883\u9886\u57df\uff0c\u4e5f\u6709\u53ef\u80fd\u662f\u7269\u7406\u73af\u5883\u9886\u57df\u6216\u5176\u4ed6\u5404\u79cd\u4e13\u4e1a\uff0c\u6211\u4eec\u9884\u671f\u8fd9\u4e2a\u5927\u6a21\u578b\u3001\u667a\u80fd\u4f53\u80fd\u6210\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u3001\u6570\u636e\u5206\u6790\u3001\u7cfb\u7edf\u89e6\u63a7\u3001\u673a\u68b0\u63a7\u5236\u7b49\u5404\u4e2a\u65b9\u9762\u4e13", "AI": {"tldr": "\u6211\u4eec\u53ef\u4ee5\u8bbe\u60f3\uff0c\u628a\u5927\u6a21\u578b\u653e\u5230\u4e0d\u540c\u9886\u57df\uff0c\u6709\u53ef\u80fd\u662f\u7f16\u7a0b\u9886\u57df\u3001\u6570\u636e\u5e93\u9886\u57df\u3001\u7cfb\u7edf\u73af\u5883\u9886\u57df\uff0c\u4e5f\u6709\u53ef\u80fd\u662f\u7269\u7406\u73af\u5883\u9886\u57df\u6216\u5176\u4ed6\u5404\u79cd\u4e13\u4e1a\uff0c\u6211\u4eec\u9884\u671f\u8fd9\u4e2a\u5927\u6a21\u578b\u3001\u667a\u80fd\u4f53\u80fd\u6210\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u3001\u6570\u636e\u5206\u6790\u3001\u7cfb\u7edf\u89e6\u63a7\u3001\u673a\u68b0\u63a7\u5236\u7b49\u5404\u4e2a\u65b9\u9762\u4e13", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.eda5c5d9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5Mzc5MDg2Mw==&mid=2658007418&idx=3&sn=453e47e41acec48881b9d06e709e4cb9&chksm=bc6e98c5ed00211b5d3362de38ce081fc59e99192ab8aa3b47715e34ab84a3d0dc289298f4a3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5Mzc5MDg2Mw==&mid=2658007418&idx=3&sn=453e47e41acec48881b9d06e709e4cb9&chksm=bc6e98c5ed00211b5d3362de38ce081fc59e99192ab8aa3b47715e34ab84a3d0dc289298f4a3#rd", "authors": ["\u77e5\u8bc6\u5c31\u662f\u529b\u91cf"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u662f\u5982\u4f55\u5b66\u4f1a\u6570\u6570\u7684\uff1f| \u79d1\u666e\u89c6\u542c", "comment": "Source: WeChat, Published: 2025-09-26 10:29:50", "summary": "\u4f60\u5e94\u8be5\u542c\u8bf4\u8fc7\uff0c\u73b0\u5728\u7684AI\uff08\u4eba\u5de5\u667a\u80fd\uff09\u5927\u6a21\u578b\u5f88\u5389\u5bb3\uff0c\u80fd\u5199\u6587\u7ae0\u3001\u4f5c\u8bd7\u3001\u7f16\u7a0b\u5e8f\uff0c\u751a\u81f3\u89e3\u7b54\u9ad8\u96be\u5ea6\u7684\u5965\u6570\u9898\u3002\u4f46\u5947\u602a\u7684\u662f\uff0c\u6709\u4e9b\u65f6\u5019\uff0c\u5b83\u5c45\u7136\u4f1a\u5728\u6700\u57fa\u7840\u7684\u5c0f\u5b66\u6570\u5b66\u9898\u4e0a\u72af\u9519\uff01", "AI": {"tldr": "\u4f60\u5e94\u8be5\u542c\u8bf4\u8fc7\uff0c\u73b0\u5728\u7684AI\uff08\u4eba\u5de5\u667a\u80fd\uff09\u5927\u6a21\u578b\u5f88\u5389\u5bb3\uff0c\u80fd\u5199\u6587\u7ae0\u3001\u4f5c\u8bd7\u3001\u7f16\u7a0b\u5e8f\uff0c\u751a\u81f3\u89e3\u7b54\u9ad8\u96be\u5ea6\u7684\u5965\u6570\u9898\u3002\u4f46\u5947\u602a\u7684\u662f\uff0c\u6709\u4e9b\u65f6\u5019\uff0c\u5b83\u5c45\u7136\u4f1a\u5728\u6700\u57fa\u7840\u7684\u5c0f\u5b66\u6570\u5b66\u9898\u4e0a\u72af\u9519\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.350ea68a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MzAwODUzOA==&mid=2651596324&idx=1&sn=02c0d25455bf445ce54f9b16c05283ad&chksm=f3c3e4e49b8bf9a2596c539c4cf29bd77c0e62e6dbd539edd1af17b6b199e8c6d433e91d7356#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MzAwODUzOA==&mid=2651596324&idx=1&sn=02c0d25455bf445ce54f9b16c05283ad&chksm=f3c3e4e49b8bf9a2596c539c4cf29bd77c0e62e6dbd539edd1af17b6b199e8c6d433e91d7356#rd", "authors": ["\u8f6f\u4ef6\u8c37\u521b\u65b0\u521b\u4e1a\u670d\u52a1\u4e2d\u5fc3"], "title": "\u8f6f\u4ef6\u8c37\u201c\u56fd\u5b57\u53f7\u201d<em class=\"highlight\">\u5927\u6a21\u578b</em>\u961f\u4f0d\u6269\u5bb9", "comment": "Source: WeChat, Published: 2025-09-26 10:11:23", "summary": "\u5927\u6a21\u578b\u4e3b\u8981\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4e3a\u6280\u672f\u5f00\u53d1\u8005\u53ca\u4f01\u4e1a\u7528\u6237\u63d0\u4f9b\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u3001\u4ee3\u7801\u4f18\u5316\u7b49\u670d\u52a1\uff0c\u5177\u5907\u8f6f\u4ef6\u7aef\u5230\u7aef\u81ea\u52a8\u5f00\u53d1\u7684\u80fd\u529b\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u4e0a\u5343\u6b3e\u5e94\u7528\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u4e3b\u8981\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4e3a\u6280\u672f\u5f00\u53d1\u8005\u53ca\u4f01\u4e1a\u7528\u6237\u63d0\u4f9b\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u3001\u4ee3\u7801\u4f18\u5316\u7b49\u670d\u52a1\uff0c\u5177\u5907\u8f6f\u4ef6\u7aef\u5230\u7aef\u81ea\u52a8\u5f00\u53d1\u7684\u80fd\u529b\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u4e0a\u5343\u6b3e\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.0ea84147", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498173&idx=1&sn=8e1227afbd8ab02a3b750bd9b17bed81&chksm=fde08dcbe3f1c331870d3591022e10b179b13b4efe5d067eacbf84af26ebd050d9225b6a930d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498173&idx=1&sn=8e1227afbd8ab02a3b750bd9b17bed81&chksm=fde08dcbe3f1c331870d3591022e10b179b13b4efe5d067eacbf84af26ebd050d9225b6a930d#rd", "authors": ["\u4e30\u519c\u4fe1\u606f"], "title": "TASE | \u519c\u4e1a\u591a\u6a21\u6001<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff1a\u73b0\u72b6\u3001\u6311\u6218\u4e0e\u672a\u6765\uff082025.09\u6700\u65b0\uff09", "comment": "Source: WeChat, Published: 2025-09-26 10:10:22", "summary": "\u519c\u4e1a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08mm-llms\uff09\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u3001\u5fae\u8c03\u4e0e\u4e0b\u6e38\u5e94\u7528 \u6838\u5fc3\u7ed3\u679c\u4e0e\u8ba8\u8bba \u7efc\u8ff0\u8868\u660e\uff0cMM-LLMs\u7684\u53d1\u5c55\u5df2\u4ece\u6700\u521d\u7684Transformer\u67b6\u6784\u6f14\u5316\u81f3\u4eca\uff0c\u50ac\u751f\u4e86\u5982OpenAI\u7684GPT\u7cfb\u5217\u3001Meta\u7684Llama\u7cfb\u5217\u3001DeepSeek\u7b49\u4f17\u591a\u5f3a\u5927\u7684\u6a21\u578b\u3002", "AI": {"tldr": "\u519c\u4e1a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08mm-llms\uff09\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u3001\u5fae\u8c03\u4e0e\u4e0b\u6e38\u5e94\u7528 \u6838\u5fc3\u7ed3\u679c\u4e0e\u8ba8\u8bba \u7efc\u8ff0\u8868\u660e\uff0cMM-LLMs\u7684\u53d1\u5c55\u5df2\u4ece\u6700\u521d\u7684Transformer\u67b6\u6784\u6f14\u5316\u81f3\u4eca\uff0c\u50ac\u751f\u4e86\u5982OpenAI\u7684GPT\u7cfb\u5217\u3001Meta\u7684Llama\u7cfb\u5217\u3001DeepSeek\u7b49\u4f17\u591a\u5f3a\u5927\u7684\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
