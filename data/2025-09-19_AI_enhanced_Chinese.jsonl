{"id": "2509.14252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14252", "abs": "https://arxiv.org/abs/2509.14252", "authors": ["Hai Huang", "Yann LeCun", "Randall Balestriero"], "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures", "comment": null, "summary": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLM-JEPA\uff0c\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u8bad\u7ec3\u76ee\u6807\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8f93\u5165\u7a7a\u95f4\u91cd\u6784\u548c\u751f\u6210\u80fd\u529b\uff0c\u800c\u89c6\u89c9\u9886\u57df\u5df2\u7ecf\u8bc1\u660e\u5d4c\u5165\u7a7a\u95f4\u8bad\u7ec3\u76ee\u6807\uff08\u5982JEPA\uff09\u8fdc\u4f18\u4e8e\u8f93\u5165\u7a7a\u95f4\u65b9\u6cd5\u3002\u8fd9\u79cd\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e0d\u5339\u914d\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u8bed\u8a00\u8bad\u7ec3\u80fd\u5426\u4ece\u89c6\u89c9\u65b9\u6cd5\u4e2d\u501f\u9274\u7ecf\u9a8c\u3002", "method": "\u5f00\u53d1\u4e86LLM-JEPA\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5d4c\u5165\u7a7a\u95f4\u8bad\u7ec3\u76ee\u6807\u800c\u975e\u4f20\u7edf\u7684\u8f93\u5165\u7a7a\u95f4\u91cd\u6784\u65b9\u6cd5\u3002", "result": "LLM-JEPA\u5728\u591a\u4e2a\u6a21\u578b\uff08Llama3\u3001OpenELM\u3001Gemma2\u3001Olmo\uff09\u548c\u6570\u636e\u96c6\uff08NL-RX\u3001GSM8K\u3001Spider\u3001RottenTomatoes\uff09\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u8bad\u7ec3\u76ee\u6807\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u5bf9\u8fc7\u62df\u5408\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5d4c\u5165\u7a7a\u95f4\u8bad\u7ec3\u76ee\u6807\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cLLM-JEPA\u4e3a\u8bed\u8a00\u8bad\u7ec3\u65b9\u6cd5\u4ece\u89c6\u89c9\u9886\u57df\u501f\u9274\u7ecf\u9a8c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7b2c\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.14265", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14265", "abs": "https://arxiv.org/abs/2509.14265", "authors": ["Siyuan Chen", "Zhichao Lu", "Qingfu Zhang"], "title": "Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models", "comment": "Technical report", "summary": "Automated kernel design is critical for overcoming software ecosystem\nbarriers in emerging hardware platforms like RISC-V. While large language\nmodels (LLMs) have shown promise for automated kernel optimization,\ndemonstrating success in CUDA domains with comprehensive technical documents\nand mature codebases, their effectiveness remains unproven for reference-scarce\ndomains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based\nevolutionary program search framework that automates kernel design for domains\nwith limited reference material. EoK mitigates reference scarcity by mining and\nformalizing reusable optimization ideas (general design principles + actionable\nthoughts) from established kernel libraries' development histories; it then\nguides parallel LLM explorations using these ideas, enriched via\nRetrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing\nhistorically effective techniques. Empirically, EoK achieves a median 1.27x\nspeedup, surpassing human experts on all 80 evaluated kernel design tasks and\nimproving upon prior LLM-based automated kernel design methods by 20%. These\nresults underscore the viability of incorporating human experience into\nemerging domains and highlight the immense potential of LLM-based automated\nkernel optimization.", "AI": {"tldr": "EoK\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8fdb\u5316\u7a0b\u5e8f\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6210\u719f\u5185\u6838\u5e93\u5f00\u53d1\u5386\u53f2\u4e2d\u6316\u6398\u53ef\u91cd\u7528\u4f18\u5316\u7406\u5ff5\uff0c\u4e3aRISC-V\u7b49\u53c2\u8003\u7a00\u7f3a\u9886\u57df\u81ea\u52a8\u5316\u5185\u6838\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e861.27\u500d\u4e2d\u503c\u52a0\u901f\uff0c\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u548c\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728RISC-V\u7b49\u53c2\u8003\u7a00\u7f3a\u786c\u4ef6\u5e73\u53f0\u4e0a\u81ea\u52a8\u5316\u5185\u6838\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u73b0\u6709LLM\u65b9\u6cd5\u5728CUDA\u7b49\u6587\u6863\u4e30\u5bcc\u7684\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u53c2\u8003\u7a00\u7f3a\u9886\u57df\u6548\u679c\u672a\u7ecf\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faEoK\u6846\u67b6\uff1a1\uff09\u4ece\u6210\u719f\u5185\u6838\u5e93\u5f00\u53d1\u5386\u53f2\u4e2d\u6316\u6398\u53ef\u91cd\u7528\u4f18\u5316\u7406\u5ff5\uff08\u901a\u7528\u8bbe\u8ba1\u539f\u5219+\u53ef\u64cd\u4f5c\u601d\u8def\uff09\uff1b2\uff09\u4f7f\u7528RAG\u589e\u5f3aRISC-V\u7279\u5b9a\u4e0a\u4e0b\u6587\uff1b3\uff09\u57fa\u4e8e\u5386\u53f2\u6709\u6548\u6280\u672f\u6307\u5bfc\u5e76\u884cLLM\u63a2\u7d22\u7684\u8fdb\u5316\u7a0b\u5e8f\u641c\u7d22\u3002", "result": "\u572880\u4e2a\u5185\u6838\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e2d\u503c1.27\u500d\u52a0\u901f\uff0c\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u6bd4\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5185\u6838\u8bbe\u8ba1\u65b9\u6cd5\u63d0\u534720%\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5c06\u4eba\u7c7b\u7ecf\u9a8c\u878d\u5165\u65b0\u5174\u9886\u57df\u7684\u53ef\u884c\u6027\uff0c\u51f8\u663e\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5185\u6838\u4f18\u5316\u7684\u5de8\u5927\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2509.14273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14273", "abs": "https://arxiv.org/abs/2509.14273", "authors": ["Swapnil Sharma Sarker", "Tanzina Taher Ifty"], "title": "Automated and Context-Aware Code Documentation Leveraging Advanced LLMs", "comment": null, "summary": "Code documentation is essential to improve software maintainability and\ncomprehension. The tedious nature of manual code documentation has led to much\nresearch on automated documentation generation. Existing automated approaches\nprimarily focused on code summarization, leaving a gap in template-based\ndocumentation generation (e.g., Javadoc), particularly with publicly available\nLarge Language Models (LLMs). Furthermore, progress in this area has been\nhindered by the lack of a Javadoc-specific dataset that incorporates modern\nlanguage features, provides broad framework/library coverage, and includes\nnecessary contextual information. This study aims to address these gaps by\ndeveloping a tailored dataset and assessing the capabilities of publicly\navailable LLMs for context-aware, template-based Javadoc generation. In this\nwork, we present a novel, context-aware dataset for Javadoc generation that\nincludes critical structural and semantic information from modern Java\ncodebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,\nPhi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and\nprovide a comparative analysis of their performance. Our results demonstrate\nthat LLaMA 3.1 performs consistently well and is a reliable candidate for\npractical, automated Javadoc generation, offering a viable alternative to\nproprietary systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684Javadoc\u751f\u6210\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e865\u4e2a\u5f00\u6e90LLM\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0LLaMA 3.1\u5728\u81ea\u52a8\u5316Javadoc\u751f\u6210\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u6587\u6863\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u6458\u8981\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6a21\u677f\u5316\u6587\u6863\u751f\u6210\uff08\u5982Javadoc\uff09\u7684\u7814\u7a76\uff0c\u4e14\u7f3a\u4e4f\u5305\u542b\u73b0\u4ee3\u8bed\u8a00\u7279\u6027\u548c\u5fc5\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4e13\u7528\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u73b0\u4ee3Java\u4ee3\u7801\u5e93\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e865\u4e2a\u5f00\u6e90LLM\uff08LLaMA-3.1\u3001Gemma-2\u3001Phi-3\u3001Mistral\u3001Qwen-2.5\uff09\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "LLaMA 3.1\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e00\u81f4\u4f18\u79c0\uff0c\u662f\u81ea\u52a8\u5316Javadoc\u751f\u6210\u7684\u53ef\u9760\u9009\u62e9\uff0c\u4e3a\u4e13\u6709\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5f00\u6e90LLM\u7279\u522b\u662fLLaMA 3.1\u5728\u6a21\u677f\u5316\u4ee3\u7801\u6587\u6863\u751f\u6210\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u8f6f\u4ef6\u7ef4\u62a4\u548c\u7406\u89e3\u3002", "topic": "swe application"}}
{"id": "2509.14289", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14289", "abs": "https://arxiv.org/abs/2509.14289", "authors": ["Lanxiao Huang", "Daksh Dave", "Ming Jin", "Tyler Cody", "Peter Beling"], "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks.", "AI": {"tldr": "\u5bf9\u591a\u79cdLLM\u4ee3\u7406\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u901a\u8fc7\u4e94\u79cd\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5757\u5316\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u8bc4\u4f30LLM\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u660e\u786e\u4e0d\u540c\u653b\u51fb\u9636\u6bb5\u7684\u6027\u80fd\u8868\u73b0\u548c\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f", "method": "\u5728\u771f\u5b9e\u6e17\u900f\u6d4b\u8bd5\u573a\u666f\u4e2d\u8bc4\u4f30\u591a\u79cdLLM\u4ee3\u7406\u67b6\u6784\uff0c\u4ece\u5355\u4ee3\u7406\u5230\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u9488\u5bf9\u6027\u589e\u5f3a\u529f\u80fd\uff08GCM\u3001IAM\u3001CCI\u3001AP\u3001RTM\uff09\u6765\u9694\u79bb\u5404\u529f\u80fd\u7684\u5f71\u54cd", "result": "\u867d\u7136\u67d0\u4e9b\u67b6\u6784\u5929\u7136\u5177\u5907\u90e8\u5206\u529f\u80fd\u7279\u6027\uff0c\u4f46\u9488\u5bf9\u6027\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5757\u5316\u4ee3\u7406\u5728\u590d\u6742\u3001\u591a\u6b65\u9aa4\u548c\u5b9e\u65f6\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6a21\u5757\u5316\u8bbe\u8ba1\u914d\u5408\u529f\u80fd\u589e\u5f3a\u6548\u679c\u663e\u8457", "topic": "agent analysis"}}
{"id": "2509.14279", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14279", "abs": "https://arxiv.org/abs/2509.14279", "authors": ["Robert Tjarko Lange", "Qi Sun", "Aaditya Prasad", "Maxence Faldor", "Yujin Tang", "David Ha"], "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization", "comment": "62 pages, 10 figures", "summary": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86robust-kbench\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u52a8\u5316CUDA\u5185\u6838\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5c06PyTorch\u4ee3\u7801\u8f6c\u6362\u4e3aCUDA\u5185\u6838\u5e76\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9ad8\u5c42\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u5e95\u5c42CUDA\u5185\u6838\u4f18\u5316\u7684\u5173\u6ce8\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u53ef\u88ab\u5229\u7528\u7684\u6f0f\u6d1e\u548c\u6d4b\u8bd5\u6761\u4ef6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86robust-kbench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6784\u5efa\u4e86\u81ea\u52a8\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ecPyTorch\u5230CUDA\u7684\u8f6c\u6362\u3001\u57fa\u4e8e\u8fdb\u5316\u5143\u751f\u6210\u7684\u8fd0\u884c\u65f6\u4f18\u5316\u3001\u4ee5\u53caLLM\u9a8c\u8bc1\u5668\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u3002", "result": "\u5728robust-kbench\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u751f\u6210\u7684CUDA\u5185\u6838\u6027\u80fd\u4f18\u4e8etorch\u5b9e\u73b0\uff0c\u80fd\u591f\u878d\u5408\u64cd\u4f5c\u5e76\u90e8\u7f72\u591a\u79cd\u8fd0\u884c\u65f6\u4f18\u5316\u7b56\u7565\uff0c\u9a8c\u8bc1\u5668\u80fd\u51c6\u786e\u5206\u7c7b\u9519\u8bef\u5185\u6838\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86CUDA\u5185\u6838\u7684\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u4f18\u5316\uff0c\u4e3a\u786c\u4ef6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86LLM\u5728\u5e95\u5c42\u4ee3\u7801\u4f18\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2509.14382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14382", "abs": "https://arxiv.org/abs/2509.14382", "authors": ["Daniel R\u00f6der", "Akhil Juneja", "Roland Roller", "Sven Schmeier"], "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06Web\u667a\u80fd\u4f53\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u9636\u6bb5\u8fdb\u884c\u8be6\u7ec6\u9519\u8bef\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u6307\u6807\u9057\u6f0f\u7684\u53ef\u64cd\u4f5c\u5f31\u70b9\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u6210\u529f\u7387\u800c\u5ffd\u7565\u4e2d\u95f4\u9519\u8bef\uff0c\u9650\u5236\u4e86\u6545\u969c\u6a21\u5f0f\u6d1e\u5bdf\u5e76\u963b\u788d\u7cfb\u7edf\u6539\u8fdb\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u9636\u6bb5\uff0c\u4ee5SeeAct\u6846\u67b6\u548cMind2Web\u6570\u636e\u96c6\u4e3a\u6848\u4f8b\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u6807\u51c6\u6307\u6807\u9057\u6f0f\u7684\u53ef\u64cd\u4f5c\u5f31\u70b9\uff0c\u4e3a\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684Web\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u6a21\u5757\u5316\u9519\u8bef\u5206\u6790\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u6545\u969c\u6a21\u5f0f\u6d1e\u5bdf\uff0c\u6709\u52a9\u4e8eWeb\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u6027\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2509.14281", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14281", "abs": "https://arxiv.org/abs/2509.14281", "authors": ["Xifeng Yao", "Dongyu Lang", "Wu Zhang", "Xintong Guo", "Huarui Xie", "Yinhao Ni", "Ping Liu", "Guang Shen", "Yi Bai", "Dandan Tu", "Changzheng Zhang"], "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems", "comment": null, "summary": "Significant advancements have been made in the capabilities of code large\nlanguage models, leading to their rapid adoption and application across a wide\nrange of domains. However, their further advancements are often constrained by\nthe scarcity of real-world coding problems. To bridge this gap, we propose a\nnovel framework for synthesizing code problems that emulate authentic\nreal-world scenarios. This framework systematically integrates domain\nknowledge, domain skills, and coding skills, all of which are meticulously\nextracted from real-world programming-related datasets, including Stack\nOverflow and Kaggle. The extracted elements serve as the foundational building\nblocks for constructing code problems. To align the generated problems with\npractical applications, application scenarios are also mined from the\naforementioned datasets. These scenarios are then utilized to construct a\nscenario-centric graph that interconnects domain knowledge, domain skills, and\ncoding skills. Based on this structured representation, a sampling strategy on\nthe graph is designed, which effectively controls the generation of a code\nproblem with complexity and diversity, reflects real-world challenges.\nExperimental results demonstrate that the proposed method consistently achieves\nsuperior performance over state-of-the-art open-source large language models of\nvarying sizes and functionalities, including both coders and general-purpose\nmodels, across a diverse set of real-world benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u7f16\u7a0b\u6570\u636e\u96c6\uff08Stack Overflow\u548cKaggle\uff09\u5408\u6210\u4ee3\u7801\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u9886\u57df\u6280\u80fd\u548c\u7f16\u7a0b\u6280\u80fd\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7f16\u7a0b\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u53d7\u5230\u771f\u5b9e\u4e16\u754c\u7f16\u7a0b\u95ee\u9898\u7a00\u7f3a\u6027\u7684\u9650\u5236\uff0c\u9700\u8981\u521b\u5efa\u80fd\u591f\u53cd\u6620\u5b9e\u9645\u6311\u6218\u7684\u591a\u6837\u5316\u4ee3\u7801\u95ee\u9898\u3002", "method": "\u4ece\u771f\u5b9e\u7f16\u7a0b\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u9886\u57df\u77e5\u8bc6\u3001\u9886\u57df\u6280\u80fd\u548c\u7f16\u7a0b\u6280\u80fd\uff0c\u6784\u5efa\u573a\u666f\u4e2d\u5fc3\u56fe\uff0c\u5e76\u8bbe\u8ba1\u56fe\u91c7\u6837\u7b56\u7565\u6765\u63a7\u5236\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u529f\u80fd\u7684\u6700\u5148\u8fdb\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6311\u6218\u7684\u4ee3\u7801\u95ee\u9898\uff0c\u4e3a\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "topic": "swe benchmark"}}
{"id": "2509.14257", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14257", "abs": "https://arxiv.org/abs/2509.14257", "authors": ["Yuanjie Lyu", "Chengyu Wang", "Jun Huang", "Tong Xu"], "title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents", "comment": null, "summary": "Large Language Model agents excel at solving complex tasks through iterative\nreasoning and tool use, but typically depend on ultra-large, costly backbones.\nExisting distillation approaches train smaller students to imitate full teacher\ntrajectories, yet reasoning and knowledge gaps between the teacher and student\noften lead to compounding errors. We propose SCoRe, a student-centered\nframework in which the student generates trajectories and the teacher\nintervenes only at the first critical error, producing training data matched to\nthe student's ability and exposing specific weaknesses. The student is first\nfine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement\nlearning starts from the verified prefix before the first critical error, with\ntarget rewards assigned at that step. This design encourages autonomous\nproblem-solving beyond imitation and improves training stability. Particularly,\non 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe\nmatches the agentic performance of a 72B-parameter teacher.", "AI": {"tldr": "SCoRe\u662f\u4e00\u4e2a\u5b66\u751f\u4e2d\u5fc3\u7684\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u53ea\u5728\u5173\u952e\u9519\u8bef\u65f6\u5e72\u9884\u6765\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f7B\u53c2\u6570\u5b66\u751f\u6a21\u578b\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523072B\u6559\u5e08\u6a21\u578b\u7684\u4ee3\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u8bad\u7ec3\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u6a21\u4eff\u5b8c\u6574\u6559\u5e08\u8f68\u8ff9\uff0c\u4f46\u5e08\u751f\u95f4\u7684\u63a8\u7406\u548c\u77e5\u8bc6\u5dee\u8ddd\u4f1a\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u84b8\u998f\u65b9\u6cd5\u6765\u7f29\u5c0f\u6a21\u578b\u89c4\u6a21\u5dee\u8ddd", "method": "\u5b66\u751f\u751f\u6210\u8f68\u8ff9\uff0c\u6559\u5e08\u53ea\u5728\u7b2c\u4e00\u4e2a\u5173\u952e\u9519\u8bef\u65f6\u5e72\u9884\uff0c\u751f\u6210\u5339\u914d\u5b66\u751f\u80fd\u529b\u7684\u6570\u636e\u3002\u5148\u5fae\u8c03\u4fee\u6b63\u8f68\u8ff9\uff0c\u7136\u540e\u4ece\u9a8c\u8bc1\u524d\u7f00\u5f00\u59cb\u77ed\u89c6\u8ddd\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5173\u952e\u9519\u8bef\u6b65\u9aa4\u5206\u914d\u76ee\u6807\u5956\u52b1", "result": "\u572812\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528SCoRe\u84b8\u998f\u76847B\u53c2\u6570\u5b66\u751f\u6a21\u578b\u5339\u914d\u4e8672B\u53c2\u6570\u6559\u5e08\u6a21\u578b\u7684\u4ee3\u7406\u6027\u80fd", "conclusion": "SCoRe\u6846\u67b6\u901a\u8fc7\u5b66\u751f\u4e2d\u5fc3\u7684\u5e72\u9884\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\u6548\u679c", "topic": "agent analysis"}}
{"id": "2509.14347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14347", "abs": "https://arxiv.org/abs/2509.14347", "authors": ["Henri A\u00efdasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI", "comment": "17 pages, 7 figures", "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.", "AI": {"tldr": "\u5bf9142,387\u4e2a\u5de5\u4e1a\u9879\u76eeCI\u4f5c\u4e1a\u7684\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c11%\u7684\u6210\u529f\u4f5c\u4e1a\u88ab\u91cd\u65b0\u8fd0\u884c\uff0c\u5176\u4e2d35%\u572824\u5c0f\u65f6\u540e\u91cd\u8dd1\uff0c\u63ed\u793a\u4e86\u9759\u9ed8\u5931\u8d25\uff08\u6210\u529f\u6807\u8bb0\u4f46\u5b9e\u9645\u5931\u8d25\uff09\u7684\u4e25\u91cd\u95ee\u9898\u3002", "motivation": "CI\u6784\u5efa\u7ed3\u679c\u7684\u53ef\u9760\u6027\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9759\u9ed8\u5931\u8d25\uff08\u4f5c\u4e1a\u6807\u8bb0\u6210\u529f\u4f46\u5b9e\u9645\u672a\u5b8c\u6210\u6240\u6709\u4efb\u52a1\uff09\u5f80\u5f80\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4bug\u8fdb\u5165\u751f\u4ea7\u73af\u5883\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5bf981\u4e2a\u5de5\u4e1a\u9879\u76ee\u7684142,387\u4e2a\u4f5c\u4e1a\u8fdb\u884c\u5206\u6790\uff0c\u4f7f\u7528\u6df7\u5408\u6548\u5e94\u6a21\u578b\u8bc4\u4f3032\u4e2a\u81ea\u53d8\u91cf\uff08AUC 85%\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u679092\u4e2a\u516c\u5f00\u95ee\u9898\u6765\u8bc6\u522b\u9759\u9ed8\u5931\u8d25\u7c7b\u522b\u3002", "result": "\u53d1\u73b011%\u7684\u6210\u529f\u4f5c\u4e1a\u88ab\u91cd\u65b0\u8fd0\u884c\uff0c\u8bc6\u522b\u51fa\u6d4b\u8bd5\u548c\u9759\u6001\u5206\u6790\u4efb\u52a1\u3001Shell\u811a\u672c\u8bed\u8a00\u4ee5\u53ca\u5f00\u53d1\u8005\u91cd\u8dd1\u503e\u5411\u662f\u5173\u952e\u56e0\u7d20\uff0c\u603b\u7ed3\u51fa11\u7c7b\u9759\u9ed8\u5931\u8d25\uff0c\u6700\u5e38\u89c1\u7684\u662f\u5de5\u4ef6\u64cd\u4f5c\u9519\u8bef\u3001\u7f13\u5b58\u9519\u8bef\u548c\u5ffd\u7565\u9000\u51fa\u7801\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86CI\u4e2d\u9759\u9ed8\u5931\u8d25\u7684\u666e\u904d\u6027\u548c\u4e25\u91cd\u6027\uff0c\u4e3a\u63d0\u9ad8CI\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2509.14373", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14373", "abs": "https://arxiv.org/abs/2509.14373", "authors": ["Huy Le", "Phong Nguyen", "Hao Do", "Tuan Nguyen", "Thien Pham", "Anh Nguyen-Duc", "Tho Quan"], "title": "CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning", "comment": null, "summary": "Context: Automated code generation using Foundation Models (FMs) offers\npromising solutions for enhancing software development efficiency. However,\nchallenges remain in ensuring domain specificity, cost-effectiveness, and\nsecurity - especially when relying on third-party APIs. This paper introduces\nCodeLSI, a framework that combines low-rank optimization and domain-specific\ninstruction tuning to address these challenges.\n  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel\napproach for generating high-quality code tailored to specific domains, using\nFMs fine-tuned on company infrastructure without dependence on external APIs.\n  Methods: CodeLSI applies low-rank adaptation techniques to reduce the\ncomputational cost of model pre-training and fine-tuning. Domain-specific\ninstruction tuning is employed to align code generation with organizational\nneeds. We implemented and tested the framework on real-world JavaScript coding\ntasks using datasets drawn from internal software projects.\n  Results: Experimental evaluations show that CodeLSI produces high-quality,\ncontext aware code. It outperforms baseline models in terms of relevance,\naccuracy, and domain fit. The use of low-rank optimization significantly\nreduced resource requirements, enabling scalable training on company-owned\ninfrastructure.\n  Conclusion: CodeLSI demonstrates that combining low-rank optimization with\ndomain specific tuning can enhance the practicality and performance of FMs for\nautomated code generation. This approach provides a secure, cost-efficient\nalternative to commercial API based solutions and supports faster, more\ntargeted innovation in software development.", "AI": {"tldr": "CodeLSI\u662f\u4e00\u4e2a\u7ed3\u5408\u4f4e\u79e9\u4f18\u5316\u548c\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u79c1\u6709\u57fa\u7840\u8bbe\u65bd\u4e0a\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9886\u57df\u7279\u5b9a\u7684\u4ee3\u7801\uff0c\u907f\u514d\u4f9d\u8d56\u7b2c\u4e09\u65b9API\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u5728\u9886\u57df\u7279\u5f02\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u7b2c\u4e09\u65b9API\u65f6\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5e94\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u964d\u4f4e\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u91c7\u7528\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u4f7f\u4ee3\u7801\u751f\u6210\u4e0e\u7ec4\u7ec7\u9700\u6c42\u5bf9\u9f50\uff0c\u5728\u771f\u5b9eJavaScript\u7f16\u7801\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "CodeLSI\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\uff0c\u5728\u76f8\u5173\u6027\u3001\u51c6\u786e\u6027\u548c\u9886\u57df\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f4e\u79e9\u4f18\u5316\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u4f4e\u79e9\u4f18\u5316\u4e0e\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u76f8\u7ed3\u5408\u53ef\u4ee5\u589e\u5f3a\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\uff0c\u63d0\u4f9b\u5b89\u5168\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2509.14547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u9a8c\u52a8\u6001\u6846\u67b6\u7528\u4e8e\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\uff0c\u901a\u8fc7Q-table\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7a7a\u95f4\uff0c\u7ed3\u5408\u4efb\u52a1\u8fdb\u5ea6\u8bc4\u4f30\u548c\u5148\u9a8c\u51b3\u7b56\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u53474.05%\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u6210\u672c\u964d\u4f4e\u81f330.68%-48.31%\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5386\u53f2\u7ecf\u9a8c\uff0c\u5728\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u80fd\u591f\u7075\u6d3b\u54cd\u5e94\u6bcf\u4e2a\u4efb\u52a1\u72ec\u7279\u7279\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Q-table\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7a7a\u95f4\uff0c\u7ed3\u5408\u4efb\u52a1\u8fdb\u5ea6\u8bc4\u4f30\u548c\u5148\u9a8c\u51b3\u7b56\u673a\u5236\uff0c\u5e76\u5f15\u5165\u51b7\u542f\u52a8\u521d\u59cb\u5316\u3001\u65e9\u505c\u548c\u526a\u679d\u7b49\u6280\u672f\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u53474.05%\uff0c\u540c\u65f6\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u548c\u63a8\u7406\u6210\u672c\u964d\u81f3\u73b0\u6709\u65b9\u6cd5\u768430.68%-48.31%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u7ecf\u9a8c\u548c\u4efb\u52a1\u7279\u6027\u54cd\u5e94\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\u3002", "topic": "agent analysis"}}
{"id": "2509.14626", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14626", "abs": "https://arxiv.org/abs/2509.14626", "authors": ["Feiran Qin", "M. M. Abid Naziri", "Hengyu Ai", "Saikat Dutta", "Marcelo d'Amorim"], "title": "Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs", "comment": null, "summary": "Deep Learning (DL) libraries such as PyTorch provide the core components to\nbuild major AI-enabled applications. Finding bugs in these libraries is\nimportant and challenging. Prior approaches have tackled this by performing\neither API-level fuzzing or model-level fuzzing, but they do not use coverage\nguidance, which limits their effectiveness and efficiency. This raises an\nintriguing question: can coverage guided fuzzing (CGF), in particular\nframeworks like LibFuzzer, be effectively applied to DL libraries, and does it\noffer meaningful improvements in code coverage, bug detection, and scalability\ncompared to prior methods?\n  We present the first in-depth study to answer this question. A key challenge\nin applying CGF to DL libraries is the need to create a test harness for each\nAPI that can transform byte-level fuzzer inputs into valid API inputs. To\naddress this, we propose FlashFuzz, a technique that leverages Large Language\nModels (LLMs) to automatically synthesize API-level harnesses by combining\ntemplates, helper functions, and API documentation. FlashFuzz uses a feedback\ndriven strategy to iteratively synthesize and repair harnesses. With this\napproach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow\nAPIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and\nTitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage\nand 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x\nspeedups in input generation. FlashFuzz has discovered 42 previously unknown\nbugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study\nconfirms that CGF can be effectively applied to DL libraries and provides a\nstrong baseline for future testing approaches.", "AI": {"tldr": "FlashFuzz\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5e93\u7684\u8986\u76d6\u7387\u5f15\u5bfc\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\uff0c\u5229\u7528LLM\u81ea\u52a8\u5408\u6210API\u7ea7\u6d4b\u8bd5harness\uff0c\u663e\u8457\u63d0\u9ad8\u4e86PyTorch\u548cTensorFlow\u7684\u4ee3\u7801\u8986\u76d6\u7387\u548cbug\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u8986\u76d6\u7387\u5f15\u5bfc\uff0c\u9650\u5236\u4e86\u6d4b\u8bd5\u6548\u679c\u548c\u6548\u7387\u3002\u7814\u7a76\u63a2\u7d22\u80fd\u5426\u5c06\u8986\u76d6\u7387\u5f15\u5bfc\u6a21\u7cca\u6d4b\u8bd5(CGF)\u6709\u6548\u5e94\u7528\u4e8eDL\u5e93\uff0c\u5e76\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4ee3\u7801\u8986\u76d6\u7387\u3001bug\u68c0\u6d4b\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6539\u8fdb\u3002", "method": "\u63d0\u51faFlashFuzz\u6280\u672f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u81ea\u52a8\u5408\u6210API\u7ea7\u6d4b\u8bd5harness\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u677f\u3001\u8f85\u52a9\u51fd\u6570\u548cAPI\u6587\u6863\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u7b56\u7565\u8fed\u4ee3\u5408\u6210\u548c\u4fee\u590dharness\u3002", "result": "\u4e3a1,151\u4e2aPyTorch\u548c662\u4e2aTensorFlow API\u5408\u6210harness\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5(ACETest\u3001PathFinder\u3001TitanFuzz)\u5b9e\u73b0101.13-212.88%\u66f4\u9ad8\u7684\u8986\u76d6\u7387\uff0c1.0x-5.4x\u66f4\u9ad8\u7684\u6709\u6548\u6027\u7387\uff0c1x-1182x\u7684\u8f93\u5165\u751f\u6210\u52a0\u901f\u3002\u53d1\u73b042\u4e2a\u672a\u77e5bug\uff0c\u5176\u4e2d8\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9eCGF\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8eDL\u5e93\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6d4b\u8bd5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u3002", "topic": "swe_application"}}
{"id": "2509.14647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14647", "abs": "https://arxiv.org/abs/2509.14647", "authors": ["NVJK Kartik", "Garvit Sapra", "Rishav Hada", "Nikhil Pareek"], "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production", "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) in automating\ncomplex, multi-agent workflows, organizations face mounting risks from errors,\nemergent behaviors, and systemic failures that current evaluation methods fail\nto capture. We present AgentCompass, the first evaluation framework designed\nspecifically for post-deployment monitoring and debugging of agentic workflows.\nAgentCompass models the reasoning process of expert debuggers through a\nstructured, multi-stage analytical pipeline: error identification and\ncategorization, thematic clustering, quantitative scoring, and strategic\nsummarization. The framework is further enhanced with a dual memory\nsystem-episodic and semantic-that enables continual learning across executions.\nThrough collaborations with design partners, we demonstrate the framework's\npractical utility on real-world deployments, before establishing its efficacy\nagainst the publicly available TRAIL benchmark. AgentCompass achieves\nstate-of-the-art results on key metrics, while uncovering critical issues\nmissed in human annotations, underscoring its role as a robust,\ndeveloper-centric tool for reliable monitoring and improvement of agentic\nsystems in production.", "AI": {"tldr": "AgentCompass\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u7684\u540e\u90e8\u7f72\u76d1\u63a7\u548c\u8c03\u8bd5\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5206\u6790\u6d41\u7a0b\u548c\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u5728TRAIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u679c\u3002", "motivation": "\u968f\u7740LLM\u5728\u81ea\u52a8\u5316\u590d\u6742\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9519\u8bef\u3001\u6d8c\u73b0\u884c\u4e3a\u548c\u7cfb\u7edf\u6027\u6545\u969c\u5e26\u6765\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7684\u76d1\u63a7\u8c03\u8bd5\u5de5\u5177\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u591a\u9636\u6bb5\u5206\u6790\u6d41\u7a0b\uff1a\u9519\u8bef\u8bc6\u522b\u5206\u7c7b\u3001\u4e3b\u9898\u805a\u7c7b\u3001\u91cf\u5316\u8bc4\u5206\u548c\u7b56\u7565\u603b\u7ed3\uff0c\u5e76\u914d\u5907\u60c5\u666f\u8bb0\u5fc6\u548c\u8bed\u4e49\u8bb0\u5fc6\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u5b9e\u7528\u6027\uff0c\u5728TRAIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6307\u6807\uff0c\u53d1\u73b0\u4eba\u5de5\u6807\u6ce8\u9057\u6f0f\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "AgentCompass\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u53d1\u8005\u4e2d\u5fc3\u5de5\u5177\uff0c\u80fd\u591f\u53ef\u9760\u5730\u76d1\u63a7\u548c\u6539\u8fdb\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2509.14744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14744", "abs": "https://arxiv.org/abs/2509.14744", "authors": ["Worawalan Chatlatanagulchai", "Kundjanasith Thonglek", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code", "comment": null, "summary": "Agentic coding tools receive goals written in natural language as input,\nbreak them down into specific tasks, and write/execute the actual code with\nminimal human intervention. Key to this process are agent manifests,\nconfiguration files (such as Claude.md) that provide agents with essential\nproject context, identity, and operational rules. However, the lack of\ncomprehensive and accessible documentation for creating these manifests\npresents a significant challenge for developers. We analyzed 253 Claude.md\nfiles from 242 repositories to identify structural patterns and common content.\nOur findings show that manifests typically have shallow hierarchies with one\nmain heading and several subsections, with content dominated by operational\ncommands, technical implementation notes, and high-level architecture.", "AI": {"tldr": "\u5206\u6790253\u4e2aClaude.md\u6587\u4ef6\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u6e05\u5355\u901a\u5e38\u5177\u6709\u6d45\u5c42\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3b\u8981\u5305\u542b\u64cd\u4f5c\u547d\u4ee4\u3001\u6280\u672f\u5b9e\u73b0\u8bf4\u660e\u548c\u9ad8\u7ea7\u67b6\u6784\u5185\u5bb9", "motivation": "\u667a\u80fd\u4f53\u7f16\u7801\u5de5\u5177\u9700\u8981\u6e05\u5355\u6587\u4ef6\u63d0\u4f9b\u9879\u76ee\u4e0a\u4e0b\u6587\u548c\u64cd\u4f5c\u89c4\u5219\uff0c\u4f46\u7f3a\u4e4f\u521b\u5efa\u8fd9\u4e9b\u6e05\u5355\u7684\u5168\u9762\u6587\u6863\uff0c\u7ed9\u5f00\u53d1\u8005\u5e26\u6765\u6311\u6218", "method": "\u5206\u6790242\u4e2a\u4ee3\u7801\u4ed3\u5e93\u4e2d\u7684253\u4e2aClaude.md\u6587\u4ef6\uff0c\u8bc6\u522b\u7ed3\u6784\u6a21\u5f0f\u548c\u5e38\u89c1\u5185\u5bb9", "result": "\u6e05\u5355\u901a\u5e38\u5177\u6709\u4e00\u4e2a\u4e3b\u6807\u9898\u548c\u82e5\u5e72\u5b50\u90e8\u5206\u7684\u6d45\u5c42\u5c42\u6b21\u7ed3\u6784\uff0c\u5185\u5bb9\u4ee5\u64cd\u4f5c\u547d\u4ee4\u3001\u6280\u672f\u5b9e\u73b0\u8bf4\u660e\u548c\u9ad8\u7ea7\u67b6\u6784\u4e3a\u4e3b", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u6e05\u5355\u7684\u5178\u578b\u7ed3\u6784\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u8005\u521b\u5efa\u66f4\u597d\u7684\u6e05\u5355\u6587\u6863\u63d0\u4f9b\u4e86\u53c2\u8003", "topic": "agent analysis"}}
{"id": "2509.14745", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14745", "abs": "https://arxiv.org/abs/2509.14745", "authors": ["Miku Watanabe", "Hao Li", "Yutaro Kashiwa", "Brittany Reid", "Hajimu Iida", "Ahmed E. Hassan"], "title": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub", "comment": null, "summary": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement.", "AI": {"tldr": "\u5bf9567\u4e2aClaude Code\u751f\u6210\u7684GitHub PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b083.8%\u7684AI\u8f85\u52a9PR\u88ab\u63a5\u53d7\u5408\u5e76\uff0c\u5176\u4e2d54.9%\u65e0\u9700\u4fee\u6539\u76f4\u63a5\u96c6\u6210\uff0c\u4f46\u4ecd\u670945.1%\u9700\u8981\u4eba\u5de5\u4fee\u8ba2\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u96c6\u6210\u5ea6\u63d0\u9ad8\uff0c\u9700\u8981\u4e86\u89e3AI\u4ee3\u7406\u751f\u6210\u7684PR\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u63a5\u53d7\u7a0b\u5ea6\u3002", "method": "\u5b9e\u8bc1\u7814\u7a76\u5206\u6790157\u4e2a\u5f00\u6e90\u9879\u76ee\u4e2d\u7684567\u4e2a\u7531Claude Code\u751f\u6210\u7684GitHub PR\uff0c\u7814\u7a76\u5176\u4efb\u52a1\u7c7b\u578b\u3001\u63a5\u53d7\u7387\u548c\u4fee\u6539\u9700\u6c42\u3002", "result": "83.8%\u7684AI\u8f85\u52a9PR\u88ab\u63a5\u53d7\u5408\u5e76\uff0c54.9%\u65e0\u9700\u4fee\u6539\u76f4\u63a5\u96c6\u6210\uff0c\u5f00\u53d1\u8005\u4e3b\u8981\u4f9d\u8d56AI\u8fdb\u884c\u91cd\u6784\u3001\u6587\u6863\u548c\u6d4b\u8bd5\u4efb\u52a1\u3002", "conclusion": "AI\u8f85\u52a9PR\u5927\u90e8\u5206\u53ef\u88ab\u63a5\u53d7\uff0c\u4f46\u4ecd\u9700\u8981\u4eba\u5de5\u76d1\u7763\u548c\u7cbe\u70bc\uff0c\u7279\u522b\u662f\u5728bug\u4fee\u590d\u3001\u6587\u6863\u548c\u9879\u76ee\u6807\u51c6\u9075\u5faa\u65b9\u9762\u3002", "topic": "swe application"}}
{"id": "2509.14585", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14585", "abs": "https://arxiv.org/abs/2509.14585", "authors": ["Minh Vu", "Konstantinos Slavakis"], "title": "Online reinforcement learning via sparse Gaussian mixture model Q-functions", "comment": null, "summary": "This paper introduces a structured and interpretable online policy-iteration\nframework for reinforcement learning (RL), built around the novel class of\nsparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work\nthat trained GMM-QFs offline, the proposed framework develops an online scheme\nthat leverages streaming data to encourage exploration. Model complexity is\nregulated through sparsification by Hadamard overparametrization, which\nmitigates overfitting while preserving expressiveness. The parameter space of\nS-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing\nfor principled parameter updates via online gradient descent on a smooth\nobjective. Numerical tests show that S-GMM-QFs match the performance of dense\ndeep RL (DeepRL) methods on standard benchmarks while using significantly fewer\nparameters, and maintain strong performance even in low-parameter-count regimes\nwhere sparsified DeepRL methods fail to generalize.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7a00\u758f\u9ad8\u65af\u6df7\u5408\u6a21\u578bQ\u51fd\u6570(S-GMM-QFs)\u7684\u7ed3\u6784\u5316\u5728\u7ebf\u7b56\u7565\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7Hadamard\u8fc7\u53c2\u6570\u5316\u5b9e\u73b0\u7a00\u758f\u5316\uff0c\u5728Riemannian\u6d41\u5f62\u4e0a\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\uff0c\u6027\u80fd\u5ab2\u7f8e\u6df1\u5ea6RL\u65b9\u6cd5\u4f46\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u5f00\u53d1\u7ed3\u6784\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u6df1\u5ea6RL\u65b9\u6cd5\u53c2\u6570\u8fc7\u591a\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u53c2\u6570\u6570\u91cf\u60c5\u51b5\u4e0b\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u9ad8\u65af\u6df7\u5408\u6a21\u578bQ\u51fd\u6570\uff0c\u901a\u8fc7Hadamard\u8fc7\u53c2\u6570\u5316\u8fdb\u884c\u7a00\u758f\u5316\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5728Riemannian\u6d41\u5f62\u7ed3\u6784\u4e0a\u8fdb\u884c\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0e\u5bc6\u96c6\u6df1\u5ea6RL\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u4f7f\u7528\u53c2\u6570\u663e\u8457\u51cf\u5c11\uff0c\u5728\u4f4e\u53c2\u6570\u6570\u91cf\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u800c\u7a00\u758f\u5316\u6df1\u5ea6RL\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u3002", "conclusion": "S-GMM-QFs\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.14829", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14829", "abs": "https://arxiv.org/abs/2509.14829", "authors": ["Shuo Jin", "Songqiang Chen", "Xiaoyuan Xie", "Shing-Chi Cheung"], "title": "RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "summary": "Automated code translation aims to convert programs between different\nprogramming languages while maintaining their functionality. Due to the\nimperfections of code translation models, the generated translations may\ncontain errors that compromise their reliability. Existing automated debugging\nmethods for code translation rely on code alignments and repair patch templates\nto locate and fix erroneous translations. However, existing methods lack\nreliable references to construct code alignments and design repair patch\ntemplates, which significantly impacts their localization accuracy and repair\neffectiveness. To address these limitations, we reintroduce code translation\nrules and propose a rule-based debugging method for code translation, called\nRulER. RulER automatically derives code translation rules from correct\ntranslations generated by LLMs, enabling the efficient collection of diverse\ntranslation rules. In addition, RulER dynamically combines the existing rules\non expandable nodes like expressions and tokens to further adaptively align\nmore statements. These rules capture clear and detailed structural\ncorrespondences between source and target programming languages. Therefore,\nthey can serve as reliable and reusable references for code alignment and\nrepair template design, enabling RulER to locate and fix translation errors\neffectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++\ntranslations produced by four code translation models demonstrates that RulER\noutperforms state-of-the-art methods, BatFix and TransMap. Our experimental\nresults show that RulER outperformed the best baseline by 20% and 272% in terms\nof error localization rates and repair success rates, respectively. RulER\nexhibits superior repair performance compared to directly prompting LLMs for\npatch generation, demonstrating a promising methodology for extracting and\nleveraging coding knowledge from LLMs.", "AI": {"tldr": "RulER\u662f\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u4ee3\u7801\u7ffb\u8bd1\u8c03\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eceLLM\u751f\u6210\u7684\u6b63\u786e\u7ffb\u8bd1\u4e2d\u81ea\u52a8\u63a8\u5bfc\u4ee3\u7801\u7ffb\u8bd1\u89c4\u5219\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9519\u8bef\u5b9a\u4f4d\u548c\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u7ffb\u8bd1\u8c03\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u9760\u7684\u53c2\u8003\u6765\u6784\u5efa\u4ee3\u7801\u5bf9\u9f50\u548c\u4fee\u590d\u8865\u4e01\u6a21\u677f\uff0c\u5f71\u54cd\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u4fee\u590d\u6548\u679c\u3002", "method": "\u4eceLLM\u751f\u6210\u7684\u6b63\u786e\u5b9a\u8bd1\u4e2d\u81ea\u52a8\u63a8\u5bfc\u4ee3\u7801\u7ffb\u8bd1\u89c4\u5219\uff0c\u52a8\u6001\u7ec4\u5408\u89c4\u5219\u4ee5\u9002\u5e94\u66f4\u591a\u8bed\u53e5\u7684\u5bf9\u9f50\uff0c\u5229\u7528\u89c4\u5219\u8fdb\u884c\u4ee3\u7801\u5bf9\u9f50\u548c\u4fee\u590d\u6a21\u677f\u8bbe\u8ba1\u3002", "result": "\u5728Java\u5230C++\u548cPython\u5230C++\u7ffb\u8bd1\u4e2d\uff0cRulER\u7684\u9519\u8bef\u5b9a\u4f4d\u7387\u548c\u4fee\u590d\u6210\u529f\u7387\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa20%\u548c272%\u3002", "conclusion": "RulER\u5c55\u793a\u4e86\u4eceLLM\u4e2d\u63d0\u53d6\u548c\u5229\u7528\u7f16\u7801\u77e5\u8bc6\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u8bba\uff0c\u5728\u4fee\u590d\u6027\u80fd\u4e0a\u4f18\u4e8e\u76f4\u63a5\u63d0\u793aLLM\u751f\u6210\u8865\u4e01\u3002", "topic": "code agent"}}
{"id": "2509.14856", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14856", "abs": "https://arxiv.org/abs/2509.14856", "authors": ["Hanyang Guo", "Xunjin Zheng", "Zihan Liao", "Hang Yu", "Peng DI", "Ziyin Zhang", "Hong-Ning Dai"], "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects", "comment": null, "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86CodeFuse-CR-Bench\uff0c\u9996\u4e2a\u9762\u5411\u4ee3\u7801\u4ed3\u5e93\u7ea7\u522b\u4ee3\u7801\u5ba1\u67e5\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b601\u4e2a\u9ad8\u8d28\u91cf\u5b9e\u4f8b\uff0c\u8986\u76d69\u4e2aPR\u95ee\u9898\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e30\u5bcc\u7684\u591a\u7ef4\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b64\u7acb\u5b50\u4efb\u52a1\u4e0a\u4f7f\u7528\u7b80\u5316\u3001\u4e0a\u4e0b\u6587\u8d2b\u4e4f\u7684\u6570\u636e\u8bc4\u4f30\u6a21\u578b\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u5ba1\u67e5\u7684\u6574\u4f53\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\uff0c\u5b58\u5728\"\u73b0\u5b9e\u5dee\u8ddd\"\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b601\u4e2a\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u63d0\u4f9bissue\u3001PR\u8be6\u60c5\u548c\u4ed3\u5e93\u72b6\u6001\u7b49\u4e30\u5bcc\u4e0a\u4e0b\u6587\uff1b\u63d0\u51fa\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u5b9a\u4f4d\u8bed\u6cd5\u68c0\u67e5\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5ba1\u67e5\u8d28\u91cf\u8bc4\u4f30\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u53d1\u73b0\uff1a(1)\u6ca1\u6709\u5355\u4e00LLM\u5728\u6240\u6709CR\u65b9\u9762\u90fd\u5360\u4f18\uff1b(2)Gemini 2.5 Pro\u7efc\u5408\u6027\u80fd\u6700\u9ad8\uff1b(3)\u4e0d\u540cLLM\u5bf9\u5197\u4f59\u4e0a\u4e0b\u6587\u7684\u9c81\u68d2\u6027\u4e0d\u540c\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8fdb\u884c\u5168\u9762\u3001\u591a\u7ef4\u5ea6\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u63a8\u8fdb\u771f\u6b63\u667a\u80fd\u4e14\u5b9e\u7528\u7684\u4ee3\u7801\u5ba1\u67e5\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002", "topic": "swe benchmark"}}
{"id": "2509.14899", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14899", "abs": "https://arxiv.org/abs/2509.14899", "authors": ["Amine Barrak", "Yosr Fourati", "Michael Olchawa", "Emna Ksontini", "Khalil Zoghlami"], "title": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models", "comment": null, "summary": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments.", "AI": {"tldr": "CARGO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684LLM\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u56de\u5f52\u5668\u548c\u53ef\u9009\u5206\u7c7b\u5668\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u9009\u62e9\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u56db\u4e2a\u4e3b\u6d41LLM\u4e0a\u8fbe\u523076.4%\u7684top-1\u8def\u7531\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u6a21\u3001\u4e13\u4e1a\u5316\u548c\u5ef6\u8fdf\u7279\u6027\u4e0a\u7684\u591a\u6837\u5316\uff0c\u5982\u4f55\u5c06\u7528\u6237\u63d0\u793a\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u6a21\u578b\u4ee5\u5e73\u8861\u6027\u80fd\u548c\u6210\u672c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u56de\u5f52\u5668\u8bad\u7ec3LLM\u5224\u65ad\u7684\u6210\u5bf9\u6bd4\u8f83\u6765\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u4e0d\u786e\u5b9a\u65f6\u8c03\u7528\u53ef\u9009\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u652f\u6301\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\u7684\u7279\u5b9a\u56de\u5f52\u5668\u3002", "result": "\u5728GPT-4o\u3001Claude 3.5 Sonnet\u3001DeepSeek V3\u548cPerplexity Sonar\u56db\u4e2a\u6a21\u578b\u4e0a\uff0cCARGO\u8fbe\u523076.4%\u7684top-1\u8def\u7531\u51c6\u786e\u7387\uff0c\u5bf9\u6297\u5355\u4e2a\u4e13\u5bb6\u7684\u80dc\u7387\u4e3a72%-89%\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u8f7b\u91cf\u7ea7\u8def\u7531\u80fd\u591f\u4ee5\u6700\u5c0f\u5f00\u9500\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u578bLLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.14956", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14956", "abs": "https://arxiv.org/abs/2509.14956", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems", "comment": "25 pages, 12 figures", "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u54e8\u5175\u4ee3\u7406\u548c\u534f\u8c03\u4ee3\u7406\u7684\u53cc\u5c42\u5b89\u5168\u67b6\u6784\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u653b\u51fb\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u5404\u79cd\u5b89\u5168\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\u3001\u5e7b\u89c9\u3001\u6570\u636e\u6cc4\u9732\u7b49\uff09\uff0c\u9700\u8981\u52a8\u6001\u81ea\u9002\u5e94\u7684\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u969c\u7cfb\u7edf\u5b89\u5168\u548c\u5b8c\u6574\u6027", "method": "\u8bbe\u8ba1\u5206\u5e03\u5f0f\u5b89\u5168\u5c42\uff1a\u54e8\u5175\u4ee3\u7406\u7f51\u7edc\u8d1f\u8d23\u8bed\u4e49\u5206\u6790\u3001\u884c\u4e3a\u5206\u6790\u3001\u5f02\u5e38\u68c0\u6d4b\uff1b\u534f\u8c03\u4ee3\u7406\u8d1f\u8d23\u7b56\u7565\u7ba1\u7406\u3001\u5a01\u80c1\u54cd\u5e94\u548c\u7cfb\u7edf\u6cbb\u7406", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6210\u529f\u68c0\u6d4b\u4e86162\u4e2a\u4e0d\u540c\u7c7b\u578b\u7684\u5408\u6210\u653b\u51fb\uff08\u63d0\u793a\u6ce8\u5165\u3001\u5e7b\u89c9\u3001\u6570\u636e\u6cc4\u9732\uff09\uff0c\u8bc1\u5b9e\u4e86\u76d1\u63a7\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u9632\u5fa1\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\uff0c\u652f\u6301\u6cd5\u89c4\u5408\u89c4\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u7b56\u7565\u7684\u6301\u7eed\u6f14\u8fdb", "topic": "agent analysis"}}
{"id": "2509.14998", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14998", "abs": "https://arxiv.org/abs/2509.14998", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "comment": "The paper has been accepted to the EMNLP 2025 Main Conference", "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.", "AI": {"tldr": "KAMAC\u662f\u4e00\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u5efa\u4e13\u5bb6\u56e2\u961f\u6765\u89e3\u51b3\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u91c7\u7528\u9759\u6001\u9884\u5206\u914d\u89d2\u8272\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u52a8\u6001\u77e5\u8bc6\u6574\u5408\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9700\u8981\u8de8\u4e13\u4e1a\u77e5\u8bc6\u7684\u590d\u6742\u4e34\u5e8a\u573a\u666f\u3002", "method": "\u63d0\u51faKAMAC\u6846\u67b6\uff0c\u4ece\u521d\u59cb\u4e13\u5bb6\u667a\u80fd\u4f53\u5f00\u59cb\uff0c\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u8ba8\u8bba\u8bc6\u522b\u77e5\u8bc6\u7f3a\u53e3\uff0c\u52a8\u6001\u62db\u52df\u989d\u5916\u4e13\u5bb6\uff0c\u652f\u6301\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u534f\u4f5c\uff0c\u6700\u7ec8\u901a\u8fc7\u5ba1\u67e5\u66f4\u65b0\u7684\u667a\u80fd\u4f53\u8bc4\u8bba\u6765\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKAMAC\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u5148\u8fdb\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u52a8\u6001\u8de8\u4e13\u4e1a\u77e5\u8bc6\u7684\u590d\u6742\u4e34\u5e8a\u573a\u666f\uff08\u5982\u764c\u75c7\u9884\u540e\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "KAMAC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u56e2\u961f\u7ec4\u5efa\u548c\u77e5\u8bc6\u9a71\u52a8\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u51b3\u7b56\u4e2d\u8de8\u4e13\u4e1a\u77e5\u8bc6\u6574\u5408\u7684\u6311\u6218\uff0c\u4e3a\u590d\u6742\u4e34\u5e8a\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.15195", "categories": ["cs.SE", "cs.AI", "cs.CR", "D.4.6; I.2.2; D.2.5"], "pdf": "https://arxiv.org/pdf/2509.15195", "abs": "https://arxiv.org/abs/2509.15195", "authors": ["Max Bazalii", "Marius Fleischer"], "title": "Orion: Fuzzing Workflow Automation", "comment": "11 pages, 3 figures, 3 tables", "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408LLM\u63a8\u7406\u4e0e\u4f20\u7edf\u5de5\u5177\uff0c\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u5728clib\u5e93\u4e2d\u53d1\u73b0\u4e24\u4e2a\u672a\u77e5\u6f0f\u6d1e", "motivation": "\u73b0\u4ee3\u6a21\u7cca\u6d4b\u8bd5\u867d\u7136\u80fd\u81ea\u52a8\u751f\u6210\u8f93\u5165\u548c\u76d1\u63a7\u6267\u884c\uff0c\u4f46\u4ece\u4ee3\u7801\u5e93\u5206\u6790\u3001\u914d\u7f6e\u6d4b\u8bd5\u5de5\u5177\u5230\u7ed3\u679c\u5206\u7c7b\u7684\u6574\u4e2a\u5de5\u4f5c\u6d41\u7a0b\u4ecd\u9700\u8981\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\uff0c\u73b0\u6709\u7814\u7a76\u53ea\u5173\u6ce8\u5355\u4e2a\u9636\u6bb5\u800c\u7f3a\u4e4f\u5b8c\u6574\u81ea\u52a8\u5316\u65b9\u6848", "method": "Orion\u6846\u67b6\u7ed3\u5408LLM\u7684\u4ee3\u7801\u63a8\u7406\u548c\u8bed\u4e49\u6307\u5bfc\u80fd\u529b\uff0c\u540c\u65f6\u4f9d\u8d56\u786e\u5b9a\u6027\u5de5\u5177\u8fdb\u884c\u9a8c\u8bc1\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u9700\u8981\u7cbe\u786e\u6027\u7684\u4efb\u52a1\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6a21\u7cca\u6d4b\u8bd5\u81ea\u52a8\u5316", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0cOrion\u5c06\u4eba\u5de5\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e8646-204\u500d\uff08\u53d6\u51b3\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u9636\u6bb5\uff09\uff0c\u5e76\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90clib\u5e93\u4e2d\u53d1\u73b0\u4e86\u4e24\u4e2a\u5148\u524d\u672a\u77e5\u7684\u6f0f\u6d1e", "conclusion": "Orion\u901a\u8fc7LLM\u4e0e\u4f20\u7edf\u5de5\u5177\u7684\u96c6\u6210\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u4eba\u5de5\u74f6\u9888\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u4eba\u7c7b\u5355\u72ec\u52aa\u529b\u4e0d\u5207\u5b9e\u9645\u7684\u573a\u666f\u4e0b\u5b9e\u73b0\u89c4\u6a21\u5316\u6a21\u7cca\u6d4b\u8bd5\u6d3b\u52a8\u7684\u53ef\u884c\u6027", "topic": "swe application"}}
{"id": "2509.14635", "categories": ["cs.CL", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14635", "abs": "https://arxiv.org/abs/2509.14635", "authors": ["Weihan Peng", "Yuling Shi", "Yuhang Wang", "Xinyun Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?", "comment": "Code and data available at\n  https://github.com/peng-weihan/SWE-QA-Bench", "summary": "Understanding and reasoning about entire software repositories is an\nessential capability for intelligent software engineering tools. While existing\nbenchmarks such as CoSQA and CodeQA have advanced the field, they predominantly\nfocus on small, self-contained code snippets. These setups fail to capture the\ncomplexity of real-world repositories, where effective understanding and\nreasoning often require navigating multiple files, understanding software\narchitecture, and grounding answers in long-range code dependencies. In this\npaper, we present SWE-QA, a repository-level code question answering (QA)\nbenchmark designed to facilitate research on automated QA systems in realistic\ncode environments. SWE-QA involves 576 high-quality question-answer pairs\nspanning diverse categories, including intention understanding, cross-file\nreasoning, and multi-hop dependency analysis. To construct SWE-QA, we first\ncrawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis\nof naturally occurring developer questions extracted from these issues, we\ndeveloped a two-level taxonomy of repository-level questions and constructed a\nset of seed questions for each category. For each category, we manually curated\nand validated questions and collected their corresponding answers. As a\nprototype application, we further develop SWE-QA-Agent, an agentic framework in\nwhich LLM agents reason and act to find answers automatically. We evaluate six\nadvanced LLMs on SWE-QA under various context augmentation strategies.\nExperimental results highlight the promise of LLMs, particularly our\nSWE-QA-Agent framework, in addressing repository-level QA, while also revealing\nopen challenges and pointing to future research directions.", "AI": {"tldr": "SWE-QA\u662f\u4e00\u4e2a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b576\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u610f\u56fe\u7406\u89e3\u3001\u8de8\u6587\u4ef6\u63a8\u7406\u548c\u591a\u8df3\u4f9d\u8d56\u5206\u6790\u7b49\u7c7b\u522b\uff0c\u65e8\u5728\u63a8\u52a8\u771f\u5b9e\u4ee3\u7801\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u95ee\u7b54\u7cfb\u7edf\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5c0f\u578b\u81ea\u5305\u542b\u4ee3\u7801\u7247\u6bb5\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u8f6f\u4ef6\u4ed3\u5e93\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u7406\u89e3\u591a\u6587\u4ef6\u3001\u8f6f\u4ef6\u67b6\u6784\u548c\u957f\u8ddd\u79bb\u4ee3\u7801\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4ece11\u4e2a\u70ed\u95e8\u4ed3\u5e93\u768477,100\u4e2aGitHub\u95ee\u9898\u4e2d\u63d0\u53d6\u5f00\u53d1\u8005\u95ee\u9898\uff0c\u6784\u5efa\u4e24\u7ea7\u5206\u7c7b\u6cd5\uff0c\u624b\u52a8\u7b56\u5212\u548c\u9a8c\u8bc1\u95ee\u9898\u5e76\u6536\u96c6\u7b54\u6848\uff0c\u5f00\u53d1SWE-QA-Agent\u4ee3\u7406\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u56de\u7b54\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e866\u4e2a\u5148\u8fdbLLM\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u589e\u5f3a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aLLMs\u7279\u522b\u662fSWE-QA-Agent\u6846\u67b6\u5728\u4ed3\u5e93\u7ea7\u95ee\u7b54\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u95ee\u7b54\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6311\u6218\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0cLLM\u4ee3\u7406\u6846\u67b6\u5c55\u73b0\u51fa\u826f\u597d\u524d\u666f\u3002", "topic": "swe benchmark"}}
{"id": "2509.14718", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14718", "abs": "https://arxiv.org/abs/2509.14718", "authors": ["Zihao Feng", "Xiaoxue Wang", "Bowen Wu", "Hailong Cao", "Tiejun Zhao", "Qun Yu", "Baoxun Wang"], "title": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning", "comment": null, "summary": "While reinforcement learning (RL) is increasingly used for LLM-based tool\nlearning, its efficiency is often hampered by an overabundance of simple\nsamples that provide diminishing learning value as training progresses.\nExisting dynamic sampling techniques are ill-suited for the multi-task\nstructure and fine-grained reward mechanisms inherent to tool learning. This\npaper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework\nspecifically designed to address this challenge by targeting the unique\ncharacteristics of tool learning: its multiple interdependent sub-tasks and\nmulti-valued reward functions. DSCL features two core components: Reward-Based\nDynamic Sampling, which uses multi-dimensional reward statistics (mean and\nvariance) to prioritize valuable data, and Task-Based Dynamic Curriculum\nLearning, which adaptively focuses training on less-mastered sub-tasks. Through\nextensive experiments, we demonstrate that DSCL significantly improves training\nefficiency and model performance over strong baselines, achieving a 3.29\\%\nimprovement on the BFCLv3 benchmark. Our method provides a tailored solution\nthat effectively leverages the complex reward signals and sub-task dynamics\nwithin tool learning to achieve superior results.", "AI": {"tldr": "DSCL\u6846\u67b6\u901a\u8fc7\u5956\u52b1\u52a8\u6001\u91c7\u6837\u548c\u4efb\u52a1\u8bfe\u7a0b\u5b66\u4e60\uff0c\u9488\u5bf9\u5de5\u5177\u5b66\u4e60\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u9762\u4e34\u7b80\u5355\u6837\u672c\u8fc7\u591a\u3001\u5b66\u4e60\u4ef7\u503c\u9012\u51cf\u7684\u95ee\u9898\uff0c\u73b0\u6709\u52a8\u6001\u91c7\u6837\u6280\u672f\u4e0d\u9002\u5408\u5de5\u5177\u5b66\u4e60\u7684\u591a\u4efb\u52a1\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236", "method": "\u63d0\u51faDSCL\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u5956\u52b1\u7684\u52a8\u6001\u91c7\u6837\uff08\u4f7f\u7528\u591a\u7ef4\u5956\u52b1\u7edf\u8ba1\uff09\u548c\u57fa\u4e8e\u4efb\u52a1\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\uff08\u81ea\u9002\u5e94\u5173\u6ce8\u672a\u638c\u63e1\u5b50\u4efb\u52a1\uff09", "result": "\u5728BFCLv3\u57fa\u51c6\u4e0a\u5b9e\u73b03.29%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "DSCL\u4e3a\u5de5\u5177\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5229\u7528\u590d\u6742\u5956\u52b1\u4fe1\u53f7\u548c\u5b50\u4efb\u52a1\u52a8\u6001\u5b9e\u73b0\u4f18\u5f02\u7ed3\u679c", "topic": "agentic reinforcement learning"}}
{"id": "2509.14477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14477", "abs": "https://arxiv.org/abs/2509.14477", "authors": ["Thales Sales Almeida", "Jo\u00e3o Guilherme Alves Santos", "Thiago Laitz", "Giovana Kerche Bon\u00e1s"], "title": "Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as task-oriented\nagents, where success depends on their ability to generate accurate function\ncalls under realistic, multilingual conditions. However, existing agent\nevaluations largely overlook cultural and linguistic diversity, often relying\non monolingual or naively translated benchmarks. We introduce Ticket-Bench, a\nbenchmark for multilingual agent evaluation in task-oriented scenarios.\nTicket-Bench simulates the domain of soccer ticket purchases across six major\nlanguages: Portuguese, English, Spanish, German, Italian, and French. Using\nlocalized teams, cities, and user profiles to provide a higher level of\nrealism. We evaluate a wide range of commercial and open-source LLMs, measuring\nfunction-calling accuracy and consistency across languages. Results show that\nreasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but\nstill exhibit notable cross-lingual disparities. These findings underscore the\nneed for culturally aware, multilingual benchmarks to guide the development of\nrobust LLM agents.", "AI": {"tldr": "Ticket-Bench\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8db3\u7403\u7968\u8d2d\u4e70\u573a\u666f\uff0c\u6db5\u76d66\u79cd\u4e3b\u8981\u8bed\u8a00\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u8de8\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u51fd\u6570\u8c03\u7528\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5ffd\u89c6\u6587\u5316\u548c\u8bed\u8a00\u591a\u6837\u6027\uff0c\u901a\u5e38\u4f9d\u8d56\u5355\u8bed\u6216\u7b80\u5355\u7ffb\u8bd1\u7684\u57fa\u51c6\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "method": "\u521b\u5efaTicket-Bench\u57fa\u51c6\uff0c\u6a21\u62df\u8db3\u7403\u7968\u8d2d\u4e70\u573a\u666f\uff0c\u6db5\u76d6\u8461\u8404\u7259\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u610f\u5927\u5229\u8bed\u548c\u6cd5\u8bed\u516d\u79cd\u8bed\u8a00\uff0c\u4f7f\u7528\u672c\u5730\u5316\u7684\u7403\u961f\u3001\u57ce\u5e02\u548c\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u63d0\u9ad8\u771f\u5b9e\u6027\u3002", "result": "\u63a8\u7406\u5bfc\u5411\u6a21\u578b\uff08\u5982GPT-5\u3001Qwen3-235B\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u8de8\u8bed\u8a00\u5dee\u5f02\uff0c\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u6587\u5316\u611f\u77e5\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6765\u6307\u5bfc\u7a33\u5065LLM\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5f53\u524d\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u4ecd\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "topic": "swe benchmark"}}
{"id": "2509.14478", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14478", "abs": "https://arxiv.org/abs/2509.14478", "authors": ["Lucas H. McCabe", "Rimon Melamed", "Thomas Hartvigsen", "H. Howie Huang"], "title": "Estimating Semantic Alphabet Size for LLM Uncertainty Quantification", "comment": null, "summary": "Many black-box techniques for quantifying the uncertainty of large language\nmodels (LLMs) rely on repeated LLM sampling, which can be computationally\nexpensive. Therefore, practical applicability demands reliable estimation from\nfew samples. Semantic entropy (SE) is a popular sample-based uncertainty\nestimator with a discrete formulation attractive for the black-box setting.\nRecent extensions of semantic entropy exhibit improved LLM hallucination\ndetection, but do so with less interpretable methods that admit additional\nhyperparameters. For this reason, we revisit the canonical discrete semantic\nentropy estimator, finding that it underestimates the \"true\" semantic entropy,\nas expected from theory. We propose a modified semantic alphabet size\nestimator, and illustrate that using it to adjust discrete semantic entropy for\nsample coverage results in more accurate semantic entropy estimation in our\nsetting of interest. Furthermore, our proposed alphabet size estimator flags\nincorrect LLM responses as well or better than recent top-performing\napproaches, with the added benefit of remaining highly interpretable.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u8bed\u4e49\u5b57\u6bcd\u8868\u5927\u5c0f\u4f30\u8ba1\u5668\u6765\u8c03\u6574\u79bb\u6563\u8bed\u4e49\u71b5\uff0c\u63d0\u9ad8LLM\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684LLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u53ef\u9760\u4f30\u8ba1\u3002\u8bed\u4e49\u71b5\u867d\u7136\u6d41\u884c\u4f46\u4f1a\u4f4e\u4f30\u771f\u5b9e\u8bed\u4e49\u71b5\uff0c\u4e14\u8fd1\u671f\u6269\u5c55\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u5dee\u4e14\u5f15\u5165\u989d\u5916\u8d85\u53c2\u6570", "method": "\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178\u79bb\u6563\u8bed\u4e49\u71b5\u4f30\u8ba1\u5668\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u8bed\u4e49\u5b57\u6bcd\u8868\u5927\u5c0f\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u8c03\u6574\u79bb\u6563\u8bed\u4e49\u71b5\u7684\u6837\u672c\u8986\u76d6\u5ea6", "result": "\u6539\u8fdb\u65b9\u6cd5\u5728\u8bed\u4e49\u71b5\u4f30\u8ba1\u4e0a\u66f4\u51c6\u786e\uff0c\u5728\u9519\u8befLLM\u54cd\u5e94\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4e0e\u9876\u7ea7\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u5b57\u6bcd\u8868\u5927\u5c0f\u4f30\u8ba1\u5668\u65e2\u80fd\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u53c8\u80fd\u4fdd\u6301\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u4f18\u52bf", "topic": "agent analysis"}}
{"id": "2509.14480", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14480", "abs": "https://arxiv.org/abs/2509.14480", "authors": ["Weiting Tan", "Xinghua Qu", "Ming Tu", "Meng Ge", "Andy T. Liu", "Philipp Koehn", "Lu Lu"], "title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents", "comment": null, "summary": "Effective interactive tool use requires agents to master Tool Integrated\nReasoning (TIR): a complex process involving multi-turn planning and\nlong-context dialogue management. To train agents for this dynamic process,\nparticularly in multi-modal contexts, we introduce a sandbox environment for\nreinforcement learning (RL) that supports interleaved speech-text rollouts. Our\ncore strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses\nthe challenge of credit assignment in long-horizon tasks by employing a Large\nLanguage Model (LLM) as a judge to provide turn-level evaluation. To enhance\nexploration, we integrate a mixed-task training curriculum with mathematical\nreasoning problems. This unified approach boosts the task pass rate on the\ntext-based $\\tau$-bench by over 6% compared to strong RL baselines. Crucially,\nwe demonstrate our framework's suitability for fine-tuning a multi-modal\nfoundation model for agentic tasks. By training a base multi-modal LLM on\ninterleaved speech-text rollouts, we equip it with tool-use abilities, paving\nthe way for more natural, voice-driven interactive agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TARL\u6846\u67b6\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u56de\u5408\u7ea7\u8bc4\u4f30\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5728\u6587\u672c\u57fa\u51c6\u4e0a\u63d0\u5347\u4efb\u52a1\u901a\u8fc7\u73876%\u4ee5\u4e0a\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u9002\u7528\u6027\u3002", "motivation": "\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u5de5\u5177\u4f7f\u7528\u9700\u8981\u4ee3\u7406\u638c\u63e1\u5de5\u5177\u96c6\u6210\u63a8\u7406(TIR)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6d89\u53ca\u591a\u8f6e\u89c4\u5212\u548c\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7ba1\u7406\u7684\u590d\u6742\u8fc7\u7a0b\u3002\u4e3a\u4e86\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8bad\u7ec3\u4ee3\u7406\u5e94\u5bf9\u8fd9\u79cd\u52a8\u6001\u8fc7\u7a0b\uff0c\u9700\u8981\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Turn-level Adjudicated Reinforcement Learning (TARL)\u7b56\u7565\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4f5c\u4e3a\u8bc4\u5224\u8005\u63d0\u4f9b\u56de\u5408\u7ea7\u8bc4\u4f30\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002\u96c6\u6210\u6df7\u5408\u4efb\u52a1\u8bad\u7ec3\u8bfe\u7a0b\u4e0e\u6570\u5b66\u63a8\u7406\u95ee\u9898\u6765\u589e\u5f3a\u63a2\u7d22\u3002", "result": "\u5728\u6587\u672c\u57fa\u51c6\u03c4-bench\u4e0a\u7684\u4efb\u52a1\u901a\u8fc7\u7387\u76f8\u6bd4\u5f3aRL\u57fa\u7ebf\u63d0\u5347\u4e86\u8d85\u8fc76%\u3002\u6210\u529f\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u7840\u591a\u6a21\u6001LLM\uff0c\u4f7f\u5176\u5177\u5907\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5fae\u8c03\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u4ee3\u7406\u4efb\u52a1\uff0c\u4e3a\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u9a71\u52a8\u4ea4\u4e92\u4ee3\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.14543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14543", "abs": "https://arxiv.org/abs/2509.14543", "authors": ["Zhengxiang Wang", "Nafis Irtiza Tripto", "Solha Park", "Zhenzhen Li", "Jiawei Zhou"], "title": "Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors", "comment": "EMNLP 2025 (Findings)", "summary": "As large language models (LLMs) become increasingly integrated into personal\nwriting tools, a critical question arises: can LLMs faithfully imitate an\nindividual's writing style from just a few examples? Personal style is often\nsubtle and implicit, making it difficult to specify through prompts yet\nessential for user-aligned generation. This work presents a comprehensive\nevaluation of state-of-the-art LLMs' ability to mimic personal writing styles\nvia in-context learning from a small number of user-authored samples. We\nintroduce an ensemble of complementary metrics-including authorship\nattribution, authorship verification, style matching, and AI detection-to\nrobustly assess style imitation. Our evaluation spans over 40000 generations\nper model across domains such as news, email, forums, and blogs, covering\nwriting samples from more than 400 real-world authors. Results show that while\nLLMs can approximate user styles in structured formats like news and email,\nthey struggle with nuanced, informal writing in blogs and forums. Further\nanalysis on various prompting strategies such as number of demonstrations\nreveal key limitations in effective personalization. Our findings highlight a\nfundamental gap in personalized LLM adaptation and the need for improved\ntechniques to support implicit, style-consistent generation. To aid future\nresearch and for reproducibility, we open-source our data and code.", "AI": {"tldr": "LLMs\u5728\u6a21\u4eff\u4e2a\u4eba\u5199\u4f5c\u98ce\u683c\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5728\u7ed3\u6784\u5316\u683c\u5f0f\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u975e\u6b63\u5f0f\u535a\u5ba2\u548c\u8bba\u575b\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u98ce\u683c\u5dee\u5f02", "motivation": "\u8bc4\u4f30LLMs\u80fd\u5426\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5fe0\u5b9e\u6a21\u4eff\u4e2a\u4eba\u5199\u4f5c\u98ce\u683c\uff0c\u8fd9\u5bf9\u4e8e\u4e2a\u6027\u5316\u5199\u4f5c\u5de5\u5177\u7684\u7528\u6237\u5bf9\u9f50\u751f\u6210\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ece\u5c11\u91cf\u7528\u6237\u5199\u4f5c\u6837\u672c\u4e2d\u6a21\u4eff\u98ce\u683c\uff0c\u91c7\u7528\u4f5c\u8005\u5f52\u5c5e\u3001\u9a8c\u8bc1\u3001\u98ce\u683c\u5339\u914d\u548cAI\u68c0\u6d4b\u7b49\u7efc\u5408\u6307\u6807\uff0c\u5728\u65b0\u95fb\u3001\u90ae\u4ef6\u3001\u8bba\u575b\u548c\u535a\u5ba2\u7b49\u9886\u57df\u7684400\u591a\u540d\u771f\u5b9e\u4f5c\u8005\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30", "result": "LLMs\u5728\u65b0\u95fb\u548c\u90ae\u4ef6\u7b49\u7ed3\u6784\u5316\u683c\u5f0f\u4e2d\u53ef\u4ee5\u8fd1\u4f3c\u7528\u6237\u98ce\u683c\uff0c\u4f46\u5728\u535a\u5ba2\u548c\u8bba\u575b\u7b49\u975e\u6b63\u5f0f\u5199\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u793a\u7b56\u7565\u5206\u6790\u663e\u793a\u4e2a\u6027\u5316\u5b58\u5728\u5173\u952e\u9650\u5236", "conclusion": "\u5f53\u524dLLM\u4e2a\u6027\u5316\u9002\u914d\u5b58\u5728\u6839\u672c\u6027\u5dee\u8ddd\uff0c\u9700\u8981\u6539\u8fdb\u6280\u672f\u6765\u652f\u6301\u9690\u5f0f\u7684\u98ce\u683c\u4e00\u81f4\u6027\u751f\u6210", "topic": "agent analysis"}}
{"id": "2509.14848", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14848", "abs": "https://arxiv.org/abs/2509.14848", "authors": ["Houssem Sifaou", "Osvaldo Simeone"], "title": "Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization", "comment": null, "summary": "Optimizing a reinforcement learning (RL) policy typically requires extensive\ninteractions with a high-fidelity simulator of the environment, which are often\ncostly or impractical. Offline RL addresses this problem by allowing training\nfrom pre-collected data, but its effectiveness is strongly constrained by the\nsize and quality of the dataset. Hybrid offline-online RL leverages both\noffline data and interactions with a single simulator of the environment. In\nmany real-world scenarios, however, multiple simulators with varying levels of\nfidelity and computational cost are available. In this work, we study\nmulti-fidelity hybrid RL for policy optimization under a fixed cost budget. We\nintroduce multi-fidelity hybrid RL via information gain maximization\n(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity\nselection based on information gain maximization through a bootstrapping\napproach. Theoretical analysis establishes the no-regret property of\nMF-HRL-IGM, while empirical evaluations demonstrate its superior performance\ncompared to existing benchmarks.", "AI": {"tldr": "\u63d0\u51faMF-HRL-IGM\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u6700\u5927\u5316\u5728\u591a\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u4e2d\u9009\u62e9\u6700\u4f18\u6a21\u62df\u5668\uff0c\u5728\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7b56\u7565", "motivation": "\u89e3\u51b3\u4f20\u7edfRL\u9700\u8981\u4e0e\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u5927\u91cf\u4ea4\u4e92\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u79bb\u7ebfRL\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8d28\u91cf\u548c\u89c4\u6a21\u7684\u95ee\u9898\uff0c\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u7684\u4f18\u52bf", "method": "\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u6700\u5927\u5316\u7684\u591a\u4fdd\u771f\u5ea6\u6df7\u5408RL\u7b97\u6cd5\uff0c\u4f7f\u7528\u81ea\u4e3e\u65b9\u6cd5\u8fdb\u884c\u4fdd\u771f\u5ea6\u9009\u62e9\uff0c\u7ed3\u5408\u79bb\u7ebf\u6570\u636e\u548c\u5728\u7ebf\u6a21\u62df\u5668\u4ea4\u4e92", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u5177\u6709\u65e0\u540e\u6094\u6027\u8d28\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5", "conclusion": "MF-HRL-IGM\u7b97\u6cd5\u80fd\u6709\u6548\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u8d44\u6e90\uff0c\u5728\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u5b9e\u73b0\u6700\u4f18\u7b56\u7565\u4f18\u5316", "topic": "agentic reinforcement learning"}}
{"id": "2509.14651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14651", "abs": "https://arxiv.org/abs/2509.14651", "authors": ["Siyu Yan", "Long Zeng", "Xuecheng Wu", "Chengcheng Han", "Kongcheng Zhang", "Chong Peng", "Xuezhi Cao", "Xunliang Cai", "Chenjuan Guo"], "title": "MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models", "comment": "EMNLP 2025 main conference", "summary": "As large language models~(LLMs) become widely adopted, ensuring their\nalignment with human values is crucial to prevent jailbreaks where adversaries\nmanipulate models to produce harmful content. While most defenses target\nsingle-turn attacks, real-world usage often involves multi-turn dialogues,\nexposing models to attacks that exploit conversational context to bypass safety\nmeasures. We introduce MUSE, a comprehensive framework tackling multi-turn\njailbreaks from both attack and defense angles. For attacks, we propose MUSE-A,\na method that uses frame semantics and heuristic tree search to explore diverse\nsemantic trajectories. For defense, we present MUSE-D, a fine-grained safety\nalignment approach that intervenes early in dialogues to reduce\nvulnerabilities. Extensive experiments on various models show that MUSE\neffectively identifies and mitigates multi-turn vulnerabilities. Code is\navailable at\n\\href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.", "AI": {"tldr": "MUSE\u6846\u67b6\u4ece\u653b\u51fb\u548c\u9632\u5fa1\u4e24\u4e2a\u89d2\u5ea6\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684LLM\u8d8a\u72f1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6846\u67b6\u8bed\u4e49\u548c\u542f\u53d1\u5f0f\u6811\u641c\u7d22\u7684\u653b\u51fb\u65b9\u6cd5MUSE-A\uff0c\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u5b89\u5168\u5bf9\u9f50\u9632\u5fa1\u65b9\u6cd5MUSE-D", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u91c7\u7528\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u9632\u5fa1\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u653b\u51fb\uff0c\u4f46\u73b0\u5b9e\u4f7f\u7528\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u5bb9\u6613\u8ba9\u653b\u51fb\u8005\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7ed5\u8fc7\u5b89\u5168\u63aa\u65bd", "method": "\u63d0\u51faMUSE\u6846\u67b6\uff1a\u653b\u51fb\u65b9\u9762\u4f7f\u7528\u6846\u67b6\u8bed\u4e49\u548c\u542f\u53d1\u5f0f\u6811\u641c\u7d22\u63a2\u7d22\u591a\u6837\u5316\u8bed\u4e49\u8f68\u8ff9(MUSE-A)\uff1b\u9632\u5fa1\u65b9\u9762\u91c7\u7528\u7ec6\u7c92\u5ea6\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u5bf9\u8bdd\u65e9\u671f\u8fdb\u884c\u5e72\u9884(MUSE-D)", "result": "\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMUSE\u80fd\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3\u591a\u8f6e\u6f0f\u6d1e", "conclusion": "MUSE\u6846\u67b6\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684LLM\u5b89\u5168\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u653b\u51fb\u548c\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027", "topic": "agent analysis"}}
{"id": "2509.14925", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14925", "abs": "https://arxiv.org/abs/2509.14925", "authors": ["Konrad Nowosadko", "Franco Ruggeri", "Ahmad Terra"], "title": "Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation", "comment": null, "summary": "Reinforcement Learning (RL) methods that incorporate deep neural networks\n(DNN), though powerful, often lack transparency. Their black-box characteristic\nhinders interpretability and reduces trustworthiness, particularly in critical\ndomains. To address this challenge in RL tasks, we propose a solution based on\nSelf-Explaining Neural Networks (SENNs) along with explanation extraction\nmethods to enhance interpretability while maintaining predictive accuracy. Our\napproach targets low-dimensionality problems to generate robust local and\nglobal explanations of the model's behaviour. We evaluate the proposed method\non the resource allocation problem in mobile networks, demonstrating that SENNs\ncan constitute interpretable solutions with competitive performance. This work\nhighlights the potential of SENNs to improve transparency and trust in\nAI-driven decision-making for low-dimensional tasks. Our approach strong\nperformance on par with the existing state-of-the-art methods, while providing\nrobust explanations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc(SENNs)\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u9488\u5bf9\u4f4e\u7ef4\u95ee\u9898\u751f\u6210\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u9ed1\u76d2\u7279\u6027\u963b\u788d\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u9886\u57df\u9700\u8981\u900f\u660e\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u81ea\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc(SENNs)\u7ed3\u5408\u89e3\u91ca\u63d0\u53d6\u65b9\u6cd5\uff0c\u9488\u5bf9\u4f4e\u7ef4\u95ee\u9898\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u79fb\u52a8\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff0cSENNs\u80fd\u591f\u63d0\u4f9b\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u540c\u65f6\u751f\u6210\u9c81\u68d2\u7684\u89e3\u91ca\u3002", "conclusion": "SENNs\u6709\u6f5c\u529b\u63d0\u9ad8\u4f4e\u7ef4\u4efb\u52a1\u4e2dAI\u9a71\u52a8\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "topic": "agent analysis"}}
{"id": "2509.15032", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15032", "abs": "https://arxiv.org/abs/2509.15032", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Yi Liu", "Dianxin Luan", "Dong Huang", "Heming Cui", "Yong Cui"], "title": "Sample Efficient Experience Replay in Non-stationary Environments", "comment": "5 pages, 3 figures", "summary": "Reinforcement learning (RL) in non-stationary environments is challenging, as\nchanging dynamics and rewards quickly make past experiences outdated.\nTraditional experience replay (ER) methods, especially those using TD-error\nprioritization, struggle to distinguish between changes caused by the agent's\npolicy and those from the environment, resulting in inefficient learning under\ndynamic conditions. To address this challenge, we propose the Discrepancy of\nEnvironment Dynamics (DoE), a metric that isolates the effects of environment\nshifts on value functions. Building on this, we introduce Discrepancy of\nEnvironment Prioritized Experience Replay (DEER), an adaptive ER framework that\nprioritizes transitions based on both policy updates and environmental changes.\nDEER uses a binary classifier to detect environment changes and applies\ndistinct prioritization strategies before and after each shift, enabling more\nsample-efficient learning. Experiments on four non-stationary benchmarks\ndemonstrate that DEER further improves the performance of off-policy algorithms\nby 11.54 percent compared to the best-performing state-of-the-art ER methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEER\u65b9\u6cd5\uff0c\u901a\u8fc7\u73af\u5883\u52a8\u6001\u5dee\u5f02\u5ea6\u91cf\u6765\u533a\u5206\u7b56\u7565\u66f4\u65b0\u548c\u73af\u5883\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u96be\u4ee5\u533a\u5206\u7b56\u7565\u53d8\u5316\u548c\u73af\u5883\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51fa\u73af\u5883\u52a8\u6001\u5dee\u5f02\u5ea6\u91cf(DoE)\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1DEER\u6846\u67b6\uff0c\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u5668\u68c0\u6d4b\u73af\u5883\u53d8\u5316\uff0c\u5728\u4e0d\u540c\u9636\u6bb5\u5e94\u7528\u4e0d\u540c\u7684\u4f18\u5148\u7ea7\u7b56\u7565", "result": "\u5728\u56db\u4e2a\u975e\u5e73\u7a33\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDEER\u76f8\u6bd4\u6700\u5148\u8fdb\u7684ER\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e8611.54%\u7684\u6027\u80fd", "conclusion": "DEER\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u533a\u5206\u7b56\u7565\u548c\u73af\u5883\u53d8\u5316\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6837\u672c\u5229\u7528", "topic": "agentic reinforcement learning"}}
{"id": "2509.14834", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14834", "abs": "https://arxiv.org/abs/2509.14834", "authors": ["Jinhee Jang", "Ayoung Moon", "Minkyoung Jung", "YoungBin Kim. Seung Jin Lee"], "title": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring", "comment": null, "summary": "The emergence of large language models (LLMs) has brought a new paradigm to\nautomated essay scoring (AES), a long-standing and practical application of\nnatural language processing in education. However, achieving human-level\nmulti-perspective understanding and judgment remains a challenge. In this work,\nwe propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework\ndesigned to perform precise and human-aligned scoring under a zero-shot\nsetting. RES constructs evaluator agents based on LLMs, each tailored to a\nspecific prompt and topic context. Each agent independently generates a\ntrait-based rubric and conducts a multi-perspective evaluation. Then, by\nsimulating a roundtable-style discussion, RES consolidates individual\nevaluations through a dialectical reasoning process to produce a final holistic\nscore that more closely aligns with human evaluation. By enabling collaboration\nand consensus among agents with diverse evaluation perspectives, RES\noutperforms prior zero-shot AES approaches. Experiments on the ASAP dataset\nusing ChatGPT and Claude show that RES achieves up to a 34.86% improvement in\naverage QWK over straightforward prompting (Vanilla) methods.", "AI": {"tldr": "RES\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5706\u684c\u8ba8\u8bba\u5b9e\u73b0\u96f6\u6837\u672c\u8bba\u6587\u81ea\u52a8\u8bc4\u5206\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728QWK\u6307\u6807\u4e0a\u63d0\u534734.86%", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bba\u6587\u8bc4\u5206\u4e2d\u96be\u4ee5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u591a\u89c6\u89d2\u7406\u89e3\u548c\u5224\u65ad\u7684\u95ee\u9898", "method": "\u6784\u5efa\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u9488\u5bf9\u7279\u5b9a\u63d0\u793a\u548c\u4e3b\u9898\u751f\u6210\u8bc4\u5206\u6807\u51c6\uff0c\u8fdb\u884c\u591a\u89c6\u89d2\u72ec\u7acb\u8bc4\u4f30\uff0c\u7136\u540e\u901a\u8fc7\u8fa9\u8bc1\u63a8\u7406\u6574\u5408\u5f97\u5230\u6700\u7ec8\u5206\u6570", "result": "\u5728ASAP\u6570\u636e\u96c6\u4e0a\u4f7f\u7528ChatGPT\u548cClaude\uff0cRES\u76f8\u6bd4\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u5728\u5e73\u5747QWK\u6307\u6807\u4e0a\u63d0\u5347\u9ad8\u8fbe34.86%", "conclusion": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u5171\u8bc6\u673a\u5236\uff0cRES\u80fd\u591f\u4ea7\u751f\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u4f30\u7684\u7cbe\u786e\u8bc4\u5206", "topic": "agent analysis"}}
{"id": "2509.15042", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15042", "abs": "https://arxiv.org/abs/2509.15042", "authors": ["Thomas Ackermann", "Moritz Spang", "Hamza A. A. Gardi"], "title": "Reinforcement Learning Agent for a 2D Shooter Game", "comment": null, "summary": "Reinforcement learning agents in complex game environments often suffer from\nsparse rewards, training instability, and poor sample efficiency. This paper\npresents a hybrid training approach that combines offline imitation learning\nwith online reinforcement learning for a 2D shooter game agent. We implement a\nmulti-head neural network with separate outputs for behavioral cloning and\nQ-learning, unified by shared feature extraction layers with attention\nmechanisms. Initial experiments using pure deep Q-Networks exhibited\nsignificant instability, with agents frequently reverting to poor policies\ndespite occasional good performance. To address this, we developed a hybrid\nmethodology that begins with behavioral cloning on demonstration data from\nrule-based agents, then transitions to reinforcement learning. Our hybrid\napproach achieves consistently above 70% win rate against rule-based opponents,\nsubstantially outperforming pure reinforcement learning methods which showed\nhigh variance and frequent performance degradation. The multi-head architecture\nenables effective knowledge transfer between learning modes while maintaining\ntraining stability. Results demonstrate that combining demonstration-based\ninitialization with reinforcement learning optimization provides a robust\nsolution for developing game AI agents in complex multi-agent environments\nwhere pure exploration proves insufficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e2D\u5c04\u51fb\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u5934\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u9762\u4e34\u7684\u7a00\u758f\u5956\u52b1\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u5934\u795e\u7ecf\u7f51\u7edc\uff0c\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5c42\u4f46\u5206\u79bb\u884c\u4e3a\u514b\u9686\u548cQ\u5b66\u4e60\u8f93\u51fa\uff0c\u5148\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u667a\u80fd\u4f53\u7684\u6f14\u793a\u6570\u636e\u8fdb\u884c\u884c\u4e3a\u514b\u9686\uff0c\u518d\u8fc7\u6e21\u5230\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u57fa\u4e8e\u89c4\u5219\u5bf9\u624b70%\u4ee5\u4e0a\u7684\u80dc\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u540e\u8005\u8868\u73b0\u51fa\u9ad8\u65b9\u5dee\u548c\u9891\u7e41\u7684\u6027\u80fd\u9000\u5316\u3002", "conclusion": "\u7ed3\u5408\u6f14\u793a\u6570\u636e\u521d\u59cb\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u6df7\u5408\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6e38\u620fAI\u5f00\u53d1\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.15110", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15110", "abs": "https://arxiv.org/abs/2509.15110", "authors": ["Dan Zhang", "Min Cai", "Jonathan Li", "Ziniu Hu", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "comment": "9 figures, 7 tables", "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.", "AI": {"tldr": "TDRM\u901a\u8fc7\u65f6\u95f4\u5dee\u5206\u6b63\u5219\u5316\u8bad\u7ec3\u66f4\u5e73\u6ed1\u53ef\u9760\u7684\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u679c\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7b56\u7565\u66f4\u65b0\u65e0\u6548\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5e73\u6ed1\u53ef\u9760\u7684\u5956\u52b1\u6a21\u578b\u3002", "method": "\u63d0\u51faTDRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6700\u5c0f\u5316\u65f6\u95f4\u5dee\u5f02\u6765\u5b66\u4e60\u5e73\u6ed1\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u5dee\u5206\u6b63\u5219\u5316\u4ea7\u751f\u5e73\u6ed1\u5956\u52b1\u5e76\u6539\u5584\u4e0e\u957f\u671f\u76ee\u6807\u7684\u5bf9\u9f50\u3002", "result": "TD\u8bad\u7ec3\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728Best-of-N\u8bbe\u7f6e\u4e2d\u63d0\u53476.6%\uff0c\u5728\u6811\u641c\u7d22\u8bbe\u7f6e\u4e2d\u63d0\u534723.7%\u3002\u4e0eRLVR\u7ed3\u5408\u65f6\uff0c\u4ec5\u97002.5k\u6570\u636e\u5373\u53ef\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd550.1k\u6570\u636e\u7684\u6027\u80fd\uff0c\u57288\u4e2a\u6a21\u578b\u53d8\u4f53\u4e0a\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u3002", "conclusion": "TDRM\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u7684\u8865\u5145\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u548c\u7b56\u7565\u8d28\u91cf\uff0c\u662f\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u7684\u6709\u6548\u6539\u8fdb\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.15157", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15157", "abs": "https://arxiv.org/abs/2509.15157", "authors": ["Shiwan Zhao", "Xuyang Zhao", "Jiaming Zhou", "Aobo Kong", "Qicheng Li", "Yong Qin"], "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an\noff-policy learning problem, where expert demonstrations come from a fixed\nbehavior policy while training aims to optimize a target policy. Importance\nsampling is the standard tool for correcting this distribution mismatch, but\nlarge policy gaps lead to high variance and training instability. Existing\napproaches mitigate this issue using KL penalties or clipping, which passively\nconstrain updates rather than actively reducing the gap. We propose a simple\nyet effective data rewriting framework that proactively shrinks the policy gap\nby keeping correct solutions as on-policy data and rewriting incorrect ones\nwith guided re-solving, falling back to expert demonstrations only when needed.\nThis aligns the training distribution with the target policy before\noptimization, reducing importance sampling variance and stabilizing off-policy\nfine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate\nconsistent and significant gains over both vanilla SFT and the state-of-the-art\nDynamic Fine-Tuning (DFT) approach. The data and code will be released at\nhttps://github.com/NKU-HLT/Off-Policy-SFT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u7f29\u5c0f\u7b56\u7565\u5dee\u8ddd\u6765\u7a33\u5b9a\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u79bb\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u79bb\u7b56\u7565\u5b66\u4e60\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4f20\u7edf\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u5728\u5927\u7b56\u7565\u5dee\u8ddd\u4e0b\u4f1a\u5bfc\u81f4\u9ad8\u65b9\u5dee\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7KL\u60e9\u7f5a\u6216\u88c1\u526a\u88ab\u52a8\u7ea6\u675f\u66f4\u65b0\uff0c\u800c\u4e0d\u662f\u4e3b\u52a8\u51cf\u5c11\u7b56\u7565\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u6570\u636e\u91cd\u5199\u6846\u67b6\uff1a\u4fdd\u6301\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u540c\u7b56\u7565\u6570\u636e\uff0c\u5bf9\u6709\u9519\u8bef\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u5f15\u5bfc\u91cd\u5199\u8fdb\u884c\u4fee\u6b63\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u56de\u9000\u5230\u4e13\u5bb6\u6f14\u793a\u3002\u8fd9\u6837\u5728\u4f18\u5316\u524d\u5c31\u5c06\u8bad\u7ec3\u5206\u5e03\u4e0e\u76ee\u6807\u7b56\u7565\u5bf9\u9f50\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u666e\u901aSFT\u548c\u6700\u5148\u8fdb\u7684\u52a8\u6001\u5fae\u8c03\u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4e3b\u52a8\u7f29\u5c0f\u7b56\u7565\u5dee\u8ddd\u7684\u6570\u636e\u91cd\u5199\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u91cd\u8981\u6027\u91c7\u6837\u65b9\u5dee\uff0c\u7a33\u5b9a\u79bb\u7b56\u7565\u5fae\u8c03\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.15207", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15207", "abs": "https://arxiv.org/abs/2509.15207", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "comment": null, "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.", "AI": {"tldr": "FlowRL\u901a\u8fc7\u6d41\u5e73\u8861\u5339\u914d\u5b8c\u6574\u5956\u52b1\u5206\u5e03\uff0c\u800c\u4e0d\u662f\u6700\u5927\u5316\u5956\u52b1\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGRPO\u548cPPO\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u6700\u5927\u5316\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\u503e\u5411\u4e8e\u8fc7\u5ea6\u4f18\u5316\u4e3b\u5bfc\u5956\u52b1\u4fe1\u53f7\uff0c\u5ffd\u89c6\u8f83\u5c11\u51fa\u73b0\u4f46\u6709\u6548\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u964d\u4f4e\u591a\u6837\u6027", "method": "\u5c06\u6807\u91cf\u5956\u52b1\u8f6c\u6362\u4e3a\u4f7f\u7528\u53ef\u5b66\u4e60\u914d\u5206\u51fd\u6570\u7684\u5f52\u4e00\u5316\u76ee\u6807\u5206\u5e03\uff0c\u7136\u540e\u6700\u5c0f\u5316\u7b56\u7565\u4e0e\u76ee\u6807\u5206\u5e03\u4e4b\u95f4\u7684\u53cd\u5411KL\u6563\u5ea6\uff0c\u5b9e\u73b0\u4e3a\u6d41\u5e73\u8861\u4f18\u5316\u65b9\u6cd5", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6bd4GRPO\u63d0\u9ad810.0%\uff0c\u6bd4PPO\u63d0\u9ad85.1%\uff0c\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\u66f4\u597d", "conclusion": "\u5956\u52b1\u5206\u5e03\u5339\u914d\u662f\u5b9e\u73b0LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u9ad8\u6548\u63a2\u7d22\u548c\u591a\u6837\u5316\u63a8\u7406\u7684\u5173\u952e\u6b65\u9aa4", "topic": "agentic reinforcement learning"}}
